,documents,assigned_topic,timestamp
0,"The limited capacity to recognize faces under occlusions is a long-standing
problem that presents a unique challenge for face recognition systems and even
for humans. The problem regarding occlusion is less covered by research when
compared to other challenges such as pose variation, different expressions,
etc. Nevertheless, occluded face recognition is imperative to exploit the full
potential of face recognition for real-world applications. In this paper, we
restrict the scope to occluded face recognition. First, we explore what the
occlusion problem is and what inherent difficulties can arise. As a part of
this review, we introduce face detection under occlusion, a preliminary step in
face recognition. Second, we present how existing face recognition methods cope
with the occlusion problem and classify them into three categories, which are
1) occlusion robust feature extraction approaches, 2) occlusion aware face
recognition approaches, and 3) occlusion recovery based face recognition
approaches. Furthermore, we analyze the motivations, innovations, pros and
cons, and the performance of representative approaches for comparison. Finally,
future challenges and method trends of occluded face recognition are thoroughly
discussed.",11,2020-06-19 20:44:02
1,"Working with Child Sexual Exploitation Material (CSEM) in forensic
applications might be benefited from the progress in automatic face
recognition. However, discriminative parts of a face in CSEM, i.e., mostly the
eyes, could be often occluded to difficult the victim's identification. Most of
the face recognition approaches cannot deal with such kind of occlusions,
resulting in inaccurate face recognition results. This document presents a
short review face recognition methods for images with natural and eye occlude
faces. The purpose is to select the best baseline approach for solving
automatic face recognition of occluded faces.",8,2021-08-26 14:37:29
2,"Face Recognition has proven to be one of the most successful technology and
has impacted heterogeneous domains. Deep learning has proven to be the most
successful at computer vision tasks because of its convolution-based
architecture. Since the advent of deep learning, face recognition technology
has had a substantial increase in its accuracy. In this paper, some of the most
impactful face recognition systems were surveyed. Firstly, the paper gives an
overview of a general face recognition system. Secondly, the survey covers
various network architectures and training losses that have had a substantial
impact. Finally, the paper talks about various databases that are used to
evaluate the capabilities of a face recognition system.",8,2022-01-09 11:47:29
3,"Face recognition has become an essential task in our lives. However, the
current COVID-19 pandemic has led to the widespread use of face masks. The
effect of wearing face masks is currently an understudied issue. The aim of
this paper is to analyze face detection, face landmarking, and face recognition
performance with masked face images. HOG and CNN face detectors are used for
face detection in combination with 5-point and 68-point face landmark
predictors and VGG16 face recognition model is used for face recognition on
masked and unmasked images. We found that the performance of face detection,
face landmarking, and face recognition is negatively impacted by face masks",8,2022-06-03 15:16:58
4,"In a world where security issues have been gaining growing importance, face
recognition systems have attracted increasing attention in multiple application
areas, ranging from forensics and surveillance to commerce and entertainment.
To help understanding the landscape and abstraction levels relevant for face
recognition systems, face recognition taxonomies allow a deeper dissection and
comparison of the existing solutions. This paper proposes a new, more
encompassing and richer multi-level face recognition taxonomy, facilitating the
organization and categorization of available and emerging face recognition
solutions; this taxonomy may also guide researchers in the development of more
efficient face recognition solutions. The proposed multi-level taxonomy
considers levels related to the face structure, feature support and feature
extraction approach. Following the proposed taxonomy, a comprehensive survey of
representative face recognition solutions is presented. The paper concludes
with a discussion on current algorithmic and application related challenges
which may define future research directions for face recognition.",8,2019-01-03 13:47:53
5,"Face recognition technology has advanced rapidly and has been widely used in
various applications. Due to the extremely huge amount of data of face images
and the large computing resources required correspondingly in large-scale face
recognition tasks, there is a requirement for a face image compression approach
that is highly suitable for face recognition tasks. In this paper, we propose a
deep convolutional autoencoder compression network for face recognition tasks.
In the compression process, deep features are extracted from the original image
by the convolutional neural networks to produce a compact representation of the
original image, which is then encoded and saved by existing codec such as PNG.
This compact representation is utilized by the reconstruction network to
generate a reconstructed image of the original one. In order to improve the
face recognition accuracy when the compression framework is used in a face
recognition system, we combine this compression framework with a existing face
recognition network for joint optimization. We test the proposed scheme and
find that after joint training, the Labeled Faces in the Wild (LFW) dataset
compressed by our compression framework has higher face verification accuracy
than that compressed by JPEG2000, and is much higher than that compressed by
JPEG.",8,2019-07-03 02:44:29
6,"With the advent of 2-dimensional Convolution Neural Networks (2D CNNs), the
face recognition accuracy has reached above 99%. However, face recognition is
still a challenge in real world conditions. A video, instead of an image, as an
input can be more useful to solve the challenges of face recognition in real
world conditions. This is because a video provides more features than an image.
However, 2D CNNs cannot take advantage of the temporal features present in the
video. We therefore, propose a framework called $Sf_{3}CNN$ for face
recognition in videos. The $Sf_{3}CNN$ framework uses 3-dimensional Residual
Network (3D Resnet) and A-Softmax loss for face recognition in videos. The use
of 3D ResNet helps to capture both spatial and temporal features into one
compact feature map. However, the 3D CNN features must be highly discriminative
for efficient face recognition. The use of A-Softmax loss helps to extract
highly discriminative features from the video for face recognition. $Sf_{3}CNN$
framework gives an increased accuracy of 99.10% on CVBL video database in
comparison to the previous 97% on the same database using 3D ResNets.",11,2021-02-02 09:47:31
7,"Face recognition technology has been widely adopted in many mission-critical
scenarios like means of human identification, controlled admission, and mobile
device access, etc. Security surveillance is a typical scenario of face
recognition technology. Because the low-resolution feature of surveillance
video and images makes it difficult for high-resolution face recognition
algorithms to extract effective feature information, Algorithms applied to
high-resolution face recognition are difficult to migrate directly to
low-resolution situations. As face recognition in security surveillance becomes
more important in the era of dense urbanization, it is essential to develop
algorithms that are able to provide satisfactory performance in processing the
video frames generated by low-resolution surveillance cameras. This paper study
on the Correlation Features-based Face Recognition (CoFFaR) method which using
for homogeneous low-resolution surveillance videos, the theory, experimental
details, and experimental results are elaborated in detail. The experimental
results validate the effectiveness of the correlation features method that
improves the accuracy of homogeneous face recognition in surveillance security
scenarios.",8,2021-11-25 17:11:52
8,"This report concerns the use of techniques for sparse signal representation
and sparse error correction for automatic face recognition. Much of the recent
interest in these techniques comes from the paper ""Robust Face Recognition via
Sparse Representation"" by Wright et al. (2009), which showed how, under certain
technical conditions, one could cast the face recognition problem as one of
seeking a sparse representation of a given input face image in terms of a
""dictionary"" of training images and images of individual pixels. In this
report, we have attempted to clarify some frequently encountered questions
about this work and particularly, on the validity of using sparse
representation techniques for face recognition.",11,2011-11-03 23:50:36
9,"Impact due to demographic factors such as age, sex, race, etc., has been
studied extensively in automated face recognition systems. However, the impact
of \textit{digitally modified} demographic and facial attributes on face
recognition is relatively under-explored. In this work, we study the effect of
attribute manipulations induced via generative adversarial networks (GANs) on
face recognition performance. We conduct experiments on the CelebA dataset by
intentionally modifying thirteen attributes using AttGAN and STGAN and
evaluating their impact on two deep learning-based face verification methods,
ArcFace and VGGFace. Our findings indicate that some attribute manipulations
involving eyeglasses and digital alteration of sex cues can significantly
impair face recognition by up to 73% and need further analysis.",8,2022-09-07 05:26:25
10,"The area of face recognition is one of the most widely researched areas in
the domain of computer vision and biometric. This is because, the non-intrusive
nature of face biometric makes it comparatively more suitable for application
in area of surveillance at public places such as airports. The application of
primitive methods in face recognition could not give very satisfactory
performance. However, with the advent of machine and deep learning methods and
their application in face recognition, several major breakthroughs were
obtained. The use of 2D Convolution Neural networks(2D CNN) in face recognition
crossed the human face recognition accuracy and reached to 99%. Still, robust
face recognition in the presence of real world conditions such as variation in
resolution, illumination and pose is a major challenge for researchers in face
recognition. In this work, we used video as input to the 3D CNN architectures
for capturing both spatial and time domain information from the video for face
recognition in real world environment. For the purpose of experimentation, we
have developed our own video dataset called CVBL video dataset. The use of 3D
CNN for face recognition in videos shows promising results with DenseNets
performing the best with an accuracy of 97% on CVBL dataset.",11,2021-02-02 11:31:40
11,"Face Recognition is a common problem in Machine Learning. This technology has
already been widely used in our lives. For example, Facebook can automatically
tag people's faces in images, and also some mobile devices use face recognition
to protect private security. Face images comes with different background,
variant illumination, different facial expression and occlusion. There are a
large number of approaches for the face recognition. Different approaches for
face recognition have been experimented with specific databases which consist
of single type, format and composition of image. Doing so, these approaches
don't suit with different face databases. One of the basic face recognition
techniques is eigenface which is quite simple, efficient, and yields generally
good results in controlled circumstances. So, this paper presents an
experimental performance comparison of face recognition using Principal
Component Analysis (PCA) and Normalized Principal Component Analysis (NPCA).
The experiments are carried out on the ORL (ATT) and Indian face database (IFD)
which contain variability in expression, pose, and facial details. The results
obtained for the two methods have been compared by varying the number of
training images. MATLAB is used for implementing algorithms also.",8,2017-05-08 08:53:30
12,"Most of the face recognition works focus on specific modules or demonstrate a
research idea. This paper presents a pose-invariant 3D-aided 2D face
recognition system (UR2D) that is robust to pose variations as large as 90? by
leveraging deep learning technology. The architecture and the interface of UR2D
are described, and each module is introduced in detail. Extensive experiments
are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms
existing 2D face recognition systems such as VGG-Face, FaceNet, and a
commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset
and 3% on the IJB-A dataset on average in face identification tasks. UR2D also
achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing
the Rank-1 accuracy score from template matching. It fills a gap by providing a
3D-aided 2D face recognition system that has compatible results with 2D face
recognition systems using deep learning techniques.",11,2017-09-19 17:02:15
13,"Face recognition in real life situations like low illumination condition is
still an open challenge in biometric security. It is well established that the
state-of-the-art methods in face recognition provide low accuracy in the case
of poor illumination. In this work, we propose an algorithm for a more robust
illumination invariant face recognition using a multi-modal approach. We
propose a new dataset consisting of aligned faces of thermal and visual images
of a hundred subjects. We then apply face detection on thermal images using the
biggest blob extraction method and apply them for fusing images of different
modalities for the purpose of face recognition. An algorithm is proposed to
implement fusion of thermal and visual images. We reason for why relying on
only one modality can give erroneous results. We use a lighter and faster CNN
model called MobileNet for the purpose of face recognition with faster
inferencing and to be able to be use it in real time biometric systems. We test
our proposed method on our own created dataset to show that real-time face
recognition on fused images shows far better results than using visual or
thermal images separately.",8,2019-02-23 15:13:16
14,"Face recognition has already been well studied under the visible light and
the infrared,in both intra-spectral and cross-spectral cases. However, how to
fuse different light bands, i.e., hyperspectral face recognition, is still an
open research problem, which has the advantages of richer information retaining
and all-weather functionality over single band face recognition. Among the very
few works for hyperspectral face recognition, traditional non-deep learning
techniques are largely used. Thus, we in this paper bring deep learning into
the topic of hyperspectral face recognition, and propose a new fusion model
(termed HyperFaceNet) especially for hyperspectral faces. The proposed fusion
model is characterized by residual dense learning, a feedback style encoder and
a recognition-oriented loss function. During the experiments, our method is
proved to be of higher recognition rates than face recognition using either
visible light or the infrared. Moreover, our fusion model is shown to be
superior to other general-purposed image fusion methods including
state-of-the-arts, in terms of both image quality and recognition performance.",11,2020-08-02 14:59:24
15,"This work explores facial expression bias as a security vulnerability of face
recognition systems. Despite the great performance achieved by state-of-the-art
face recognition systems, the algorithms are still sensitive to a large range
of covariates. We present a comprehensive analysis of how facial expression
bias impacts the performance of face recognition technologies. Our study
analyzes: i) facial expression biases in the most popular face recognition
databases; and ii) the impact of facial expression in face recognition
performances. Our experimental framework includes two face detectors, three
face recognition models, and three different databases. Our results demonstrate
a huge facial expression bias in the most widely used databases, as well as a
related impact of face expression in the performance of state-of-the-art
algorithms. This work opens the door to new research lines focused on
mitigating the observed vulnerability.",8,2020-11-17 18:12:41
16,"We present the improved network architecture, data augmentation, and training
strategies for the Webface track and Insightface/Glint360K track of the masked
face recognition challenge of ICCV2021. One of the key goals is to have a
balanced performance of masked and standard face recognition. In order to
prevent the overfitting for the masked face recognition, we control the total
number of masked faces by not more than 10\% of the total face recognition in
the training dataset. We propose a few key changes to the face recognition
network including a new stem unit, drop block, face detection and alignment
using YOLO5Face, feature concatenation, a cycle cosine learning rate, etc. With
this strategy, we achieve good and balanced performance for both masked and
standard face recognition.",11,2021-10-04 15:41:05
17,"Deep networks trained on millions of facial images are believed to be closely
approaching human-level performance in face recognition. However, open world
face recognition still remains a challenge. Although, 3D face recognition has
an inherent edge over its 2D counterpart, it has not benefited from the recent
developments in deep learning due to the unavailability of large training as
well as large test datasets. Recognition accuracies have already saturated on
existing 3D face datasets due to their small gallery sizes. Unlike 2D
photographs, 3D facial scans cannot be sourced from the web causing a
bottleneck in the development of deep 3D face recognition networks and
datasets. In this backdrop, we propose a method for generating a large corpus
of labeled 3D face identities and their multiple instances for training and a
protocol for merging the most challenging existing 3D datasets for testing. We
also propose the first deep CNN model designed specifically for 3D face
recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our
test dataset comprises 1,853 identities with a single 3D scan in the gallery
and another 31K scans as probes, which is several orders of magnitude larger
than existing ones. Without fine tuning on this dataset, our network already
outperforms state of the art face recognition by over 10%. We fine tune our
network on the gallery set to perform end-to-end large scale 3D face
recognition which further improves accuracy. Finally, we show the efficacy of
our method for the open world face recognition problem.",8,2017-11-16 06:07:10
18,"With the recent success of deep neural networks, remarkable progress has been
achieved on face recognition. However, collecting large-scale real-world
training data for face recognition has turned out to be challenging, especially
due to the label noise and privacy issues. Meanwhile, existing face recognition
datasets are usually collected from web images, lacking detailed annotations on
attributes (e.g., pose and expression), so the influences of different
attributes on face recognition have been poorly investigated. In this paper, we
address the above-mentioned issues in face recognition using synthetic face
images, i.e., SynFace. Specifically, we first explore the performance gap
between recent state-of-the-art face recognition models trained with synthetic
and real face images. We then analyze the underlying causes behind the
performance gap, e.g., the poor intra-class variations and the domain gap
between synthetic and real face images. Inspired by this, we devise the SynFace
with identity mixup (IM) and domain mixup (DM) to mitigate the above
performance gap, demonstrating the great potentials of synthetic data for face
recognition. Furthermore, with the controllable face synthesis model, we can
easily manage different factors of synthetic face generation, including pose,
expression, illumination, the number of identities, and samples per identity.
Therefore, we also perform a systematically empirical analysis on synthetic
face images to provide some insights on how to effectively utilize synthetic
data for face recognition.",8,2021-08-18 03:41:54
19,"This paper implements and compares different techniques for face detection
and recognition. One is find where the face is located in the images that is
face detection and second is face recognition that is identifying the person.
We study three techniques in this paper: Face detection using self organizing
map (SOM), Face recognition by projection and nearest neighbor and Face
recognition using SVM.",8,2016-04-19 03:16:14
20,"This work presents an analysis of the efficiency of image augmentations for
the face recognition problem from limited data. We considered basic
manipulations, generative methods, and their combinations for augmentations.
Our results show that augmentations, in general, can considerably improve the
quality of face recognition systems and the combination of generative and basic
approaches performs better than the other tested techniques.",11,2021-05-18 19:33:17
21,"In order to establish face recognition system in rehabilitation nursing
robots beds and achieve real-time monitor the patient on the bed. We propose a
face recognition method based on partial matching Hu moments which apply for
rehabilitation nursing robots beds. Firstly we using Haar classifier to detect
human faces automatically in dynamic video frames. Secondly we using Otsu
threshold method to extract facial features (eyebrows, eyes, mouth) in the face
image and its Hu moments. Finally, we using Hu moment feature set to achieve
the automatic face recognition. Experimental results show that this method can
efficiently identify face in a dynamic video and it has high practical value
(the accuracy rate is 91% and the average recognition time is 4.3s).",8,2015-08-02 14:09:02
22,"Face recognition has achieved considerable progress in recent years thanks to
the development of deep neural networks, but it has recently been discovered
that deep neural networks are vulnerable to adversarial examples. This means
that face recognition models or systems based on deep neural networks are also
susceptible to adversarial examples. However, the existing methods of attacking
face recognition models or systems with adversarial examples can effectively
complete white-box attacks but not black-box impersonation attacks, physical
attacks, or convenient attacks, particularly on commercial face recognition
systems. In this paper, we propose a new method to attack face recognition
models or systems called RSTAM, which enables an effective black-box
impersonation attack using an adversarial mask printed by a mobile and compact
printer. First, RSTAM enhances the transferability of the adversarial masks
through our proposed random similarity transformation strategy. Furthermore, we
propose a random meta-optimization strategy for ensembling several pre-trained
face models to generate more general adversarial masks. Finally, we conduct
experiments on the CelebA-HQ, LFW, Makeup Transfer (MT), and CASIA-FaceV5
datasets. The performance of the attacks is also evaluated on state-of-the-art
commercial face recognition systems: Face++, Baidu, Aliyun, Tencent, and
Microsoft. Extensive experiments show that RSTAM can effectively perform
black-box impersonation attacks on face recognition models or systems.",8,2022-06-25 08:16:55
23,"Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.",8,2014-03-03 16:56:53
24,"In the beginning stage, face verification is done using easy method of
geometric algorithm models, but the verification route has now developed into a
scientific progress of complicated geometric representation and identical
procedure. In recent years the technologies have boosted face recognition
system into the healthy focus. Researchers currently undergoing strong research
on finding face recognition system for wider area information taken under
hysterical elucidation dissimilarity. The proposed face recognition system
consists of a narrative expositionindiscreet preprocessing method, a hybrid
Fourier-based facial feature extraction and a score fusion scheme. We have
verified the face recognition in different lightening conditions (day or night)
and at different locations (indoor or outdoor). Preprocessing, Image detection,
Feature- extraction and Face recognition are the methods used for face
verification system. This paper focuses mainly on the issue of toughness to
lighting variations. The proposed system has obtained an average of 88.1%
verification rate on Two-Dimensional images under different lightening
conditions.",8,2014-03-04 04:47:37
25,"Developing a reliable and practical face recognition system is a
long-standing goal in computer vision research. Existing literature suggests
that pixel-wise face alignment is the key to achieve high-accuracy face
recognition. By assuming a human face as piece-wise planar surfaces, where each
surface corresponds to a facial part, we develop in this paper a Constrained
Part-based Alignment (CPA) algorithm for face recognition across pose and/or
expression. Our proposed algorithm is based on a trainable CPA model, which
learns appearance evidence of individual parts and a tree-structured shape
configuration among different parts. Given a probe face, CPA simultaneously
aligns all its parts by fitting them to the appearance evidence with
consideration of the constraint from the tree-structured shape configuration.
This objective is formulated as a norm minimization problem regularized by
graph likelihoods. CPA can be easily integrated with many existing classifiers
to perform part-based face recognition. Extensive experiments on benchmark face
datasets show that CPA outperforms or is on par with existing methods for
robust face recognition across pose, expression, and/or illumination changes.",11,2015-01-20 06:05:01
26,"This paper explores multi-task learning (MTL) for face recognition. We answer
the questions of how and why MTL can improve the face recognition performance.
First, we propose a multi-task Convolutional Neural Network (CNN) for face
recognition where identity classification is the main task and pose,
illumination, and expression estimations are the side tasks. Second, we develop
a dynamic-weighting scheme to automatically assign the loss weight to each side
task, which is a crucial problem in MTL. Third, we propose a pose-directed
multi-task CNN by grouping different poses to learn pose-specific identity
features, simultaneously across all poses. Last but not least, we propose an
energy-based weight analysis method to explore how CNN-based MTL works. We
observe that the side tasks serve as regularizations to disentangle the
variations from the learnt identity features. Extensive experiments on the
entire Multi-PIE dataset demonstrate the effectiveness of the proposed
approach. To the best of our knowledge, this is the first work using all data
in Multi-PIE for face recognition. Our approach is also applicable to
in-the-wild datasets for pose-invariant face recognition and achieves
comparable or better performance than state of the art on LFW, CFP, and IJB-A
datasets.",8,2017-02-15 18:41:21
27,"This paper presents a multi-pose face recognition approach using hybrid face
features descriptors (HFFD). The HFFD is a face descriptor containing of rich
discriminant information that is created by fusing some frequency-based
features extracted using both wavelet and DCT analysis of several different
poses of 2D face images. The main aim of this method is to represent the
multi-pose face images using a dominant frequency component with still having
reasonable achievement compared to the recent multi-pose face recognition
methods. The HFFD based face recognition tends to achieve better performance
than that of the recent 2D-based face recognition method. In addition, the
HFFD-based face recognition also is sufficiently to handle large face
variability due to face pose variations .",8,2017-03-12 02:58:07
28,"Existing convolutional neural network (CNN) based face recognition algorithms
typically learn a discriminative feature mapping, using a loss function that
enforces separation of features from different classes and/or aggregation of
features within the same class. However, they may suffer from bias in the
training data such as uneven sampling density, because they optimize the
adjacency relationship of the learned features without considering the
proximity of the underlying faces. Moreover, since they only use facial images
for training, the learned feature mapping may not correctly indicate the
relationship of other attributes such as gender and ethnicity, which can be
important for some face recognition applications. In this paper, we propose a
new CNN-based face recognition approach that incorporates such attributes into
the training process. Using an attribute-aware loss function that regularizes
the feature mapping using attribute proximity, our approach learns more
discriminative features that are correlated with the attributes. We train our
face recognition model on a large-scale RGB-D data set with over 100K
identities captured under real application conditions. By comparing our
approach with other methods on a variety of experiments, we demonstrate that
depth channel and attribute-aware loss greatly improve the accuracy and
robustness of face recognition.",11,2018-11-24 15:07:12
29,"Deep neural networks have been widely used in numerous computer vision
applications, particularly in face recognition. However, deploying deep neural
network face recognition on mobile devices has recently become a trend but
still limited since most high-accuracy deep models are both time and GPU
consumption in the inference stage. Therefore, developing a lightweight deep
neural network is one of the most practical solutions to deploy face
recognition on mobile devices. Such the lightweight deep neural network
requires efficient memory with small number of weights representation and low
cost operators. In this paper, a novel deep neural network named MobiFace, a
simple but effective approach, is proposed for productively deploying face
recognition on mobile devices. The experimental results have shown that our
lightweight MobiFace is able to achieve high performance with 99.73% on LFW
database and 91.3% on large-scale challenging Megaface database. It is also
eventually competitive against large-scale deep-networks face recognition while
significant reducing computational time and memory consumption.",8,2018-11-27 16:34:01
30,"Face liveness detection is an essential prerequisite for face recognition
applications. Previous face liveness detection methods usually train a binary
classifier to differentiate between a fake face and a real face before face
recognition. The client identity information is not utilized in previous face
liveness detection methods. However, in practical face recognition
applications, face spoofing attacks are always aimed at a specific client, and
the client identity information can provide useful clues for face liveness
detection. In this paper, we propose a face liveness detection method based on
the client identity using Siamese network. We detect face liveness after face
recognition instead of before face recognition, that is, we detect face
liveness with the client identity information. We train a Siamese network with
image pairs. Each image pair consists of two real face images or one real and
one fake face images. The face images in each pair come from a same client.
Given a test face image, the face image is firstly recognized by face
recognition system, then the real face image of the identified client is
retrieved to help the face liveness detection. Experiment results demonstrate
the effectiveness of our method.",8,2019-03-13 09:12:38
31,"We introduce a deep convolutional neural networks (CNN) architecture to
classify facial attributes and recognize face images simultaneously via a
shared learning paradigm to improve the accuracy for facial attribute
prediction and face recognition performance. In this method, we use facial
attributes as an auxiliary source of information to assist CNN features
extracted from the face images to improve the face recognition performance.
Specifically, we use a shared CNN architecture that jointly predicts facial
attributes and recognize face images simultaneously via a shared learning
parameters, and then we use facial attribute features an an auxiliary source of
information concatenated by face features to increase the discrimination of the
CNN for face recognition. This process assists the CNN classifier to better
recognize face images. The experimental results show that our model increases
both the face recognition and facial attribute prediction performance,
especially for the identity attributes such as gender and race. We evaluated
our method on several standard datasets labeled by identities and face
attributes and the results show that the proposed method outperforms
state-of-the-art face recognition models.",8,2019-09-28 18:25:41
32,"Face recognition systems are present in many modern solutions and thousands
of applications in our daily lives. However, current solutions are not easily
scalable, especially when it comes to the addition of new targeted people. We
propose a cluster-matching-based approach for face recognition in video. In our
approach, we use unsupervised learning to cluster the faces present in both the
dataset and targeted videos selected for face recognition. Moreover, we design
a cluster matching heuristic to associate clusters in both sets that is also
capable of identifying when a face belongs to a non-registered person. Our
method has achieved a recall of 99.435% and a precision of 99.131% in the task
of video face recognition. Besides performing face recognition, it can also be
used to determine the video segments where each person is present.",11,2020-10-20 00:44:54
33,"Knowledge Distillation (KD) refers to transferring knowledge from a large
model to a smaller one, which is widely used to enhance model performance in
machine learning. It tries to align embedding spaces generated from the teacher
and the student model (i.e. to make images corresponding to the same semantics
share the same embedding across different models). In this work, we focus on
its application in face recognition. We observe that existing knowledge
distillation models optimize the proxy tasks that force the student to mimic
the teacher's behavior, instead of directly optimizing the face recognition
accuracy. Consequently, the obtained student models are not guaranteed to be
optimal on the target task or able to benefit from advanced constraints, such
as large margin constraints (e.g. margin-based softmax). We then propose a
novel method named ProxylessKD that directly optimizes face recognition
accuracy by inheriting the teacher's classifier as the student's classifier to
guide the student to learn discriminative embeddings in the teacher's embedding
space. The proposed ProxylessKD is very easy to implement and sufficiently
generic to be extended to other tasks beyond face recognition. We conduct
extensive experiments on standard face recognition benchmarks, and the results
demonstrate that ProxylessKD achieves superior performance over existing
knowledge distillation methods.",8,2020-10-31 13:14:34
34,"3D face recognition has shown its potential in many application scenarios.
Among numerous 3D face recognition methods, deep-learning-based methods have
developed vigorously in recent years. In this paper, an end-to-end deep
learning network entitled Sur3dNet-Face for point-cloud-based 3D face
recognition is proposed. The network uses PointNet as the backbone, which is a
successful point cloud classification solution but does not work properly in
face recognition. Supplemented with modifications in network architecture and a
few-data guided learning framework based on Gaussian process morphable model,
the backbone is successfully modified for 3D face recognition. Different from
existing methods training with a large amount of data in multiple datasets, our
method uses Spring2003 subset of FRGC v2.0 for training which contains only 943
facial scans, and the network is well trained with the guidance of such a small
amount of real data. Without fine-tuning on the test set, the Rank-1
Recognition Rate (RR1) is achieved as follows: 98.85% on FRGC v2.0 dataset and
99.33% on Bosphorus dataset, which proves the effectiveness and the
potentiality of our method.",11,2021-03-31 09:26:14
35,"As the deep learning makes big progresses in still-image face recognition,
unconstrained video face recognition is still a challenging task due to low
quality face images caused by pose, blur, occlusion, illumination etc. In this
paper we propose a network for face recognition which gives an explicit and
quantitative quality score at the same time when a feature vector is extracted.
To our knowledge this is the first network that implements these two functions
in one network online. This network is very simple by adding a quality network
branch to the baseline network of face recognition. It does not require
training datasets with annotated face quality labels. We evaluate this network
on both still-image face datasets and video face datasets and achieve the
state-of-the-art performance in many cases. This network enables a lot of
applications where an explicit face quality scpre is used. We demonstrate three
applications of the explicit face quality, one of which is a progressive
feature aggregation scheme in online video face recognition. We design an
experiment to prove the benefits of using the face quality in this application.
Code will be available at \url{https://github.com/deepcam-cn/facequality}.",11,2021-05-03 05:45:57
36,"Face recognition has been greatly facilitated by the development of deep
neural networks (DNNs) and has been widely applied to many safety-critical
applications. However, recent studies have shown that DNNs are very vulnerable
to adversarial examples, raising serious concerns on the security of real-world
face recognition. In this work, we study sticker-based physical attacks on face
recognition for better understanding its adversarial robustness. To this end,
we first analyze in-depth the complicated physical-world conditions confronted
by attacking face recognition, including the different variations of stickers,
faces, and environmental conditions. Then, we propose a novel robust physical
attack framework, dubbed PadvFace, to model these challenging variations
specifically. Furthermore, considering the difference in attack complexity, we
propose an efficient Curriculum Adversarial Attack (CAA) algorithm that
gradually adapts adversarial stickers to environmental variations from easy to
complex. Finally, we construct a standardized testing protocol to facilitate
the fair evaluation of physical attacks on face recognition, and extensive
experiments on both dodging and impersonation attacks demonstrate the superior
performance of the proposed method.",8,2021-09-20 06:49:52
37,"Recent successful adversarial attacks on face recognition show that, despite
the remarkable progress of face recognition models, they are still far behind
the human intelligence for perception and recognition. It reveals the
vulnerability of deep convolutional neural networks (CNNs) as state-of-the-art
building block for face recognition models against adversarial examples, which
can cause certain consequences for secure systems. Gradient-based adversarial
attacks are widely studied before and proved to be successful against face
recognition models. However, finding the optimized perturbation per each face
needs to submitting the significant number of queries to the target model. In
this paper, we propose recursive adversarial attack on face recognition using
automatic face warping which needs extremely limited number of queries to fool
the target model. Instead of a random face warping procedure, the warping
functions are applied on specific detected regions of face like eyebrows, nose,
lips, etc. We evaluate the robustness of proposed method in the decision-based
black-box attack setting, where the attackers have no access to the model
parameters and gradients, but hard-label predictions and confidence scores are
provided by the target model.",8,2022-07-04 00:22:45
38,"The main finding of this work is that the standard image classification
pipeline, which consists of dictionary learning, feature encoding, spatial
pyramid pooling and linear classification, outperforms all state-of-the-art
face recognition methods on the tested benchmark datasets (we have tested on
AR, Extended Yale B, the challenging FERET, and LFW-a datasets). This
surprising and prominent result suggests that those advances in generic image
classification can be directly applied to improve face recognition systems. In
other words, face recognition may not need to be viewed as a separate object
classification problem.
  While recently a large body of residual based face recognition methods focus
on developing complex dictionary learning algorithms, in this work we show that
a dictionary of randomly extracted patches (even from non-face images) can
achieve very promising results using the image classification pipeline. That
means, the choice of dictionary learning methods may not be important. Instead,
we find that learning multiple dictionaries using different low-level image
features often improve the final classification accuracy. Our proposed face
recognition approach offers the best reported results on the widely-used face
recognition benchmark datasets. In particular, on the challenging FERET and
LFW-a datasets, we improve the best reported accuracies in the literature by
about 20% and 30% respectively.",8,2013-09-22 11:52:03
39,"Previous works have shown that face recognition with high accurate 3D data is
more reliable and insensitive to pose and illumination variations. Recently,
low-cost and portable 3D acquisition techniques like ToF(Time of Flight) and
DoE based structured light systems enable us to access 3D data easily, e.g.,
via a mobile phone. However, such devices only provide sparse(limited speckles
in structured light system) and noisy 3D data which can not support face
recognition directly. In this paper, we aim at achieving high-performance face
recognition for devices equipped with such modules which is very meaningful in
practice as such devices will be very popular. We propose a framework to
perform face recognition by fusing a sequence of low-quality 3D data. As 3D
data are sparse and noisy which can not be well handled by conventional methods
like the ICP algorithm, we design a PointNet-like Deep Registration
Network(DRNet) which works with ordered 3D point coordinates while preserving
the ability of mining local structures via convolution. Meanwhile we develop a
novel loss function to optimize our DRNet based on the quaternion expression
which obviously outperforms other widely used functions. For face recognition,
we design a deep convolutional network which takes the fused 3D depth-map as
input based on AMSoftmax model. Experiments show that our DRNet can achieve
rotation error 0.95{\deg} and translation error 0.28mm for registration. The
face recognition on fused data also achieves rank-1 accuracy 99.2% , FAR-0.001
97.5% on Bosphorus dataset which is comparable with state-of-the-art
high-quality data based recognition performance.",8,2018-10-23 04:58:48
40,"In this paper we develop a Quality Assessment approach for face recognition
based on deep learning. The method consists of a Convolutional Neural Network,
FaceQnet, that is used to predict the suitability of a specific input image for
face recognition purposes. The training of FaceQnet is done using the VGGFace2
database. We employ the BioLab-ICAO framework for labeling the VGGFace2 images
with quality information related to their ICAO compliance level. The
groundtruth quality labels are obtained using FaceNet to generate comparison
scores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making
it capable of returning a numerical quality measure for each input image.
Finally, we verify if the FaceQnet scores are suitable to predict the expected
performance when employing a specific image for face recognition with a COTS
face recognition system. Several conclusions can be drawn from this work, most
notably: 1) we managed to employ an existing ICAO compliance framework and a
pretrained CNN to automatically label data with quality information, 2) we
trained FaceQnet for quality estimation by fine-tuning a pre-trained face
recognition network (ResNet-50), and 3) we have shown that the predictions from
FaceQnet are highly correlated with the face recognition accuracy of a
state-of-the-art commercial system not used during development. FaceQnet is
publicly available in GitHub.",8,2019-04-03 02:12:31
41,"Modern face recognition systems leverage datasets containing images of
hundreds of thousands of specific individuals' faces to train deep
convolutional neural networks to learn an embedding space that maps an
arbitrary individual's face to a vector representation of their identity. The
performance of a face recognition system in face verification (1:1) and face
identification (1:N) tasks is directly related to the ability of an embedding
space to discriminate between identities. Recently, there has been significant
public scrutiny into the source and privacy implications of large-scale face
recognition training datasets such as MS-Celeb-1M and MegaFace, as many people
are uncomfortable with their face being used to train dual-use technologies
that can enable mass surveillance. However, the impact of an individual's
inclusion in training data on a derived system's ability to recognize them has
not previously been studied. In this work, we audit ArcFace, a
state-of-the-art, open source face recognition system, in a large-scale face
identification experiment with more than one million distractor images. We find
a Rank-1 face identification accuracy of 79.71% for individuals present in the
model's training data and an accuracy of 75.73% for those not present. This
modest difference in accuracy demonstrates that face recognition systems using
deep learning work better for individuals they are trained on, which has
serious privacy implications when one considers all major open source face
recognition training datasets do not obtain informed consent from individuals
during their collection.",8,2020-01-09 15:50:28
42,"In order to effectively prevent the spread of COVID-19 virus, almost everyone
wears a mask during coronavirus epidemic. This almost makes conventional facial
recognition technology ineffective in many cases, such as community access
control, face access control, facial attendance, facial security checks at
train stations, etc. Therefore, it is very urgent to improve the recognition
performance of the existing face recognition technology on the masked faces.
Most current advanced face recognition approaches are designed based on deep
learning, which depend on a large number of face samples. However, at present,
there are no publicly available masked face recognition datasets. To this end,
this work proposes three types of masked face datasets, including Masked Face
Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD)
and Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best
of our knowledge, RMFRD is currently theworld's largest real-world masked face
dataset. These datasets are freely available to industry and academia, based on
which various applications on masked faces can be developed. The
multi-granularity masked face recognition model we developed achieves 95%
accuracy, exceeding the results reported by the industry. Our datasets are
available at: https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.",11,2020-03-20 04:15:19
43,"Face recognition is a classical problem in Computer Vision that has
experienced significant progress. Yet, in digital videos, face recognition is
complicated by occlusion, pose and lighting variations, and persons
entering/leaving the scene. The thesis's goal is to develop a fast method for
face recognition in digital videos that is applicable to large datasets. The
thesis introduces several methods to address the problems associated with video
face recognition. First, to address issues associated with pose and lighting
variations, a collection of face prototypes is associated with each student.
Second, to speed up the process, sampling, K-means Clustering, and a
combination of both are used to reduce the number of face prototypes per
student. Third, the videos are processed at different frame rates. Fourth, the
thesis proposes the use of active sets to address occlusion and to eliminate
face recognition application on video frames with slow face motions. Fifth, the
thesis develops a group face detector that recognizes students within a
collaborative learning group, while rejecting out-of-group face detections.
Sixth, the thesis introduces a face DeID for protecting the students'
identities. Seventh, the thesis uses data augmentation to increase the training
set's size. The different methods are combined using multi-objective
optimization to guarantee that the full method remains fast without sacrificing
accuracy. To test the approach, the thesis develops the AOLME dataset of 138
student faces (81 boys and 57 girls) of ages 10 to 14, who are predominantly
Latina/o students. Compared to the baseline method, the final optimized method
resulted in fast recognition times with significant improvements in face
recognition accuracy. Using face prototype sampling only, the proposed method
achieved an accuracy of 71.8% compared to 62.3% for the baseline system, while
running 11.6 times faster.",11,2021-10-26 01:54:38
44,"Deep learning approaches have achieved highly accurate face recognition by
training the models with very large face image datasets. Unlike the
availability of large 2D face image datasets, there is a lack of large 3D face
datasets available to the public. Existing public 3D face datasets were usually
collected with few subjects, leading to the over-fitting problem. This paper
proposes two CNN models to improve the RGB-D face recognition task. The first
is a segmentation-aware depth estimation network, called DepthNet, which
estimates depth maps from RGB face images by including semantic segmentation
information for more accurate face region localization. The other is a novel
mask-guided RGB-D face recognition model that contains an RGB recognition
branch, a depth map recognition branch, and an auxiliary segmentation mask
branch with a spatial attention module. Our DepthNet is used to augment a large
2D face image dataset to a large RGB-D face dataset, which is used for training
an accurate RGB-D face recognition model. Furthermore, the proposed mask-guided
RGB-D face recognition model can fully exploit the depth map and segmentation
mask information and is more robust against pose variation than previous
methods. Our experimental results show that DepthNet can produce more reliable
depth maps from face images with the segmentation mask. Our mask-guided face
recognition model outperforms state-of-the-art methods on several public 3D
face datasets.",8,2021-12-22 07:46:23
45,"Knowledge distillation is an effective method to improve the performance of a
lightweight neural network (i.e., student model) by transferring the knowledge
of a well-performed neural network (i.e., teacher model), which has been widely
applied in many computer vision tasks, including face recognition.
Nevertheless, the current face recognition distillation methods usually utilize
the Feature Consistency Distillation (FCD) (e.g., L2 distance) on the learned
embeddings extracted by the teacher and student models for each sample, which
is not able to fully transfer the knowledge from the teacher to the student for
face recognition. In this work, we observe that mutual relation knowledge
between samples is also important to improve the discriminative ability of the
learned representation of the student model, and propose an effective face
recognition distillation method called CoupleFace by additionally introducing
the Mutual Relation Distillation (MRD) into existing distillation framework.
Specifically, in MRD, we first propose to mine the informative mutual
relations, and then introduce the Relation-Aware Distillation (RAD) loss to
transfer the mutual relation knowledge of the teacher model to the student
model. Extensive experimental results on multiple benchmark datasets
demonstrate the effectiveness of our proposed CoupleFace for face recognition.
Moreover, based on our proposed CoupleFace, we have won the first place in the
ICCV21 Masked Face Recognition Challenge (MS1M track).",8,2022-04-12 03:25:42
46,"Automatic face recognition is a research area with high popularity. Many
different face recognition algorithms have been proposed in the last thirty
years of intensive research in the field. With the popularity of deep learning
and its capability to solve a huge variety of different problems, face
recognition researchers have concentrated effort on creating better models
under this paradigm. From the year 2015, state-of-the-art face recognition has
been rooted in deep learning models. Despite the availability of large-scale
and diverse datasets for evaluating the performance of face recognition
algorithms, many of the modern datasets just combine different factors that
influence face recognition, such as face pose, occlusion, illumination, facial
expression and image quality. When algorithms produce errors on these datasets,
it is not clear which of the factors has caused this error and, hence, there is
no guidance in which direction more research is required. This work is a
followup from our previous works developed in 2014 and eventually published in
2016, showing the impact of various facial aspects on face recognition
algorithms. By comparing the current state-of-the-art with the best systems
from the past, we demonstrate that faces under strong occlusions, some types of
illumination, and strong expressions are problems mastered by deep learning
algorithms, whereas recognition with low-resolution images, extreme pose
variations, and open-set recognition is still an open problem. To show this, we
run a sequence of experiments using six different datasets and five different
face recognition algorithms in an open-source and reproducible manner. We
provide the source code to run all of our experiments, which is easily
extensible so that utilizing your own deep network in our evaluation is just a
few minutes away.",8,2022-08-08 10:40:29
47,"Psychophysical studies suggest that face recognition takes place in a narrow
band of low spatial frequencies (``critical band''). Here, we examined the
recognition performance of an artificial face recognition system as a function
of the size of the input images. Recognition performance was quantified with
three discriminability measures: Fisher Linear Discriminant Analysis, non
Parametric Discriminant Analysis, and mutual information. All of the three
measures revealed a maximum at the same image sizes. Since spatial frequency
content is a function of image size, our data consistently predict the range of
psychophysical found frequencies. Our results therefore support the notion that
the critical band of spatial frequencies for face recognition in humans and
machines follows from inherent properties of face images.",8,2006-12-01 20:37:56
48,"The state-of-the-art of face recognition has been significantly advanced by
the emergence of deep learning. Very deep neural networks recently achieved
great success on general object recognition because of their superb learning
capacity. This motivates us to investigate their effectiveness on face
recognition. This paper proposes two very deep neural network architectures,
referred to as DeepID3, for face recognition. These two architectures are
rebuilt from stacked convolution and inception layers proposed in VGG net and
GoogLeNet to make them suitable to face recognition. Joint face
identification-verification supervisory signals are added to both intermediate
and final feature extraction layers during training. An ensemble of the
proposed two architectures achieves 99.53% LFW face verification accuracy and
96.0% LFW rank-1 face identification accuracy, respectively. A further
discussion of LFW face verification result is given in the end.",8,2015-02-03 14:28:55
49,"The capacity to recognize faces under varied poses is a fundamental human
ability that presents a unique challenge for computer vision systems. Compared
to frontal face recognition, which has been intensively studied and has
gradually matured in the past few decades, pose-invariant face recognition
(PIFR) remains a largely unsolved problem. However, PIFR is crucial to
realizing the full potential of face recognition for real-world applications,
since face recognition is intrinsically a passive biometric technology for
recognizing uncooperative subjects. In this paper, we discuss the inherent
difficulties in PIFR and present a comprehensive review of established
techniques. Existing PIFR methods can be grouped into four categories, i.e.,
pose-robust feature extraction approaches, multi-view subspace learning
approaches, face synthesis approaches, and hybrid approaches. The motivations,
strategies, pros/cons, and performance of representative approaches are
described and compared. Moreover, promising directions for future research are
discussed.",11,2015-02-15 23:25:32
50,"This paper proposes to learn high-performance deep ConvNets with sparse
neural connections, referred to as sparse ConvNets, for face recognition. The
sparse ConvNets are learned in an iterative way, each time one additional layer
is sparsified and the entire model is re-trained given the initial weights
learned in previous iterations. One important finding is that directly training
the sparse ConvNet from scratch failed to find good solutions for face
recognition, while using a previously learned denser model to properly
initialize a sparser model is critical to continue learning effective features
for face recognition. This paper also proposes a new neural correlation-based
weight selection criterion and empirically verifies its effectiveness in
selecting informative connections from previously learned models in each
iteration. When taking a moderately sparse structure (26%-76% of weights in the
dense model), the proposed sparse ConvNet model significantly improves the face
recognition performance of the previous state-of-the-art DeepID2+ models given
the same training data, while it keeps the performance of the baseline model
with only 12% of the original parameters.",8,2015-12-07 02:56:27
51,"Even though face recognition in frontal view and normal lighting condition
works very well, the performance degenerates sharply in extreme conditions.
Recently there are many work dealing with pose and illumination problems,
respectively. However both the lighting and pose variation will always be
encountered at the same time. Accordingly we propose an end-to-end face
recognition method to deal with pose and illumination simultaneously based on
convolutional networks where the discriminative nonlinear features that are
invariant to pose and illumination are extracted. Normally the global structure
for images taken in different views is quite diverse. Therefore we propose to
use the 1*1 convolutional kernel to extract the local features. Furthermore the
parallel multi-stream multi-layer 1*1 convolution network is developed to
extract multi-hierarchy features. In the experiments we obtained the average
face recognition rate of 96.9% on multiPIE dataset,which improves the
state-of-the-art of face recognition across poses and illumination by 7.5%.
Especially for profile-wise positions, the average recognition rate of our
proposed network is 97.8%, which increases the state-of-the-art recognition
rate by 19%.",1,2016-07-12 03:52:30
52,"To address the sequential changes of images including poses, in this paper we
propose a recurrent regression neural network(RRNN) framework to unify two
classic tasks of cross-pose face recognition on still images and video-based
face recognition. To imitate the changes of images, we explicitly construct the
potential dependencies of sequential images so as to regularize the final
learning model. By performing progressive transforms for sequentially adjacent
images, RRNN can adaptively memorize and forget the information that benefits
for the final classification. For face recognition of still images, given any
one image with any one pose, we recurrently predict the images with its
sequential poses to expect to capture some useful information of others poses.
For video-based face recognition, the recurrent regression takes one entire
sequence rather than one image as its input. We verify RRNN in static face
dataset MultiPIE and face video dataset YouTube Celebrities(YTC). The
comprehensive experimental results demonstrate the effectiveness of the
proposed RRNN method.",11,2016-07-24 05:11:40
53,"Robust face representation is imperative to highly accurate face recognition.
In this work, we propose an open source face recognition method with deep
representation named as VIPLFaceNet, which is a 10-layer deep convolutional
neural network with 7 convolutional layers and 3 fully-connected layers.
Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training
time and 60% testing time, but achieves 40\% drop in error rate on the
real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean
accuracy on LFW using one single network. An open-source C++ SDK based on
VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process
one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a
state-of-the-art start point for both academic and industrial face recognition
applications.",8,2016-09-13 15:13:55
54,"Plenty of effective methods have been proposed for face recognition during
the past decade. Although these methods differ essentially in many aspects, a
common practice of them is to specifically align the facial area based on the
prior knowledge of human face structure before feature extraction. In most
systems, the face alignment module is implemented independently. This has
actually caused difficulties in the designing and training of end-to-end face
recognition models. In this paper we study the possibility of alignment
learning in end-to-end face recognition, in which neither prior knowledge on
facial landmarks nor artificially defined geometric transformations are
required. Specifically, spatial transformer layers are inserted in front of the
feature extraction layers in a Convolutional Neural Network (CNN) for face
recognition. Only human identity clues are used for driving the neural network
to automatically learn the most suitable geometric transformation and the most
appropriate facial area for the recognition task. To ensure reproducibility,
our model is trained purely on the publicly available CASIA-WebFace dataset,
and is tested on the Labeled Face in the Wild (LFW) dataset. We have achieved a
verification accuracy of 99.08\% which is comparable to state-of-the-art single
model based methods.",8,2017-01-25 06:10:41
55,"We propose a novel 3D face recognition algorithm using a deep convolutional
neural network (DCNN) and a 3D augmentation technique. The performance of 2D
face recognition algorithms has significantly increased by leveraging the
representational power of deep neural networks and the use of large-scale
labeled training data. As opposed to 2D face recognition, training
discriminative deep features for 3D face recognition is very difficult due to
the lack of large-scale 3D face datasets. In this paper, we show that transfer
learning from a CNN trained on 2D face images can effectively work for 3D face
recognition by fine-tuning the CNN with a relatively small number of 3D facial
scans. We also propose a 3D face augmentation technique which synthesizes a
number of different facial expressions from a single 3D face scan. Our proposed
method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC
datasets, without using hand-crafted features. The 3D identification using our
deep features also scales well for large databases.",8,2017-03-30 23:49:23
56,"We present a longitudinal study of face recognition performance on Children
Longitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects,
in the age group [2, 18] years. Each subject has at least four face images
acquired over a time span of up to six years. Face comparison scores are
obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source
matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTS-A
and FaceNet matchers. To improve the performance of the open-source FaceNet
matcher for child face recognition, we were able to fine-tune it on an
independent training set of 3,294 face images of 1,119 children in the age
group [3, 18] years. Multilevel statistical models are fit to genuine
comparison scores from the CLF dataset to determine the decrease in face
recognition accuracy over time. Additionally, we analyze both the verification
and open-set identification accuracies in order to evaluate state-of-the-art
face recognition technology for tracing and identifying children lost at a
young age as victims of child trafficking or abduction.",8,2017-11-10 19:19:55
57,"This paper presents face recognition using maximum a posteriori (MAP)
discriminant on YCbCr color space. The YCbCr color space is considered in order
to cover the skin information of face image on the recognition process. The
proposed method is employed to improve the recognition rate and equal error
rate (EER) of the gray scale based face recognition. In this case, the face
features vector consisting of small part of dominant frequency elements which
is extracted by non-blocking DCT is implemented as dimensional reduction of the
raw face images. The matching process between the query face features and the
trained face features is performed using maximum a posteriori (MAP)
discriminant. From the experimental results on data from four face databases
containing 2268 images with 196 classes show that the face recognition YCbCr
color space provide better recognition rate and lesser EER than those of gray
scale based face recognition which improve the first rank of grayscale based
method result by about 4%. However, it requires three times more computation
time than that of grayscale based method.",1,2018-07-05 18:19:30
58,"Spectral imaging has recently gained traction for face recognition in
biometric systems. We investigate the merits of spectral imaging for face
recognition and the current challenges that hamper the widespread deployment of
spectral sensors for face recognition. The reliability of conventional face
recognition systems operating in the visible range is compromised by
illumination changes, pose variations and spoof attacks. Recent works have
reaped the benefits of spectral imaging to counter these limitations in
surveillance activities (defence, airport security checks, etc.). However, the
implementation of this technology for biometrics, is still in its infancy due
to multiple reasons. We present an overview of the existing work in the domain
of spectral imaging for face recognition, different types of modalities and
their assessment, availability of public databases for sake of reproducible
research as well as evaluation of algorithms, and recent advancements in the
field, such as, the use of deep learning-based methods for recognizing faces
from spectral images.",11,2018-07-16 10:24:28
59,"With the broad use of face recognition, its weakness gradually emerges that
it is able to be attacked. So, it is important to study how face recognition
networks are subject to attacks. In this paper, we focus on a novel way to do
attacks against face recognition network that misleads the network to identify
someone as the target person not misclassify inconspicuously. Simultaneously,
for this purpose, we introduce a specific attentional adversarial attack
generative network to generate fake face images. For capturing the semantic
information of the target person, this work adds a conditional variational
autoencoder and attention modules to learn the instance-level correspondences
between faces. Unlike traditional two-player GAN, this work introduces face
recognition networks as the third player to participate in the competition
between generator and discriminator which allows the attacker to impersonate
the target person better. The generated faces which are hard to arouse the
notice of onlookers can evade recognition by state-of-the-art networks and most
of them are recognized as the target person.",8,2018-11-29 09:14:56
60,"Hyperspectral imaging systems collect and process information from specific
wavelengths across the electromagnetic spectrum. The fusion of multi-spectral
bands in the visible spectrum has been exploited to improve face recognition
performance over all the conventional broad band face images. In this book
chapter, we propose a new Convolutional Neural Network (CNN) framework which
adopts a structural sparsity learning technique to select the optimal spectral
bands to obtain the best face recognition performance over all of the spectral
bands. Specifically, in this method, images from all bands are fed to a CNN,
and the convolutional filters in the first layer of the CNN are then
regularized by employing a group Lasso algorithm to zero out the redundant
bands during the training of the network. Contrary to other methods which
usually select the useful bands manually or in a greedy fashion, our method
selects the optimal spectral bands automatically to achieve the best face
recognition performance over all spectral bands. Moreover, experimental results
demonstrate that our method outperforms state of the art band selection methods
for face recognition on several publicly-available hyperspectral face image
datasets.",8,2019-08-15 23:51:33
61,"Face recognition algorithms have demonstrated very high recognition
performance, suggesting suitability for real world applications. Despite the
enhanced accuracies, robustness of these algorithms against attacks and bias
has been challenged. This paper summarizes different ways in which the
robustness of a face recognition algorithm is challenged, which can severely
affect its intended working. Different types of attacks such as physical
presentation attacks, disguise/makeup, digital adversarial attacks, and
morphing/tampering using GANs have been discussed. We also present a discussion
on the effect of bias on face recognition models and showcase that factors such
as age and gender variations affect the performance of modern algorithms. The
paper also presents the potential reasons for these challenges and some of the
future research directions for increasing the robustness of face recognition
models.",8,2020-02-07 18:21:59
62,"Visible face recognition systems achieve nearly perfect recognition
accuracies using deep learning. However, in lack of light, these systems
perform poorly. A way to deal with this problem is thermal to visible
cross-domain face matching. This is a desired technology because of its
usefulness in night time surveillance. Nevertheless, due to differences between
two domains, it is a very challenging face recognition problem. In this paper,
we present a deep autoencoder based system to learn the mapping between visible
and thermal face images. Also, we assess the impact of alignment in thermal to
visible face recognition. For this purpose, we manually annotate the facial
landmarks on the Carl and EURECOM datasets. The proposed approach is
extensively tested on three publicly available datasets: Carl, UND-X1, and
EURECOM. Experimental results show that the proposed approach improves the
state-of-the-art significantly. We observe that alignment increases the
performance by around 2%. Annotated facial landmark positions in this study can
be downloaded from the following link:
github.com/Alpkant/Thermal-to-Visible-Face-Recognition-Using-Deep-Autoencoders .",8,2020-02-10 11:58:36
63,"In this paper, we address a key limitation of existing 2D face recognition
methods: robustness to occlusions. To accomplish this task, we systematically
analyzed the impact of facial attributes on the performance of a
state-of-the-art face recognition method and through extensive experimentation,
quantitatively analyzed the performance degradation under different types of
occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach
learned discriminative facial templates despite the presence of such
occlusions. First, an attention mechanism was proposed that extracted local
identity-related region. The local features were then aggregated with the
global representations to form a single template. Second, a simple, yet
effective, training strategy was introduced to balance the non-occluded and
occluded facial images. Extensive experiments demonstrated that OREO improved
the generalization ability of face recognition under occlusions by (10.17%) in
a single-image-based setting and outperformed the baseline by approximately
(2%) in terms of rank-1 accuracy in an image-set-based scenario.",8,2020-06-11 20:17:23
64,"Recent news articles have accused face recognition of being ""biased"",
""sexist"" or ""racist"". There is consensus in the research literature that face
recognition accuracy is lower for females, who often have both a higher false
match rate and a higher false non-match rate. However, there is little
published research aimed at identifying the cause of lower accuracy for
females. For instance, the 2019 Face Recognition Vendor Test that documents
lower female accuracy across a broad range of algorithms and datasets also
lists ""Analyze cause and effect"" under the heading ""What we did not do"". We
present the first experimental analysis to identify major causes of lower face
recognition accuracy for females on datasets where previous research has
observed this result. Controlling for equal amount of visible face in the test
images reverses the apparent higher false non-match rate for females. Also,
principal component analysis indicates that images of two different females are
inherently more similar than of two different males, potentially accounting for
a difference in false match rates.",8,2020-08-16 20:29:05
65,"Using the face as a biometric identity trait is motivated by the contactless
nature of the capture process and the high accuracy of the recognition
algorithms. After the current COVID-19 pandemic, wearing a face mask has been
imposed in public places to keep the pandemic under control. However, face
occlusion due to wearing a mask presents an emerging challenge for face
recognition systems. In this paper, we present a solution to improve masked
face recognition performance. Specifically, we propose the Embedding Unmasking
Model (EUM) operated on top of existing face recognition models. We also
propose a novel loss function, the Self-restrained Triplet (SRT), which enabled
the EUM to produce embeddings similar to these of unmasked faces of the same
identities. The achieved evaluation results on three face recognition models,
two real masked datasets, and two synthetically generated masked face datasets
proved that our proposed approach significantly improves the performance in
most experimental settings.",8,2021-03-02 13:43:11
66,"The existing face recognition datasets usually lack occlusion samples, which
hinders the development of face recognition. Especially during the COVID-19
coronavirus epidemic, wearing a mask has become an effective means of
preventing the virus spread. Traditional CNN-based face recognition models
trained on existing datasets are almost ineffective for heavy occlusion. To
this end, we pioneer a simulated occlusion face recognition dataset. In
particular, we first collect a variety of glasses and masks as occlusion, and
randomly combine the occlusion attributes (occlusion objects, textures,and
colors) to achieve a large number of more realistic occlusion types. We then
cover them in the proper position of the face image with the normal occlusion
habit. Furthermore, we reasonably combine original normal face images and
occluded face images to form our final dataset, termed as Webface-OCC. It
covers 804,704 face images of 10,575 subjects, with diverse occlusion types to
ensure its diversity and stability. Extensive experiments on public datasets
show that the ArcFace retrained by our dataset significantly outperforms the
state-of-the-arts. Webface-OCC is available at
https://github.com/Baojin-Huang/Webface-OCC.",8,2021-03-04 03:07:42
67,"In the past years, face recognition technologies have shown impressive
recognition performance, mainly due to recent developments in deep
convolutional neural networks. Notwithstanding those improvements, several
challenges which affect the performance of face recognition systems remain. In
this work, we investigate the impact that facial tattoos and paintings have on
current face recognition systems. To this end, we first collected an
appropriate database containing image-pairs of individuals with and without
facial tattoos or paintings. The assembled database was used to evaluate how
facial tattoos and paintings affect the detection, quality estimation, as well
as the feature extraction and comparison modules of a face recognition system.
The impact on these modules was evaluated using state-of-the-art open-source
and commercial systems. The obtained results show that facial tattoos and
paintings affect all the tested modules, especially for images where a large
area of the face is covered with tattoos or paintings. Our work is an initial
case-study and indicates a need to design algorithms which are robust to the
visual changes caused by facial tattoos and paintings.",8,2021-03-17 22:38:13
68,"DNN-based face recognition models require large centrally aggregated face
datasets for training. However, due to the growing data privacy concerns and
legal restrictions, accessing and sharing face datasets has become exceedingly
difficult. We propose FedFace, a federated learning (FL) framework for
collaborative learning of face recognition models in a privacy-aware manner.
FedFace utilizes the face images available on multiple clients to learn an
accurate and generalizable face recognition model where the face images stored
at each client are neither shared with other clients nor the central host and
each client is a mobile device containing face images pertaining to only the
owner of the device (one identity per client). Our experiments show the
effectiveness of FedFace in enhancing the verification performance of
pre-trained face recognition system on standard face verification benchmarks
namely LFW, IJB-A, and IJB-C.",8,2021-04-07 09:25:32
69,"Face recognition has been extensively studied in computer vision and
artificial intelligence communities in recent years. An important issue of face
recognition is data privacy, which receives more and more public concerns. As a
common privacy-preserving technique, Federated Learning is proposed to train a
model cooperatively without sharing data between parties. However, as far as we
know, it has not been successfully applied in face recognition. This paper
proposes a framework named FedFace to innovate federated learning for face
recognition. Specifically, FedFace relies on two major innovative algorithms,
Partially Federated Momentum (PFM) and Federated Validation (FV). PFM locally
applies an estimated equivalent global momentum to approximating the
centralized momentum-SGD efficiently. FV repeatedly searches for better
federated aggregating weightings via testing the aggregated models on some
private validation datasets, which can improve the model's generalization
ability. The ablation study and extensive experiments validate the
effectiveness of the FedFace method and show that it is comparable to or even
better than the centralized baseline in performance.",8,2021-05-06 08:07:25
70,"The proliferation of automated face recognition in the commercial and
government sectors has caused significant privacy concerns for individuals. One
approach to address these privacy concerns is to employ evasion attacks against
the metric embedding networks powering face recognition systems: Face
obfuscation systems generate imperceptibly perturbed images that cause face
recognition systems to misidentify the user. Perturbed faces are generated on
metric embedding networks, which are known to be unfair in the context of face
recognition. A question of demographic fairness naturally follows: are there
demographic disparities in face obfuscation system performance? We answer this
question with an analytical and empirical exploration of recent face
obfuscation systems. Metric embedding networks are found to be demographically
aware: face embeddings are clustered by demographic. We show how this
clustering behavior leads to reduced face obfuscation utility for faces in
minority groups. An intuitive analytical model yields insight into these
phenomena.",20,2021-08-05 16:18:15
71,"During the COVID-19 coronavirus epidemic, almost everyone wears a facial
mask, which poses a huge challenge to deep face recognition. In this workshop,
we organize Masked Face Recognition (MFR) challenge and focus on bench-marking
deep face recognition methods under the existence of facial masks. In the MFR
challenge, there are two main tracks: the InsightFace track and the WebFace260M
track. For the InsightFace track, we manually collect a large-scale masked face
test set with 7K identities. In addition, we also collect a children test set
including 14K identities and a multi-racial test set containing 242K
identities. By using these three test sets, we build up an online model testing
system, which can give a comprehensive evaluation of face recognition models.
To avoid data privacy problems, no test image is released to the public. As the
challenge is still under-going, we will keep on updating the top-ranked
solutions as well as this report on the arxiv.",11,2021-08-18 15:14:44
72,"Face recognition is one of the most studied research topics in the community.
In recent years, the research on face recognition has shifted to using 3D
facial surfaces, as more discriminating features can be represented by the 3D
geometric information. This survey focuses on reviewing the 3D face recognition
techniques developed in the past ten years which are generally categorized into
conventional methods and deep learning methods. The categorized techniques are
evaluated using detailed descriptions of the representative works. The
advantages and disadvantages of the techniques are summarized in terms of
accuracy, complexity and robustness to face variation (expression, pose and
occlusions, etc). The main contribution of this survey is that it
comprehensively covers both conventional methods and deep learning methods on
3D face recognition. In addition, a review of available 3D face databases is
provided, along with the discussion of future research challenges and
directions.",11,2021-08-25 07:00:59
73,"Despite the unprecedented improvement of face recognition, existing face
recognition models still show considerably low performances in determining
whether a pair of child and adult images belong to the same identity. Previous
approaches mainly focused on increasing the similarity between child and adult
images of a given identity to overcome the discrepancy of facial appearances
due to aging. However, we observe that reducing the similarity between child
images of different identities is crucial for learning distinct features among
children and thus improving face recognition performance in child-adult pairs.
Based on this intuition, we propose a novel loss function called the
Inter-Prototype loss which minimizes the similarity between child images.
Unlike the previous studies, the Inter-Prototype loss does not require
additional child images or training additional learnable parameters. Our
extensive experiments and in-depth analyses show that our approach outperforms
existing baselines in face recognition with child-adult pairs. Our code and
newly-constructed test sets of child-adult pairs are available at
https://github.com/leebebeto/Inter-Prototype.",8,2021-10-22 07:31:14
74,"With increasing appealing to privacy issues in face recognition, federated
learning has emerged as one of the most prevalent approaches to study the
unconstrained face recognition problem with private decentralized data.
However, conventional decentralized federated algorithm sharing whole
parameters of networks among clients suffers from privacy leakage in face
recognition scene. In this work, we introduce a framework, FedGC, to tackle
federated learning for face recognition and guarantees higher privacy. We
explore a novel idea of correcting gradients from the perspective of backward
propagation and propose a softmax-based regularizer to correct gradients of
class embeddings by precisely injecting a cross-client gradient term.
Theoretically, we show that FedGC constitutes a valid loss function similar to
standard softmax. Extensive experiments have been conducted to validate the
superiority of FedGC which can match the performance of conventional
centralized methods utilizing full training dataset on several popular
benchmark datasets.",11,2021-12-14 09:19:29
75,"Media reports have accused face recognition of being ''biased'', ''sexist''
and ''racist''. There is consensus in the research literature that face
recognition accuracy is lower for females, who often have both a higher false
match rate and a higher false non-match rate. However, there is little
published research aimed at identifying the cause of lower accuracy for
females. For instance, the 2019 Face Recognition Vendor Test that documents
lower female accuracy across a broad range of algorithms and datasets also
lists ''Analyze cause and effect'' under the heading ''What we did not do''. We
present the first experimental analysis to identify major causes of lower face
recognition accuracy for females on datasets where previous research has
observed this result. Controlling for equal amount of visible face in the test
images mitigates the apparent higher false non-match rate for females.
Additional analysis shows that makeup-balanced datasets further improves
females to achieve lower false non-match rates. Finally, a clustering
experiment suggests that images of two different females are inherently more
similar than of two different males, potentially accounting for a difference in
false match rates.",8,2021-12-29 17:07:33
76,"Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.",8,2022-03-09 10:21:40
77,"Face recognition is one of the most active tasks in computer vision and has
been widely used in the real world. With great advances made in convolutional
neural networks (CNN), lots of face recognition algorithms have achieved high
accuracy on various face datasets. However, existing face recognition
algorithms based on CNNs are vulnerable to noise. Noise corrupted image
patterns could lead to false activations, significantly decreasing face
recognition accuracy in noisy situations. To equip CNNs with built-in
robustness to noise of different levels, we proposed a Median Pixel Difference
Convolutional Network (MeDiNet) by replacing some traditional convolutional
layers with the proposed novel Median Pixel Difference Convolutional Layer
(MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional
multiscale median filtering with deep CNNs. The MeDiNet is tested on the four
face datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur
kernels, noise intensities, scales, and JPEG quality factors. Extensive
experiments show that our MeDiNet can effectively remove noisy pixels in the
feature map and suppress the negative impact of noise, leading to achieving
limited accuracy loss under these practical noises compared with the standard
CNN under clean conditions.",8,2022-05-30 13:15:49
78,"To recognize the masked face, one of the possible solutions could be to
restore the occluded part of the face first and then apply the face recognition
method. Inspired by the recent image inpainting methods, we propose an
end-to-end hybrid masked face recognition system, namely HiMFR, consisting of
three significant parts: masked face detector, face inpainting, and face
recognition. The masked face detector module applies a pretrained Vision
Transformer (ViT\_b32) to detect whether faces are covered with masked or not.
The inpainting module uses a fine-tune image inpainting model based on a
Generative Adversarial Network (GAN) to restore faces. Finally, the hybrid face
recognition module based on ViT with an EfficientNetB3 backbone recognizes the
faces. We have implemented and evaluated our proposed method on four different
publicly available datasets: CelebA, SSDMNV2, MAFA, {Pubfig83} with our locally
collected small dataset, namely Face5. Comprehensive experimental results show
the efficacy of the proposed HiMFR method with competitive performance. Code is
available at https://github.com/mdhosen/HiMFR",11,2022-09-19 11:26:49
79,"Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.",8,2007-04-26 11:29:19
80,"Face recognition systems must be robust to the variation of various factors
such as facial expression, illumination, head pose and aging. Especially, the
robustness against illumination variation is one of the most important problems
to be solved for the practical use of face recognition systems. Gabor wavelet
is widely used in face detection and recognition because it gives the
possibility to simulate the function of human visual system. In this paper, we
propose a method for extracting Gabor wavelet features which is stable under
the variation of local illumination and show experiment results demonstrating
its effectiveness.",8,2012-12-11 13:19:54
81,"In this paper, we proposed a novel pipeline for image-level classification in
the hyperspectral images. By doing this, we show that the discriminative
spectral information at image-level features lead to significantly improved
performance in a face recognition task. We also explored the potential of
traditional feature descriptors in the hyperspectral images. From our
evaluations, we observe that SIFT features outperform the state-of-the-art
hyperspectral face recognition methods, and also the other descriptors. With
the increasing deployment of hyperspectral sensors in a multitude of
applications, we believe that our approach can effectively exploit the spectral
information in hyperspectral images, thus beneficial to more accurate
classification.",8,2016-05-11 13:18:22
82,"In this paper we describe a solution to our entry for the emotion recognition
challenge EmotiW 2017. We propose an ensemble of several models, which capture
spatial and audio features from videos. Spatial features are captured by
convolutional neural networks, pretrained on large face recognition datasets.
We show that usage of strong industry-level face recognition networks increases
the accuracy of emotion recognition. Using our ensemble we improve on the
previous best result on the test set by about 1 %, achieving a 60.03 %
classification accuracy without any use of visual temporal information.",11,2017-11-13 14:38:24
83,"Starting in the seventies, face recognition has become one of the most
researched topics in computer vision and biometrics. Traditional methods based
on hand-crafted features and traditional machine learning techniques have
recently been superseded by deep neural networks trained with very large
datasets. In this paper we provide a comprehensive and up-to-date literature
review of popular face recognition methods including both traditional
(geometry-based, holistic, feature-based and hybrid methods) and deep learning
methods.",8,2018-10-31 20:58:39
84,"This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.",11,2019-01-19 20:33:35
85,"In this paper we present an efficient implementation using triplet loss for
face recognition. We conduct the practical experiment to analyze the factors
that influence the training of triplet loss. All models are trained on
CASIA-Webface dataset and tested on LFW. We analyze the experiment results and
give some insights to help others balance the factors when they apply triplet
loss to their own problem especially for face recognition task. Code has been
released in https://github.com/yule-li/MassFace.",11,2019-02-28 10:49:39
86,"Traditionally, researchers in automatic face recognition and biometric
technologies have focused on developing accurate algorithms. With this
technology being integrated into operational systems, engineers and scientists
are being asked, do these systems meet societal norms? The origin of this line
of inquiry is `trust' of artificial intelligence (AI) systems. In this paper,
we concentrate on adapting explainable AI to face recognition and biometrics,
and we present four principles of explainable AI to face recognition and
biometrics. The principles are illustrated by $\it{four}$ case studies, which
show the challenges and issues in developing algorithms that can produce
explanations.",4,2020-02-03 21:03:20
87,"Over the past few decades, interest in algorithms for face recognition has
been growing rapidly and has even surpassed human-level performance. Despite
their accomplishments, their practical integration with a real-time
performance-hungry system is not feasible due to high computational costs. So
in this paper, we explore the recent, fast, and accurate face recognition
system that can be easily integrated with real-time devices, and tested the
algorithms on robot hardware platforms to confirm their robustness and speed.",8,2022-04-19 16:26:48
88,"Deep learning, in particular Convolutional Neural Network (CNN), has achieved
promising results in face recognition recently. However, it remains an open
question: why CNNs work well and how to design a 'good' architecture. The
existing works tend to focus on reporting CNN architectures that work well for
face recognition rather than investigate the reason. In this work, we conduct
an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a
common ground to make our work easily reproducible. Specifically, we use public
database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing
CNNs trained on private databases. We propose three CNN architectures which are
the first reported architectures trained using LFW data. This paper
quantitatively compares the architectures of CNNs and evaluate the effect of
different implementation choices. We identify several useful properties of
CNN-FRS. For instance, the dimensionality of the learned features can be
significantly reduced without adverse effect on face recognition accuracy. In
addition, traditional metric learning method exploiting CNN-learned features is
evaluated. Experiments show two crucial factors to good CNN-FRS performance are
the fusion of multiple CNNs and metric learning. To make our work reproducible,
source code and models will be made publicly available.",8,2015-04-09 15:27:49
89,"Despite rapid advances in face recognition, there remains a clear gap between
the performance of still image-based face recognition and video-based face
recognition, due to the vast difference in visual quality between the domains
and the difficulty of curating diverse large-scale video datasets. This paper
addresses both of those challenges, through an image to video feature-level
domain adaptation approach, to learn discriminative video frame
representations. The framework utilizes large-scale unlabeled video data to
reduce the gap between different domains while transferring discriminative
knowledge from large-scale labeled still images. Given a face recognition
network that is pretrained in the image domain, the adaptation is achieved by
(i) distilling knowledge from the network to a video adaptation network through
feature matching, (ii) performing feature restoration through synthetic data
augmentation and (iii) learning a domain-invariant feature through a domain
adversarial discriminator. We further improve performance through a
discriminator-guided feature fusion that boosts high-quality frames while
eliminating those degraded by video domain-specific factors. Experiments on the
YouTube Faces and IJB-A datasets demonstrate that each module contributes to
our feature-level domain adaptation framework and substantially improves video
face recognition performance to achieve state-of-the-art accuracy. We
demonstrate qualitatively that the network learns to suppress diverse artifacts
in videos such as pose, illumination or occlusion without being explicitly
trained for them.",11,2017-08-07 16:36:54
90,"This work tackles the face recognition task on images captured using thermal
camera sensors which can operate in the non-light environment. While it can
greatly increase the scope and benefits of the current security surveillance
systems, performing such a task using thermal images is a challenging problem
compared to face recognition task in the Visible Light Domain (VLD). This is
partly due to the much smaller amount number of thermal imagery data collected
compared to the VLD data. Unfortunately, direct application of the existing
very strong face recognition models trained using VLD data into the thermal
imagery data will not produce a satisfactory performance. This is due to the
existence of the domain gap between the thermal and VLD images. To this end, we
propose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is
able to transform thermal face images into their corresponding VLD images
whilst maintaining identity information which is sufficient enough for the
existing VLD face recognition models to perform recognition. Some examples are
presented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an
explicit closed-set face recognition loss to regularize the discriminator
network training. This information will then be conveyed into the generator
network in the forms of gradient loss. In the experiment, we show that by using
this additional explicit regularization for the discriminator network, the
TV-GAN is able to preserve more identity information when translating a thermal
image of a person which is not seen before by the TV-GAN.",8,2017-12-07 07:06:39
91,"Recent advances in deep learning have significantly increased the performance
of face recognition systems. The performance and reliability of these models
depend heavily on the amount and quality of the training data. However, the
collection of annotated large datasets does not scale well and the control over
the quality of the data decreases with the size of the dataset. In this work,
we explore how synthetically generated data can be used to decrease the number
of real-world images needed for training deep face recognition systems. In
particular, we make use of a 3D morphable face model for the generation of
images with arbitrary amounts of facial identities and with full control over
image variations, such as pose, illumination, and background. In our
experiments with an off-the-shelf face recognition software we observe the
following phenomena: 1) The amount of real training data needed to train
competitive deep face recognition systems can be reduced significantly. 2)
Combining large-scale real-world data with synthetic data leads to an increased
performance. 3) Models trained only on synthetic data with strong variations in
pose, illumination, and background perform very well across different datasets
even without dataset adaptation. 4) The real-to-virtual performance gap can be
closed when using synthetic data for pre-training, followed by fine-tuning with
real-world images. 5) There are no observable negative effects of pre-training
with synthetic data. Thus, any face recognition system in our experiments
benefits from using synthetic face images. The synthetic data generator, as
well as all experiments, are publicly available.",11,2018-02-16 11:05:18
92,"Deep neural network (DNN) architecture based models have high expressive
power and learning capacity. However, they are essentially a black box method
since it is not easy to mathematically formulate the functions that are learned
within its many layers of representation. Realizing this, many researchers have
started to design methods to exploit the drawbacks of deep learning based
algorithms questioning their robustness and exposing their singularities. In
this paper, we attempt to unravel three aspects related to the robustness of
DNNs for face recognition: (i) assessing the impact of deep architectures for
face recognition in terms of vulnerabilities to attacks inspired by commonly
observed distortions in the real world that are well handled by shallow
learning methods along with learning based adversaries; (ii) detecting the
singularities by characterizing abnormal filter response behavior in the hidden
layers of deep networks; and (iii) making corrections to the processing
pipeline to alleviate the problem. Our experimental evaluation using multiple
open-source DNN-based face recognition networks, including OpenFace and
VGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates
that the performance of deep learning based face recognition algorithms can
suffer greatly in the presence of such distortions. The proposed method is also
compared with existing detection algorithms and the results show that it is
able to detect the attacks with very high accuracy by suitably designing a
classifier using the response of the hidden layers in the network. Finally, we
present several effective countermeasures to mitigate the impact of adversarial
attacks and improve the overall robustness of DNN-based face recognition.",11,2018-02-22 08:03:26
93,"Accurate face recognition techniques make a series of critical applications
possible: policemen could employ it to retrieve criminals' faces from
surveillance video streams; cross boarder travelers could pass a face
authentication inspection line without the involvement of officers.
Nonetheless, when public security heavily relies on such intelligent systems,
the designers should deliberately consider the emerging attacks aiming at
misleading those systems employing face recognition.
  We propose a kind of brand new attack against face recognition systems, which
is realized by illuminating the subject using infrared according to the
adversarial examples worked out by our algorithm, thus face recognition systems
can be bypassed or misled while simultaneously the infrared perturbations
cannot be observed by raw eyes. Through launching this kind of attack, an
attacker not only can dodge surveillance cameras. More importantly, he can
impersonate his target victim and pass the face authentication system, if only
the victim's photo is acquired by the attacker. Again, the attack is totally
unobservable by nearby people, because not only the light is invisible, but
also the device we made to launch the attack is small enough. According to our
study on a large dataset, attackers have a very high success rate with a over
70\% success rate for finding such an adversarial example that can be
implemented by infrared. To the best of our knowledge, our work is the first
one to shed light on the severity of threat resulted from infrared adversarial
examples against face recognition.",8,2018-03-13 08:51:01
94,"Although deep learning approaches have achieved performance surpassing humans
for still image-based face recognition, unconstrained video-based face
recognition is still a challenging task due to large volume of data to be
processed and intra/inter-video variations on pose, illumination, occlusion,
scene, blur, video quality, etc. In this work, we consider challenging
scenarios for unconstrained video-based face recognition from multiple-shot
videos and surveillance videos with low-quality frames. To handle these
problems, we propose a robust and efficient system for unconstrained
video-based face recognition, which is composed of modules for face/fiducial
detection, face association, and face recognition. First, we use multi-scale
single-shot face detectors to efficiently localize faces in videos. The
detected faces are then grouped respectively through carefully designed face
association methods, especially for multi-shot videos. Finally, the faces are
recognized by the proposed face matcher based on an unsupervised subspace
learning approach and a subspace-to-subspace similarity metric. Extensive
experiments on challenging video datasets, such as Multiple Biometric Grand
Challenge (MBGC), Face and Ocular Challenge Series (FOCS), IARPA Janus
Surveillance Video Benchmark (IJB-S) for low-quality surveillance videos and
IARPA JANUS Benchmark B (IJB-B) for multiple-shot videos, demonstrate that the
proposed system can accurately detect and associate faces from unconstrained
videos and effectively learn robust and discriminative features for
recognition.",11,2018-12-10 19:51:38
95,"Faces play a magnificent role in human robot interaction, as they do in our
daily life. The inherent ability of the human mind facilitates us to recognize
a person by exploiting various challenges such as bad illumination, occlusions,
pose variation etc. which are involved in face recognition. But it is a very
complex task in nature to identify a human face by humanoid robots. The recent
literatures on face biometric recognition are extremely rich in its application
on structured environment for solving human identification problem. But the
application of face biometric on mobile robotics is limited for its inability
to produce accurate identification in uneven circumstances. The existing face
recognition problem has been tackled with our proposed component based
fragmented face recognition framework. The proposed framework uses only a
subset of the full face such as eyes, nose and mouth to recognize a person.
It's less searching cost, encouraging accuracy and ability to handle various
challenges of face recognition offers its applicability on humanoid robots. The
second problem in face recognition is the face spoofing, in which a face
recognition system is not able to distinguish between a person and an imposter
(photo/video of the genuine user). The problem will become more detrimental
when robots are used as an authenticator. A depth analysis method has been
investigated in our research work to test the liveness of imposters to
discriminate them from the legitimate users. The implication of the previous
earned techniques has been used with respect to criminal identification with
NAO robot. An eyewitness can interact with NAO through a user interface. NAO
asks several questions about the suspect, such as age, height, her/his facial
shape and size etc., and then making a guess about her/his face.",11,2018-12-08 11:00:13
96,"Face recognition has obtained remarkable progress in recent years due to the
great improvement of deep convolutional neural networks (CNNs). However, deep
CNNs are vulnerable to adversarial examples, which can cause fateful
consequences in real-world face recognition applications with
security-sensitive purposes. Adversarial attacks are widely studied as they can
identify the vulnerability of the models before they are deployed. In this
paper, we evaluate the robustness of state-of-the-art face recognition models
in the decision-based black-box attack setting, where the attackers have no
access to the model parameters and gradients, but can only acquire hard-label
predictions by sending queries to the target model. This attack setting is more
practical in real-world face recognition systems. To improve the efficiency of
previous methods, we propose an evolutionary attack algorithm, which can model
the local geometries of the search directions and reduce the dimension of the
search space. Extensive experiments demonstrate the effectiveness of the
proposed method that induces a minimum perturbation to an input face image with
fewer queries. We also apply the proposed method to attack a real-world face
recognition system successfully.",8,2019-04-09 02:45:35
97,"There are many factors affecting visual face recognition, such as low
resolution images, aging, illumination and pose variance, etc. One of the most
important problem is low resolution face images which can result in bad
performance on face recognition. Most of the general face recognition
algorithms usually assume a sufficient resolution for the face images. However,
in practice many applications often do not have sufficient image resolutions.
The modern face hallucination models demonstrate reasonable performance to
reconstruct high-resolution images from its corresponding low resolution
images. However, they do not consider identity level information during
hallucination which directly affects results of the recognition of low
resolution faces. To address this issue, we propose a Face Hallucination
Generative Adversarial Network (FH-GAN) which improves the quality of low
resolution face images and accurately recognize those low quality images.
Concretely, we make the following contributions: 1) we propose FH-GAN network,
an end-to-end system, that improves both face hallucination and face
recognition simultaneously. The novelty of this proposed network depends on
incorporating identity information in a GAN-based face hallucination algorithm
via combining a face recognition network for identity preserving. 2) We also
propose a new face hallucination network, namely Dense Sparse Network (DSNet),
which improves upon the state-of-art in face hallucination. 3) We demonstrate
benefits of training the face recognition and GAN-based DSNet jointly by
reporting good result on face hallucination and recognition.",8,2019-05-16 05:49:01
98,"Near-infrared-visible (NIR-VIS) heterogeneous face recognition matches NIR to
corresponding VIS face images. However, due to the sensing gap, NIR images
often lose some identity information so that the recognition issue is more
difficult than conventional VIS face recognition. Recently, NIR-VIS
heterogeneous face recognition has attracted considerable attention in the
computer vision community because of its convenience and adaptability in
practical applications. Various deep learning-based methods have been proposed
and substantially increased the recognition performance, but the lack of
NIR-VIS training samples leads to the difficulty of the model training process.
In this paper, we propose a new Large-Scale Multi-Pose High-Quality NIR-VIS
database LAMP-HQ containing 56,788 NIR and 16,828 VIS images of 573 subjects
with large diversities in pose, illumination, attribute, scene and accessory.
We furnish a benchmark along with the protocol for NIR-VIS face recognition via
generation on LAMP-HQ, including Pixel2Pixel, CycleGAN, and ADFL. Furthermore,
we propose a novel exemplar-based variational spectral attention network to
produce high-fidelity VIS images from NIR data. A spectral conditional
attention module is introduced to reduce the domain gap between NIR and VIS
data and then improve the performance of NIR-VIS heterogeneous face recognition
on various databases including the LAMP-HQ.",11,2019-12-17 04:01:18
99,"Face image quality is an important factor to enable high performance face
recognition systems. Face quality assessment aims at estimating the suitability
of a face image for recognition. Previous work proposed supervised solutions
that require artificially or human labelled quality values. However, both
labelling mechanisms are error-prone as they do not rely on a clear definition
of quality and may not know the best characteristics for the utilized face
recognition system. Avoiding the use of inaccurate quality labels, we proposed
a novel concept to measure face quality based on an arbitrary face recognition
model. By determining the embedding variations generated from random
subnetworks of a face model, the robustness of a sample representation and
thus, its quality is estimated. The experiments are conducted in a
cross-database evaluation setting on three publicly available databases. We
compare our proposed solution on two face embeddings against six
state-of-the-art approaches from academia and industry. The results show that
our unsupervised solution outperforms all other approaches in the majority of
the investigated scenarios. In contrast to previous works, the proposed
solution shows a stable performance over all scenarios. Utilizing the deployed
face recognition model for our face quality assessment methodology avoids the
training phase completely and further outperforms all baseline approaches by a
large margin. Our solution can be easily integrated into current face
recognition systems and can be modified to other tasks beyond face recognition.",8,2020-03-20 16:50:30
100,"Face recognition has been of great importance in many applications as a
biometric for its throughput, convenience, and non-invasiveness. Recent
advancements in deep Convolutional Neural Network (CNN) architectures have
boosted significantly the performance of face recognition based on
two-dimensional (2D) facial texture images and outperformed the previous state
of the art using conventional methods. However, the accuracy of 2D face
recognition is still challenged by the change of pose, illumination, make-up,
and expression. On the other hand, the geometric information contained in
three-dimensional (3D) face data has the potential to overcome the fundamental
limitations of 2D face data.
  We propose a multi-Channel deep 3D face network for face recognition based on
3D face data. We compute the geometric information of a 3D face based on its
piecewise-linear triangular mesh structure and then conformally flatten
geometric information along with the color from 3D to 2D plane to leverage the
state-of-the-art deep CNN architectures. We modify the input layer of the
network to take images with nine channels instead of three only such that more
geometric information can be explicitly fed to it. We pre-train the network
using images from the VGG-Face \cite{Parkhi2015} and then fine-tune it with the
generated multi-channel face images. The face recognition accuracy of the
multi-Channel deep 3D face network has achieved 98.6. The experimental results
also clearly show that the network performs much better when a 9-channel image
is flattened to plane based on the conformal map compared with the orthographic
projection.",8,2020-09-30 15:29:05
101,"Recently, significant advancements have been made in face recognition
technologies using Deep Neural Networks. As a result, companies such as
Microsoft, Amazon, and Naver offer highly accurate commercial face recognition
web services for diverse applications to meet the end-user needs. Naturally,
however, such technologies are threatened persistently, as virtually any
individual can quickly implement impersonation attacks. In particular, these
attacks can be a significant threat for authentication and identification
services, which heavily rely on their underlying face recognition technologies'
accuracy and robustness. Despite its gravity, the issue regarding deepfake
abuse using commercial web APIs and their robustness has not yet been
thoroughly investigated. This work provides a measurement study on the
robustness of black-box commercial face recognition APIs against Deepfake
Impersonation (DI) attacks using celebrity recognition APIs as an example case
study. We use five deepfake datasets, two of which are created by us and
planned to be released. More specifically, we measure attack performance based
on two scenarios (targeted and non-targeted) and further analyze the differing
system behaviors using fidelity, confidence, and similarity metrics.
Accordingly, we demonstrate how vulnerable face recognition technologies from
popular companies are to DI attack, achieving maximum success rates of 78.0%
and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with
any celebrity) attacks, respectively. Moreover, we propose practical defense
strategies to mitigate DI attacks, reducing the attack success rates to as low
as 0% and 0.02% for targeted and non-targeted attacks, respectively.",8,2021-03-01 08:40:10
102,"Near-infrared to visible (NIR-VIS) face recognition is the most common case
in heterogeneous face recognition, which aims to match a pair of face images
captured from two different modalities. Existing deep learning based methods
have made remarkable progress in NIR-VIS face recognition, while it encounters
certain newly-emerged difficulties during the pandemic of COVID-19, since
people are supposed to wear facial masks to cut off the spread of the virus. We
define this task as NIR-VIS masked face recognition, and find it problematic
with the masked face in the NIR probe image. First, the lack of masked face
data is a challenging issue for the network training. Second, most of the
facial parts (cheeks, mouth, nose etc.) are fully occluded by the mask, which
leads to a large amount of loss of information. Third, the domain gap still
exists in the remaining facial parts. In such scenario, the existing methods
suffer from significant performance degradation caused by the above issues. In
this paper, we aim to address the challenge of NIR-VIS masked face recognition
from the perspectives of training data and training method. Specifically, we
propose a novel heterogeneous training method to maximize the mutual
information shared by the face representation of two domains with the help of
semi-siamese networks. In addition, a 3D face reconstruction based approach is
employed to synthesize masked face from the existing NIR image. Resorting to
these practices, our solution provides the domain-invariant face representation
which is also robust to the mask occlusion. Extensive experiments on three
NIR-VIS face datasets demonstrate the effectiveness and
cross-dataset-generalization capacity of our method.",11,2021-04-14 10:40:09
103,"This research presents an improved real-time face recognition system at a low
resolution of 15 pixels with pose and emotion and resolution variations. We
have designed our datasets named LRD200 and LRD100, which have been used for
training and classification. The face detection part uses the Viola-Jones
algorithm, and the face recognition part receives the face image from the face
detection part to process it using the Local Binary Pattern Histogram (LBPH)
algorithm with preprocessing using contrast limited adaptive histogram
equalization (CLAHE) and face alignment. The face database in this system can
be updated via our custom-built standalone android app and automatic restarting
of the training and recognition process with an updated database. Using our
proposed algorithm, a real-time face recognition accuracy of 78.40% at 15 px
and 98.05% at 45 px have been achieved using the LRD200 database containing 200
images per person. With 100 images per person in the database (LRD100) the
achieved accuracies are 60.60% at 15 px and 95% at 45 px respectively. A facial
deflection of about 30 degrees on either side from the front face showed an
average face recognition precision of 72.25% - 81.85%. This face recognition
system can be employed for law enforcement purposes, where the surveillance
camera captures a low-resolution image because of the distance of a person from
the camera. It can also be used as a surveillance system in airports, bus
stations, etc., to reduce the risk of possible criminal threats.",8,2021-04-15 04:54:29
104,"With the recent advancement of deep convolutional neural networks,
significant progress has been made in general face recognition. However, the
state-of-the-art general face recognition models do not generalize well to
occluded face images, which are exactly the common cases in real-world
scenarios. The potential reasons are the absences of large-scale occluded face
data for training and specific designs for tackling corrupted features brought
by occlusions. This paper presents a novel face recognition method that is
robust to occlusions based on a single end-to-end deep neural network. Our
approach, named FROM (Face Recognition with Occlusion Masks), learns to
discover the corrupted features from the deep convolutional neural networks,
and clean them by the dynamically learned masks. In addition, we construct
massive occluded face images to train FROM effectively and efficiently. FROM is
simple yet powerful compared to the existing methods that either rely on
external detectors to discover the occlusions or employ shallow models which
are less discriminative. Experimental results on the LFW, Megaface challenge 1,
RMF2, AR dataset and other simulated occluded/masked datasets confirm that FROM
dramatically improves the accuracy under occlusions, and generalizes well on
general face recognition. Code is available at
https://github.com/haibo-qiu/FROM",8,2021-08-21 09:08:41
105,"This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- ""characteristic gradient detachment""
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.",11,2021-09-12 17:07:54
106,"Learning discriminative face features plays a major role in building
high-performing face recognition models. The recent state-of-the-art face
recognition solutions proposed to incorporate a fixed penalty margin on
commonly used classification loss function, softmax loss, in the normalized
hypersphere to increase the discriminative power of face recognition models, by
minimizing the intra-class variation and maximizing the inter-class variation.
Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the
geodesic distance between and within the different identities can be equally
learned using a fixed penalty margin. However, such a learning objective is not
realistic for real data with inconsistent inter-and intra-class variation,
which might limit the discriminative and generalizability of the face
recognition model. In this paper, we relax the fixed penalty margin constrain
by proposing elastic penalty margin loss (ElasticFace) that allows flexibility
in the push for class separability. The main idea is to utilize random margin
values drawn from a normal distribution in each training iteration. This aims
at giving the decision boundary chances to extract and retract to allow space
for flexible class separability learning. We demonstrate the superiority of our
ElasticFace loss over ArcFace and CosFace losses, using the same geometric
transformation, on a large set of mainstream benchmarks. From a wider
perspective, our ElasticFace has advanced the state-of-the-art face recognition
performance on seven out of nine mainstream benchmarks.",8,2021-09-20 10:31:50
107,"Face recognition systems have to deal with large variabilities (such as
different poses, illuminations, and expressions) that might lead to incorrect
matching decisions. These variabilities can be measured in terms of face image
quality which is defined over the utility of a sample for recognition. Previous
works on face recognition either do not employ this valuable information or
make use of non-inherently fit quality estimates. In this work, we propose a
simple and effective face recognition solution (QMagFace) that combines a
quality-aware comparison score with a recognition model based on a
magnitude-aware angular margin loss. The proposed approach includes
model-specific face image qualities in the comparison process to enhance the
recognition performance under unconstrained circumstances. Exploiting the
linearity between the qualities and their comparison scores induced by the
utilized loss, our quality-aware comparison function is simple and highly
generalizable. The experiments conducted on several face recognition databases
and benchmarks demonstrate that the introduced quality-awareness leads to
consistent improvements in the recognition performance. Moreover, the proposed
QMagFace approach performs especially well under challenging circumstances,
such as cross-pose, cross-age, or cross-quality. Consequently, it leads to
state-of-the-art performances on several face recognition benchmarks, such as
98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace
is publicly available",8,2021-11-26 12:44:54
108,"Current state-of-the-art deep learning based face recognition (FR) models
require a large number of face identities for central training. However, due to
the growing privacy awareness, it is prohibited to access the face images on
user devices to continually improve face recognition models. Federated Learning
(FL) is a technique to address the privacy issue, which can collaboratively
optimize the model without sharing the data between clients. In this work, we
propose a FL based framework called FedFR to improve the generic face
representation in a privacy-aware manner. Besides, the framework jointly
optimizes personalized models for the corresponding clients via the proposed
Decoupled Feature Customization module. The client-specific personalized model
can serve the need of optimized face recognition experience for registered
identities at the local device. To the best of our knowledge, we are the first
to explore the personalized face recognition in FL setup. The proposed
framework is validated to be superior to previous approaches on several generic
and personalized face recognition benchmarks with diverse FL scenarios. The
source codes and our proposed personalized FR benchmark under FL setup are
available at https://github.com/jackie840129/FedFR.",11,2021-12-23 12:42:38
109,"The 3D face recognition has long been considered secure for its resistance to
current physical adversarial attacks, like adversarial patches. However, this
paper shows that a 3D face recognition system can be easily attacked, leading
to evading and impersonation attacks. We are the first to propose a physically
realizable attack for the 3D face recognition system, named structured light
imaging attack (SLIA), which exploits the weakness of structured-light-based 3D
scanning devices. SLIA utilizes the projector in the structured light imaging
system to create adversarial illuminations to contaminate the reconstructed
point cloud. Firstly, we propose a 3D transform-invariant loss function (3D-TI)
to generate adversarial perturbations that are more robust to head movements.
Then we integrate the 3D imaging process into the attack optimization, which
minimizes the total pixel shifting of fringe patterns. We realize both dodging
and impersonation attacks on a real-world 3D face recognition system. Our
methods need fewer modifications on projected patterns compared with Chamfer
and Chamfer+kNN-based methods and achieve average attack success rates of 0.47
(impersonation) and 0.89 (dodging). This paper exposes the insecurity of
present structured light imaging technology and sheds light on designing secure
3D face recognition authentication systems.",8,2022-05-26 15:06:14
110,"Deep neural network based face recognition models have been shown to be
vulnerable to adversarial examples. However, many of the past attacks require
the adversary to solve an input-dependent optimization problem using gradient
descent which makes the attack impractical in real-time. These adversarial
examples are also tightly coupled to the attacked model and are not as
successful in transferring to different models. In this work, we propose
ReFace, a real-time, highly-transferable attack on face recognition models
based on Adversarial Transformation Networks (ATNs). ATNs model adversarial
example generation as a feed-forward neural network. We find that the white-box
attack success rate of a pure U-Net ATN falls substantially short of
gradient-based attacks like PGD on large face recognition datasets. We
therefore propose a new architecture for ATNs that closes this gap while
maintaining a 10000x speedup over PGD. Furthermore, we find that at a given
perturbation magnitude, our ATN adversarial perturbations are more effective in
transferring to new face recognition models than PGD. ReFace attacks can
successfully deceive commercial face recognition services in a transfer attack
setting and reduce face identification accuracy from 82% to 16.4% for AWS
SearchFaces API and Azure face verification accuracy from 91% to 50.1%.",8,2022-06-09 22:25:34
111,"Recent deep face recognition models proposed in the literature utilized
large-scale public datasets such as MS-Celeb-1M and VGGFace2 for training very
deep neural networks, achieving state-of-the-art performance on mainstream
benchmarks. Recently, many of these datasets, e.g., MS-Celeb-1M and VGGFace2,
are retracted due to credible privacy and ethical concerns. This motivates this
work to propose and investigate the feasibility of using a privacy-friendly
synthetically generated face dataset to train face recognition models. Towards
this end, we utilize a class-conditional generative adversarial network to
generate class-labeled synthetic face images, namely SFace. To address the
privacy aspect of using such data to train a face recognition model, we provide
extensive evaluation experiments on the identity relation between the synthetic
dataset and the original authentic dataset used to train the generative model.
Our reported evaluation proved that associating an identity of the authentic
dataset to one with the same class label in the synthetic dataset is hardly
possible. We also propose to train face recognition on our privacy-friendly
dataset, SFace, using three different learning strategies, multi-class
classification, label-free knowledge transfer, and combined learning of
multi-class classification and knowledge transfer. The reported evaluation
results on five authentic face benchmarks demonstrated that the
privacy-friendly synthetic dataset has high potential to be used for training
face recognition models, achieving, for example, a verification accuracy of
91.87\% on LFW using multi-class classification and 99.13\% using the combined
learning strategy.",8,2022-06-21 16:42:04
112,"Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.",8,2022-07-15 07:15:36
113,"Deep learning based face recognition has achieved significant progress in
recent years. Yet, the practical model production and further research of deep
face recognition are in great need of corresponding public support. For
example, the production of face representation network desires a modular
training scheme to consider the proper choice from various candidates of
state-of-the-art backbone and training supervision subject to the real-world
face recognition demand; for performance analysis and comparison, the standard
and automatic evaluation with a bunch of models on multiple benchmarks will be
a desired tool as well; besides, a public groundwork is welcomed for deploying
the face recognition in the shape of holistic pipeline. Furthermore, there are
some newly-emerged challenges, such as the masked face recognition caused by
the recent world-wide COVID-19 pandemic, which draws increasing attention in
practical applications. A feasible and elegant solution is to build an
easy-to-use unified framework to meet the above demands. To this end, we
introduce a novel open-source framework, named FaceX-Zoo, which is oriented to
the research-development community of face recognition. Resorting to the highly
modular and scalable design, FaceX-Zoo provides a training module with various
supervisory heads and backbones towards state-of-the-art face recognition, as
well as a standardized evaluation module which enables to evaluate the models
in most of the popular benchmarks just by editing a simple configuration. Also,
a simple yet fully functional face SDK is provided for the validation and
primary application of the trained models. Rather than including as many as
possible of the prior techniques, we enable FaceX-Zoo to easily upgrade and
extend along with the development of face related domains. The source code and
models are available at https://github.com/JDAI-CV/FaceX-Zoo.",11,2021-01-12 11:06:50
114,"Facial expressions convey non-verbal cues, which play an important role in
interpersonal relations. Automatic recognition of human face based on facial
expression can be an important component of natural human-machine interface. It
may also be used in behavioural science. Although human can recognize the face
practically without any effort, but reliable face recognition by machine is a
challenge. This paper presents a new approach for recognizing the face of a
person considering the expressions of the same human face at different
instances of time. This methodology is developed combining Eigenface method for
feature extraction and modified k-Means clustering for identification of the
human face. This method endowed the face recognition without using the
conventional distance measure classifiers. Simulation results show that
proposed face recognition using perception of k-Means clustering is useful for
face images with different facial expressions.",8,2011-04-07 03:17:08
115,"While existing face recognition systems based on local features are robust to
issues such as misalignment, they can exhibit accuracy degradation when
comparing images of differing resolutions. This is common in surveillance
environments where a gallery of high resolution mugshots is compared to low
resolution CCTV probe images, or where the size of a given image is not a
reliable indicator of the underlying resolution (eg. poor optics). To alleviate
this degradation, we propose a compensation framework which dynamically chooses
the most appropriate face recognition system for a given pair of image
resolutions. This framework applies a novel resolution detection method which
does not rely on the size of the input images, but instead exploits the
sensitivity of local features to resolution using a probabilistic multi-region
histogram approach. Experiments on a resolution-modified version of the
""Labeled Faces in the Wild"" dataset show that the proposed resolution detector
frontend obtains a 99% average accuracy in selecting the most appropriate face
recognition system, resulting in higher overall face discrimination accuracy
(across several resolutions) compared to the individual baseline face
recognition systems.",8,2013-04-08 08:36:55
116,"Hybrid approach has a special status among Face Recognition Systems as they
combine different recognition approaches in an either serial or parallel to
overcome the shortcomings of individual methods. This paper explores the area
of Hybrid Face Recognition using score based strategy as a combiner/fusion
process. In proposed approach, the recognition system operates in two modes:
training and classification. Training mode involves normalization of the face
images (training set), extracting appropriate features using Principle
Component Analysis (PCA) and Independent Component Analysis (ICA). The
extracted features are then trained in parallel using Back-propagation neural
networks (BPNNs) to partition the feature space in to different face classes.
In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face
image(s). The score based strategy which works as a combiner is applied to the
results of both PCA BPNN and ICA BPNN to classify given new face image(s)
according to face classes obtained during the training mode. The proposed
approach has been tested on ORL and other face databases; the experimented
results show that the proposed system has higher accuracy than face recognition
systems using single feature extractor.",8,2014-01-02 09:21:24
117,"Face recognition is a widely used biometric approach. Face recognition
technology has developed rapidly in recent years and it is more direct, user
friendly and convenient compared to other methods. But face recognition systems
are vulnerable to spoof attacks made by non-real faces. It is an easy way to
spoof face recognition systems by facial pictures such as portrait photographs.
A secure system needs Liveness detection in order to guard against such
spoofing. In this work, face liveness detection approaches are categorized
based on the various types techniques used for liveness detection. This
categorization helps understanding different spoof attacks scenarios and their
relation to the developed solutions. A review of the latest works regarding
face liveness detection works is presented. The main aim is to provide a simple
path for the future development of novel and more secured face liveness
detection approach.",20,2014-05-09 13:47:09
118,"The reduction of the cost of infrared (IR) cameras in recent years has made
IR imaging a highly viable modality for face recognition in practice. A
particularly attractive advantage of IR-based over conventional, visible
spectrum-based face recognition stems from its invariance to visible
illumination. In this paper we argue that the main limitation of previous work
on face recognition using IR lies in its ad hoc approach to treating different
nuisance factors which affect appearance, prohibiting a unified approach that
is capable of handling concurrent changes in multiple (or indeed all) major
extrinsic sources of variability, which is needed in practice. We describe the
first approach that attempts to achieve this - the framework we propose
achieves outstanding recognition performance in the presence of variable (i)
pose, (ii) facial expression, (iii) physiological state, (iv) partial occlusion
due to eye-wear, and (v) quasi-occlusion due to facial hair growth.",8,2014-07-28 04:20:24
119,"Face recognition performance improves rapidly with the recent deep learning
technique developing and underlying large training dataset accumulating. In
this paper, we report our observations on how big data impacts the recognition
performance. According to these observations, we build our Megvii Face
Recognition System, which achieves 99.50% accuracy on the LFW benchmark,
outperforming the previous state-of-the-art. Furthermore, we report the
performance in a real-world security certification scenario. There still exists
a clear gap between machine recognition and human performance. We summarize our
experiments and present three challenges lying ahead in recent face
recognition. And we indicate several possible solutions towards these
challenges. We hope our work will stimulate the community's discussion of the
difference between research benchmark and real-world applications.",11,2015-01-20 01:15:02
120,"This paper presents a real-time face recognition system using kinect sensor.
The algorithm is implemented on GPU using opencl and significant speed
improvements are observed. We use kinect depth image to increase the robustness
and reduce computational cost of conventional LBP based face recognition. The
main objective of this paper was to perform robust, high speed fusion based
face recognition and tracking. The algorithm is mainly composed of three steps.
First step is to detect all faces in the video using viola jones algorithm. The
second step is online database generation using a tracking window on the face.
A modified LBP feature vector is calculated using fusion information from depth
and greyscale image on GPU. This feature vector is used to train a svm
classifier. Third step involves recognition of multiple faces based on our
modified feature vector.",11,2015-04-08 09:34:30
121,"Recent face recognition experiments on the LFW benchmark show that face
recognition is performing stunningly well, surpassing human recognition rates.
In this paper, we study face recognition at scale. Specifically, we have
collected from Flickr a \textbf{Million} faces and evaluated state of the art
face recognition algorithms on this dataset. We found that the performance of
algorithms varies--while all perform great on LFW, once evaluated at scale
recognition rates drop drastically for most algorithms. Interestingly, deep
learning based approach by \cite{schroff2015facenet} performs much better, but
still gets less robust at scale. We consider both verification and
identification problems, and evaluate how pose affects recognition at scale.
Moreover, we ran an extensive human study on Mechanical Turk to evaluate human
recognition at scale, and report results. All the photos are creative commons
photos and is released at \small{\url{http://megaface.cs.washington.edu/}} for
research and further experiments.",11,2015-05-08 17:39:23
122,"Face analysis techniques have become a crucial component of human-machine
interaction in the fields of assistive and humanoid robotics. However, the
variations in head-pose that arise naturally in these environments are still a
great challenge. In this paper, we present a real-time capable 3D face
modelling framework for 2D in-the-wild images that is applicable for robotics.
The fitting of the 3D Morphable Model is based exclusively on automatically
detected landmarks. After fitting, the face can be corrected in pose and
transformed back to a frontal 2D representation that is more suitable for face
recognition. We conduct face recognition experiments with non-frontal images
from the MUCT database and uncontrolled, in the wild images from the PaSC
database, the most challenging face recognition database to date, showing an
improved performance. Finally, we present our SCITOS G5 robot system, which
incorporates our framework as a means of image pre-processing for face
analysis.",8,2016-06-01 21:28:27
123,"Face recognition approaches that are based on deep convolutional neural
networks (CNN) have been dominating the field. The performance improvements
they have provided in the so called in-the-wild datasets are significant,
however, their performance under image quality degradations have not been
assessed, yet. This is particularly important, since in real-world face
recognition applications, images may contain various kinds of degradations due
to motion blur, noise, compression artifacts, color distortions, and occlusion.
In this work, we have addressed this problem and analyzed the influence of
these image degradations on the performance of deep CNN-based face recognition
approaches using the standard LFW closed-set identification protocol. We have
evaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and
GoogLeNet. Results have indicated that blur, noise, and occlusion cause a
significant decrease in performance, while deep CNN models are found to be
robust to distortions, such as color distortions and change in color balance.",11,2016-08-18 11:48:26
124,"Face recognition has made great progress with the development of deep
learning. However, video face recognition (VFR) is still an ongoing task due to
various illumination, low-resolution, pose variations and motion blur. Most
existing CNN-based VFR methods only obtain a feature vector from a single image
and simply aggregate the features in a video, which less consider the
correlations of face images in one video. In this paper, we propose a novel
Attention-Set based Metric Learning (ASML) method to measure the statistical
characteristics of image sets. It is a promising and generalized extension of
Maximum Mean Discrepancy with memory attention weighting. First, we define an
effective distance metric on image sets, which explicitly minimizes the
intra-set distance and maximizes the inter-set distance simultaneously. Second,
inspired by Neural Turing Machine, a Memory Attention Weighting is proposed to
adapt set-aware global contents. Then ASML is naturally integrated into CNNs,
resulting in an end-to-end learning scheme. Our method achieves
state-of-the-art performance for the task of video face recognition on the
three widely used benchmarks including YouTubeFace, YouTube Celebrities and
Celebrity-1000.",11,2017-04-12 15:54:24
125,"Face recognition has the perception of a solved problem, however when tested
at the million-scale exhibits dramatic variation in accuracies across the
different algorithms. Are the algorithms very different? Is access to good/big
training data their secret weapon? Where should face recognition improve? To
address those questions, we created a benchmark, MF2, that requires all
algorithms to be trained on same data, and tested at the million scale. MF2 is
a public large-scale set with 672K identities and 4.7M photos created with the
goal to level playing field for large scale face recognition. We contrast our
results with findings from the other two large-scale benchmarks MegaFace
Challenge and MS-Celebs-1M where groups were allowed to train on any
private/public/big/small set. Some key discoveries: 1) algorithms, trained on
MF2, were able to achieve state of the art and comparable results to algorithms
trained on massive private sets, 2) some outperformed themselves once trained
on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace,
identifying the need for larger age variations possibly within identities or
adjustment of algorithms in future testings.",8,2017-05-01 01:04:53
126,"Recognition of low resolution face images is a challenging problem in many
practical face recognition systems. Methods have been proposed in the face
recognition literature for the problem which assume that the probe is low
resolution, but a high resolution gallery is available for recognition. These
attempts have been aimed at modifying the probe image such that the resultant
image provides better discrimination. We formulate the problem differently by
leveraging the information available in the high resolution gallery image and
propose a dictionary learning approach for classifying the low-resolution probe
image. An important feature of our algorithm is that it can handle resolution
change along with illumination variations. Furthermore, we also kernelize the
algorithm to handle non-linearity in data and present a joint dictionary
learning technique for robust recognition at low resolutions. The effectiveness
of the proposed method is demonstrated using standard datasets and a
challenging outdoor face dataset. It is shown that our method is efficient and
can perform significantly better than many competitive low resolution face
recognition algorithms.",8,2017-07-10 08:12:07
127,"Representation based classification (RC) methods such as sparse RC (SRC) have
shown great potential in face recognition in recent years. Most previous RC
methods are based on the conventional regression models, such as lasso
regression, ridge regression or group lasso regression. These regression models
essentially impose a predefined assumption on the distribution of the noise
variable in the query sample, such as the Gaussian or Laplacian distribution.
However, the complicated noises in practice may violate the assumptions and
impede the performance of these RC methods. In this paper, we propose a modal
regression based atomic representation and classification (MRARC) framework to
alleviate such limitation. Unlike previous RC methods, the MRARC framework does
not require the noise variable to follow any specific predefined distributions.
This gives rise to the capability of MRARC in handling various complex noises
in reality. Using MRARC as a general platform, we also develop four novel RC
methods for unimodal and multimodal face recognition, respectively. In
addition, we devise a general optimization algorithm for the unified MRARC
framework based on the alternating direction method of multipliers (ADMM) and
half-quadratic theory. The experiments on real-world data validate the efficacy
of MRARC for robust face recognition.",11,2017-11-05 02:27:43
128,"Smile detection from unconstrained facial images is a specialized and
challenging problem. As one of the most informative expressions, smiles convey
basic underlying emotions, such as happiness and satisfaction, which lead to
multiple applications, e.g., human behavior analysis and interactive
controlling. Compared to the size of databases for face recognition, far less
labeled data is available for training smile detection systems. To leverage the
large amount of labeled data from face recognition datasets and to alleviate
overfitting on smile detection, an efficient transfer learning-based smile
detection approach is proposed in this paper. Unlike previous works which use
either hand-engineered features or train deep convolutional networks from
scratch, a well-trained deep face recognition model is explored and fine-tuned
for smile detection in the wild. Three different models are built as a result
of fine-tuning the face recognition model with different inputs, including
aligned, unaligned and grayscale images generated from the GENKI-4K dataset.
Experiments show that the proposed approach achieves improved state-of-the-art
performance. Robustness of the model to noise and blur artifacts is also
evaluated in this paper.",8,2018-01-17 19:58:26
129,"Face recognition achieves exceptional success thanks to the emergence of deep
learning. However, many contemporary face recognition models still perform
relatively poor in processing profile faces compared to frontal faces. A key
reason is that the number of frontal and profile training faces are highly
imbalanced - there are extensively more frontal training samples compared to
profile ones. In addition, it is intrinsically hard to learn a deep
representation that is geometrically invariant to large pose variations. In
this study, we hypothesize that there is an inherent mapping between frontal
and profile faces, and consequently, their discrepancy in the deep
representation space can be bridged by an equivariant mapping. To exploit this
mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,
which is capable of adaptively adding residuals to the input deep
representation to transform a profile face representation to a canonical pose
that simplifies recognition. The DREAM block consistently enhances the
performance of profile face recognition for many strong deep networks,
including ResNet models, without deliberately augmenting training data of
profile faces. The block is easy to use, light-weight, and can be implemented
with a negligible computational overhead.",11,2018-03-02 13:25:34
130,"We present a new method of primate face recognition, and evaluate this method
on several endangered primates, including golden monkeys, lemurs, and
chimpanzees. The three datasets contain a total of 11,637 images of 280
individual primates from 14 species. Primate face recognition performance is
evaluated using two existing state-of-the-art open-source systems, (i) FaceNet
and (ii) SphereFace, (iii) a lemur face recognition system from literature, and
(iv) our new convolutional neural network (CNN) architecture called PrimNet.
Three recognition scenarios are considered: verification (1:1 comparison), and
both open-set and closed-set identification (1:N search). We demonstrate that
PrimNet outperforms all of the other systems in all three scenarios for all
primate species tested. Finally, we implement an Android application of this
recognition system to assist primate researchers and conservationists in the
wild for individual recognition of primates.",8,2018-04-24 00:19:32
131,"Depth information has been proven useful for face recognition. However,
existing depth-image-based face recognition methods still suffer from noisy
depth values and varying poses and expressions. In this paper, we propose a
novel method for normalizing facial depth images to frontal pose and neutral
expression and extracting robust features from the normalized depth images. The
method is implemented via two deep convolutional neural networks (DCNN),
normalization network ($Net_{N}$) and feature extraction network ($Net_{F}$).
Given a facial depth image, $Net_{N}$ first converts it to an HHA image, from
which the 3D face is reconstructed via a DCNN. $Net_{N}$ then generates a
pose-and-expression normalized (PEN) depth image from the reconstructed 3D
face. The PEN depth image is finally passed to $Net_{F}$, which extracts a
robust feature representation via another DCNN for face recognition. Our
preliminary evaluation results demonstrate the superiority of the proposed
method in recognizing faces of arbitrary poses and expressions with depth
images.",11,2018-05-01 16:02:50
132,"Deep CNNs have been pushing the frontier of visual recognition over past
years. Besides recognition accuracy, strong demands in understanding deep CNNs
in the research community motivate developments of tools to dissect pre-trained
models to visualize how they make predictions. Recent works further push the
interpretability in the network learning stage to learn more meaningful
representations. In this work, focusing on a specific area of visual
recognition, we report our efforts towards interpretable face recognition. We
propose a spatial activation diversity loss to learn more structured face
representations. By leveraging the structure, we further design a feature
activation diversity loss to push the interpretable representations to be
discriminative and robust to occlusions. We demonstrate on three face
recognition benchmarks that our proposed method is able to improve face
recognition accuracy with easily interpretable face representations.",11,2018-05-02 03:46:47
133,"The growing scale of face recognition datasets empowers us to train strong
convolutional networks for face recognition. While a variety of architectures
and loss functions have been devised, we still have a limited understanding of
the source and consequence of label noise inherent in existing datasets. We
make the following contributions: 1) We contribute cleaned subsets of popular
face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new
large-scale noise-controlled IMDb-Face dataset. 2) With the original datasets
and cleaned subsets, we profile and analyze label noise properties of MegaFace
and MS-Celeb-1M. We show that a few orders more samples are needed to achieve
the same accuracy yielded by a clean subset. 3) We study the association
between different types of noise, i.e., label flips and outliers, with the
accuracy of face recognition models. 4) We investigate ways to improve data
cleanliness, including a comprehensive user study on the influence of data
labeling strategies to annotation accuracy. The IMDb-Face dataset has been
released on https://github.com/fwang91/IMDb-Face.",8,2018-07-31 03:43:11
134,"In this paper, we propose a novel Global Norm-Aware Pooling (GNAP) block,
which reweights local features in a convolutional neural network (CNN)
adaptively according to their L2 norms and outputs a global feature vector with
a global average pooling layer. Our GNAP block is designed to give dynamic
weights to local features in different spatial positions without losing spatial
symmetry. We use a GNAP block in a face feature embedding CNN to produce
discriminative face feature vectors for pose-robust face recognition. The GNAP
block is of very cheap computational cost, but it is very powerful for
frontal-profile face recognition. Under the CFP frontal-profile protocol, the
GNAP block can not only reduce EER dramatically but also boost TPR@FPR=0.1%
(TPR i.e. True Positive Rate, FPR i.e. False Positive Rate) substantially. Our
experiments show that the GNAP block greatly promotes pose-robust face
recognition over the base model especially at low false positive rate.",1,2018-08-01 17:32:31
135,"Face recognition in images is an active area of interest among the computer
vision researchers. However, recognizing human face in an unconstrained
environment, is a relatively less-explored area of research. Multiple face
recognition in unconstrained environment is a challenging task, due to the
variation of view-point, scale, pose, illumination and expression of the face
images. Partial occlusion of faces makes the recognition task even more
challenging. The contribution of this paper is two-folds: introducing a
challenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition
in unconstrained environment and evaluating the performance of state-of-the-art
hand-designed and deep learning based face descriptors on the dataset. The
proposed IIITS MFace dataset contains faces with challenges like pose
variation, occlusion, mask, spectacle, expressions, change of illumination,
etc. We experiment with several state-of-the-art face descriptors, including
recent deep learning based face descriptors like VGGFace, and compare with the
existing benchmark face datasets. Results of the experiments clearly show that
the difficulty level of the proposed dataset is much higher compared to the
benchmark datasets.",11,2018-09-30 07:04:59
136,"As facial appearance is subject to significant intra-class variations caused
by the aging process over time, age-invariant face recognition (AIFR) remains a
major challenge in face recognition community. To reduce the intra-class
discrepancy caused by the aging, in this paper we propose a novel approach
(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep
face features. Specifically, we decompose deep face features into two
orthogonal components to represent age-related and identity-related features.
As a result, identity-related features that are robust to aging are then used
for AIFR. Besides, for complementing the existing cross-age datasets and
advancing the research in this field, we construct a brand-new large-scale
Cross-Age Face dataset (CAF). Extensive experiments conducted on the three
public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have
shown the effectiveness of the proposed approach and the value of the
constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most
popular general face recognition (GFR) dataset LFW additionally demonstrates
the comparable generalization performance on GFR.",8,2018-10-17 15:08:11
137,"Face recognition is one of the most widely publicized feature in the devices
today and hence represents an important problem that should be studied with the
utmost priority. As per the recent trends, the Convolutional Neural Network
(CNN) based approaches are highly successful in many tasks of Computer Vision
including face recognition. The loss function is used on the top of CNN to
judge the goodness of any network. In this paper, we present a performance
comparison of different loss functions such as Cross-Entropy, Angular Softmax,
Additive-Margin Softmax, ArcFace and Marginal Loss for face recognition. The
experiments are conducted with two CNN architectures namely, ResNet and
MobileNet. Two widely used face datasets namely, CASIA-Webface and MS-Celeb-1M
are used for the training and benchmark Labeled Faces in the Wild (LFW) face
dataset is used for the testing.",8,2019-01-01 03:00:10
138,"Biometrics-related research has been accelerated significantly by deep
learning technology. However, there are limited open-source resources to help
researchers evaluate their deep learning-based biometrics algorithms
efficiently, especially for the face recognition tasks. In this work, we design
and implement a light-weight, maintainable, scalable, generalizable, and
extendable face recognition evaluation toolbox named FaRE that supports both
online and offline evaluation to provide feedback to algorithm development and
accelerate biometrics-related research. FaRE consists of a set of evaluation
metric functions and provides various APIs for commonly-used face recognition
datasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be
easily extended to include other customized datasets. The package and the
pre-trained baseline models will be released for public academic research use
after obtaining university approval.",11,2019-01-27 21:53:26
139,"Recurrent networks have been successful in analyzing temporal data and have
been widely used for video analysis. However, for video face recognition, where
the base CNNs trained on large-scale data already provide discriminative
features, using Long Short-Term Memory (LSTM), a popular recurrent network, for
feature learning could lead to overfitting and degrade the performance instead.
We propose a Recurrent Embedding Aggregation Network (REAN) for set to set face
recognition. Compared with LSTM, REAN is robust against overfitting because it
only learns how to aggregate the pre-trained embeddings rather than learning
representations from scratch. Compared with quality-aware aggregation methods,
REAN can take advantage of the context information to circumvent the noise
introduced by redundant video frames. Empirical results on three public domain
video face recognition datasets, IJB-S, YTF, and PaSC show that the proposed
REAN significantly outperforms naive CNN-LSTM structure and quality-aware
aggregation methods.",11,2019-04-26 19:22:41
140,"With the development of convolutional neural network, significant progress
has been made in computer vision tasks. However, the commonly used loss
function softmax loss and highly efficient network architecture for common
visual tasks are not as effective for face recognition. In this paper, we
propose a novel loss function named Li-ArcFace based on ArcFace. Li-ArcFace
takes the value of the angle through linear function as the target logit rather
than through cosine function, which has better convergence and performance on
low dimensional embedding feature learning for face recognition. In terms of
network architecture, we improved the the perfomance of MobileFaceNet by
increasing the network depth, width and adding attention module. Besides, we
found some useful training tricks for face recognition. With all the above
results, we won the second place in the deepglint-light challenge of LFR2019.",8,2019-07-29 08:04:37
141,"In this paper, we propose a novel attribute-guided cross-resolution
(low-resolution to high-resolution) face recognition framework that leverages a
coupled generative adversarial network (GAN) structure with adversarial
training to find the hidden relationship between the low-resolution and
high-resolution images in a latent common embedding subspace. The coupled GAN
framework consists of two sub-networks, one dedicated to the low-resolution
domain and the other dedicated to the high-resolution domain. Each sub-network
aims to find a projection that maximizes the pair-wise correlation between the
two feature domains in a common embedding subspace. In addition to projecting
the images into a common subspace, the coupled network also predicts facial
attributes to improve the cross-resolution face recognition. Specifically, our
proposed coupled framework exploits facial attributes to further maximize the
pair-wise correlation by implicitly matching facial attributes of the low and
high-resolution images during the training, which leads to a more
discriminative embedding subspace resulting in performance enhancement for
cross-resolution face recognition. The efficacy of our approach compared with
the state-of-the-art is demonstrated using the LFWA, Celeb-A, SCFace and UCCS
datasets.",8,2019-08-05 18:10:55
142,"Face recognition is a biometric which is attracting significant research,
commercial and government interest, as it provides a discreet, non-intrusive
way of detecting, and recognizing individuals, without need for the subject's
knowledge or consent. This is due to reduced cost, and evolution in hardware
and algorithms which have improved their ability to handle unconstrained
conditions. Evidently affordable and efficient applications are required.
However, there is much debate over which methods are most appropriate,
particularly in the context of the growing importance of deep neural
network-based face recognition systems. This systematic review attempts to
provide clarity on both issues by organizing the plethora of research and data
in this field to clarify current research trends, state-of-the-art methods, and
provides an outline of their benefits and shortcomings. Overall, this research
covered 1,330 relevant studies, showing an increase of over 200% in research
interest in the field of face recognition over the past 6 years. Our results
also demonstrated that deep learning methods are the prime focus of modern
research due to improvements in hardware databases and increasing understanding
of neural networks. In contrast, traditional methods have lost favor amongst
researchers due to their inherent limitations in accuracy, and lack of
efficiency when handling large amounts of data.",8,2019-07-12 23:54:50
143,"Rather than the visual images, the face recognition of the caricatures is far
from the performance of the visual images. The challenge is the extreme
non-rigid distortions of the caricatures introduced by exaggerating the facial
features to strengthen the characters. In this paper, we propose dynamic
multi-task learning based on deep CNNs for cross-modal caricature-visual face
recognition. Instead of the conventional multi-task learning with fixed weights
of the tasks, the proposed dynamic multi-task learning dynamically updates the
weights of tasks according to the importance of the tasks, which enables the
training of the networks focus on the hard task instead of being stuck in the
overtraining of the easy task. The experimental results demonstrate the
effectiveness of the dynamic multi-task learning for caricature-visual face
recognition. The performance evaluated on the datasets CaVI and WebCaricature
show the superiority over the state-of-art methods. The implementation code is
available here.",8,2019-11-08 16:04:08
144,"We address the problem of bias in automated face recognition and demographic
attribute estimation algorithms, where errors are lower on certain cohorts
belonging to specific demographic groups. We present a novel de-biasing
adversarial network (DebFace) that learns to extract disentangled feature
representations for both unbiased face recognition and demographics estimation.
The proposed network consists of one identity classifier and three demographic
classifiers (for gender, age, and race) that are trained to distinguish
identity and demographic attributes, respectively. Adversarial learning is
adopted to minimize correlation among feature factors so as to abate bias
influence from other factors. We also design a new scheme to combine
demographics with identity features to strengthen robustness of face
representation in different demographic groups. The experimental results show
that our approach is able to reduce bias in face recognition as well as
demographics estimation while achieving state-of-the-art performance.",8,2019-11-19 03:44:34
145,"In this paper, we propose a framework for disentangling the appearance and
geometry representations in the face recognition task. To provide supervision
for this aim, we generate geometrically identical faces by incorporating
spatial transformations. We demonstrate that the proposed approach enhances the
performance of deep face recognition models by assisting the training process
in two ways. First, it enforces the early and intermediate convolutional layers
to learn more representative features that satisfy the properties of
disentangled embeddings. Second, it augments the training set by altering faces
geometrically. Through extensive experiments, we demonstrate that integrating
the proposed approach into state-of-the-art face recognition methods
effectively improves their performance on challenging datasets, such as LFW,
YTF, and MegaFace. Both theoretical and practical aspects of the method are
analyzed rigorously by concerning ablation studies and knowledge transfer
tasks. Furthermore, we show that the knowledge leaned by the proposed method
can favor other face-related tasks, such as attribute prediction.",8,2020-01-13 23:19:58
146,"Face recognition performance has seen a tremendous gain in recent years,
mostly due to the availability of large-scale face images dataset that can be
exploited by deep neural networks to learn powerful face representations.
However, recent research has shown differences in face recognition performance
across different ethnic groups mostly due to the racial imbalance in the
training datasets where Caucasian identities largely dominate other
ethnicities. This is actually symptomatic of the under-representation of
non-Caucasian ethnic groups in the celebdom from which face datasets are
usually gathered, rendering the acquisition of labeled data of the
under-represented groups challenging. In this paper, we propose an Asymmetric
Rejection Loss, which aims at making full use of unlabeled images of those
under-represented groups, to reduce the racial bias of face recognition models.
We view each unlabeled image as a unique class, however as we cannot guarantee
that two unlabeled samples are from a distinct class we exploit both labeled
and unlabeled data in an asymmetric manner in our loss formalism. Extensive
experiments show our method's strength in mitigating racial bias, outperforming
state-of-the-art semi-supervision methods. Performance on the under-represented
ethnicity groups increases while that on the well-represented group is nearly
unchanged.",8,2020-02-09 04:01:03
147,"In recent years, large ""in the wild"" face datasets have been released in an
attempt to facilitate progress in tasks such as face detection, face
recognition, and other tasks. Most of these datasets are acquired from webpages
with automatic procedures. As a consequence, noisy data are often found.
Furthermore, in these large face datasets, the annotation of identities is
important as they are used for training face recognition algorithms. But due to
the automatic way of gathering these datasets and due to their large size, many
identities folder contain mislabeled samples which deteriorates the quality of
the datasets. In this work, it is presented a semi-automatic method for
cleaning the noisy large face datasets with the use of face recognition. This
methodology is applied to clean the CelebA dataset and show its effectiveness.
Furthermore, the list with the mislabelled samples in the CelebA dataset is
made available.",8,2020-03-24 13:01:13
148,"Active illumination is a prominent complement to enhance 2D face recognition
and make it more robust, e.g., to spoofing attacks and low-light conditions. In
the present work we show that it is possible to adopt active illumination to
enhance state-of-the-art 2D face recognition approaches with 3D features, while
bypassing the complicated task of 3D reconstruction. The key idea is to project
over the test face a high spatial frequency pattern, which allows us to
simultaneously recover real 3D information plus a standard 2D facial image.
Therefore, state-of-the-art 2D face recognition solution can be transparently
applied, while from the high frequency component of the input image,
complementary 3D facial features are extracted. Experimental results on ND-2006
dataset show that the proposed ideas can significantly boost face recognition
performance and dramatically improve the robustness to spoofing attacks.",8,2020-04-03 20:17:14
149,"State-of-the-art deep networks implicitly encode gender information while
being trained for face recognition. Gender is often viewed as an important
attribute with respect to identifying faces. However, the implicit encoding of
gender information in face descriptors has two major issues: (a.) It makes the
descriptors susceptible to privacy leakage, i.e. a malicious agent can be
trained to predict the face gender from such descriptors. (b.) It appears to
contribute to gender bias in face recognition, i.e. we find a significant
difference in the recognition accuracy of DCNNs on male and female faces.
Therefore, we present a novel `Adversarial Gender De-biasing algorithm
(AGENDA)' to reduce the gender information present in face descriptors obtained
from previously trained face recognition networks. We show that AGENDA
significantly reduces gender predictability of face descriptors. Consequently,
we are also able to reduce gender bias in face verification while maintaining
reasonable recognition performance.",8,2020-06-14 08:54:03
150,"Side-view face recognition is a challenging problem with many applications. Especially in real-life scenarios where the environment is uncontrolled, coping with pose variations up to side-view positions is an important task for face recognition. In this paper we discuss the use of side view face recognition techniques to be used in house safety applications. Our aim is to recognize people as they pass through a door, and estimate their location in the house. Here, we compare available databases appropriate for this task, and review current methods for profile face recognition",11,2010-01-01 00:00:00
151,"Deep CNNs have been pushing the frontier of visual recognition over past
years. Besides recognition accuracy, strong demands in understanding deep CNNs
in the research community motivate developments of tools to dissect pre-trained
models to visualize how they make predictions. Recent works further push the
interpretability in the network learning stage to learn more meaningful
representations. In this work, focusing on a specific area of visual
recognition, we report our efforts towards interpretable face recognition. We
propose a spatial activation diversity loss to learn more structured face
representations. By leveraging the structure, we further design a feature
activation diversity loss to push the interpretable representations to be
discriminative and robust to occlusions. We demonstrate on three face
recognition benchmarks that our proposed method is able to improve face
recognition accuracy with easily interpretable face representations",11,2019-08-17 01:00:00
152,"Human recognition from video traces is an important task in forensic investigations and evidence evaluations. Compared with other biometric traits, face is one of the most popularly used modalities for human recognition due to the fact that its collection is non-intrusive and requires less cooperation from the subjects. Moreover, face images taken at a long distance can still provide reasonable resolution, while most biometric modalities, such as iris and fingerprint, do not have this merit. In this chapter, we discuss automatic face recognition technologies for evidential evaluations of video traces. We first introduce the general concepts in both forensic and automatic face recognition , then analyse the difficulties in face recognition from videos . We summarise and categorise the approaches for handling different uncontrollable factors in difficult recognition conditions. Finally we discuss some challenges and trends in face recognition research in both forensics and biometrics . Given its merits tested in many deployed systems and great potential in other emerging applications, considerable research and development efforts are expected to be devoted in face recognition in the near future",4,2017-01-01 00:00:00
153,"Recent research indicates face recognition ability varies within the normal population.  To date, two factors have been identified that influence this cognitive process: the age and gender of the perceiver.  In this paper, we examine the influence of socio-emotional functioning on face recognition ability.  We invited participants with high and low levels of empathy (as indicated by the Empathy Quotient) to take part in a face recognition test.  Participants were asked to study a set of faces, and at test viewed the studied faces intermixed with novel faces.  As predicted, high empaths achieved higher scores in the face recognition test compared to low empaths.  This pattern of findings provides further evidence that face recognition ability varies within the normal population, and suggests socio-emotional functioning may be an additional factor that influences face recognition ability",4,2010-01-01 00:00:00
154,"In a world where security issues have been gaining growing importance, face
recognition systems have attracted increasing attention in multiple application
areas, ranging from forensics and surveillance to commerce and entertainment.
To help understanding the landscape and abstraction levels relevant for face
recognition systems, face recognition taxonomies allow a deeper dissection and
comparison of the existing solutions. This paper proposes a new, more
encompassing and richer multi-level face recognition taxonomy, facilitating the
organization and categorization of available and emerging face recognition
solutions; this taxonomy may also guide researchers in the development of more
efficient face recognition solutions. The proposed multi-level taxonomy
considers levels related to the face structure, feature support and feature
extraction approach. Following the proposed taxonomy, a comprehensive survey of
representative face recognition solutions is presented. The paper concludes
with a discussion on current algorithmic and application related challenges
which may define future research directions for face recognition",8,2019-01-03 00:00:00
155,"Principle Component Analysis PCA is a classical feature extraction and data
representation technique widely used in pattern recognition. It is one of the
most successful techniques in face recognition. But it has drawback of high
computational especially for big size database. This paper conducts a study to
optimize the time complexity of PCA (eigenfaces) that does not affects the
recognition performance. The authors minimize the participated eigenvectors
which consequently decreases the computational time. A comparison is done to
compare the differences between the recognition time in the original algorithm
and in the enhanced algorithm. The performance of the original and the enhanced
proposed algorithm is tested on face94 face database. Experimental results show
that the recognition time is reduced by 35% by applying our proposed enhanced
algorithm. DET Curves are used to illustrate the experimental results",8,2012-06-07 00:00:00
156,"Beside a few papers which focus on the forensic aspects of automatic face recognition, there is not much published about it in contrast to the literature on developing new techniques and methodologies for biometric face recognition. In this report, we review forensic facial identification which is the forensic experts‟ way of manual facial comparison. Then we review famous works in the domain of forensic face recognition. Some of these papers describe general trends in forensics [1], guidelines for manual forensic facial comparison and training of face examiners who will be required to verify the outcome of automatic forensic face recognition system [2]. Some proposes theoretical framework for application of face recognition technology in forensics [3] and automatic forensic facial comparison [4, 5]. Bayesian framework is discussed in detail and it is elaborated how it can be adapted to forensic face recognition. Several issues related with court admissibility and reliability of system are also discussed. \ud
Until now, there is no operational system available which automatically compare image of a suspect with mugshot database and provide result usable in court. The fact that biometric face recognition can in most cases be used for forensic purpose is true but the issues related to integration of technology with legal system of court still remain to be solved. There is a great need for research which is multi-disciplinary in nature and which will integrate the face recognition technology with existing legal systems. In this report we present a review of the existing literature in this domain and discuss various aspects and requirements for forensic face recognition systems particularly focusing on Bayesian framework",4,2010-01-01 00:00:00
157,"Much research has been conducted on both face identification and face
verification, with greater focus on the latter. Research on face identification
has mostly focused on using closed-set protocols, which assume that all probe
images used in evaluation contain identities of subjects that are enrolled in
the gallery. Real systems, however, where only a fraction of probe sample
identities are enrolled in the gallery, cannot make this closed-set assumption.
Instead, they must assume an open set of probe samples and be able to
reject/ignore those that correspond to unknown identities. In this paper, we
address the widespread misconception that thresholding verification-like scores
is a good way to solve the open-set face identification problem, by formulating
an open-set face identification protocol and evaluating different strategies
for assessing similarity. Our open-set identification protocol is based on the
canonical labeled faces in the wild (LFW) dataset. Additionally to the known
identities, we introduce the concepts of known unknowns (known, but
uninteresting persons) and unknown unknowns (people never seen before) to the
biometric community. We compare three algorithms for assessing similarity in a
deep feature space under an open-set protocol: thresholded verification-like
scores, linear discriminant analysis (LDA) scores, and an extreme value machine
(EVM) probabilities. Our findings suggest that thresholding EVM probabilities,
which are open-set by design, outperforms thresholding verification-like
scores",8,2017-05-18 00:00:00
158,"To address the sequential changes of images including poses, in this paper we
propose a recurrent regression neural network(RRNN) framework to unify two
classic tasks of cross-pose face recognition on still images and video-based
face recognition. To imitate the changes of images, we explicitly construct the
potential dependencies of sequential images so as to regularize the final
learning model. By performing progressive transforms for sequentially adjacent
images, RRNN can adaptively memorize and forget the information that benefits
for the final classification. For face recognition of still images, given any
one image with any one pose, we recurrently predict the images with its
sequential poses to expect to capture some useful information of others poses.
For video-based face recognition, the recurrent regression takes one entire
sequence rather than one image as its input. We verify RRNN in static face
dataset MultiPIE and face video dataset YouTube Celebrities(YTC). The
comprehensive experimental results demonstrate the effectiveness of the
proposed RRNN method",11,2016-07-24 01:00:00
159,"The gap between sensing patterns of different face modalities remains a
challenging problem in heterogeneous face recognition (HFR). This paper
proposes an adversarial discriminative feature learning framework to close the
sensing gap via adversarial learning on both raw-pixel space and compact
feature space. This framework integrates cross-spectral face hallucination and
discriminative feature learning into an end-to-end adversarial network. In the
pixel space, we make use of generative adversarial networks to perform
cross-spectral face hallucination. An elaborate two-path model is introduced to
alleviate the lack of paired images, which gives consideration to both global
structures and local textures. In the feature space, an adversarial loss and a
high-order variance discrepancy loss are employed to measure the global and
local discrepancy between two heterogeneous distributions respectively. These
two losses enhance domain-invariant feature learning and modality independent
noise removing. Experimental results on three NIR-VIS databases show that our
proposed approach outperforms state-of-the-art HFR methods, without requiring
of complex network or large-scale training dataset",8,2017-09-11 01:00:00
160,"The objective of this work is set-based face recognition, i.e. to decide if
two sets of images of a face are of the same person or not. Conventionally, the
set-wise feature descriptor is computed as an average of the descriptors from
individual face images within the set. In this paper, we design a neural
network architecture that learns to aggregate based on both ""visual"" quality
(resolution, illumination), and ""content"" quality (relative importance for
discriminative classification). To this end, we propose a Multicolumn Network
(MN) that takes a set of images (the number in the set can vary) as input, and
learns to compute a fix-sized feature descriptor for the entire set. To
encourage high-quality representations, each individual input image is first
weighted by its ""visual"" quality, determined by a self-quality assessment
module, and followed by a dynamic recalibration based on ""content"" qualities
relative to the other images within the set. Both of these qualities are learnt
implicitly during training for set-wise classification. Comparing with the
previous state-of-the-art architectures trained with the same dataset
(VGGFace2), our Multicolumn Networks show an improvement of between 2-6% on the
IARPA IJB face recognition benchmarks, and exceed the state of the art for all
methods on these benchmarks",8,2018-01-01 00:00:00
161,"In this work, we define a new pilgrims face recognition dataset, called HUFRD
dataset. The new developed dataset presents various pilgrims' images taken from
outside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrah
seasons. Such dataset will be used to test our developed facial recognition and
detection algorithms, as well as assess in the missing and found recognition
system \cite{crowdsensing}",11,2012-12-29 00:00:00
162,"Deep networks trained on millions of facial images are believed to be closely
approaching human-level performance in face recognition. However, open world
face recognition still remains a challenge. Although, 3D face recognition has
an inherent edge over its 2D counterpart, it has not benefited from the recent
developments in deep learning due to the unavailability of large training as
well as large test datasets. Recognition accuracies have already saturated on
existing 3D face datasets due to their small gallery sizes. Unlike 2D
photographs, 3D facial scans cannot be sourced from the web causing a
bottleneck in the development of deep 3D face recognition networks and
datasets. In this backdrop, we propose a method for generating a large corpus
of labeled 3D face identities and their multiple instances for training and a
protocol for merging the most challenging existing 3D datasets for testing. We
also propose the first deep CNN model designed specifically for 3D face
recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our
test dataset comprises 1,853 identities with a single 3D scan in the gallery
and another 31K scans as probes, which is several orders of magnitude larger
than existing ones. Without fine tuning on this dataset, our network already
outperforms state of the art face recognition by over 10%. We fine tune our
network on the gallery set to perform end-to-end large scale 3D face
recognition which further improves accuracy. Finally, we show the efficacy of
our method for the open world face recognition problem",8,2018-07-05 01:00:00
163,"The main finding of this work is that the standard image classification
pipeline, which consists of dictionary learning, feature encoding, spatial
pyramid pooling and linear classification, outperforms all state-of-the-art
face recognition methods on the tested benchmark datasets (we have tested on
AR, Extended Yale B, the challenging FERET, and LFW-a datasets). This
surprising and prominent result suggests that those advances in generic image
classification can be directly applied to improve face recognition systems. In
other words, face recognition may not need to be viewed as a separate object
classification problem.
  While recently a large body of residual based face recognition methods focus
on developing complex dictionary learning algorithms, in this work we show that
a dictionary of randomly extracted patches (even from non-face images) can
achieve very promising results using the image classification pipeline. That
means, the choice of dictionary learning methods may not be important. Instead,
we find that learning multiple dictionaries using different low-level image
features often improve the final classification accuracy. Our proposed face
recognition approach offers the best reported results on the widely-used face
recognition benchmark datasets. In particular, on the challenging FERET and
LFW-a datasets, we improve the best reported accuracies in the literature by
about 20% and 30% respectively",8,2013-09-29 01:00:00
164,"Developing a reliable and practical face recognition system is a
long-standing goal in computer vision research. Existing literature suggests
that pixel-wise face alignment is the key to achieve high-accuracy face
recognition. By assuming a human face as piece-wise planar surfaces, where each
surface corresponds to a facial part, we develop in this paper a Constrained
Part-based Alignment (CPA) algorithm for face recognition across pose and/or
expression. Our proposed algorithm is based on a trainable CPA model, which
learns appearance evidence of individual parts and a tree-structured shape
configuration among different parts. Given a probe face, CPA simultaneously
aligns all its parts by fitting them to the appearance evidence with
consideration of the constraint from the tree-structured shape configuration.
This objective is formulated as a norm minimization problem regularized by
graph likelihoods. CPA can be easily integrated with many existing classifiers
to perform part-based face recognition. Extensive experiments on benchmark face
datasets show that CPA outperforms or is on par with existing methods for
robust face recognition across pose, expression, and/or illumination changes",11,2015-01-20 00:00:00
165,"Heterogeneous face recognition (HFR) refers to matching face images acquired
from different sources (i.e., different sensors or different wavelengths) for
identification. HFR plays an important role in both biometrics research and
industry. In spite of promising progresses achieved in recent years, HFR is
still a challenging problem due to the difficulty to represent two
heterogeneous images in a homogeneous manner. Existing HFR methods either
represent an image ignoring the spatial information, or rely on a
transformation procedure which complicates the recognition task. Considering
these problems, we propose a novel graphical representation based HFR method
(G-HFR) in this paper. Markov networks are employed to represent heterogeneous
image patches separately, which takes the spatial compatibility between
neighboring image patches into consideration. A coupled representation
similarity metric (CRSM) is designed to measure the similarity between obtained
graphical representations. Extensive experiments conducted on multiple HFR
scenarios (viewed sketch, forensic sketch, near infrared image, and thermal
infrared image) show that the proposed method outperforms state-of-the-art
methods",8,2016-03-14 00:00:00
166,"We present a longitudinal study of face recognition performance on Children
Longitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects,
in the age group [2, 18] years. Each subject has at least four face images
acquired over a time span of up to six years. Face comparison scores are
obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source
matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTS-A
and FaceNet matchers. To improve the performance of the open-source FaceNet
matcher for child face recognition, we were able to fine-tune it on an
independent training set of 3,294 face images of 1,119 children in the age
group [3, 18] years. Multilevel statistical models are fit to genuine
comparison scores from the CLF dataset to determine the decrease in face
recognition accuracy over time. Additionally, we analyze both the verification
and open-set identification accuracies in order to evaluate state-of-the-art
face recognition technology for tracing and identifying children lost at a
young age as victims of child trafficking or abduction",8,2017-11-10 00:00:00
167,"The objective of this paper is to learn a compact representation of image
sets for template-based face recognition. We make the following contributions:
first, we propose a network architecture which aggregates and embeds the face
descriptors produced by deep convolutional neural networks into a compact
fixed-length representation. This compact representation requires minimal
memory storage and enables efficient similarity computation. Second, we propose
a novel GhostVLAD layer that includes {\em ghost clusters}, that do not
contribute to the aggregation. We show that a quality weighting on the input
faces emerges automatically such that informative images contribute more than
those with low quality, and that the ghost clusters enhance the network's
ability to deal with poor quality images. Third, we explore how input feature
dimension, number of clusters and different training techniques affect the
recognition performance. Given this analysis, we train a network that far
exceeds the state-of-the-art on the IJB-B face recognition dataset. This is
currently one of the most challenging public benchmarks, and we surpass the
state-of-the-art on both the identification and verification protocols",8,2018-10-23 01:00:00
168,"Most of the face recognition works focus on specific modules or demonstrate a
research idea. This paper presents a pose-invariant 3D-aided 2D face
recognition system (UR2D) that is robust to pose variations as large as 90? by
leveraging deep learning technology. The architecture and the interface of UR2D
are described, and each module is introduced in detail. Extensive experiments
are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms
existing 2D face recognition systems such as VGG-Face, FaceNet, and a
commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset
and 3% on the IJB-A dataset on average in face identification tasks. UR2D also
achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing
the Rank-1 accuracy score from template matching. It fills a gap by providing a
3D-aided 2D face recognition system that has compatible results with 2D face
recognition systems using deep learning techniques",11,2017-09-19 01:00:00
169,"We propose a novel 3D face recognition algorithm using a deep convolutional
neural network (DCNN) and a 3D augmentation technique. The performance of 2D
face recognition algorithms has significantly increased by leveraging the
representational power of deep neural networks and the use of large-scale
labeled training data. As opposed to 2D face recognition, training
discriminative deep features for 3D face recognition is very difficult due to
the lack of large-scale 3D face datasets. In this paper, we show that transfer
learning from a CNN trained on 2D face images can effectively work for 3D face
recognition by fine-tuning the CNN with a relatively small number of 3D facial
scans. We also propose a 3D face augmentation technique which synthesizes a
number of different facial expressions from a single 3D face scan. Our proposed
method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC
datasets, without using hand-crafted features. The 3D identification using our
deep features also scales well for large databases",8,2017-03-30 00:00:00
170,"Face recognition has been studied extensively; however, real-world face recognition still remains a challenging task. The demand for unconstrained practical face recognition is rising with the explosion of online multimedia such as social networks, and video surveillance footage where face analysis is of significant importance. In this paper, we approach face recognition in the context of graph theory. We recognize an unknown face using an external reference face graph (RFG). An RFG is generated and recognition of a given face is achieved by comparing it to the faces in the constructed RFG. Centrality measures are utilized to identify distinctive faces in the reference face graph. The proposed RFG-based face recognition algorithm is robust to the changes in pose and it is also alignment free. The RFG recognition is used in conjunction with DCT locality sensitive hashing for efficient retrieval to ensure scalability. Experiments are conducted on several publicly available databases and the results show that the proposed approach outperforms the state-of-the-art methods without any preprocessing necessities such as face alignment. Due to the richness in the reference set construction, the proposed method can also handle illumination and expression variation",11,2014-12-01 00:00:00
171,"Convolutional neural networks (CNNs) have achieved a great success in face
recognition, which unfortunately comes at the cost of massive computation and
storage consumption. Many compact face recognition networks are thus proposed
to resolve this problem. Triplet loss is effective to further improve the
performance of those compact models. However, it normally employs a fixed
margin to all the samples, which neglects the informative similarity structures
between different identities. In this paper, we propose an enhanced version of
triplet loss, named triplet distillation, which exploits the capability of a
teacher model to transfer the similarity information to a small model by
adaptively varying the margin between positive and negative pairs. Experiments
on LFW, AgeDB, and CPLFW datasets show the merits of our method compared to the
original triplet loss",11,2019-05-19 01:00:00
172,"Eigenvalues of sample covariance matrices are often used in biometrics. It has been known for several decades that even though the sample covariance matrix is an unbiased estimate of the real covariance matrix [Fukunaga,1990], the eigenvalues of the sample covariance matrix are biased estimates of the real eigenvalues [Silverstein,1986]. This bias is particularly dominant when the number of samples used for estimation is in the same order as the number of dimensions, as is often the case in biometrics. We investigate the effects of this bias on error rates in verification experiments and show that eigenvalue correction can improve recognition performance",8,2008-01-01 00:00:00
173,"Good registration (alignment to a reference) is essential for accurate face recognition. We use the locations of facial features (eyes, nose, mouth, etc) as landmarks for registration. Two landmarking methods are explored and compared: (1) the Most Likely-Landmark Locator (MLLL), based on maximizing the likelihood ratio [1], and (2) Viola-Jones detection [2]. Further, a landmark-correction method based on projection into a subspace is introduced. Both landmarking methods have been trained on the landmarked images in the BioID database [3]. The MLLL has been trained for locating 17 landmarks and the Viola-Jones method for 5 landmarks. The localization error and effects on the equal-error rate (EER) have been measured. In these experiments ground- truth data has been used as a reference. The results are described as follows:\ud
1. The localization errors obtained on the FRGC database are 4.2, 8.6 and 4.6 pixels for the Viola-Jones, the MLLL, and the MLLL after landmark correction, respectively. The inter-eye distance of the reference face is 100 pixels. The MLLL with landmark correction scores best in the verification experiment.\ud
2. Using more landmarks decreases the average localization error and the EER",1,2005-01-01 00:00:00
174,"In this paper we develop a Quality Assessment approach for face recognition
based on deep learning. The method consists of a Convolutional Neural Network,
FaceQnet, that is used to predict the suitability of a specific input image for
face recognition purposes. The training of FaceQnet is done using the VGGFace2
database. We employ the BioLab-ICAO framework for labeling the VGGFace2 images
with quality information related to their ICAO compliance level. The
groundtruth quality labels are obtained using FaceNet to generate comparison
scores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making
it capable of returning a numerical quality measure for each input image.
Finally, we verify if the FaceQnet scores are suitable to predict the expected
performance when employing a specific image for face recognition with a COTS
face recognition system. Several conclusions can be drawn from this work, most
notably: 1) we managed to employ an existing ICAO compliance framework and a
pretrained CNN to automatically label data with quality information, 2) we
trained FaceQnet for quality estimation by fine-tuning a pre-trained face
recognition network (ResNet-50), and 3) we have shown that the predictions from
FaceQnet are highly correlated with the face recognition accuracy of a
state-of-the-art commercial system not used during development. FaceQnet is
publicly available in GitHub",8,2019-04-04 01:00:00
175,"Deep learning, in particular Convolutional Neural Network (CNN), has achieved
promising results in face recognition recently. However, it remains an open
question: why CNNs work well and how to design a 'good' architecture. The
existing works tend to focus on reporting CNN architectures that work well for
face recognition rather than investigate the reason. In this work, we conduct
an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a
common ground to make our work easily reproducible. Specifically, we use public
database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing
CNNs trained on private databases. We propose three CNN architectures which are
the first reported architectures trained using LFW data. This paper
quantitatively compares the architectures of CNNs and evaluate the effect of
different implementation choices. We identify several useful properties of
CNN-FRS. For instance, the dimensionality of the learned features can be
significantly reduced without adverse effect on face recognition accuracy. In
addition, traditional metric learning method exploiting CNN-learned features is
evaluated. Experiments show two crucial factors to good CNN-FRS performance are
the fusion of multiple CNNs and metric learning. To make our work reproducible,
source code and models will be made publicly available",8,2015-04-09 01:00:00
176,"Good registration (alignment to a reference) is essential for accurate face recognition. The effects of the number of landmarks on the mean localization error and the recognition performance are studied. Two landmarking methods are explored and compared for that purpose: (1) the most likely-landmark locator (MLLL), based on maximizing the likelihood ratio, and (2) Viola-Jones detection. Both use the locations of facial features (eyes, nose, mouth, etc) as landmarks. Further, a landmark-correction method (BILBO) based on projection into a subspace is introduced. The MLLL has been trained for locating 17 landmarks and the Viola-Jones method for 5. The mean localization errors and effects on the verification performance have been measured. It was found that on the eyes, the Viola-Jones detector is about 1% of the interocular distance more accurate than the MLLL-BILBO combination. On the nose and mouth, the MLLL-BILBO combination is about 0.5% of the inter-ocular distance more accurate than the Viola-Jones detector. Using more landmarks will result in lower equal-error rates, even when the landmarking is not so accurate. If the same landmarks are used, the most accurate landmarking method gives the best verification performance",1,2006-01-01 00:00:00
177,This paper investigates the performance degradation of facial recognition systems due to the influence of age. A comparative analysis of verification performance is conducted for four subspace projection techniques combined with four different distance metrics. The experimental results based on a subset of the MORPH-II database show that the choice of subspace projection technique and associated distance metric can have a significant impact on the performance of the face recognition system for particular age groups,8,2013-09-01 00:00:00
178,"Recently, the face recognizers based on linear representations have been
shown to deliver state-of-the-art performance. In real-world applications,
however, face images usually suffer from expressions, disguises and random
occlusions. The problematic facial parts undermine the validity of the
linear-subspace assumption and thus the recognition performance deteriorates
significantly. In this work, we address the problem in a
learning-inference-mixed fashion. By observing that the linear-subspace
assumption is more reliable on certain face patches rather than on the holistic
face, some Bayesian Patch Representations (BPRs) are randomly generated and
interpreted according to the Bayes' theory. We then train an ensemble model
over the patch-representations by minimizing the empirical risk w.r.t the
""leave-one-out margins"". The obtained model is termed Optimal Representation
Ensemble (ORE), since it guarantees the optimality from the perspective of
Empirical Risk Minimization. To handle the unknown patterns in test faces, a
robust version of BPR is proposed by taking the non-face category into
consideration. Equipped with the Robust-BPRs, the inference ability of ORE is
increased dramatically and several record-breaking accuracies (99.9% on Yale-B
and 99.5% on AR) and desirable efficiencies (below 20 ms per face in Matlab)
are achieved. It also overwhelms other modular heuristics on the faces with
random occlusions, extreme expressions and disguises. Furthermore, to
accommodate immense BPRs sets, a boosting-like algorithm is also derived. The
boosted model, a.k.a Boosted-ORE, obtains similar performance to its prototype.
Besides the empirical superiorities, two desirable features of the proposed
methods, namely, the training-determined model-selection and the
data-weight-free boosting procedure, are also theoretically verified",8,2011-10-03 00:00:00
179,"The state-of-the-art of face recognition has been significantly advanced by
the emergence of deep learning. Very deep neural networks recently achieved
great success on general object recognition because of their superb learning
capacity. This motivates us to investigate their effectiveness on face
recognition. This paper proposes two very deep neural network architectures,
referred to as DeepID3, for face recognition. These two architectures are
rebuilt from stacked convolution and inception layers proposed in VGG net and
GoogLeNet to make them suitable to face recognition. Joint face
identification-verification supervisory signals are added to both intermediate
and final feature extraction layers during training. An ensemble of the
proposed two architectures achieves 99.53% LFW face verification accuracy and
96.0% LFW rank-1 face identification accuracy, respectively. A further
discussion of LFW face verification result is given in the end",8,2015-02-03 00:00:00
180,"There are many factors affecting visual face recognition, such as low
resolution images, aging, illumination and pose variance, etc. One of the most
important problem is low resolution face images which can result in bad
performance on face recognition. Most of the general face recognition
algorithms usually assume a sufficient resolution for the face images. However,
in practice many applications often do not have sufficient image resolutions.
The modern face hallucination models demonstrate reasonable performance to
reconstruct high-resolution images from its corresponding low resolution
images. However, they do not consider identity level information during
hallucination which directly affects results of the recognition of low
resolution faces. To address this issue, we propose a Face Hallucination
Generative Adversarial Network (FH-GAN) which improves the quality of low
resolution face images and accurately recognize those low quality images.
Concretely, we make the following contributions: 1) we propose FH-GAN network,
an end-to-end system, that improves both face hallucination and face
recognition simultaneously. The novelty of this proposed network depends on
incorporating identity information in a GAN-based face hallucination algorithm
via combining a face recognition network for identity preserving. 2) We also
propose a new face hallucination network, namely Dense Sparse Network (DSNet),
which improves upon the state-of-art in face hallucination. 3) We demonstrate
benefits of training the face recognition and GAN-based DSNet jointly by
reporting good result on face hallucination and recognition",8,2019-05-16 01:00:00
181,"Face recognition has obtained remarkable progress in recent years due to the
great improvement of deep convolutional neural networks (CNNs). However, deep
CNNs are vulnerable to adversarial examples, which can cause fateful
consequences in real-world face recognition applications with
security-sensitive purposes. Adversarial attacks are widely studied as they can
identify the vulnerability of the models before they are deployed. In this
paper, we evaluate the robustness of state-of-the-art face recognition models
in the decision-based black-box attack setting, where the attackers have no
access to the model parameters and gradients, but can only acquire hard-label
predictions by sending queries to the target model. This attack setting is more
practical in real-world face recognition systems. To improve the efficiency of
previous methods, we propose an evolutionary attack algorithm, which can model
the local geometries of the search directions and reduce the dimension of the
search space. Extensive experiments demonstrate the effectiveness of the
proposed method that induces a minimum perturbation to an input face image with
fewer queries. We also apply the proposed method to attack a real-world face
recognition system successfully",8,2019-04-08 01:00:00
182,"Face recognition is a widely used biometric approach. Face recognition
technology has developed rapidly in recent years and it is more direct, user
friendly and convenient compared to other methods. But face recognition systems
are vulnerable to spoof attacks made by non-real faces. It is an easy way to
spoof face recognition systems by facial pictures such as portrait photographs.
A secure system needs Liveness detection in order to guard against such
spoofing. In this work, face liveness detection approaches are categorized
based on the various types techniques used for liveness detection. This
categorization helps understanding different spoof attacks scenarios and their
relation to the developed solutions. A review of the latest works regarding
face liveness detection works is presented. The main aim is to provide a simple
path for the future development of novel and more secured face liveness
detection approach",20,2014-05-09 00:00:00
183,"Tato práce se zabývá detekci tváře ve statickém obrazu. Teoretická část práce je zaměřena na barevné modely využívané pro detekci kůže v obraze (RGB, HSI, YCbCr), metodami využívající barevnou složku obrázků k detekci kůže (explicitní, parametrické či neparametrické metody), metrikou obrazu, detekci hran, matematickou morfologií, metodami pro klasifikaci tváře (příznakové metody, invariantní metody, znalostní metody, metody založené na porovnávání šablon). Praktická část obsahuje konkrétní návrh a praktickou realizaci dvou algoritmů detekující barvu kůže v obraze (jednoduchá metoda založená na Cr chrominační složce a statistická metoda). Praktická část také obsahuje návrh a praktickou realizaci dvou klasifikátorů tváře (příznaková metoda a metoda porovnávání šablon).This thesis is focused on face detection in static picture. Theoretical part contains color spaces (RGB, HSI, YCbCr), methods for skin detection (explicit, parametric or non-parametric methods), image metric, edge detection, mathematical morphology, methods for classification faces (appearance-based methods, feature invariant approaches, knowledge-based methods, template matching methods). Practical part of this thesis contains concept and practical realization two algorithms for segmentation skin in static image (simple method based on Cr chroma components and statistical method). Practical part contains concept and practical realization two algorithms for classification face (appearance-based method and template matching method) too.",1,2008-01-01 00:00:00
184,"Tato bakalářská práce se zabývá problematikou rozpoznání obličeje. Cílem práce bylo nastudovat různé metody extrakce příznaků a zjistit jejich vliv na úspěšnost rozpoznání. Mezi zkoumané metody extrakce příznaků patří Local Binary Pattern, Histogram orientovaných gradientů a Gaborovy filtry. Dále práce vysvětluje způsoby rozpoznání obličejů podle podobnosti obrazových dat. Jako klasifikátor obličejů byl při experimentech použit Support Vectore Machines. Experimentálně zjištěné nejúspěšnější kombinace parametrů metod extrakce příznaků a klasifikátoru byly využity v systému pro jednoduché rozpoznávání obličejů.The thesis deals with Face Recognition. The aim was to study the various methods of feature extraction and determine their influence on the success of recognition. The methods of feature extraction include the Local Binary Pattern, Histogram Of Oriented Gradients and Gabor Filter. Face recognition of image similarity will be described. Support Vectore Machines was used in the experiments. Experimentally determined parameters of the most successful methods were used in the system for simple Face Recognition.",8,2012-01-01 00:00:00
185,"Previous works have shown that face recognition with high accurate 3D data is
more reliable and insensitive to pose and illumination variations. Recently,
low-cost and portable 3D acquisition techniques like ToF(Time of Flight) and
DoE based structured light systems enable us to access 3D data easily, e.g.,
via a mobile phone. However, such devices only provide sparse(limited speckles
in structured light system) and noisy 3D data which can not support face
recognition directly. In this paper, we aim at achieving high-performance face
recognition for devices equipped with such modules which is very meaningful in
practice as such devices will be very popular. We propose a framework to
perform face recognition by fusing a sequence of low-quality 3D data. As 3D
data are sparse and noisy which can not be well handled by conventional methods
like the ICP algorithm, we design a PointNet-like Deep Registration
Network(DRNet) which works with ordered 3D point coordinates while preserving
the ability of mining local structures via convolution. Meanwhile we develop a
novel loss function to optimize our DRNet based on the quaternion expression
which obviously outperforms other widely used functions. For face recognition,
we design a deep convolutional network which takes the fused 3D depth-map as
input based on AMSoftmax model. Experiments show that our DRNet can achieve
rotation error 0.95{\deg} and translation error 0.28mm for registration. The
face recognition on fused data also achieves rank-1 accuracy 99.2% , FAR-0.001
97.5% on Bosphorus dataset which is comparable with state-of-the-art
high-quality data based recognition performance",8,2019-04-04 01:00:00
186,"Recent face recognition experiments on the LFW benchmark show that face
recognition is performing stunningly well, surpassing human recognition rates.
In this paper, we study face recognition at scale. Specifically, we have
collected from Flickr a \textbf{Million} faces and evaluated state of the art
face recognition algorithms on this dataset. We found that the performance of
algorithms varies--while all perform great on LFW, once evaluated at scale
recognition rates drop drastically for most algorithms. Interestingly, deep
learning based approach by \cite{schroff2015facenet} performs much better, but
still gets less robust at scale. We consider both verification and
identification problems, and evaluate how pose affects recognition at scale.
Moreover, we ran an extensive human study on Mechanical Turk to evaluate human
recognition at scale, and report results. All the photos are creative commons
photos and is released at \small{\url{http://megaface.cs.washington.edu/}} for
research and further experiments",11,2015-09-07 01:00:00
187,"There are many Local texture features each very in way they implement and
each of the Algorithm trying improve the performance. An attempt is made in
this paper to represent a theoretically very simple and computationally
effective approach for face recognition. In our implementation the face image
is divided into 3x3 sub-regions from which the features are extracted using the
Local Binary Pattern (LBP) over a window, fuzzy membership function and at the
central pixel. The LBP features possess the texture discriminative property and
their computational cost is very low. By utilising the information from LBP,
membership function, and central pixel, the limitations of traditional LBP is
eliminated. The bench mark database like ORL and Sheffield Databases are used
for the evaluation of proposed features with SVM classifier. For the proposed
approach K-fold and ROC curves are obtained and results are compared",8,2015-09-23 01:00:00
188,"After intensive research, heterogenous face recognition is still a
challenging problem. The main difficulties are owing to the complex
relationship between heterogenous face image spaces. The heterogeneity is
always tightly coupled with other variations, which makes the relationship of
heterogenous face images highly nonlinear. Many excellent methods have been
proposed to model the nonlinear relationship, but they apt to overfit to the
training set, due to limited samples. Inspired by the unsupervised algorithms
in deep learning, this paper proposes an novel framework for heterogeneous face
recognition. We first extract Gabor features at some localized facial points,
and then use Restricted Boltzmann Machines (RBMs) to learn a shared
representation locally to remove the heterogeneity around each facial point.
Finally, the shared representations of local RBMs are connected together and
processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases
are selected to evaluate the proposed method. For Sketch-Photo problem, we
obtain perfect results on the CUFS database. For NIR-VIS problem, we produce
new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases",8,2014-06-04 01:00:00
189,"Heterogeneous face matching is a challenge issue in face recognition due to
large domain difference as well as insufficient pairwise images in different
modalities during training. This paper proposes a coupled deep learning (CDL)
approach for the heterogeneous face matching. CDL seeks a shared feature space
in which the heterogeneous face matching problem can be approximately treated
as a homogeneous face matching problem. The objective function of CDL mainly
includes two parts. The first part contains a trace norm and a block-diagonal
prior as relevance constraints, which not only make unpaired images from
multiple modalities be clustered and correlated, but also regularize the
parameters to alleviate overfitting. An approximate variational formulation is
introduced to deal with the difficulties of optimizing low-rank constraint
directly. The second part contains a cross modal ranking among triplet domain
specific images to maximize the margin for different identities and increase
data for a small amount of training samples. Besides, an alternating
minimization method is employed to iteratively update the parameters of CDL.
Experimental results show that CDL achieves better performance on the
challenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch
database, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF),
which significantly outperforms state-of-the-art heterogeneous face recognition
methods",8,2017-11-16 00:00:00
190,"Occlusion in face recognition is a common yet challenging problem. While
sparse representation based classification (SRC) has been shown promising
performance in laboratory conditions (i.e. noiseless or random pixel
corrupted), it performs much worse in practical scenarios. In this paper, we
consider the practical face recognition problem, where the occlusions are
predictable and available for sampling. We propose the structured occlusion
coding (SOC) to address occlusion problems. The structured coding here lies in
two folds. On one hand, we employ a structured dictionary for recognition. On
the other hand, we propose to use the structured sparsity in this formulation.
Specifically, SOC simultaneously separates the occlusion and classifies the
image. In this way, the problem of recognizing an occluded image is turned into
seeking a structured sparse solution on occlusion-appended dictionary. In order
to construct a well-performing occlusion dictionary, we propose an occlusion
mask estimating technique via locality constrained dictionary (LCD), showing
striking improvement in occlusion sample. On a category-specific occlusion
dictionary, we replace norm sparsity with the structured sparsity which is
shown more robust, further enhancing the robustness of our approach. Moreover,
SOC achieves significant improvement in handling large occlusion in real world.
Extensive experiments are conducted on public data sets to validate the
superiority of the proposed algorithm",11,2015-07-25 01:00:00
191,"This paper presents a face recognition method based on a sequence of images.
Face shape is reconstructed from images using a combination of
structure-from-motion and multi-view stereo methods. The reconstructed 3D face
model is compared against models held in a gallery. The novel element in the
presented approach is the fact, that the reconstruction is based only on input
images and doesn't require a generic, deformable face model. Experimental
verification of the proposed method is also included",8,2018-09-28 01:00:00
192,"The boosting framework has shown good performance in face recognition. By combining a set of features with Adaboost, a similarity function is developed which determines if a pair of face images belongs to the same person or not. Recently, many features have been used in combination with Adaboost, achieving good results on the FERET database. In this paper we compare the results of several features on the same database and discuss our solutions on some of the\ud
open issues in this method. We compare the boosting framework with some standard algorithms and test the boosting algorithm under difficult circumstances,\ud
like illumination and registration noise.\u",8,2007-01-01 00:00:00
193,"We present an improved Locality Preserving Projections (LPP) method, named
Gloablity-Locality Preserving Projections (GLPP), to preserve both the global
and local geometric structures of data. In our approach, an additional
constraint of the geometry of classes is imposed to the objective function of
conventional LPP for respecting some more global manifold structures. Moreover,
we formulate a two-dimensional extension of GLPP (2D-GLPP) as an example to
show how to extend GLPP with some other statistical techniques. We apply our
works to face recognition on four popular face databases, namely ORL, Yale,
FERET and LFW-A databases, and extensive experimental results demonstrate that
the considered global manifold information can significantly improve the
performance of LPP and the proposed face recognition methods outperform the
state-of-the-arts",8,2013-11-05 00:00:00
194,"Facial recognition technologies are implemented in many areas, including but
not limited to, citizen surveillance, crime control, activity monitoring, and
facial expression evaluation. However, processing biometric information is a
resource-intensive task that often involves third-party servers, which can be
accessed by adversaries with malicious intent. Biometric information delivered
to untrusted third-party servers in an uncontrolled manner can be considered a
significant privacy leak (i.e. uncontrolled information release) as biometrics
can be correlated with sensitive data such as healthcare or financial records.
In this paper, we propose a privacy-preserving technique for ""controlled
information release"", where we disguise an original face image and prevent
leakage of the biometric features while identifying a person. We introduce a
new privacy-preserving face recognition protocol named PEEP (Privacy using
EigEnface Perturbation) that utilizes local differential privacy. PEEP applies
perturbation to Eigenfaces utilizing differential privacy and stores only the
perturbed data in the third-party servers to run a standard Eigenface
recognition algorithm. As a result, the trained model will not be vulnerable to
privacy attacks such as membership inference and model memorization attacks.
Our experiments show that PEEP exhibits a classification accuracy of around 70%
- 90% under standard privacy settings",8,2020-07-04 01:00:00
195,"This paper proposes to learn high-performance deep ConvNets with sparse
neural connections, referred to as sparse ConvNets, for face recognition. The
sparse ConvNets are learned in an iterative way, each time one additional layer
is sparsified and the entire model is re-trained given the initial weights
learned in previous iterations. One important finding is that directly training
the sparse ConvNet from scratch failed to find good solutions for face
recognition, while using a previously learned denser model to properly
initialize a sparser model is critical to continue learning effective features
for face recognition. This paper also proposes a new neural correlation-based
weight selection criterion and empirically verifies its effectiveness in
selecting informative connections from previously learned models in each
iteration. When taking a moderately sparse structure (26%-76% of weights in the
dense model), the proposed sparse ConvNet model significantly improves the face
recognition performance of the previous state-of-the-art DeepID2+ models given
the same training data, while it keeps the performance of the baseline model
with only 12% of the original parameters",8,2015-12-06 00:00:00
196,"Sparse Representation (or coding) based Classification (SRC) has gained great
success in face recognition in recent years. However, SRC emphasizes the
sparsity too much and overlooks the correlation information which has been
demonstrated to be critical in real-world face recognition problems. Besides,
some work considers the correlation but overlooks the discriminative ability of
sparsity. Different from these existing techniques, in this paper, we propose a
framework called Adaptive Sparse Representation based Classification (ASRC) in
which sparsity and correlation are jointly considered. Specifically, when the
samples are of low correlation, ASRC selects the most discriminative samples
for representation, like SRC; when the training samples are highly correlated,
ASRC selects most of the correlated and discriminative samples for
representation, rather than choosing some related samples randomly. In general,
the representation model is adaptive to the correlation structure, which
benefits from both $\ell_1$-norm and $\ell_2$-norm.
  Extensive experiments conducted on publicly available data sets verify the
effectiveness and robustness of the proposed algorithm by comparing it with
state-of-the-art methods",11,2014-04-18 00:00:00
197,"We evaluate a new approach to face recognition using a variety of surface representations of three-dimensional facial structure. Applying principal component analysis (PCA), we show that high levels of recognition accuracy can be achieved on a large database of 3D face models, captured under conditions that present typical difficulties to more conventional two-dimensional approaches. Applying a ran-c of image processing, techniques we identify the most effective surface representation for use in such application areas as security surveillance, data compression and archive searching",8,2004-01-01 00:00:00
198,"Biometrics-related research has been accelerated significantly by deep
learning technology. However, there are limited open-source resources to help
researchers evaluate their deep learning-based biometrics algorithms
efficiently, especially for the face recognition tasks. In this work, we design
and implement a light-weight, maintainable, scalable, generalizable, and
extendable face recognition evaluation toolbox named FaRE that supports both
online and offline evaluation to provide feedback to algorithm development and
accelerate biometrics-related research. FaRE consists of a set of evaluation
metric functions and provides various APIs for commonly-used face recognition
datasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be
easily extended to include other customized datasets. The package and the
pre-trained baseline models will be released for public academic research use
after obtaining university approval",11,2019-01-27 00:00:00
199,"This paper proposes a data driven model to predict the performance of a face
recognition system based on image quality features. We model the relationship
between image quality features (e.g. pose, illumination, etc.) and recognition
performance measures using a probability density function. To address the issue
of limited nature of practical training data inherent in most data driven
models, we have developed a Bayesian approach to model the distribution of
recognition performance measures in small regions of the quality space. Since
the model is based solely on image quality features, it can predict performance
even before the actual recognition has taken place. We evaluate the performance
predictive capabilities of the proposed model for six face recognition systems
(two commercial and four open source) operating on three independent data sets:
MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can
accurately predict performance using an accurate and unbiased Image Quality
Assessor (IQA). Furthermore, our experiments highlight the impact of the
unaccounted quality space -- the image quality features not considered by IQA
-- in contributing to performance prediction errors",8,2015-10-24 01:00:00
200,"By coding a query sample as a sparse linear combination of all training
samples and then classifying it by evaluating which class leads to the minimal
coding residual, sparse representation based classification (SRC) leads to
interesting results for robust face recognition. It is widely believed that the
l1- norm sparsity constraint on coding coefficients plays a key role in the
success of SRC, while its use of all training samples to collaboratively
represent the query sample is rather ignored. In this paper we discuss how SRC
works, and show that the collaborative representation mechanism used in SRC is
much more crucial to its success of face classification. The SRC is a special
case of collaborative representation based classification (CRC), which has
various instantiations by applying different norms to the coding residual and
coding coefficient. More specifically, the l1 or l2 norm characterization of
coding residual is related to the robustness of CRC to outlier facial pixels,
while the l1 or l2 norm characterization of coding coefficient is related to
the degree of discrimination of facial features. Extensive experiments were
conducted to verify the face recognition accuracy and efficiency of CRC with
different instantiations",11,2014-03-10 00:00:00
201,"The BASIS project is about the secure application of transparent biometrics in the home environment. Due to transparency and home-setting requirements there is variance in appearance of the subject. An other problem which needs attention is the extraction of features. The quality of the extracted features is not only depending on the proper preprocessing of the input data but also on the suitability of the extraction algorithm for this problem. Possible approaches to address problems due to transparency requirements are the use of active appearance models in face recognition, smart segmentation, multi-camera solutions and tracking. In this paper an inventory of problems and possible solution will be give",8,2004-01-01 00:00:00
202,"By the widespread popularity of electronic devices, the emergence of
biometric technology has brought significant convenience to user authentication
compared with the traditional password and mode unlocking. Among many
biological characteristics, the face is a universal and irreplaceable feature
that does not need too much cooperation and can significantly improve the
user's experience at the same time. Face recognition is one of the main
functions of electronic equipment propaganda. Hence it's virtually worth
researching in computer vision. Previous work in this field has focused on two
directions: converting loss function to improve recognition accuracy in
traditional deep convolution neural networks (Resnet); combining the latest
loss function with the lightweight system (MobileNet) to reduce network size at
the minimal expense of accuracy. But none of these has changed the network
structure. With the development of AutoML, neural architecture search (NAS) has
shown excellent performance in the benchmark of image classification. In this
paper, we integrate NAS technology into face recognition to customize a more
suitable network. We quote the framework of neural architecture search which
trains child and controller network alternately. At the same time, we mutate
NAS by incorporating evaluation latency into rewards of reinforcement learning
and utilize policy gradient algorithm to search the architecture automatically
with the most classical cross-entropy loss. The network architectures we
searched out have got state-of-the-art accuracy in the large-scale face
dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with
relatively small network size. To the best of our knowledge, this proposal is
the first attempt to use NAS to solve the problem of Deep Face Recognition and
achieve the best results in this domain",8,2019-04-26 01:00:00
203,"Recognizing a face based on its attributes is an easy task for a human to
perform as it is a cognitive process. In recent years, Face Recognition is
achieved with different kinds of facial features which were used separately or
in a combined manner. Currently, Feature fusion methods and parallel methods
are the facial features used and performed by integrating multiple feature sets
at different levels. However, this integration and the combinational methods do
not guarantee better result. Hence to achieve better results, the feature
fusion model with multiple weighted facial attribute set is selected. For this
feature model, face images from predefined data set has been taken from
Olivetti Research Laboratory (ORL) and applied on different methods like
Principal Component Analysis (PCA) based Eigen feature extraction technique,
Discrete Cosine Transformation (DCT) based feature extraction technique,
Histogram Based Feature Extraction technique and Simple Intensity based
features. The extracted feature set obtained from these methods were compared
and tested for accuracy. In this work we have developed a model which will use
the above set of feature extraction techniques with different levels of weights
to attain better accuracy. The results show that the selection of optimum
weight for a particular feature will lead to improvement in recognition rate",8,2010-06-01 00:00:00
204,"Collaborative Representation Classification (CRC) for face recognition
attracts a lot attention recently due to its good recognition performance and
fast speed. Compared to Sparse Representation Classification (SRC), CRC
achieves a comparable recognition performance with 10-1000 times faster speed.
In this paper, we propose to ensemble several CRC models to promote the
recognition rate, where each CRC model uses different and divergent randomly
generated biologically-inspired features as the face representation. The
proposed ensemble algorithm calculates an ensemble weight for each CRC model
that guided by the underlying classification rule of CRC. The obtained weights
reflect the confidences of those CRC models where the more confident CRC models
have larger weights. The proposed weighted ensemble method proves to be very
effective and improves the performance of each CRC model significantly.
Extensive experiments are conducted to show the superior performance of the
proposed method",11,2015-07-29 01:00:00
205,"With increasing security threats, Biometric systems have importance in different fields. This appears clearly exactly after the rapid development that happened in power of computing. In this paper, the Design and implementation of a real-time face recognition system are presented. In such a system, Kernel principal component analysis (KPCA) and Local binary pattern (LBP) are used as feature extraction methods with the aid of support vector machine (SVM) to work as a classifier. A comparison between traditional feature extraction methods as (PCA and LDA) and a proposal methods are performed as well as a comparison between support vector neural network and artificial neural network classifier are also implemented. Two types of experiments, On-line, and Off-line experiments are done. In the On-line experiment, a new database is created and used. While in the off-line experiment, two types of databases (ORL and YALE) are used to estimate the performance and efficiency of the system. The combinations of these methods together enhances the experimental results in compare with other methods",8,2017-02-01 00:00:00
206,"Assistive robots collaborating with people demand strong Human-Robot interaction capabilities. In this way, recognizing the person the robot has to interact with is paramount to provide a personalized service and reach a satisfactory end-user experience. 
To this end, face recognition: a non-intrusive, automatic mechanism of identification using biometric identifiers from an user's face, has gained relevance in the recent years, as the advances in machine learning and the creation of huge public datasets have considerably improved the state-of-the-art performance.
 In this work we study different open-source implementations of the typical components of state-of-the-art face recognition pipelines, including face detection, feature extraction and classification, and propose a recognition system integrating the most suitable methods for their utilization in assistant robots. 
 Concretely, for face detection we have considered MTCNN, OpenCV's DNN, and OpenPose, while for feature extraction we have analyzed InsightFace and Facenet.
 We have made public an implementation of the proposed recognition framework, ready to be used by any robot running the Robot Operating System (ROS).
 The methods in the spotlight have been compared in terms of accuracy and performance in common benchmark datasets, namely FDDB and LFW, to aid the choice of the final system implementation, which has been tested in a real robotic platform.This work is supported by the Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech, the research projects WISER ([DPI2017-84827-R]),funded by the Spanish Government, and financed by European RegionalDevelopment’s funds (FEDER), and MoveCare ([ICT-26-2016b-GA-732158]), funded by the European H2020 program, and by a postdoc contract from the I-PPIT-UMA program financed by the University of Málaga",8,2020-03-11 00:00:00
207,"This paper addresses deep face recognition (FR) problem under open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. However, few existing algorithms can effectively achieve this
criterion. To this end, we propose the angular softmax (A-Softmax) loss that
enables convolutional neural networks (CNNs) to learn angularly discriminative
features. Geometrically, A-Softmax loss can be viewed as imposing
discriminative constraints on a hypersphere manifold, which intrinsically
matches the prior that faces also lie on a manifold. Moreover, the size of
angular margin can be quantitatively adjusted by a parameter $m$. We further
derive specific $m$ to approximate the ideal feature criterion. Extensive
analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF)
and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The
code has also been made publicly available",11,2018-01-29 00:00:00
208,"With the broad use of face recognition, its weakness gradually emerges that
it is able to be attacked. So, it is important to study how face recognition
networks are subject to attacks. In this paper, we focus on a novel way to do
attacks against face recognition network that misleads the network to identify
someone as the target person not misclassify inconspicuously. Simultaneously,
for this purpose, we introduce a specific attentional adversarial attack
generative network to generate fake face images. For capturing the semantic
information of the target person, this work adds a conditional variational
autoencoder and attention modules to learn the instance-level correspondences
between faces. Unlike traditional two-player GAN, this work introduces face
recognition networks as the third player to participate in the competition
between generator and discriminator which allows the attacker to impersonate
the target person better. The generated faces which are hard to arouse the
notice of onlookers can evade recognition by state-of-the-art networks and most
of them are recognized as the target person",8,2018-11-29 00:00:00
209,"Recognition of low resolution face images is a challenging problem in many
practical face recognition systems. Methods have been proposed in the face
recognition literature for the problem which assume that the probe is low
resolution, but a high resolution gallery is available for recognition. These
attempts have been aimed at modifying the probe image such that the resultant
image provides better discrimination. We formulate the problem differently by
leveraging the information available in the high resolution gallery image and
propose a dictionary learning approach for classifying the low-resolution probe
image. An important feature of our algorithm is that it can handle resolution
change along with illumination variations. Furthermore, we also kernelize the
algorithm to handle non-linearity in data and present a joint dictionary
learning technique for robust recognition at low resolutions. The effectiveness
of the proposed method is demonstrated using standard datasets and a
challenging outdoor face dataset. It is shown that our method is efficient and
can perform significantly better than many competitive low resolution face
recognition algorithms",8,2017-07-10 01:00:00
210,"This paper presents face recognition using maximum a posteriori (MAP)
discriminant on YCbCr color space. The YCbCr color space is considered in order
to cover the skin information of face image on the recognition process. The
proposed method is employed to improve the recognition rate and equal error
rate (EER) of the gray scale based face recognition. In this case, the face
features vector consisting of small part of dominant frequency elements which
is extracted by non-blocking DCT is implemented as dimensional reduction of the
raw face images. The matching process between the query face features and the
trained face features is performed using maximum a posteriori (MAP)
discriminant. From the experimental results on data from four face databases
containing 2268 images with 196 classes show that the face recognition YCbCr
color space provide better recognition rate and lesser EER than those of gray
scale based face recognition which improve the first rank of grayscale based
method result by about 4%. However, it requires three times more computation
time than that of grayscale based method",1,2018-07-05 01:00:00
211,"A method of autonomic face recognition based on the biologically plausible
network of networks (NoN) model of information processing is presented. The NoN
model is based on locally parallel and globally coordinated transformations in
which the neurons or computational units form distributed networks, which
themselves link to form larger networks. This models the structures in the
cerebral cortex described by Mountcastle and the architecture based on that
proposed for information processing by Sutton. In the proposed implementation,
face images are processed by a nested family of locally operating networks
along with a hierarchically superior network that classifies the information
from each of the local networks. The results of the experiments yielded a
maximum of 98.5% recognition accuracy and an average of 97.4% recognition
accuracy on a benchmark database",8,2006-03-09 00:00:00
212,"A novel algorithm for 3D face recognition based point cloud rotations, multiple projections, and voted keypoint matching is proposed and evaluated. The basic idea is to rotate each 3D point cloud representing an individual’s face around the x, y or z axes, iteratively projecting the 3D points onto multiple 2.5D images at each step of the rotation. Labelled keypoints are then extracted from the resulting collection of 2.5D images, and this much smaller set of keypoints replaces the original face scan and its projections in the face database. Unknown test faces are recognised firstly by performing the same multiview keypoint extraction technique, and secondly, the application of a new weighted keypoint matching algorithm. In an extensive evaluation using the GavabDB 3D face recognition dataset (61 subjects, 9 scans per subject), our method achieves up to 95% recognition accuracy for faces with neutral expressions only, and over 90% accuracy for face recognition where expressions (such as a smile or a strong laugh) and random faceoccluding gestures are permitted",8,2009-01-01 00:00:00
213,"Attendance Management System (AMS) can be made into smarter way by using face
recognition technique, where we use a CCTV camera to be fixed at the entry
point of a classroom, which automatically captures the image of the person and
checks the observed image with the face database using android enhanced smart
phone. It is typically used for two purposes. Firstly, marking attendance for
student by comparing the face images produced recently and secondly,
recognition of human who are strange to the environment i.e. an unauthorized
person For verification of image, a newly emerging trend 3D Face Recognition is
used which claims to provide more accuracy in matching the image databases and
has an ability to recognize a subject at different view angles",8,2013-11-13 00:00:00
214,"Although deep learning approaches have achieved performance surpassing humans
for still image-based face recognition, unconstrained video-based face
recognition is still a challenging task due to large volume of data to be
processed and intra/inter-video variations on pose, illumination, occlusion,
scene, blur, video quality, etc. In this work, we consider challenging
scenarios for unconstrained video-based face recognition from multiple-shot
videos and surveillance videos with low-quality frames. To handle these
problems, we propose a robust and efficient system for unconstrained
video-based face recognition, which is composed of modules for face/fiducial
detection, face association, and face recognition. First, we use multi-scale
single-shot face detectors to efficiently localize faces in videos. The
detected faces are then grouped respectively through carefully designed face
association methods, especially for multi-shot videos. Finally, the faces are
recognized by the proposed face matcher based on an unsupervised subspace
learning approach and a subspace-to-subspace similarity metric. Extensive
experiments on challenging video datasets, such as Multiple Biometric Grand
Challenge (MBGC), Face and Ocular Challenge Series (FOCS), IARPA Janus
Surveillance Video Benchmark (IJB-S) for low-quality surveillance videos and
IARPA JANUS Benchmark B (IJB-B) for multiple-shot videos, demonstrate that the
proposed system can accurately detect and associate faces from unconstrained
videos and effectively learn robust and discriminative features for
recognition",11,2019-08-09 01:00:00
215,"Despite rapid advances in face recognition, there remains a clear gap between
the performance of still image-based face recognition and video-based face
recognition, due to the vast difference in visual quality between the domains
and the difficulty of curating diverse large-scale video datasets. This paper
addresses both of those challenges, through an image to video feature-level
domain adaptation approach, to learn discriminative video frame
representations. The framework utilizes large-scale unlabeled video data to
reduce the gap between different domains while transferring discriminative
knowledge from large-scale labeled still images. Given a face recognition
network that is pretrained in the image domain, the adaptation is achieved by
(i) distilling knowledge from the network to a video adaptation network through
feature matching, (ii) performing feature restoration through synthetic data
augmentation and (iii) learning a domain-invariant feature through a domain
adversarial discriminator. We further improve performance through a
discriminator-guided feature fusion that boosts high-quality frames while
eliminating those degraded by video domain-specific factors. Experiments on the
YouTube Faces and IJB-A datasets demonstrate that each module contributes to
our feature-level domain adaptation framework and substantially improves video
face recognition performance to achieve state-of-the-art accuracy. We
demonstrate qualitatively that the network learns to suppress diverse artifacts
in videos such as pose, illumination or occlusion without being explicitly
trained for them",11,2017-08-07 00:00:00
216,An optical network is described that is capable of recognizing at standard video rates the identity of faces for which it has been trained. The faces are presented under a wide variety of conditions to the system and the classification performance is measured. The system is trained by gradually adapting photorefractive holograms,8,1993-09-10 00:00:00
217,"This paper presents a Neural Aggregation Network (NAN) for video face
recognition. The network takes a face video or face image set of a person with
a variable number of face images as its input, and produces a compact,
fixed-dimension feature representation for recognition. The whole network is
composed of two modules. The feature embedding module is a deep Convolutional
Neural Network (CNN) which maps each face image to a feature vector. The
aggregation module consists of two attention blocks which adaptively aggregate
the feature vectors to form a single feature inside the convex hull spanned by
them. Due to the attention mechanism, the aggregation is invariant to the image
order. Our NAN is trained with a standard classification or verification loss
without any extra supervision signal, and we found that it automatically learns
to advocate high-quality face images while repelling low-quality ones such as
blurred, occluded and improperly exposed faces. The experiments on IJB-A,
YouTube Face, Celebrity-1000 video face recognition benchmarks show that it
consistently outperforms naive aggregation methods and achieves the
state-of-the-art accuracy",11,2017-08-02 00:00:00
218,"In order to establish face recognition system in rehabilitation nursing
robots beds and achieve real-time monitor the patient on the bed. We propose a
face recognition method based on partial matching Hu moments which apply for
rehabilitation nursing robots beds. Firstly we using Haar classifier to detect
human faces automatically in dynamic video frames. Secondly we using Otsu
threshold method to extract facial features (eyebrows, eyes, mouth) in the face
image and its Hu moments. Finally, we using Hu moment feature set to achieve
the automatic face recognition. Experimental results show that this method can
efficiently identify face in a dynamic video and it has high practical value
(the accuracy rate is 91% and the average recognition time is 4.3s)",8,2015-08-02 01:00:00
219,"Recently, it has been shown that performance of a face recognition system depends on the quality of both face images participating in the recognition process: the reference and the test image. In the context of forensic face recognition, this observation has two implications: a) the quality of the trace (extracted from CCTV footage) constrains the performance achievable using a particular face recognition system; b) the quality of the suspect reference set (to which the trace is matched against) can be judiciously chosen to approach optimal recognition performance under such a constraint. Motivated by these recent findings, we propose a framework for forensic face recognition that is based on calibrating the recognition performance for the quality of pairs of images. The application of this framework to several mock-up forensic cases, created entirely from the MultiPIE dataset, shows that optimal recognition performance, under such a constraint, can be achieved by matching the quality (pose, illumination, and, imaging device) of the reference set to that of the trace. This improvement in recognition performance helps reduce the rate of misleading interpretation of the evidence",4,2012-01-01 00:00:00
220,Face recognition is one of biometrical research area that is still interesting. This study discusses the Complex-Valued Backpropagation algorithm for face recognition. Complex-Valued Backpropagation is an algorithm modified from Real-Valued Backpropagation algorithm where the weights and activation functions used are complex. The dataset used in this study consist of 250 images that is classified in 5 classes. The performance of face recognition using Complex-Valued Backpropagation is also compared with Real-Valued Backpropagation algorithm. Experimental results have shown that Complex-Valued Backpropagation performance is better than Real-Valued Backpropagation,8,2018-01-01 00:00:00
221,"Smile detection from unconstrained facial images is a specialized and
challenging problem. As one of the most informative expressions, smiles convey
basic underlying emotions, such as happiness and satisfaction, which lead to
multiple applications, e.g., human behavior analysis and interactive
controlling. Compared to the size of databases for face recognition, far less
labeled data is available for training smile detection systems. To leverage the
large amount of labeled data from face recognition datasets and to alleviate
overfitting on smile detection, an efficient transfer learning-based smile
detection approach is proposed in this paper. Unlike previous works which use
either hand-engineered features or train deep convolutional networks from
scratch, a well-trained deep face recognition model is explored and fine-tuned
for smile detection in the wild. Three different models are built as a result
of fine-tuning the face recognition model with different inputs, including
aligned, unaligned and grayscale images generated from the GENKI-4K dataset.
Experiments show that the proposed approach achieves improved state-of-the-art
performance. Robustness of the model to noise and blur artifacts is also
evaluated in this paper",8,2018-01-17 00:00:00
222,"Mirzaei, M. E. (2020). Review of six popular method: Face Recognition. Iberian Journal of Applied Sciences and Innovations, 1(1).Simulation of human ability has always been attractive topic for researchers but had often been limited to hardware part. With the advancement of technology and emerge of deep learning and machine learning, hope to simulate human perception increased. Face recognition is one of the most important capability of human perception that use in routine. Many researches have been done that led to proposing different approach and various methods. This paper is a survey of face recognition methods that proposed in past decades and also categorize them in meaningful approachespublishersversionpublishe",11,2020-01-01 00:00:00
223,"This work tackles the face recognition task on images captured using thermal
camera sensors which can operate in the non-light environment. While it can
greatly increase the scope and benefits of the current security surveillance
systems, performing such a task using thermal images is a challenging problem
compared to face recognition task in the Visible Light Domain (VLD). This is
partly due to the much smaller amount number of thermal imagery data collected
compared to the VLD data. Unfortunately, direct application of the existing
very strong face recognition models trained using VLD data into the thermal
imagery data will not produce a satisfactory performance. This is due to the
existence of the domain gap between the thermal and VLD images. To this end, we
propose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is
able to transform thermal face images into their corresponding VLD images
whilst maintaining identity information which is sufficient enough for the
existing VLD face recognition models to perform recognition. Some examples are
presented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an
explicit closed-set face recognition loss to regularize the discriminator
network training. This information will then be conveyed into the generator
network in the forms of gradient loss. In the experiment, we show that by using
this additional explicit regularization for the discriminator network, the
TV-GAN is able to preserve more identity information when translating a thermal
image of a person which is not seen before by the TV-GAN",8,2017-12-07 00:00:00
224,"University of Technology Sydney. Faculty of Engineering and Information Technology.Face recognition is one of the most important and promising biometric techniques. In face recognition, a similarity score is automatically calculated between face images to further decide their identity. Due to its non-invasive characteristics and ease of use, it has shown great potential in many real-world applications, e.g., video surveillance, access control systems, forensics and security, and social networks. This thesis addresses key challenges inherent in real-world face recognition systems including pose and illumination variations, occlusion, and image blur. To tackle these challenges, a series of robust face recognition algorithms are proposed. These can be summarized as follows:
In Chapter 2, we present a novel, manually designed face image descriptor named “Dual-Cross Patterns” (DCP). DCP efficiently encodes the seconder-order statistics of facial textures in the most informative directions within a face image. It proves to be more descriptive and discriminative than previous descriptors. We further extend DCP into a comprehensive face representation scheme named “Multi-Directional Multi-Level Dual-Cross Patterns” (MDML-DCPs). MDML-DCPs efficiently encodes the invariant characteristics of a face image from multiple levels into patterns that are highly discriminative of inter-personal differences but robust to intra-personal variations. MDML-DCPs achieves the best performance on the challenging FERET, FRGC 2.0, CAS-PEAL-R1, and LFW databases.
In Chapter 3, we develop a deep learning-based face image descriptor named “Multimodal Deep Face Representation” (MM-DFR) to automatically learn face representations from multimodal image data. In brief, convolutional neural networks (CNNs) are designed to extract complementary information from the original holistic face image, the frontal pose image rendered by 3D modeling, and uniformly sampled image patches. The recognition ability of each CNN is optimized by carefully integrating a number of published or newly developed tricks. A feature level fusion approach using stacked auto-encoders is designed to fuse the features extracted from the set of CNNs, which is advantageous for non-linear dimension reduction. MM-DFR achieves over 99% recognition rate on LFW using publicly available training data.
In Chapter 4, based on our research on handcrafted face image descriptors, we propose a powerful pose-invariant face recognition (PIFR) framework capable of handling the full range of pose variations within ±90° of yaw. The framework has two parts: the first is Patch-based Partial Representation (PBPR), and the second is Multi-task Feature Transformation Learning (MtFTL). PBPR transforms the original PIFR problem into a partial frontal face recognition problem. A robust patch-based face representation scheme is developed to represent the synthesized partial frontal faces. For each patch, a transformation dictionary is learnt under the MtFTL scheme. The transformation dictionary transforms the features of different poses into a discriminative subspace in which face matching is performed. The PBPR-MtFTL framework outperforms previous state-of-the-art PIFR methods on the FERET, CMU-PIE, and Multi-PIE databases.
In Chapter 5, based on our research on deep learning-based face image descriptors, we design a novel framework named Trunk-Branch Ensemble CNN (TBE-CNN) to handle challenges in video-based face recognition (VFR) under surveillance circumstances. Three major challenges are considered: image blur, occlusion, and pose variation. First, to learn blur-robust face representations, we artificially blur training data composed of clear still images to account for a shortfall in real-world video training data. Second, to enhance the robustness of CNN features to pose variations and occlusion, we propose the TBE-CNN architecture, which efficiently extracts complementary information from holistic face images and patches cropped around facial components. Third, to further promote the discriminative power of the representations learnt by TBE-CNN, we propose an improved triplet loss function. With the proposed techniques, TBE-CNN achieves state-of-the-art performance on three popular video face databases: PaSC, COX Face, and YouTube Faces",11,2016-01-01 00:00:00
225,"From birth, infants prefer to look at faces that engage them in direct eye contact. In adults, direct gaze is known to modulate the processing of faces, including the recognition of individuals. In the present study, we investigate whether direction of gaze has any effect on face recognition in four-month-old infants. Four-month infants were shown faces with both direct and averted gaze, and subsequently given a preference test involving the same face and a novel one. A novelty preference during test was only found following initial exposure to a face with direct gaze. Further, face recognition was also generally enhanced for faces with both direct and with averted gaze when the infants started the task with the direct gaze condition. Together, these results indicate that the direction of the gaze modulates face recognition in early infancy",4,2007-01-01 00:00:00
226,"Automatic age estimation from real-world and unconstrained face images is
rapidly gaining importance. In our proposed work, a deep CNN model that was
trained on a database for face recognition task is used to estimate the age
information on the Adience database. This paper has three significant
contributions in this field. (1) This work proves that a CNN model, which was
trained for face recognition task, can be utilized for age estimation to
improve performance; (2) Over fitting problem can be overcome by employing a
pretrained CNN on a large database for face recognition task; (3) Not only the
number of training images and the number subjects in a training database effect
the performance of the age estimation model, but also the pre-training task of
the employed CNN determines the performance of the model",8,2017-09-05 01:00:00
227,"Imaging system is suitable for different purposes, depending upon their final application. Digital cameras, camcorders, webcams, security cameras or infrared (IR) cameras are well-known imaging systems. For complementary metal oxide semiconductor (CMOS) image sensors, its performance is more promising compared to charged coupled device (CCD) due to its low power consumption, low cost, and on chip functionality and compatibility with standard CMOS technology [1]. In face recognition mode, three dimensional cameras replaced two-dimensional images to measure the object size, distance and shape with time-of-flight (TOF) cameras. Depth information can be extracted to be compared with database [2]. Therefore, CMOS image sensor with its architecture on pixel array, signal processors, row and column selector and timing control [3] will be the main key",15,2020-01-01 00:00:00
228,"Do very high accuracies of deep networks suggest pride of effective AI or are
deep networks prejudiced? Do they suffer from in-group biases (own-race-bias
and own-age-bias), and mimic the human behavior? Is in-group specific
information being encoded sub-consciously by the deep networks?
  This research attempts to answer these questions and presents an in-depth
analysis of `bias' in deep learning based face recognition systems. This is the
first work which decodes if and where bias is encoded for face recognition.
Taking cues from cognitive studies, we inspect if deep networks are also
affected by social in- and out-group effect. Networks are analyzed for own-race
and own-age bias, both of which have been well established in human beings. The
sub-conscious behavior of face recognition models is examined to understand if
they encode race or age specific features for face recognition. Analysis is
performed based on 36 experiments conducted on multiple datasets. Four deep
learning networks either trained from scratch or pre-trained on over 10M images
are used. Variations across class activation maps and feature visualizations
provide novel insights into the functioning of deep learning systems,
suggesting behavior similar to humans. It is our belief that a better
understanding of state-of-the-art deep learning networks would enable
researchers to address the given challenge of bias in AI, and develop fairer
systems",11,2019-06-19 01:00:00
229,"In this paper, we present a deep coupled learning frame- work to address the
problem of matching polarimetric ther- mal face photos against a gallery of
visible faces. Polariza- tion state information of thermal faces provides the
miss- ing textural and geometrics details in the thermal face im- agery which
exist in visible spectrum. we propose a coupled deep neural network
architecture which leverages relatively large visible and thermal datasets to
overcome the problem of overfitting and eventually we train it by a
polarimetric thermal face dataset which is the first of its kind. The pro-
posed architecture is able to make full use of the polari- metric thermal
information to train a deep model compared to the conventional shallow
thermal-to-visible face recogni- tion methods. Proposed coupled deep neural
network also finds global discriminative features in a nonlinear embed- ding
space to relate the polarimetric thermal faces to their corresponding visible
faces. The results show the superior- ity of our method compared to the
state-of-the-art models in cross thermal-to-visible face recognition
algorithms",8,2018-01-04 00:00:00
230,"Multimodal biometric identification has been grown a great attention in the
most interests in the security fields. In the real world there exist modern
system devices that are able to detect, recognize, and classify the human
identities with reliable and fast recognition rates. Unfortunately most of
these systems rely on one modality, and the reliability for two or more
modalities are further decreased. The variations of face images with respect to
different poses are considered as one of the important challenges in face
recognition systems. In this paper, we propose a multimodal biometric system
that able to detect the human face images that are not only one view face
image, but also multi-view face images. Each subject entered to the system
adjusted their face at front of the three cameras, and then the features of the
face images are extracted based on Speeded Up Robust Features (SURF) algorithm.
We utilize Multi-Layer Perceptron (MLP) and combined classifiers based on both
Learning Vector Quantization (LVQ), and Radial Basis Function (RBF) for
classification purposes. The proposed system has been tested using SDUMLA-HMT,
and CASIA datasets. Furthermore, we collected a database of multi-view face
images by which we take the additive white Gaussian noise into considerations.
The results indicated the reliability, robustness of the proposed system with
different poses and variations including noise images",8,2017-06-01 01:00:00
231,"Face recognition has the perception of a solved problem, however when tested
at the million-scale exhibits dramatic variation in accuracies across the
different algorithms. Are the algorithms very different? Is access to good/big
training data their secret weapon? Where should face recognition improve? To
address those questions, we created a benchmark, MF2, that requires all
algorithms to be trained on same data, and tested at the million scale. MF2 is
a public large-scale set with 672K identities and 4.7M photos created with the
goal to level playing field for large scale face recognition. We contrast our
results with findings from the other two large-scale benchmarks MegaFace
Challenge and MS-Celebs-1M where groups were allowed to train on any
private/public/big/small set. Some key discoveries: 1) algorithms, trained on
MF2, were able to achieve state of the art and comparable results to algorithms
trained on massive private sets, 2) some outperformed themselves once trained
on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace,
identifying the need for larger age variations possibly within identities or
adjustment of algorithms in future testings",8,2017-04-30 00:00:00
232,"Benefit from large-scale training datasets, deep Convolutional Neural
Networks(CNNs) have achieved impressive results in face recognition(FR).
However, tremendous scale of datasets inevitably lead to noisy data, which
obviously reduce the performance of the trained CNN models. Kicking out wrong
labels from large-scale FR datasets is still very expensive, although some
cleaning approaches are proposed. According to the analysis of the whole
process of training CNN models supervised by angular margin based loss(AM-Loss)
functions, we find that the $\theta$ distribution of training samples
implicitly reflects their probability of being clean. Thus, we propose a novel
training paradigm that employs the idea of weighting samples based on the above
probability. Without any prior knowledge of noise, we can train high
performance CNN models with large-scale FR datasets. Experiments demonstrate
the effectiveness of our training paradigm. The codes are available at
https://github.com/huangyangyu/NoiseFace",8,2019-03-26 00:00:00
233,Volume 4 Issue 1 (January 2016,0,2016-01-01 00:00:00
234,"Inspired by biological vision systems, the over-complete local features with
huge cardinality are increasingly used for face recognition during the last
decades. Accordingly, feature selection has become more and more important and
plays a critical role for face data description and recognition. In this paper,
we propose a trainable feature selection algorithm based on the regularized
frame for face recognition. By enforcing a sparsity penalty term on the minimum
squared error (MSE) criterion, we cast the feature selection problem into a
combinatorial sparse approximation problem, which can be solved by greedy
methods or convex relaxation methods. Moreover, based on the same frame, we
propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal
sparse solution and the corresponding margin vector of the MSE criterion. The
proposed methods are used for selecting the most informative Gabor features of
face images for recognition and the experimental results on benchmark face
databases demonstrate the effectiveness of the proposed methods",8,2011-02-14 00:00:00
235,"Low-resolution face recognition (LRFR) has received increasing attention over
the past few years. Its applications lie widely in the real-world environment
when high-resolution or high-quality images are hard to capture. One of the
biggest demands for LRFR technologies is video surveillance. As the the number
of surveillance cameras in the city increases, the videos that captured will
need to be processed automatically. However, those videos or images are usually
captured with large standoffs, arbitrary illumination condition, and diverse
angles of view. Faces in these images are generally small in size. Several
studies addressed this problem employed techniques like super resolution,
deblurring, or learning a relationship between different resolution domains. In
this paper, we provide a comprehensive review of approaches to low-resolution
face recognition in the past five years. First, a general problem definition is
given. Later, systematically analysis of the works on this topic is presented
by catogory. In addition to describing the methods, we also focus on datasets
and experiment settings. We further address the related works on unconstrained
low-resolution face recognition and compare them with the result that use
synthetic low-resolution data. Finally, we summarized the general limitations
and speculate a priorities for the future effort",8,2019-03-28 00:00:00
236,"Various factors, such as identities, views (poses), and illuminations, are
coupled in face images. Disentangling the identity and view representations is
a major challenge in face recognition. Existing face recognition systems either
use handcrafted features or learn features discriminatively to improve
recognition accuracy. This is different from the behavior of human brain.
Intriguingly, even without accessing 3D data, human not only can recognize face
identity, but can also imagine face images of a person under different
viewpoints given a single 2D image, making face perception in the brain robust
to view changes. In this sense, human brain has learned and encoded 3D face
models from 2D images. To take into account this instinct, this paper proposes
a novel deep neural net, named multi-view perceptron (MVP), which can untangle
the identity and view features, and infer a full spectrum of multi-view images
in the meanwhile, given a single 2D face image. The identity features of MVP
achieve superior performance on the MultiPIE dataset. MVP is also capable to
interpolate and predict images under viewpoints that are unobserved in the
training data",8,2014-06-26 01:00:00
237,"Face recognition is very helpful in many applications such as video surveillance, forensic applications criminal investigations, and in many other fields. The most common methods includes PCA approach based Eigenface, Linear Discriminant Analysis(LDA), Hidden Markov Model(HMM),DWT, geometry based and template matching approaches.In this paper we are using sparse representation approach to attain more robustness to variation in lighting, directions and expressions. This survey paper performs analysis on different approaches and factors affecting the face recognition",11,2015-02-01 00:00:00
238,"With the rapid development of digital imaging and communication technologies,
image set based face recognition (ISFR) is becoming increasingly important. One
key issue of ISFR is how to effectively and efficiently represent the query
face image set by using the gallery face image sets. The set-to-set distance
based methods ignore the relationship between gallery sets, while representing
the query set images individually over the gallery sets ignores the correlation
between query set images. In this paper, we propose a novel image set based
collaborative representation and classification method for ISFR. By modeling
the query set as a convex or regularized hull, we represent this hull
collaboratively over all the gallery sets. With the resolved representation
coefficients, the distance between the query set and each gallery set can then
be calculated for classification. The proposed model naturally and effectively
extends the image based collaborative representation to an image set based one,
and our extensive experiments on benchmark ISFR databases show the superiority
of the proposed method to state-of-the-art ISFR methods under different set
sizes in terms of both recognition rate and efficiency",11,2013-08-30 01:00:00
239,"This literature review was commissioned to explore the psychological literature relating to facial image comparison with a particular emphasis on whether individuals can be trained to improve performance on this task. Surprisingly few studies have addressed this question directly. As a consequence, this review has been extended to cover training of face recognition and training of different kinds of perceptual comparisons where we are of the opinion that the methodologies or findings of such studies are informative. The majority of studies of face processing have examined face recognition, which relies heavily on memory. This may be memory for a face that was learned recently (e.g. minutes or hours previously) or for a face learned longer ago, perhaps after many exposures (e.g. friends, family members, celebrities). Successful face recognition, irrespective of the type of face, relies on the ability to retrieve the to-berecognised face from long-term memory. This memory is then compared to the physically present image to reach a recognition decision. In contrast, in face matching task two physical representations of a face (live, photographs, movies) are compared and so long-term memory is not involved. Because the comparison is between two present stimuli rather than between a present stimulus and a memory, one might expect that face matching, even if not an easy task, would be easier to do and easier to learn than face recognition. In support of this, there is evidence that judgment tasks where a presented stimulus must be judged by a remembered standard are generally more cognitively demanding than judgments that require comparing two presented stimuli Davies &amp; Parasuraman, 1982; Parasuraman &amp; Davies, 1977; Warm and Dember, 1998). Is there enough overlap between face recognition and matching that it is useful to look at the literature recognition? No study has directly compared face recognition and face matching, so we turn to research in which people decided whether two non-face stimuli were the same or different. In these studies, accuracy of comparison is not always better when the comparator is present than when it is remembered. Further, all perceptual factors that were found to affect comparisons of simultaneously presented objects also affected comparisons of successively presented objects in qualitatively the same way. Those studies involved judgments about colour (Newhall, Burnham &amp; Clark, 1957; Romero, Hita &amp; Del Barco, 1986), and shape (Larsen, McIlhagga &amp; Bundesen, 1999; Lawson, Bülthoff &amp; Dumbell, 2003; Quinlan, 1995). Although one must be cautious in generalising from studies of object processing to studies of face processing (see, e.g., section comparing face processing to object processing), from these kinds of studies there is no evidence to suggest that there are qualitative differences in the perceptual aspects of how recognition and matching are done. As a result, this review will include studies of face recognition skill as well as face matching skill. The distinction between face recognition involving memory and face matching not involving memory is clouded in many recognition studies which require observers to decide which of many presented faces matches a remembered face (e.g., eyewitness studies). And of course there are other forensic face-matching tasks that will require comparison to both presented and remembered comparators (e.g., deciding whether any person in a video showing a crowd is the target person). For this reason, too, we choose to include studies of face recognition as well as face matching in our revie",4,2011-01-01 00:00:00
240,"We present in this paper a biometric system of face detection and recognition
in color images. The face detection technique is based on skin color
information and fuzzy classification. A new algorithm is proposed in order to
detect automatically face features (eyes, mouth and nose) and extract their
correspondent geometrical points. These fiducial points are described by sets
of wavelet components which are used for recognition. To achieve the face
recognition, we use neural networks and we study its performances for different
inputs. We compare the two types of features used for recognition: geometric
distances and Gabor coefficients which can be used either independently or
jointly. This comparison shows that Gabor coefficients are more powerful than
geometric distances. We show with experimental results how the importance
recognition ratio makes our system an effective tool for automatic face
detection and recognition",8,2009-07-28 00:00:00
241,"Starting in the seventies, face recognition has become one of the most
researched topics in computer vision and biometrics. Traditional methods based
on hand-crafted features and traditional machine learning techniques have
recently been superseded by deep neural networks trained with very large
datasets. In this paper we provide a comprehensive and up-to-date literature
review of popular face recognition methods including both traditional
(geometry-based, holistic, feature-based and hybrid methods) and deep learning
methods",8,2018-10-31 00:00:00
242,"In PCA-based face recognition, there is often a trade-off between selecting the most relevant parts of a face image for recognition and not discarding information which may be useful. The work presented in this paper proposes a method to automatically determine the most discriminative coefficients in a DWT/PCA-based face recognition system, based on their inter-class and intra-class standard deviations. In addition, the eigenfaces used for recognition are generally chosen based on the value of their associated eigenvalues. However, the variance indicated by the eigenvalues may be due to factors such as variation in illumination levels between training set faces, rather than differences that are useful for identification. The work presented proposes a method to automatically determine the most discriminative eigenfaces, based on the inter-class and intra-class standard deviations of the training set eigenface weight vectors. The results obtained using the AT&T database show an improvement over existing DWT/PCA coefficient selection techniques",8,2008-01-01 00:00:00
243,"The pose problem is one of the bottlenecks in automatic face recognition. We
argue that one of the diffculties in this problem is the severe misalignment in
face images or feature vectors with different poses. In this paper, we propose
that this problem can be statistically solved or at least mitigated by
maximizing the intra-subject across-pose correlations via canonical correlation
analysis (CCA). In our method, based on the data set with coupled face images
of the same identities and across two different poses, CCA learns
simultaneously two linear transforms, each for one pose. In the transformed
subspace, the intra-subject correlations between the different poses are
maximized, which implies pose-invariance or pose-robustness is achieved. The
experimental results show that our approach could considerably improve the
recognition performance. And if further enhanced with holistic+local feature
representation, the performance could be comparable to the state-of-the-art",8,2015-07-29 01:00:00
244,"Heterogeneous face recognition between color image and depth image is a much
desired capacity for real world applications where shape information is looked
upon as merely involved in gallery. In this paper, we propose a cross-modal
deep learning method as an effective and efficient workaround for this
challenge. Specifically, we begin with learning two convolutional neural
networks (CNNs) to extract 2D and 2.5D face features individually. Once
trained, they can serve as pre-trained models for another two-way CNN which
explores the correlated part between color and depth for heterogeneous
matching. Compared with most conventional cross-modal approaches, our method
additionally conducts accurate depth image reconstruction from single color
image with Conditional Generative Adversarial Nets (cGAN), and further enhances
the recognition performance by fusing multi-modal matching results. Through
both qualitative and quantitative experiments on benchmark FRGC 2D/3D face
database, we demonstrate that the proposed pipeline outperforms
state-of-the-art performance on heterogeneous face recognition and ensures a
drastically efficient on-line stage",11,2017-09-13 01:00:00
245,"A key recent advance in face recognition models a test face image as a sparse
linear combination of a set of training face images. The resulting sparse
representations have been shown to possess robustness against a variety of
distortions like random pixel corruption, occlusion and disguise. This approach
however makes the restrictive (in many scenarios) assumption that test faces
must be perfectly aligned (or registered) to the training data prior to
classification. In this paper, we propose a simple yet robust local block-based
sparsity model, using adaptively-constructed dictionaries from local features
in the training data, to overcome this misalignment problem. Our approach is
inspired by human perception: we analyze a series of local discriminative
features and combine them to arrive at the final classification decision. We
propose a probabilistic graphical model framework to explicitly mine the
conditional dependencies between these distinct sparse local features. In
particular, we learn discriminative graphs on sparse representations obtained
from distinct local slices of a face. Conditional correlations between these
sparse features are first discovered (in the training phase), and subsequently
exploited to bring about significant improvements in recognition rates.
Experimental results obtained on benchmark face databases demonstrate the
effectiveness of the proposed algorithms in the presence of multiple
registration errors (such as translation, rotation, and scaling) as well as
under variations of pose and illumination",8,2011-11-08 00:00:00
246,"The recent explosive growth in convolutional neural network (CNN) research
has produced a variety of new architectures for deep learning. One intriguing
new architecture is the bilinear CNN (B-CNN), which has shown dramatic
performance gains on certain fine-grained recognition problems [15]. We apply
this new CNN to the challenging new face recognition benchmark, the IARPA Janus
Benchmark A (IJB-A) [12]. It features faces from a large number of identities
in challenging real-world conditions. Because the face images were not
identified automatically using a computerized face detection system, it does
not have the bias inherent in such a database. We demonstrate the performance
of the B-CNN model beginning from an AlexNet-style network pre-trained on
ImageNet. We then show results for fine-tuning using a moderate-sized and
public external database, FaceScrub [17]. We also present results with
additional fine-tuning on the limited training data provided by the protocol.
In each case, the fine-tuned bilinear model shows substantial improvements over
the standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a
large face database, the recently released VGG-Face model [20], can be
converted into a B-CNN without any additional feature training. This B-CNN
improves upon the CNN performance on the IJB-A benchmark, achieving 89.5%
rank-1 recall",11,2016-03-28 00:00:00
247,"Face Recognition has been studied for many decades. As opposed to traditional
hand-crafted features such as LBP and HOG, much more sophisticated features can
be learned automatically by deep learning methods in a data-driven way. In this
paper, we propose a two-stage approach that combines a multi-patch deep CNN and
deep metric learning, which extracts low dimensional but very discriminative
features for face verification and recognition. Experiments show that this
method outperforms other state-of-the-art methods on LFW dataset, achieving
99.77% pair-wise verification accuracy and significantly better accuracy under
other two more practical protocols. This paper also discusses the importance of
data size and the number of patches, showing a clear path to practical
high-performance face recognition systems in real world",8,2015-07-22 01:00:00
248,"There has been an increasing research interest in age-invariant face
recognition. However, matching faces with big age gaps remains a challenging
problem, primarily due to the significant discrepancy of face appearances
caused by aging. To reduce such a discrepancy, in this paper we propose a novel
algorithm to remove age-related components from features mixed with both
identity and age information. Specifically, we factorize a mixed face feature
into two uncorrelated components: identity-dependent component and
age-dependent component, where the identity-dependent component includes
information that is useful for face recognition. To implement this idea, we
propose the Decorrelated Adversarial Learning (DAL) algorithm, where a
Canonical Mapping Module (CMM) is introduced to find the maximum correlation
between the paired features generated by a backbone network, while the backbone
network and the factorization module are trained to generate features reducing
the correlation. Thus, the proposed model learns the decomposed features of age
and identity whose correlation is significantly reduced. Simultaneously, the
identity-dependent feature and the age-dependent feature are respectively
supervised by ID and age preserving signals to ensure that they both contain
the correct information. Extensive experiments are conducted on popular
public-domain face aging datasets (FG-NET, MORPH Album 2, and CACD-VS) to
demonstrate the effectiveness of the proposed approach",8,2019-04-09 01:00:00
249,"Computer-based human face detection and recognition systems proliferate in various sectors such as universities, factories and other organisation [1]. This system refers to a branch of computer technology that can recognise human faces in digital images..",8,2020-01-01 00:00:00
250,"Cross modal face matching between the thermal and visible spectrum is a much
desired capability for night-time surveillance and security applications. Due
to a very large modality gap, thermal-to-visible face recognition is one of the
most challenging face matching problem. In this paper, we present an approach
to bridge this modality gap by a significant margin. Our approach captures the
highly non-linear relationship between the two modalities by using a deep
neural network. Our model attempts to learn a non-linear mapping from visible
to thermal spectrum while preserving the identity information. We show
substantive performance improvement on three difficult thermal-visible face
datasets. The presented approach improves the state-of-the-art by more than
10\% on UND-X1 dataset and by more than 15-30\% on NVESD dataset in terms of
Rank-1 identification. Our method bridges the drop in performance due to the
modality gap by more than 40\%",8,2016-07-07 00:00:00
251,"Examplers of a face are formed from multiple gallery images of a person and
are used in the process of classification of a test image. We incorporate such
examplers in forming a biologically inspired local binary decisions on
similarity based face recognition method. As opposed to single model approaches
such as face averages the exampler based approach results in higher recognition
accu- racies and stability. Using multiple training samples per person, the
method shows the following recognition accuracies: 99.0% on AR, 99.5% on FERET,
99.5% on ORL, 99.3% on EYALE, 100.0% on YALE and 100.0% on CALTECH face
databases. In addition to face recognition, the method also detects the natural
variability in the face images which can find application in automatic tagging
of face images",8,2012-01-28 00:00:00
252,"The model of low-dimensional manifold and sparse representation are two
well-known concise models that suggest each data can be described by a few
characteristics. Manifold learning is usually investigated for dimension
reduction by preserving some expected local geometric structures from the
original space to a low-dimensional one. The structures are generally
determined by using pairwise distance, e.g., Euclidean distance. Alternatively,
sparse representation denotes a data point as a linear combination of the
points from the same subspace. In practical applications, however, the nearby
points in terms of pairwise distance may not belong to the same subspace, and
vice versa. Consequently, it is interesting and important to explore how to get
a better representation by integrating these two models together. To this end,
this paper proposes a novel coding algorithm, called Locality-Constrained
Collaborative Representation (LCCR), which improves the robustness and
discrimination of data representation by introducing a kind of local
consistency. The locality term derives from a biologic observation that the
similar inputs have similar code. The objective function of LCCR has an
analytical solution, and it does not involve local minima. The empirical
studies based on four public facial databases, ORL, AR, Extended Yale B, and
Multiple PIE, show that LCCR is promising in recognizing human faces from
frontal views with varying expression and illumination, as well as various
corruptions and occlusions",11,2013-03-30 00:00:00
253,"This paper presents a computationally efficient yet powerful binary framework
for robust facial representation based on image gradients. It is termed as
structural binary gradient patterns (SBGP). To discover underlying local
structures in the gradient domain, we compute image gradients from multiple
directions and simplify them into a set of binary strings. The SBGP is derived
from certain types of these binary strings that have meaningful local
structures and are capable of resembling fundamental textural information. They
detect micro orientational edges and possess strong orientation and locality
capabilities, thus enabling great discrimination. The SBGP also benefits from
the advantages of the gradient domain and exhibits profound robustness against
illumination variations. The binary strategy realized by pixel correlations in
a small neighborhood substantially simplifies the computational complexity and
achieves extremely efficient processing with only 0.0032s in Matlab for a
typical face image. Furthermore, the discrimination power of the SBGP can be
enhanced on a set of defined orientational image gradient magnitudes, further
enforcing locality and orientation. Results of extensive experiments on various
benchmark databases illustrate significant improvements of the SBGP based
representations over the existing state-of-the-art local descriptors in the
terms of discrimination, robustness and complexity. Codes for the SBGP methods
will be available at
http://www.eee.manchester.ac.uk/research/groups/sisp/software/",8,2015-06-01 01:00:00
254,"This paper proposes a face recognition system that can be used to effectively match a face image scanned from an identity (ID) doc-ument against the face image stored in the biometric chip of such a document. The purpose of this specific face recognition algorithm is to aid the automatic detection of forged ID documents where the photography printed on the document’s surface has been altered or replaced. The proposed algorithm uses a novel combination of texture and shape features together with sub-space representation techniques. In addition, the robustness of the proposed algorithm when dealing with more general face recognition tasks has been proven with the Good, the Bad & the Ugly (GBU) dataset, one of the most challenging datasets containing frontal faces. The proposed algorithm has been complement-ed with a novel method that adopts two operating points to enhance the reliability of the algorithm’s final verification decision.Final Accepted Versio",8,2016-07-28 00:00:00
255,"Recurrent networks have been successful in analyzing temporal data and have
been widely used for video analysis. However, for video face recognition, where
the base CNNs trained on large-scale data already provide discriminative
features, using Long Short-Term Memory (LSTM), a popular recurrent network, for
feature learning could lead to overfitting and degrade the performance instead.
We propose a Recurrent Embedding Aggregation Network (REAN) for set to set face
recognition. Compared with LSTM, REAN is robust against overfitting because it
only learns how to aggregate the pre-trained embeddings rather than learning
representations from scratch. Compared with quality-aware aggregation methods,
REAN can take advantage of the context information to circumvent the noise
introduced by redundant video frames. Empirical results on three public domain
video face recognition datasets, IJB-S, YTF, and PaSC show that the proposed
REAN significantly outperforms naive CNN-LSTM structure and quality-aware
aggregation methods",11,2019-06-25 01:00:00
256,"Face recognition is a long standing challenge in the field of Artificial
Intelligence (AI). The goal is to create systems that accurately detect,
recognize, verify, and understand human faces. There are significant technical
hurdles in making these systems accurate, particularly in unconstrained
settings due to confounding factors related to pose, resolution, illumination,
occlusion, and viewpoint. However, with recent advances in neural networks,
face recognition has achieved unprecedented accuracy, largely built on
data-driven deep learning methods. While this is encouraging, a critical aspect
that is limiting facial recognition accuracy and fairness is inherent facial
diversity. Every face is different. Every face reflects something unique about
us. Aspects of our heritage - including race, ethnicity, culture, geography -
and our individual identify - age, gender, and other visible manifestations of
self-expression, are reflected in our faces. We expect face recognition to work
equally accurately for every face. Face recognition needs to be fair. As we
rely on data-driven methods to create face recognition technology, we need to
ensure necessary balance and coverage in training data. However, there are
still scientific questions about how to represent and extract pertinent facial
features and quantitatively measure facial diversity. Towards this goal,
Diversity in Faces (DiF) provides a data set of one million annotated human
face images for advancing the study of facial diversity. The annotations are
generated using ten well-established facial coding schemes from the scientific
literature. The facial coding schemes provide human-interpretable quantitative
measures of facial features. We believe that by making the extracted coding
schemes available on a large set of faces, we can accelerate research and
development towards creating more fair and accurate facial recognition systems",11,2019-04-08 01:00:00
257,"Pushing by big data and deep convolutional neural network (CNN), the
performance of face recognition is becoming comparable to human. Using private
large scale training datasets, several groups achieve very high performance on
LFW, i.e., 97% to 99%. While there are many open source implementations of CNN,
none of large scale face dataset is publicly available. The current situation
in the field of face recognition is that data is more important than algorithm.
To solve this problem, this paper proposes a semi-automatical way to collect
face images from Internet and builds a large scale dataset containing about
10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database,
we use a 11-layer CNN to learn discriminative representation and obtain
state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will
attract more research groups entering this field and accelerate the development
of face recognition in the wild",8,2014-11-28 00:00:00
258,"Face recognition systems are the need of time. We have applied de-noising technique and GLD method in conjunction with Viola-Jones, PCA-ANN and achieved recognition rate beyond expectation",8,2016-03-01 00:00:00
259,"In this paper, a novel technique named random subspace two-dimensional LDA
(RS-2DLDA) is developed for face recognition. This approach offers a number of
improvements over the random subspace two-dimensional PCA (RS2DPCA) framework
introduced by Nguyen et al. [5]. Firstly, the eigenvectors from 2DLDA have more
discriminative power than those from 2DPCA, resulting in higher accuracy for
the RS-2DLDA method over RS-2DPCA. Various distance metrics are evaluated, and
a weighting scheme is developed to further boost accuracy. A series of
experiments on the MORPH-II and ORL datasets are conducted to demonstrate the
effectiveness of this approach",11,2017-11-01 00:00:00
260,"In this paper we present an efficient implementation using triplet loss for
face recognition. We conduct the practical experiment to analyze the factors
that influence the training of triplet loss. All models are trained on
CASIA-Webface dataset and tested on LFW. We analyze the experiment results and
give some insights to help others balance the factors when they apply triplet
loss to their own problem especially for face recognition task. Code has been
released in https://github.com/yule-li/MassFace",11,2019-02-28 00:00:00
261,"In this paper, we propose a method to apply the popular cascade classifier
into face recognition to improve the computational efficiency while keeping
high recognition rate. In large scale face recognition systems, because the
probability of feature templates coming from different subjects is very high,
most of the matching pairs will be rejected by the early stages of the cascade.
Therefore, the cascade can improve the matching speed significantly. On the
other hand, using the nested structure of the cascade, we could drop some
stages at the end of feature to reduce the memory and bandwidth usage in some
resources intensive system while not sacrificing the performance too much. The
cascade is learned by two steps. Firstly, some kind of prepared features are
grouped into several nested stages. And then, the threshold of each stage is
learned to achieve user defined verification rate (VR). In the paper, we take a
landmark based Gabor+LDA face recognition system as baseline to illustrate the
process and advantages of the proposed method. However, the use of this method
is very generic and not limited in face recognition, which can be easily
generalized to other biometrics as a post-processing module. Experiments on the
FERET database show the good performance of our baseline and an experiment on a
self-collected large scale database illustrates that the cascade can improve
the matching speed significantly",8,2013-02-28 00:00:00
262,"Facial images in surveillance or mobile scenarios often have large view-point
variations in terms of pitch and yaw angles. These jointly occurred angle
variations make face recognition challenging. Current public face databases
mainly consider the case of yaw variations. In this paper, a new large-scale
Multi-yaw Multi-pitch high-quality database is proposed for Facial Pose
Analysis (M2FPA), including face frontalization, face rotation, facial pose
estimation and pose-invariant face recognition. It contains 397,544 images of
229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is
the most comprehensive multi-view face database for facial pose analysis.
Further, we provide an effective benchmark for face frontalization and
pose-invariant face recognition on M2FPA with several state-of-the-art methods,
including DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and
benchmark can significantly push forward the advance of facial pose analysis in
real-world applications. Moreover, a simple yet effective parsing guided
discriminator is introduced to capture the local consistency during GAN
optimization. Extensive quantitative and qualitative results on M2FPA and
Multi-PIE demonstrate the superiority of our face frontalization method.
Baseline results for both face synthesis and face recognition from
state-of-theart methods demonstrate the challenge offered by this new database",11,2019-10-20 01:00:00
263,"In this paper we present Deep Secure Encoding: a framework for secure
classification using deep neural networks, and apply it to the task of
biometric template protection for faces. Using deep convolutional neural
networks (CNNs), we learn a robust mapping of face classes to high entropy
secure codes. These secure codes are then hashed using standard hash functions
like SHA-256 to generate secure face templates. The efficacy of the approach is
shown on two face databases, namely, CMU-PIE and Extended Yale B, where we
achieve state of the art matching performance, along with cancelability and
high security with no unrealistic assumptions. Furthermore, the scheme can work
in both identification and verification modes",8,2015-06-13 01:00:00
264,"Face recognition is one of the most successful applications of image analysis and understanding and has gained much attention in recent years. Among many approaches to the problem of face recognition, appearance-based subspace analysis still gives the most promising results. In this paper we study the three most popular appearance-based face recognition projection methods (PCA, LDA and ICA). All methods are tested in equal working conditions regarding preprocessing and algorithm implementation on the FERET data set with its standard tests. We also compare the ICA method with its whitening preprocess and find out that there is no significant difference between them. When we compare different projection with different metrics we found out that the LDA+COS combination is the most promising for all tasks. The L1 metric gives the best results in

combination with PCA and ICA1, and COS is superior to any other metric when used with LDA and ICA2. Our results are compared to other studies and some discrepancies are pointed ou",11,2006-01-01 00:00:00
265,"Facial expressions convey non-verbal cues, which play an important role in
interpersonal relations. Automatic recognition of human face based on facial
expression can be an important component of natural human-machine interface. It
may also be used in behavioural science. Although human can recognize the face
practically without any effort, but reliable face recognition by machine is a
challenge. This paper presents a new approach for recognizing the face of a
person considering the expressions of the same human face at different
instances of time. This methodology is developed combining Eigenface method for
feature extraction and modified k-Means clustering for identification of the
human face. This method endowed the face recognition without using the
conventional distance measure classifiers. Simulation results show that
proposed face recognition using perception of k-Means clustering is useful for
face images with different facial expressions",8,2011-04-06 00:00:00
266,"Soodamani Ramalingam, 'Fuzzy interval-valued multi criteria based decision making for ranking features in multi-modal 3D face recognition', Fuzzy Sets and Systems, In Press version available online 13 June 2017. This is an Open Access paper, made available under the Creative Commons license CC BY 4.0 https://creativecommons.org/licenses/by/4.0/This paper describes an application of multi-criteria decision making (MCDM) for multi-modal fusion of features in a 3D face recognition system. A decision making process is outlined that is based on the performance of multi-modal features in a face recognition task involving a set of 3D face databases. In particular, the fuzzy interval valued MCDM technique called TOPSIS is applied for ranking and deciding on the best choice of multi-modal features at the decision stage. It provides a formal mechanism of benchmarking their performances against a set of criteria. The technique demonstrates its ability in scaling up the multi-modal features.Peer reviewedProo",8,2017-06-13 00:00:00
267,"In this paper, we propose a novel Global Norm-Aware Pooling (GNAP) block,
which reweights local features in a convolutional neural network (CNN)
adaptively according to their L2 norms and outputs a global feature vector with
a global average pooling layer. Our GNAP block is designed to give dynamic
weights to local features in different spatial positions without losing spatial
symmetry. We use a GNAP block in a face feature embedding CNN to produce
discriminative face feature vectors for pose-robust face recognition. The GNAP
block is of very cheap computational cost, but it is very powerful for
frontal-profile face recognition. Under the CFP frontal-profile protocol, the
GNAP block can not only reduce EER dramatically but also boost TPR@FPR=0.1%
(TPR i.e. True Positive Rate, FPR i.e. False Positive Rate) substantially. Our
experiments show that the GNAP block greatly promotes pose-robust face
recognition over the base model especially at low false positive rate",1,2018-08-01 01:00:00
268,"Face analysis techniques have become a crucial component of human-machine
interaction in the fields of assistive and humanoid robotics. However, the
variations in head-pose that arise naturally in these environments are still a
great challenge. In this paper, we present a real-time capable 3D face
modelling framework for 2D in-the-wild images that is applicable for robotics.
The fitting of the 3D Morphable Model is based exclusively on automatically
detected landmarks. After fitting, the face can be corrected in pose and
transformed back to a frontal 2D representation that is more suitable for face
recognition. We conduct face recognition experiments with non-frontal images
from the MUCT database and uncontrolled, in the wild images from the PaSC
database, the most challenging face recognition database to date, showing an
improved performance. Finally, we present our SCITOS G5 robot system, which
incorporates our framework as a means of image pre-processing for face
analysis",8,2016-06-01 01:00:00
269,"Face recognition difficulties are frequently documented in children with autism spectrum disorders (ASD). It has been hypothesized that these difficulties result from a reduced interest in faces early in life, leading to decreased cortical specialization and atypical development of the neural circuitry for face processing. However, a recent study by our lab demonstrated that infants at increased familial risk for ASD, irrespective of their diagnostic status at 3 years, exhibit a clear orienting response to faces. The present study was conducted as a follow-up on the same cohort to investigate how measures of early engagement with faces relate to face-processing abilities later in life. We also investigated whether face recognition difficulties are specifically related to an ASD diagnosis, or whether they are present at a higher rate in all those at familial risk. At 3 years we found a reduced ability to recognize unfamiliar faces in the high-risk group that was not specific to those children who received an ASD diagnosis, consistent with face recognition difficulties being an endophenotype of the disorder. Furthermore, we found that longer looking at faces at 7 months was associated with poorer performance on the face recognition task at 3 years in the high- risk group. These findings suggest that longer looking at faces in infants at risk for ASD might reflect early face-processing difficulties and predicts difficulties with recognizing faces later in life",4,1970-12-19 01:00:00
270,"In this paper we describe a solution to our entry for the emotion recognition
challenge EmotiW 2017. We propose an ensemble of several models, which capture
spatial and audio features from videos. Spatial features are captured by
convolutional neural networks, pretrained on large face recognition datasets.
We show that usage of strong industry-level face recognition networks increases
the accuracy of emotion recognition. Using our ensemble we improve on the
previous best result on the test set by about 1 %, achieving a 60.03 %
classification accuracy without any use of visual temporal information",11,2017-11-13 00:00:00
271,"Face recognition difficulties are frequently documented in children with autism spectrum disorders (ASD). It has been hypothesized that these difficulties result from a reduced interest in faces early in life, leading to decreased cortical specialization and atypical development of the neural circuitry for face processing. However, a recent study by our lab demonstrated that infants at increased familial risk for ASD, irrespective of their diagnostic status at 3 years, exhibit a clear orienting response to faces. The present study was conducted as a follow-up on the same cohort to investigate how measures of early engagement with faces relate to face-processing abilities later in life. We also investigated whether face recognition difficulties are specifically related to an ASD diagnosis, or whether they are present at a higher rate in all those at familial risk. At 3 years we found a reduced ability to recognize unfamiliar faces in the high-risk group that was not specific to those children who received an ASD diagnosis, consistent with face recognition difficulties being an endophenotype of the disorder. Furthermore, we found that longer looking at faces at 7 months was associated with poorer performance on the face recognition task at 3 years in the high- risk group. These findings suggest that longer looking at faces in infants at risk for ASD might reflect early face-processing difficulties and predicts difficulties with recognizing faces later in life",4,2014-07-01 00:00:00
272,"Face recognition approaches that are based on deep convolutional neural
networks (CNN) have been dominating the field. The performance improvements
they have provided in the so called in-the-wild datasets are significant,
however, their performance under image quality degradations have not been
assessed, yet. This is particularly important, since in real-world face
recognition applications, images may contain various kinds of degradations due
to motion blur, noise, compression artifacts, color distortions, and occlusion.
In this work, we have addressed this problem and analyzed the influence of
these image degradations on the performance of deep CNN-based face recognition
approaches using the standard LFW closed-set identification protocol. We have
evaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and
GoogLeNet. Results have indicated that blur, noise, and occlusion cause a
significant decrease in performance, while deep CNN models are found to be
robust to distortions, such as color distortions and change in color balance",11,2016-08-18 00:00:00
273,"Subspace-based holistic registration is introduced as an alternative to landmark-based face registration, which has a poor performance on low-resolution images, as obtained in camera surveillance applications. The proposed registration method finds the alignment by maximizing the similarity score between a probe and a gallery image. We use a novel probabilistic framework for both user-independent as well as user-specific face registration. The similarity is calculated using the probability that the face image is correctly aligned in a face subspace, but additionally we take the probability into account that the face is misaligned based on the residual error in the dimensions perpendicular to the face subspace. We perform extensive experiments on the FRGCv2 database to evaluate the impact that the face registration methods have on face recognition. Subspace-based holistic registration on low-resolution images can improve face recognition in comparison with landmark-based registration on high-resolution images. The performance of the tested face recognition methods after subspace-based holistic registration on a low-resolution version of the FRGC database is similar to that after manual registration",8,2010-01-01 00:00:00
274,"We propose a very simple, efficient yet surprisingly effective feature
extraction method for face recognition (about 20 lines of Matlab code), which
is mainly inspired by spatial pyramid pooling in generic image classification.
We show that features formed by simply pooling local patches over a multi-level
pyramid, coupled with a linear classifier, can significantly outperform most
recent face recognition methods. The simplicity of our feature extraction
procedure is demonstrated by the fact that no learning is involved (except PCA
whitening). We show that, multi-level spatial pooling and dense extraction of
multi-scale patches play critical roles in face image classification. The
extracted facial features can capture strong structural information of
individual faces with no label information being used. We also find that,
pre-processing on local image patches such as contrast normalization can have
an important impact on the classification accuracy. In particular, on the
challenging face recognition datasets of FERET and LFW-a, our method improves
previous best results by more than 10% and 20%, respectively",8,2014-09-17 01:00:00
275,"The performance of modern face recognition systems is a function of the
dataset on which they are trained. Most datasets are largely biased toward
""near-frontal"" views with benign lighting conditions, negatively effecting
recognition performance on images that do not meet these criteria. The proposed
approach demonstrates how a baseline training set can be augmented to increase
pose and lighting variability using semi-synthetic images with simulated pose
and lighting conditions. The semi-synthetic images are generated using a fast
and robust 3-d shape estimation and rendering pipeline which includes the full
head and background. Various methods of incorporating the semi-synthetic
renderings into the training procedure of a state of the art deep neural
network-based recognition system without modifying the structure of the network
itself are investigated. Quantitative results are presented on the challenging
IJB-A identification dataset using a state of the art recognition pipeline as a
baseline",8,2017-04-13 01:00:00
276,"We present a low-rank transformation approach to compensate for face
variations due to changes in visual domains, such as pose and illumination. The
key idea is to learn discriminative linear transformations for face images
using matrix rank as the optimization criteria. The learned linear
transformations restore a shared low-rank structure for faces from the same
subject, and, at the same time, force a high-rank structure for faces from
different subjects. In this way, among the transformed faces, we reduce
variations caused by domain changes within the classes, and increase
separations between the classes for better face recognition across domains.
Extensive experiments using public datasets are presented to demonstrate the
effectiveness of our approach for face recognition across domains. The
potential of the approach for feature extraction in generic object recognition
and coded aperture design are discussed as well",8,2013-08-01 01:00:00
277,"This article has been made available through the Brunel Open Access Publishing Fund.Face recognition is an interesting and a challenging problem that has been widely studied in the field of pattern recognition and computer vision. It has many applications such as biometric authentication, video surveillance, and others. In the past decade, several methods for face recognition were proposed. However, these methods suffer from pose and illumination variations. In order to address these problems, this paper proposes a novel methodology to recognize the face images. Since image gradients are invariant to illumination and pose variations, the proposed approach uses gradient orientation to handle these effects. The Schur decomposition is used for matrix decomposition and then Schurvalues and Schurvectors are extracted for subspace projection. We call this subspace projection of face features as Schurfaces, which is numerically stable and have the ability of handling defective matrices. The Hausdorff distance is used with the nearest neighbor classifier to measure the similarity between different faces. Experiments are conducted with Yale face database and ORL face database. The results show that the proposed approach is highly discriminant and achieves a promising accuracy for face recognition than the state-of-the-art approaches",8,2014-01-01 00:00:00
278,"Despite significant recent advances in the field of face recognition,
implementing face verification and recognition efficiently at scale presents
serious challenges to current approaches. In this paper we present a system,
called FaceNet, that directly learns a mapping from face images to a compact
Euclidean space where distances directly correspond to a measure of face
similarity. Once this space has been produced, tasks such as face recognition,
verification and clustering can be easily implemented using standard techniques
with FaceNet embeddings as feature vectors.
  Our method uses a deep convolutional network trained to directly optimize the
embedding itself, rather than an intermediate bottleneck layer as in previous
deep learning approaches. To train, we use triplets of roughly aligned matching
/ non-matching face patches generated using a novel online triplet mining
method. The benefit of our approach is much greater representational
efficiency: we achieve state-of-the-art face recognition performance using only
128-bytes per face.
  On the widely used Labeled Faces in the Wild (LFW) dataset, our system
achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves
95.12%. Our system cuts the error rate in comparison to the best published
result by 30% on both datasets.
  We also introduce the concept of harmonic embeddings, and a harmonic triplet
loss, which describe different versions of face embeddings (produced by
different networks) that are compatible to each other and allow for direct
comparison between each other",8,2015-06-17 00:00:00
279,"PCANet was proposed as a lightweight deep learning network that mainly
leverages Principal Component Analysis (PCA) to learn multistage filter banks
followed by binarization and block-wise histograming. PCANet was shown worked
surprisingly well in various image classification tasks. However, PCANet is
data-dependence hence inflexible. In this paper, we proposed a
data-independence network, dubbed DCTNet for face recognition in which we adopt
Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is
motivated by the fact that 2D DCT basis is indeed a good approximation for high
ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated
sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is
free from learning as 2D DCT bases can be computed in advance. Besides that, we
also proposed an effective method to regulate the block-wise histogram feature
vector of DCTNet for robustness. It is shown to provide surprising performance
boost when the probe image is considerably different in appearance from the
gallery image. We evaluate the performance of DCTNet extensively on a number of
benchmark face databases and being able to achieve on par with or often better
accuracy performance than PCANet",8,2015-09-29 00:00:00
280,"Face recognition has been widely studied due to its importance in different
applications; however, most of the proposed methods fail when face images are
occluded or captured under illumination and pose variations. Recently several
low-rank dictionary learning methods have been proposed and achieved promising
results for noisy observations. While these methods are mostly developed for
single-modality scenarios, recent studies demonstrated the advantages of
feature fusion from multiple inputs. We propose a multi-modal structured
low-rank dictionary learning method for robust face recognition, using raw
pixels of face images and their illumination invariant representation. The
proposed method learns robust and discriminative representations from
contaminated face images, even if there are few training samples with large
intra-class variations. Extensive experiments on different datasets validate
the superior performance and robustness of our method to severe illumination
variations and occlusion",8,2017-03-14 00:00:00
281,"In this paper, we propose a non-frontal model based approach which ensures that a face recognition system always gets to compare images having similar view (or pose). This requires a virtual suspect reference set that consists of non-frontal suspect images having pose similar to the surveillance view trace image. We apply the 3D model reconstruction followed by image synthesis approach to the frontal view mug shot images in the suspect reference set in order to create such a virtual suspect reference set. This strategy not only ensures a stable 3D face model reconstruction because of the relatively good quality mug shot suspect images but also provides a practical solution for forensic cases where the trace is often of very low quality. For most face recognition algorithms, the relative pose difference between the test and reference image is one of the major causes of severe degradation in recognition performance. Moreover, given appropriate training, comparing a pair of non-frontal images is no more difficult that comparing frontal view images",4,2012-01-01 00:00:00
282,"In this paper, we present a deep coupled framework to address the problem of
matching sketch image against a gallery of mugshots. Face sketches have the
essential in- formation about the spatial topology and geometric details of
faces while missing some important facial attributes such as ethnicity, hair,
eye, and skin color. We propose a cou- pled deep neural network architecture
which utilizes facial attributes in order to improve the sketch-photo
recognition performance. The proposed Attribute-Assisted Deep Con- volutional
Neural Network (AADCNN) method exploits the facial attributes and leverages the
loss functions from the facial attributes identification and face verification
tasks in order to learn rich discriminative features in a common em- bedding
subspace. The facial attribute identification task increases the inter-personal
variations by pushing apart the embedded features extracted from individuals
with differ- ent facial attributes, while the verification task reduces the
intra-personal variations by pulling together all the fea- tures that are
related to one person. The learned discrim- inative features can be well
generalized to new identities not seen in the training data. The proposed
architecture is able to make full use of the sketch and complementary fa- cial
attribute information to train a deep model compared to the conventional
sketch-photo recognition methods. Exten- sive experiments are performed on
composite (E-PRIP) and semi-forensic (IIIT-D semi-forensic) datasets. The
results show the superiority of our method compared to the state- of-the-art
models in sketch-photo recognition algorithm",8,2018-07-31 01:00:00
283,"This paper demonstrates two different fusion techniques at two different
levels of a human face recognition process. The first one is called data fusion
at lower level and the second one is the decision fusion towards the end of the
recognition process. At first a data fusion is applied on visual and
corresponding thermal images to generate fused image. Data fusion is
implemented in the wavelet domain after decomposing the images through
Daubechies wavelet coefficients (db2). During the data fusion maximum of
approximate and other three details coefficients are merged together. After
that Principle Component Analysis (PCA) is applied over the fused coefficients
and finally two different artificial neural networks namely Multilayer
Perceptron(MLP) and Radial Basis Function(RBF) networks have been used
separately to classify the images. After that, for decision fusion based
decisions from both the classifiers are combined together using Bayesian
formulation. For experiments, IRIS thermal/visible Face Database has been used.
Experimental results show that the performance of multiple classifier system
along with decision fusion works well over the single classifier system",8,2011-06-17 00:00:00
284,"This paper proposes a novel face recognition algorithm based on large-scale
supervised hierarchical feature learning. The approach consists of two parts:
hierarchical feature learning and large-scale model learning. The hierarchical
feature learning searches feature in three levels of granularity in a
supervised way. First, face images are modeled by receptive field theory, and
the representation is an image with many channels of Gaussian receptive maps.
We activate a few most distinguish channels by supervised learning. Second, the
face image is further represented by patches of picked channels, and we search
from the over-complete patch pool to activate only those most discriminant
patches. Third, the feature descriptor of each patch is further projected to
lower dimension subspace with discriminant subspace analysis.
  Learned feature of activated patches are concatenated to get a full face
representation.A linear classifier is learned to separate face pairs from same
subjects and different subjects. As the number of face pairs are extremely
large, we introduce ADMM (alternative direction method of multipliers) to train
the linear classifier on a computing cluster. Experiments show that more
training samples will bring notable accuracy improvement.
  We conduct experiments on FRGC and LFW. Results show that the proposed
approach outperforms existing algorithms under the same protocol notably.
Besides, the proposed approach is small in memory footprint, and low in
computing cost, which makes it suitable for embedded applications",8,2014-07-06 01:00:00
285,"Face recognition (FR) systems for video surveillance (VS) applications
attempt to accurately detect the presence of target individuals over a
distributed network of cameras. In video-based FR systems, facial models of
target individuals are designed a priori during enrollment using a limited
number of reference still images or video data. These facial models are not
typically representative of faces being observed during operations due to large
variations in illumination, pose, scale, occlusion, blur, and to camera
inter-operability. Specifically, in still-to-video FR application, a single
high-quality reference still image captured with still camera under controlled
conditions is employed to generate a facial model to be matched later against
lower-quality faces captured with video cameras under uncontrolled conditions.
Current video-based FR systems can perform well on controlled scenarios, while
their performance is not satisfactory in uncontrolled scenarios mainly because
of the differences between the source (enrollment) and the target (operational)
domains. Most of the efforts in this area have been toward the design of robust
video-based FR systems in unconstrained surveillance environments. This chapter
presents an overview of recent advances in still-to-video FR scenario through
deep convolutional neural networks (CNNs). In particular, deep learning
architectures proposed in the literature based on triplet-loss function (e.g.,
cross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and
supervised autoencoders (e.g., canonical face representation CNN) are reviewed
and compared in terms of accuracy and computational complexity",11,2018-06-27 01:00:00
286,"Face detection and recognition benchmarks have shifted toward more difficult
environments. The challenge presented in this paper addresses the next step in
the direction of automatic detection and identification of people from outdoor
surveillance cameras. While face detection has shown remarkable success in
images collected from the web, surveillance cameras include more diverse
occlusions, poses, weather conditions and image blur. Although face
verification or closed-set face identification have surpassed human
capabilities on some datasets, open-set identification is much more complex as
it needs to reject both unknown identities and false accepts from the face
detector. We show that unconstrained face detection can approach high detection
rates albeit with moderate false accept rates. By contrast, open-set face
recognition is currently weak and requires much more attention",8,2018-01-01 00:00:00
287,"Face recognition systems must be robust to the variation of various factors
such as facial expression, illumination, head pose and aging. Especially, the
robustness against illumination variation is one of the most important problems
to be solved for the practical use of face recognition systems. Gabor wavelet
is widely used in face detection and recognition because it gives the
possibility to simulate the function of human visual system. In this paper, we
propose a method for extracting Gabor wavelet features which is stable under
the variation of local illumination and show experiment results demonstrating
its effectiveness",8,2012-12-11 00:00:00
288,"Face recognition achieves exceptional success thanks to the emergence of deep
learning. However, many contemporary face recognition models still perform
relatively poor in processing profile faces compared to frontal faces. A key
reason is that the number of frontal and profile training faces are highly
imbalanced - there are extensively more frontal training samples compared to
profile ones. In addition, it is intrinsically hard to learn a deep
representation that is geometrically invariant to large pose variations. In
this study, we hypothesize that there is an inherent mapping between frontal
and profile faces, and consequently, their discrepancy in the deep
representation space can be bridged by an equivariant mapping. To exploit this
mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,
which is capable of adaptively adding residuals to the input deep
representation to transform a profile face representation to a canonical pose
that simplifies recognition. The DREAM block consistently enhances the
performance of profile face recognition for many strong deep networks,
including ResNet models, without deliberately augmenting training data of
profile faces. The block is easy to use, light-weight, and can be implemented
with a negligible computational overhead",11,2018-03-02 00:00:00
289,"Representation based classification (RC) methods such as sparse RC (SRC) have
shown great potential in face recognition in recent years. Most previous RC
methods are based on the conventional regression models, such as lasso
regression, ridge regression or group lasso regression. These regression models
essentially impose a predefined assumption on the distribution of the noise
variable in the query sample, such as the Gaussian or Laplacian distribution.
However, the complicated noises in practice may violate the assumptions and
impede the performance of these RC methods. In this paper, we propose a modal
regression based atomic representation and classification (MRARC) framework to
alleviate such limitation. Unlike previous RC methods, the MRARC framework does
not require the noise variable to follow any specific predefined distributions.
This gives rise to the capability of MRARC in handling various complex noises
in reality. Using MRARC as a general platform, we also develop four novel RC
methods for unimodal and multimodal face recognition, respectively. In
addition, we devise a general optimization algorithm for the unified MRARC
framework based on the alternating direction method of multipliers (ADMM) and
half-quadratic theory. The experiments on real-world data validate the efficacy
of MRARC for robust face recognition",11,2017-11-04 00:00:00
290,"As facial appearance is subject to significant intra-class variations caused
by the aging process over time, age-invariant face recognition (AIFR) remains a
major challenge in face recognition community. To reduce the intra-class
discrepancy caused by the aging, in this paper we propose a novel approach
(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep
face features. Specifically, we decompose deep face features into two
orthogonal components to represent age-related and identity-related features.
As a result, identity-related features that are robust to aging are then used
for AIFR. Besides, for complementing the existing cross-age datasets and
advancing the research in this field, we construct a brand-new large-scale
Cross-Age Face dataset (CAF). Extensive experiments conducted on the three
public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have
shown the effectiveness of the proposed approach and the value of the
constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most
popular general face recognition (GFR) dataset LFW additionally demonstrates
the comparable generalization performance on GFR",8,2018-10-17 01:00:00
291,"Cross modal face matching between the thermal and visible spectrum is a much
de- sired capability for night-time surveillance and security applications. Due
to a very large modality gap, thermal-to-visible face recognition is one of the
most challenging face matching problem. In this paper, we present an approach
to bridge this modality gap by a significant margin. Our approach captures the
highly non-linear relationship be- tween the two modalities by using a deep
neural network. Our model attempts to learn a non-linear mapping from visible
to thermal spectrum while preserving the identity in- formation. We show
substantive performance improvement on a difficult thermal-visible face
dataset. The presented approach improves the state-of-the-art by more than 10%
in terms of Rank-1 identification and bridge the drop in performance due to the
modality gap by more than 40%",8,2015-07-10 01:00:00
292,"Face recognition has made extraordinary progress owing to the advancement of
deep convolutional neural networks (CNNs). The central task of face
recognition, including face verification and identification, involves face
feature discrimination. However, the traditional softmax loss of deep CNNs
usually lacks the power of discrimination. To address this problem, recently
several loss functions such as center loss, large margin softmax loss, and
angular softmax loss have been proposed. All these improved losses share the
same idea: maximizing inter-class variance and minimizing intra-class variance.
In this paper, we propose a novel loss function, namely large margin cosine
loss (LMCL), to realize this idea from a different perspective. More
specifically, we reformulate the softmax loss as a cosine loss by $L_2$
normalizing both features and weight vectors to remove radial variations, based
on which a cosine margin term is introduced to further maximize the decision
margin in the angular space. As a result, minimum intra-class variance and
maximum inter-class variance are achieved by virtue of normalization and cosine
decision margin maximization. We refer to our model trained with LMCL as
CosFace. Extensive experimental evaluations are conducted on the most popular
public-domain face recognition datasets such as MegaFace Challenge, Youtube
Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art
performance on these benchmarks, which confirms the effectiveness of our
proposed approach",11,2018-04-03 01:00:00
293,"Heterogeneous Face Recognition (HFR) is a challenging issue because of the
large domain discrepancy and a lack of heterogeneous data. This paper considers
HFR as a dual generation problem, and proposes a novel Dual Variational
Generation (DVG) framework. It generates large-scale new paired heterogeneous
images with the same identity from noise, for the sake of reducing the domain
gap of HFR. Specifically, we first introduce a dual variational autoencoder to
represent a joint distribution of paired heterogeneous images. Then, in order
to ensure the identity consistency of the generated paired heterogeneous
images, we impose a distribution alignment in the latent space and a pairwise
identity preserving in the image space. Moreover, the HFR network reduces the
domain discrepancy by constraining the pairwise feature distances between the
generated paired heterogeneous images. Extensive experiments on four HFR
databases show that our method can significantly improve state-of-the-art
results. The related code is available at https://github.com/BradyFU/DVG",8,2019-12-01 00:00:00
294,"The objective of this work is to provide an efficient face recognition scheme useful for video indexing applications. In
particular we are addressing the following problem: given a set of known images and given a video sequence to be
indexed, find where the corresponding persons appear in the sequence. Conventional face detection schemes are not
well suited for this application and alternate and more efficient schemes have to be developed. In this paper we have
modified our original generic eigenface-based recognition scheme presented in [1] by introducing the concept of selfeigenfaces.
The resulting scheme is very efficient to find specific face images and to cope with the different face
conditions present in a video sequence. The main and final objective is to develop a tool to be used in the MPEG-7
standardization effort to help video indexing activities. Good results have been obtained using the video test sequences
used in the MPEG-7 evaluation group.Peer ReviewedPostprint (published version",8,2000-01-01 00:00:00
