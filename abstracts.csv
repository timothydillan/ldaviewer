,abstract
0,"Compressive Sensing (CS) theory asserts that sparse signal reconstruction is
possible from a small number of linear measurements. Although CS enables
low-cost linear sampling, it requires non-linear and costly reconstruction.
Recent literature works show that compressive image classification is possible
in CS domain without reconstruction of the signal. In this work, we introduce a
DCT base method that extracts binary discriminative features directly from CS
measurements. These CS measurements can be obtained by using (i) a random or a
pseudo-random measurement matrix, or (ii) a measurement matrix whose elements
are learned from the training data to optimize the given classification task.
We further introduce feature fusion by concatenating Bag of Words (BoW)
representation of our binary features with one of the two state-of-the-art
CNN-based feature vectors. We show that our fused feature outperforms the
state-of-the-art in both cases.Comment: 6 pages, submitted/accepted, EUVIP 201"
1,"Convolutional Neural Network(CNN) has been widely used for image recognition
with great success. However, there are a number of limitations of the current
CNN based image recognition paradigm. First, the receptive field of CNN is
generally fixed, which limits its recognition capacity when the input image is
very large. Second, it lacks the computational scalability for dealing with
images with different sizes. Third, it is quite different from human visual
system for image recognition, which involves both feadforward and recurrent
proprocessing. This paper proposes a different paradigm of image recognition,
which can take advantages of variable scales of the input images, has more
computational scalabilities, and is more similar to image recognition by human
visual system. It is based on recurrent neural network (RNN) defined on image
scale with an embeded base CNN, which is named Scale Recurrent Neural
Network(SRNN). This RNN based approach makes it easier to deal with images with
variable sizes, and allows us to borrow existing RNN techniques, such as LSTM
and GRU, to further enhance the recognition accuracy. Our experiments show that
the recognition accuracy of a base CNN can be significantly boosted using the
proposed SRNN models. It also significantly outperforms the scale ensemble
method, which integrate the results of performing CNN to the input image at
different scales, although the computational overhead of using SRNN is
negligible"
2,"This paper presents an integration of image forgery detection with image
facial recognition using black propagation neural network (BPNN). We observed
that facial image recognition by itself will always give a matching output or
closest possible output image for every input image irrespective of the
authenticity or otherwise not of the testing input image. Based on this, we are
proposing the combination of the blind but powerful automation image forgery
detection for entire input images for the BPNN recognition program. Hence, an
input image must first be authenticated before being fed into the recognition
program. Thus, an image security identification and authentication requirement,
any image that fails the authentication/verification stage are not to be used
as an input/test image. In addition, the universal smart GUI tool is proposed
and designed to perform image forgery detection with the high accuracy of 2%
error rate.Comment: Circuit and Systems (CAS) Conference. Pp.120-125. ISBN:
  978-1-4673-311"
3,"Traditional image recognition involves identifying the key object in a
portrait-type image with a single object focus (ILSVRC, AlexNet, and VGG). More
recent approaches consider dense image recognition - segmenting an image with
appropriate bounding boxes and performing image recognition within these
bounding boxes (Semantic segmentation). The Visual Genome dataset [5] is an
attempt to bridge these various approaches to a cohesive dataset for each
subtask - bounding box generation, image recognition, captioning, and a new
operation: scene graph generation. Our focus is on using such scene graphs to
perform graph search on image databases to holistically retrieve images based
on a search criteria. We develop a method to store scene graphs and metadata in
graph databases (using Neo4J) and to perform fast approximate retrieval of
images based on a graph search query. We process more complex queries than
single object search, e.g. ""girl eating cake"" retrieves images that contain the
specified relation as well as variations"
4,"Any subset of the plane can be approximated by a set of square pixels. This
transition from a shape to its pixelation is rather brutal since it destroys
geometric and topological information about the shape. Using a technique
inspired by Morse Theory, we algorithmically produce a PL approximation of the
original shape using only information from its pixelation. This approximation
converges to the original shape in a very strong sense: as the size of the
pixels goes to zero we can recover important geometric and topological
invariants of the original shape such as Betti numbers, area, perimeter and
curvature measures"
5,"Place recognition is one of the most fundamental topics in computer vision
and robotics communities, where the task is to accurately and efficiently
recognize the location of a given query image. Despite years of wisdom
accumulated in this field, place recognition still remains an open problem due
to the various ways in which the appearance of real-world places may differ.
This paper presents an overview of the place recognition literature. Since
condition invariant and viewpoint invariant features are essential factors to
long-term robust visual place recognition system, We start with traditional
image description methodology developed in the past, which exploit techniques
from image retrieval field. Recently, the rapid advances of related fields such
as object detection and image classification have inspired a new technique to
improve visual place recognition system, i.e., convolutional neural networks
(CNNs). Thus we then introduce recent progress of visual place recognition
system based on CNNs to automatically learn better image representations for
places. Eventually, we close with discussions and future work of place
recognition.Comment: Applied Sciences (2018"
6,"Document image has been the area of research for a couple of decades because
of its potential application in the area of text recognition, line recognition
or any other shape recognition from the image. For most of these purposes
binarization of image becomes mandatory as far as recognition is concerned.
Throughout couple decades standard algorithms have already been developed for
this purpose. Some of these algorithms are applicable to degraded image also.
Our objective behind this work is to study the existing techniques, compare
them in view of advantages and disadvantages and modify some of these
algorithms to optimize time or performance.Comment: National Conference on Computing and Communication Systems
  (COCOSYS-09), UIT, Burdwan, January 02-04, 2009, pp. 71-7"
7,"Visual recognition under adverse conditions is a very important and
challenging problem of high practical value, due to the ubiquitous existence of
quality distortions during image acquisition, transmission, or storage. While
deep neural networks have been extensively exploited in the techniques of
low-quality image restoration and high-quality image recognition tasks
respectively, few studies have been done on the important problem of
recognition from very low-quality images. This paper proposes a deep learning
based framework for improving the performance of image and video recognition
models under adverse conditions, using robust adverse pre-training or its
aggressive variant. The robust adverse pre-training algorithms leverage the
power of pre-training and generalizes conventional unsupervised pre-training
and data augmentation methods. We further develop a transfer learning approach
to cope with real-world datasets of unknown adverse conditions. The proposed
framework is comprehensively evaluated on a number of image and video
recognition benchmarks, and obtains significant performance improvements under
various single or mixed adverse conditions. Our visualization and analysis
further add to the explainability of results"
8,"Recognition of low resolution face images is a challenging problem in many
practical face recognition systems. Methods have been proposed in the face
recognition literature for the problem which assume that the probe is low
resolution, but a high resolution gallery is available for recognition. These
attempts have been aimed at modifying the probe image such that the resultant
image provides better discrimination. We formulate the problem differently by
leveraging the information available in the high resolution gallery image and
propose a dictionary learning approach for classifying the low-resolution probe
image. An important feature of our algorithm is that it can handle resolution
change along with illumination variations. Furthermore, we also kernelize the
algorithm to handle non-linearity in data and present a joint dictionary
learning technique for robust recognition at low resolutions. The effectiveness
of the proposed method is demonstrated using standard datasets and a
challenging outdoor face dataset. It is shown that our method is efficient and
can perform significantly better than many competitive low resolution face
recognition algorithms"
9,"We present an experimental study to demonstrate the effect of the time difference in image acquisition for gallery and probe on the performance of ear recognition. This experimental research is the first study on the time effect on ear biometrics. For the purpose of recognition, we convolve banana wavelets with an ear image and then apply local binary pattern on the convolved image. The histograms of the produced image are then used as features to describe an ear. A histogram intersection technique is then applied on the histograms of two ears to measure the ear similarity for the recognition purposes. We also use analysis of variance (ANOVA) to select features to identify the best banana wavelets for the recognition process. The experimental results show that the recognition rate is only slightly reduced by time. The average recognition rate of 98.5% is achieved for an eleven month-difference between gallery and probe on an un-occluded ear dataset of 1491 images of ears selected from Southampton University ear database"
10,"Attendance Management System (AMS) can be made into smarter way by using face
recognition technique, where we use a CCTV camera to be fixed at the entry
point of a classroom, which automatically captures the image of the person and
checks the observed image with the face database using android enhanced smart
phone. It is typically used for two purposes. Firstly, marking attendance for
student by comparing the face images produced recently and secondly,
recognition of human who are strange to the environment i.e. an unauthorized
person For verification of image, a newly emerging trend 3D Face Recognition is
used which claims to provide more accuracy in matching the image databases and
has an ability to recognize a subject at different view angles.Comment: 20 pages,4 figure"
11,"State recognition of food images can be considered as one of the promising
applications of object recognition and fine-grained image classification in
computer vision. In this paper, evidence is provided for the power of
convolutional neural network (CNN) for food state recognition, even with a
small data set. In this study, we fine-tuned a CNN initially trained on a large
natural image recognition dataset (Imagenet ILSVRC) and transferred the learned
feature representations to the food state recognition task. A small-scale
dataset consisting of 5978 images of seven categories was constructed and
annotated manually. Data augmentation was applied to increase the size of the
data.Comment: 5 pages, 7 figure"
12,"In order to perform object recognition it is necessary to learn representations of the underlying components of images.  Such components correspond to objects, object-parts, or features.  Non-negative matrix factorisation is a generative model that has been specifically proposed for finding such meaningful representations of image data, through the use of non-negativity constraints on the factors.  This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints"
13,"The main finding of this work is that the standard image classification
pipeline, which consists of dictionary learning, feature encoding, spatial
pyramid pooling and linear classification, outperforms all state-of-the-art
face recognition methods on the tested benchmark datasets (we have tested on
AR, Extended Yale B, the challenging FERET, and LFW-a datasets). This
surprising and prominent result suggests that those advances in generic image
classification can be directly applied to improve face recognition systems. In
other words, face recognition may not need to be viewed as a separate object
classification problem.
  While recently a large body of residual based face recognition methods focus
on developing complex dictionary learning algorithms, in this work we show that
a dictionary of randomly extracted patches (even from non-face images) can
achieve very promising results using the image classification pipeline. That
means, the choice of dictionary learning methods may not be important. Instead,
we find that learning multiple dictionaries using different low-level image
features often improve the final classification accuracy. Our proposed face
recognition approach offers the best reported results on the widely-used face
recognition benchmark datasets. In particular, on the challenging FERET and
LFW-a datasets, we improve the best reported accuracies in the literature by
about 20% and 30% respectively.Comment: 10 page"
14,"Face sketch synthesis has wide applications ranging from digital
entertainments to law enforcements. Objective image quality assessment scores
and face recognition accuracy are two mainly used tools to evaluate the
synthesis performance. In this paper, we proposed a synthesized face sketch
recognition framework based on full-reference image quality assessment metrics.
Synthesized sketches generated from four state-of-the-art methods are utilized
to test the performance of the proposed recognition framework. For the image
quality assessment metrics, we employed the classical structured similarity
index metric and other three prevalent metrics: visual information fidelity,
feature similarity index metric and gradient magnitude similarity deviation.
Extensive experiments compared with baseline methods illustrate the
effectiveness of the proposed synthesized face sketch recognition framework.
Data and implementation code in this paper are available online at
www.ihitworld.com/WNN/IQA_Sketch.zip"
15,"The Low-Power Image Recognition Challenge (LPIRC,
https://rebootingcomputing.ieee.org/lpirc) is an annual competition started in
2015. The competition identifies the best technologies that can classify and
detect objects in images efficiently (short execution time and low energy
consumption) and accurately (high precision). Over the four years, the winners'
scores have improved more than 24 times. As computer vision is widely used in
many battery-powered systems (such as drones and mobile phones), the need for
low-power computer vision will become increasingly important. This paper
summarizes LPIRC 2018 by describing the three different tracks and the winners'
solutions.Comment: 13 pages, workshop in 2018 CVPR, competition, low-power, image
  recognitio"
16,"Optical Character Recognition deals in recognition and classification of
characters from an image. For the recognition to be accurate, certain
topological and geometrical properties are calculated, based on which a
character is classified and recognized. Also, the Human psychology perceives
characters by its overall shape and features such as strokes, curves,
protrusions, enclosures etc. These properties, also called Features are
extracted from the image by means of spatial pixel-based calculation. A
collection of such features, called Vectors, help in defining a character
uniquely, by means of an Artificial Neural Network that uses these Feature
Vectors.Comment: Signal & Image Processing : An International Journal (SIPIJ) Vol.3,
  No.5, October 201"
17,"Face image quality is an important factor in facial recognition systems as
its verification and recognition accuracy is highly dependent on the quality of
image presented. Rejecting low quality images can significantly increase the
accuracy of any facial recognition system. In this project, a simple approach
is presented to train a deep convolutional neural network to perform end-to-end
face image quality assessment. The work is done in 2 stages : First, generation
of quality score label and secondly, training a deep convolutional neural
network in a supervised manner to predict quality score between 0 and 1. The
generation of quality labels is done by comparing the face image with a
template of best quality images and then evaluating the normalized score based
on the similarity.Comment: Course project repor"
18,"Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC & COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.Comment: Tech repor"
19,"Automated facial identification and facial expression recognition have been
topics of active research over the past few decades. Facial and expression
recognition find applications in human-computer interfaces, subject tracking,
real-time security surveillance systems and social networking. Several holistic
and geometric methods have been developed to identify faces and expressions
using public and local facial image databases. In this work we present the
evolution in facial image data sets and the methodologies for facial
identification and recognition of expressions such as anger, sadness,
happiness, disgust, fear and surprise. We observe that most of the earlier
methods for facial and expression recognition aimed at improving the
recognition rates for facial feature-based methods using static images.
However, the recent methodologies have shifted focus towards robust
implementation of facial/expression recognition from large image databases that
vary with space (gathered from the internet) and time (video recordings). The
evolution trends in databases and methodologies for facial and expression
recognition can be useful for assessing the next-generation topics that may
have applications in security systems or personal identification systems that
involve ""Quantitative face"" assessments.Comment: 16 pages, 4 figures, 3 tables, International Journal of Computer
  Science and Engineering Survey, October, 201"
20,"The convolution layer has been the dominant feature extractor in computer
vision for years. However, the spatial aggregation in convolution is basically
a pattern matching process that applies fixed filters which are inefficient at
modeling visual elements with varying spatial distributions. This paper
presents a new image feature extractor, called the local relation layer, that
adaptively determines aggregation weights based on the compositional
relationship of local pixel pairs. With this relational approach, it can
composite visual elements into higher-level entities in a more efficient manner
that benefits semantic inference. A network built with local relation layers,
called the Local Relation Network (LR-Net), is found to provide greater
modeling capacity than its counterpart built with regular convolution on
large-scale recognition tasks such as ImageNet classification"
21,"In the domain of Biometrics, recognition systems based on iris, fingerprint
or palm print scans etc. are often considered more dependable due to extremely
low variance in the properties of these entities with respect to time. However,
over the last decade data processing capability of computers has increased
manifold, which has made real-time video content analysis possible. This shows
that the need of the hour is a robust and highly automated Face Detection and
Recognition algorithm with credible accuracy rate. The proposed Face Detection
and Recognition system using Discrete Wavelet Transform (DWT) accepts face
frames as input from a database containing images from low cost devices such as
VGA cameras, webcams or even CCTV's, where image quality is inferior. Face
region is then detected using properties of L*a*b* color space and only Frontal
Face is extracted such that all additional background is eliminated. Further,
this extracted image is converted to grayscale and its dimensions are resized
to 128 x 128 pixels. DWT is then applied to entire image to obtain the
coefficients. Recognition is carried out by comparison of the DWT coefficients
belonging to the test image with those of the registered reference image. On
comparison, Euclidean distance classifier is deployed to validate the test
image from the database. Accuracy for various levels of DWT Decomposition is
obtained and hence, compared.Comment: discrete wavelet transform, face detection, face recognition, person
  identificatio"
22,"Recently, deep convolutional neural network (DCNN) achieved increasingly
remarkable success and rapidly developed in the field of natural image
recognition. Compared with the natural image, the scale of remote sensing image
is larger and the scene and the object it represents are more macroscopic. This
study inquires whether remote sensing scene and natural scene recognitions
differ and raises the following questions: What are the key factors in remote
sensing scene recognition? Is the DCNN recognition mechanism centered on object
recognition still applicable to the scenarios of remote sensing scene
understanding? We performed several experiments to explore the influence of the
DCNN structure and the scale of remote sensing scene understanding from the
perspective of scene complexity. Our experiment shows that understanding a
complex scene depends on an in-depth network and multiple-scale perception.
Using a visualization method, we qualitatively and quantitatively analyze the
recognition mechanism in a complex remote sensing scene and demonstrate the
importance of multi-objective joint semantic support"
23,"In this paper a vision-based vehicles recognition method is presented.
Proposed method uses fuzzy description of image segments for automatic
recognition of vehicles recorded in image data. The description takes into
account selected geometrical properties and shape coefficients determined for
segments of reference image (vehicle model). The proposed method was
implemented using reasoning system with fuzzy rules. A vehicles recognition
algorithm was developed based on the fuzzy rules describing shape and
arrangement of the image segments that correspond to visible parts of a
vehicle. An extension of the algorithm with set of fuzzy rules defined for
different reference images (and various vehicle shapes) enables vehicles
classification in traffic scenes. The devised method is suitable for
application in video sensors for road traffic control and surveillance systems.Comment: The final publication is available at http://www.springerlink.co"
24,"This paper investigates, using prior shape models and the concept of ball
scale (b-scale), ways of automatically recognizing objects in 3D images without
performing elaborate searches or optimization. That is, the goal is to place
the model in a single shot close to the right pose (position, orientation, and
scale) in a given image so that the model boundaries fall in the close vicinity
of object boundaries in the image. This is achieved via the following set of
key ideas: (a) A semi-automatic way of constructing a multi-object shape model
assembly. (b) A novel strategy of encoding, via b-scale, the pose relationship
between objects in the training images and their intensity patterns captured in
b-scale images. (c) A hierarchical mechanism of positioning the model, in a
one-shot way, in a given image from a knowledge of the learnt pose relationship
and the b-scale image of the given image to be segmented. The evaluation
results on a set of 20 routine clinical abdominal female and male CT data sets
indicate the following: (1) Incorporating a large number of objects improves
the recognition accuracy dramatically. (2) The recognition algorithm can be
thought as a hierarchical framework such that quick replacement of the model
assembly is defined as coarse recognition and delineation itself is known as
finest recognition. (3) Scale yields useful information about the relationship
between the model assembly and any given image such that the recognition
results in a placement of the model close to the actual pose without doing any
elaborate searches or optimization. (4) Effective object recognition can make
delineation most accurate.Comment: This paper was published and presented in SPIE Medical Imaging 201"
25,"This paper proposes a data driven model to predict the performance of a face
recognition system based on image quality features. We model the relationship
between image quality features (e.g. pose, illumination, etc.) and recognition
performance measures using a probability density function. To address the issue
of limited nature of practical training data inherent in most data driven
models, we have developed a Bayesian approach to model the distribution of
recognition performance measures in small regions of the quality space. Since
the model is based solely on image quality features, it can predict performance
even before the actual recognition has taken place. We evaluate the performance
predictive capabilities of the proposed model for six face recognition systems
(two commercial and four open source) operating on three independent data sets:
MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can
accurately predict performance using an accurate and unbiased Image Quality
Assessor (IQA). Furthermore, our experiments highlight the impact of the
unaccounted quality space -- the image quality features not considered by IQA
-- in contributing to performance prediction errors.Comment: Submitted to TPAMI journal on Apr. 22, 2015. Decision of ""Revise and
  resubmit as new"" received on Sep. 10, 2015. At present, updating the paper to
  address the feedback and concerns of the two reviewers. The re-submitted
  paper will be uploaded as version 2 on arXi"
26,"Over many decades, researchers working in object recognition have longed for
an end-to-end automated system that will simply accept 2D or 3D image or videos
as inputs and output the labels of objects in the input data. Computer vision
methods that use representations derived based on geometric, radiometric and
neural considerations and statistical and structural matchers and artificial
neural network-based methods where a multi-layer network learns the mapping
from inputs to class labels have provided competing approaches for image
recognition problems. Over the last four years, methods based on Deep
Convolutional Neural Networks (DCNNs) have shown impressive performance
improvements on object detection/recognition challenge problems. This has been
made possible due to the availability of large annotated data, a better
understanding of the non-linear mapping between image and class labels as well
as the affordability of GPUs. In this paper, we present a brief history of
developments in computer vision and artificial neural networks over the last
forty years for the problem of image-based recognition. We then present the
design details of a deep learning system for end-to-end unconstrained face
verification/recognition. Some open issues regarding DCNNs for object
recognition problems are then discussed. We caution the readers that the views
expressed in this paper are from the authors and authors only!Comment: 7 page"
27,"We present a new approach for face recognition system. The method is based on
2D face image features using subset of non-correlated and Orthogonal Gabor
Filters instead of using the whole Gabor Filter Bank, then compressing the
output feature vector using Linear Discriminant Analysis (LDA). The face image
has been enhanced using multi stage image processing technique to normalize it
and compensate for illumination variation. Experimental results show that the
proposed system is effective for both dimension reduction and good recognition
performance when compared to the complete Gabor filter bank. The system has
been tested using CASIA, ORL and Cropped YaleB 2D face images Databases and
achieved average recognition rate of 98.9 %"
28,"CONFIGR (CONtour FIgure GRound) is a computational model based on principles of biological vision that completes sparse and  noisy image figures. Within an integrated vision/recognition system, CONFIGR posits an initial recognition stage which identifies figure pixels from spatially local input information. The resulting, and typically incomplete, figure is fed back to the “early vision” stage for long-range completion via filling-in. The reconstructed image is then re-presented to the recognition system for global functions such as object recognition. In the CONFIGR algorithm, the smallest independent image unit is the visible pixel, whose size defines a computational spatial scale. Once pixel size is fixed, the entire algorithm is fully determined, with no additional parameter choices. Multi-scale simulations illustrate the vision/recognition system. Open-source CONFIGR code is available online, but all examples can be derived analytically, and the design principles applied at each step are transparent. The model balances filling-in as figure against complementary filling-in as ground, which blocks spurious figure completions. Lobe computations occur on a subpixel spatial scale. Originally designed to fill-in missing contours in an incomplete image such as a dashed line, the same CONFIGR system connects and segments sparse dots, and unifies occluded objects from pieces locally identified as figure in the initial recognition stage. The model self-scales its completion distances, filling-in across gaps of any length, where unimpeded, while limiting connections among dense image-figure pixel groups that already have intrinsic form. Long-range image completion promises to play an important role in adaptive processors that reconstruct images from highly compressed video and still camera images.Air Force Office of Scientific Research (F49620-01-1-0423); National Geospatial-Intelligence Agency (NMA 201-01-1-0216); National Science Foundation (SBE-0354378); Office of Naval Research (N000014-01-1-0624"
29,"We address the task of recognizing objects from video input. This important
problem is relatively unexplored, compared with image-based object recognition.
To this end, we make the following contributions. First, we introduce two
comprehensive datasets for video-based object recognition. Second, we propose
Latent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-based
object recognition. LBSVM is based on Structured-Output SVM, but extends it to
handle noisy video data and ensure consistency of the output decision
throughout time. We apply LBSVM to recognize office objects and museum
sculptures, and we demonstrate its benefits over image-based, set-based, and
other video-based object recognition"
30,"The analysis of historical documents is still a topical issue given the
importance of information that can be extracted and also the importance given
by the institutions to preserve their heritage. The main idea in order to
characterize the content of the images of ancient documents after attempting to
clean the image is segmented blocks texts from the same image and tries to find
similar blocks in either the same image or the entire image database. Most
approaches of offline handwriting recognition proceed by segmenting words into
smaller pieces (usually characters) which are recognized separately.
Recognition of a word then requires the recognition of all characters (OCR)
that compose it. Our work focuses mainly on the characterization of classes in
images of old documents. We use Som toolbox for finding classes in documents.
We applied also fractal dimensions and points of interest to categorize and
match ancient documents.Comment: 10 page"
31,"This book constitutes the refereed proceedings of the 18th Scandinavian Conference on Image Analysis, SCIA 2013, held in Espoo, Finland, in June 2013. The 67 revised full papers presented were carefully reviewed and selected from 132 submissions. The papers are organized in topical sections on feature extraction and segmentation, pattern recognition and machine learning, medical and biomedical image analysis, faces and gestures, object and scene recognition, matching, registration, and alignment, 3D vision, color and multispectral image analysis, motion analysis, systems and applications, human-centered computing, and video and multimedia analysis"
32,"The popular Biometric used to authenticate a person is Fingerprint which is
unique and permanent throughout a person's life. A minutia matching is widely
used for fingerprint recognition and can be classified as ridge ending and
ridge bifurcation. In this paper we projected Fingerprint Recognition using
Minutia Score Matching method (FRMSM). For Fingerprint thinning, the Block
Filter is used, which scans the image at the boundary to preserves the quality
of the image and extract the minutiae from the thinned image. The false
matching ratio is better compared to the existing algorithm.Comment: 8 Page"
33,"Weather Recognition plays an important role in our daily lives and many
computer vision applications. However, recognizing the weather conditions from
a single image remains challenging and has not been studied thoroughly.
Generally, most previous works treat weather recognition as a single-label
classification task, namely, determining whether an image belongs to a specific
weather class or not. This treatment is not always appropriate, since more than
one weather conditions may appear simultaneously in a single image. To address
this problem, we make the first attempt to view weather recognition as a
multi-label classification task, i.e., assigning an image more than one labels
according to the displayed weather conditions. Specifically, a CNN-RNN based
multi-label classification approach is proposed in this paper. The
convolutional neural network (CNN) is extended with a channel-wise attention
model to extract the most correlated visual features. The Recurrent Neural
Network (RNN) further processes the features and excavates the dependencies
among weather classes. Finally, the weather labels are predicted step by step.
Besides, we construct two datasets for the weather recognition task and explore
the relationships among different weather conditions. Experimental results
demonstrate the superiority and effectiveness of the proposed approach. The new
constructed datasets will be available at
https://github.com/wzgwzg/Multi-Label-Weather-Recognition.Comment: One weather recognition dataset is constructe"
34,"Poster presentation: Introduction We here focus on constructing a hierarchical neural system for position-invariant recognition, which is one of the most fundamental invariant recognition achieved in visual processing [1,2]. The invariant recognition have been hypothesized to be done by matching a sensory image of a particular object stimulated on the retina to the most suitable representation stored in memory of the higher visual cortical area. Here arises a general problem: In such a visual processing, the position of the object image on the retina must be initially uncertain. Furthermore, the retinal activities possessing sensory information are being far from the ones in the higher area with a loss of the sensory object information. Nevertheless, with such recognition ambiguity, the particular object can effortlessly and easily be recognized. Our aim in this work is an attempt to resolve such a general recognition problem. .."
35,"In this paper, we generate and control semantically interpretable filters
that are directly learned from natural images in an unsupervised fashion. Each
semantic filter learns a visually interpretable local structure in conjunction
with other filters. The significance of learning these interpretable filter
sets is demonstrated on two contrasting applications. The first application is
image recognition under progressive decolorization, in which recognition
algorithms should be color-insensitive to achieve a robust performance. The
second application is image quality assessment where objective methods should
be sensitive to color degradations. In the proposed work, the sensitivity and
lack thereof are controlled by weighing the semantic filters based on the local
structures they represent. To validate the proposed approach, we utilize the
CURE-TSR dataset for image recognition and the TID 2013 dataset for image
quality assessment. We show that the proposed semantic filter set achieves
state-of-the-art performances in both datasets while maintaining its robustness
across progressive distortions.Comment: 5 pages, 5 figures, 1 tabl"
36,"Heterogeneous face recognition between color image and depth image is a much
desired capacity for real world applications where shape information is looked
upon as merely involved in gallery. In this paper, we propose a cross-modal
deep learning method as an effective and efficient workaround for this
challenge. Specifically, we begin with learning two convolutional neural
networks (CNNs) to extract 2D and 2.5D face features individually. Once
trained, they can serve as pre-trained models for another two-way CNN which
explores the correlated part between color and depth for heterogeneous
matching. Compared with most conventional cross-modal approaches, our method
additionally conducts accurate depth image reconstruction from single color
image with Conditional Generative Adversarial Nets (cGAN), and further enhances
the recognition performance by fusing multi-modal matching results. Through
both qualitative and quantitative experiments on benchmark FRGC 2D/3D face
database, we demonstrate that the proposed pipeline outperforms
state-of-the-art performance on heterogeneous face recognition and ensures a
drastically efficient on-line stage"
37,"In the paper, we combined DSP processor with image processing algorithm and
studied the method of water meter character recognition. We collected water
meter image through camera at a fixed angle, and the projection method is used
to recognize those digital images. The experiment results show that the method
can recognize the meter characters accurately and artificial meter reading is
replaced by automatic digital recognition, which improves working efficiency"
38,"What is the current state-of-the-art for image restoration and enhancement
applied to degraded images acquired under less than ideal circumstances? Can
the application of such algorithms as a pre-processing step to improve image
interpretability for manual analysis or automatic visual recognition to
classify scene content? While there have been important advances in the area of
computational photography to restore or enhance the visual quality of an image,
the capabilities of such techniques have not always translated in a useful way
to visual recognition tasks. Consequently, there is a pressing need for the
development of algorithms that are designed for the joint problem of improving
visual appearance and recognition, which will be an enabling factor for the
deployment of visual recognition tools in many real-world scenarios. To address
this, we introduce the UG^2 dataset as a large-scale benchmark composed of
video imagery captured under challenging conditions, and two enhancement tasks
designed to test algorithmic impact on visual quality and automatic object
recognition. Furthermore, we propose a set of metrics to evaluate the joint
improvement of such tasks as well as individual algorithmic advances, including
a novel psychophysics-based evaluation regime for human assessment and a
realistic set of quantitative measures for object recognition performance. We
introduce six new algorithms for image restoration or enhancement, which were
created as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR
2018. Under the proposed evaluation regime, we present an in-depth analysis of
these algorithms and a host of deep learning-based and classic baseline
approaches. From the observed results, it is evident that we are in the early
days of building a bridge between computational photography and visual
recognition, leaving many opportunities for innovation in this area.Comment: CVPR Prize Challenge: http://www.ug2challenge.or"
39,"International Conference Image Analysis and Recognition (ICIAR 2018, Póvoa de Varzim, Portugal"
40,"In many real-world applications, image data often come with noises,
corruptions or large errors. One approach to deal with noise image data is to
use data recovery techniques which aim to recover the true uncorrupted signals
from the observed noise images. In this paper, we first introduce a novel
corruption recovery transformation (CRT) model which aims to recover multiple
(or a collection of) corrupted images using a single affine transformation.
Then, we show that the introduced CRT can be efficiently constructed through
learning from training data. Once CRT is learned, we can recover the true
signals from the new incoming/test corrupted images explicitly. As an
application, we apply our CRT to image recognition task. Experimental results
on six image datasets demonstrate that the proposed CRT model is effective in
recovering noise image data and thus leads to better recognition results"
41,"Deep neural networks demonstrate to have a high performance on image
classification tasks while being more difficult to train. Due to the complexity
and vanishing gradient problem, it normally takes a lot of time and more
computational power to train deeper neural networks. Deep residual networks
(ResNets) can make the training process faster and attain more accuracy
compared to their equivalent neural networks. ResNets achieve this improvement
by adding a simple skip connection parallel to the layers of convolutional
neural networks. In this project we first design a ResNet model that can
perform the image classification task on the Tiny ImageNet dataset with a high
accuracy, then we compare the performance of this ResNet model with its
equivalent Convolutional Network (ConvNet). Our findings illustrate that
ResNets are more prone to overfitting despite their higher accuracy. Several
methods to prevent overfitting such as adding dropout layers and stochastic
augmentation of the training dataset has been studied in this work.Comment: 6 pages, 9 figure"
42,"In this paper, we propose the use of a semantic image, an improved
representation for video analysis, principally in combination with Inception
networks. The semantic image is obtained by applying localized sparse
segmentation using global clustering (LSSGC) prior to the approximate rank
pooling which summarizes the motion characteristics in single or multiple
images. It incorporates the background information by overlaying a static
background from the window onto the subsequent segmented frames. The idea is to
improve the action-motion dynamics by focusing on the region which is important
for action recognition and encoding the temporal variances using the frame
ranking method. We also propose the sequential combination of
Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the
temporal variances for improved recognition performance. Extensive analysis has
been carried out on UCF101 and HMDB51 datasets which are widely used in action
recognition studies. We show that (i) the semantic image generates better
activations and converges faster than its original variant, (ii) using
segmentation prior to approximate rank pooling yields better recognition
performance, (iii) The use of LSTM leverages the temporal variance information
from approximate rank pooling to model the action behavior better than the base
network, (iv) the proposed representations can be adaptive as they can be used
with existing methods such as temporal segment networks to improve the
recognition performance, and (v) our proposed four-stream network architecture
comprising of semantic images and semantic optical flows achieves
state-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101
and HMDB51, respectively.Comment: 30 page"
43,"Recent studies have shown that a Deep Convolutional Neural Network (DCNN)
pretrained on a large image dataset can be used as a universal image
descriptor, and that doing so leads to impressive performance for a variety of
image classification tasks. Most of these studies adopt activations from a
single DCNN layer, usually the fully-connected layer, as the image
representation. In this paper, we proposed a novel way to extract image
representations from two consecutive convolutional layers: one layer is
utilized for local feature extraction and the other serves as guidance to pool
the extracted features. By taking different viewpoints of convolutional layers,
we further develop two schemes to realize this idea. The first one directly
uses convolutional layers from a DCNN. The second one applies the pretrained
CNN on densely sampled image regions and treats the fully-connected activations
of each image region as convolutional feature activations. We then train
another convolutional layer on top of that as the pooling-guidance
convolutional layer. By applying our method to three popular visual
classification tasks, we find our first scheme tends to perform better on the
applications which need strong discrimination on subtle object patterns within
small regions while the latter excels in the cases that require discrimination
on category-level patterns. Overall, the proposed method achieves superior
performance over existing ways of extracting image representations from a DCNN.Comment: Fixed typos. Journal extension of arXiv:1411.7466. Accepted to IEEE
  Transactions on Pattern Analysis and Machine Intelligenc"
44,"Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset"
45,"We have benchmarked the maximum obtainable recognition accuracy on various
word image datasets using manual segmentation and a currently available
commercial OCR. We have developed a Matlab program, with graphical user
interface, for semi-automated pixel level segmentation of word images. We
discuss the advantages of pixel level annotation. We have covered five
databases adding up to over 3600 word images. These word images have been
cropped from camera captured scene, born-digital and street view images. We
recognize the segmented word image using the trial version of Nuance Omnipage
OCR. We also discuss, how the degradations introduced during acquisition or
inaccuracies introduced during creation of word images affect the recognition
of the word present in the image. Word images for different kinds of
degradations and correction for slant and curvy nature of words are also
discussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,
Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,
88.5% and 86.7% respectively.Comment: 16 pages, 4 figure"
46,"The task of multi-label image recognition is to predict a set of object
labels that present in an image. As objects normally co-occur in an image, it
is desirable to model the label dependencies to improve the recognition
performance. To capture and explore such important dependencies, we propose a
multi-label classification model based on Graph Convolutional Network (GCN).
The model builds a directed graph over the object labels, where each node
(label) is represented by word embeddings of a label, and GCN is learned to map
this label graph into a set of inter-dependent object classifiers. These
classifiers are applied to the image descriptors extracted by another sub-net,
enabling the whole network to be end-to-end trainable. Furthermore, we propose
a novel re-weighted scheme to create an effective label correlation matrix to
guide information propagation among the nodes in GCN. Experiments on two
multi-label image recognition datasets show that our approach obviously
outperforms other existing state-of-the-art methods. In addition, visualization
analyses reveal that the classifiers learned by our model maintain meaningful
semantic topology.Comment: To appear at CVPR 2019 (Source codes have been released on
  https://github.com/chenzhaomin123/ML_GCN"
47,"In this paper we develop a Quality Assessment approach for face recognition
based on deep learning. The method consists of a Convolutional Neural Network,
FaceQnet, that is used to predict the suitability of a specific input image for
face recognition purposes. The training of FaceQnet is done using the VGGFace2
database. We employ the BioLab-ICAO framework for labeling the VGGFace2 images
with quality information related to their ICAO compliance level. The
groundtruth quality labels are obtained using FaceNet to generate comparison
scores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making
it capable of returning a numerical quality measure for each input image.
Finally, we verify if the FaceQnet scores are suitable to predict the expected
performance when employing a specific image for face recognition with a COTS
face recognition system. Several conclusions can be drawn from this work, most
notably: 1) we managed to employ an existing ICAO compliance framework and a
pretrained CNN to automatically label data with quality information, 2) we
trained FaceQnet for quality estimation by fine-tuning a pre-trained face
recognition network (ResNet-50), and 3) we have shown that the predictions from
FaceQnet are highly correlated with the face recognition accuracy of a
state-of-the-art commercial system not used during development. FaceQnet is
publicly available in GitHub.Comment: Preprint version of a paper accepted at ICB 201"
48,"This paper presents a cluster oriented image retrieval system with context recognition mechanism for selection subspaces of color features. Our idea to implement a context in the image retrieval system is how to recognize the most important features in the image search by connecting the user impression to the query. We apply a context recognition with Mathematical Model of Meaning (MMM) and then make a projection to the color features with a color impression metric. After a user gives a context, the MMM retrieves the highest correlated words to the context. These representative words are projected to the color impression metric to obtain the most significant colors for subspace feature selection. After applying subspace selection, the system then clusters the image database using Pillar-Kmeans algorithm. The centroids of clustering results are used for calculating the similarity measurements to the image query. We perform our proposed system for experimental purpose with the Ukiyo-e image datasets from Tokyo Metropolitan Library for representing the Japanese cultural image collections"
49,"In order to improve the accuracy of face recognition under varying
illumination conditions, a local texture enhanced illumination normalization
method based on fusion of differential filtering images (FDFI-LTEIN) is
proposed to weaken the influence caused by illumination changes. Firstly, the
dynamic range of the face image in dark or shadowed regions is expanded by
logarithmic transformation. Then, the global contrast enhanced face image is
convolved with difference of Gaussian filters and difference of bilateral
filters, and the filtered images are weighted and merged using a coefficient
selection rule based on the standard deviation (SD) of image, which can enhance
image texture information while filtering out most noise. Finally, the local
contrast equalization (LCE) is performed on the fused face image to reduce the
influence caused by over or under saturated pixel values in highlight or dark
regions. Experimental results on the Extended Yale B face database and CMU PIE
face database demonstrate that the proposed method is more robust to
illumination changes and achieve higher recognition accuracy when compared with
other illumination normalization methods and a deep CNNs based illumination
invariant face recognition methodComment: 10 page"
50,"Deep convolutional neural networks have recently achieved state-of-the-art
performance on a number of image recognition benchmarks, including the ImageNet
Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on
the localization sub-task was a network that predicts a single bounding box and
a confidence score for each object category in the image. Such a model captures
the whole-image context around the objects but cannot handle multiple instances
of the same object in the image without naively replicating the number of
outputs for each instance. In this work, we propose a saliency-inspired neural
network model for detection, which predicts a set of class-agnostic bounding
boxes along with a single score for each box, corresponding to its likelihood
of containing any object of interest. The model naturally handles a variable
number of instances for each class and allows for cross-class generalization at
the highest levels of the network. We are able to obtain competitive
recognition performance on VOC2007 and ILSVRC2012, while using only the top few
predicted locations in each image and a small number of neural network
evaluations"
51,"Automatic face recognition has received significant performance improvement
by developing specialised facial image representations. On the other hand,
generic object recognition has rarely been applied to the face recognition.
Spatial pyramid pooling of features encoded by an over-complete dictionary has
been the key component of many state-of-the-art image classification systems.
Inspired by its success, in this work we develop a new face image
representation method inspired by the second-order pooling in Carreira et al.
[1], which was originally proposed for image segmentation. The proposed method
differs from the previous methods in that, we encode the densely extracted
local patches by a small-size dictionary; and the facial image signatures are
obtained by pooling the second-order statistics of the encoded features. We
show the importance of pooling on encoded features, which is bypassed by the
original second-order pooling method to avoid the high computational cost.
Equipped with a simple linear classifier, the proposed method outperforms the
state-of-the-art face identification performance by large margins. For example,
on the LFW databases, the proposed method performs better than the previous
best by around 13% accuracy.Comment: 9 page"
52,"The requirement of large amounts of annotated images has become one grand
challenge while training deep neural network models for various visual
detection and recognition tasks. This paper presents a novel image synthesis
technique that aims to generate a large amount of annotated scene text images
for training accurate and robust scene text detection and recognition models.
The proposed technique consists of three innovative designs. First, it realizes
""semantic coherent"" synthesis by embedding texts at semantically sensible
regions within the background image, where the semantic coherence is achieved
by leveraging the semantic annotations of objects and image regions that have
been created in the prior semantic segmentation research. Second, it exploits
visual saliency to determine the embedding locations within each semantic
sensible region, which coincides with the fact that texts are often placed
around homogeneous regions for better visibility in scenes. Third, it designs
an adaptive text appearance model that determines the color and brightness of
embedded texts by learning from the feature of real scene text images
adaptively. The proposed technique has been evaluated over five public datasets
and the experiments show its superior performance in training accurate and
robust scene text detection and recognition models.Comment: 14 pages, ECCV2018, datasets:
  https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scene"
53,"We describe CITlab's recognition system for the HTRtS competition attached to
the 13. International Conference on Document Analysis and Recognition, ICDAR
2015. The task comprises the recognition of historical handwritten documents.
The core algorithms of our system are based on multi-dimensional recurrent
neural networks (MDRNN) and connectionist temporal classification (CTC). The
software modules behind that as well as the basic utility technologies are
essentially powered by PLANET's ARGUS framework for intelligent text
recognition and image processing.Comment: Description of CITlab's System for the HTRtS 2015 Task : Handwritten
  Text Recognition on the tranScriptorium Datase"
54,"In this paper, we design and evaluate a convolutional autoencoder that
perturbs an input face image to impart privacy to a subject. Specifically, the
proposed autoencoder transforms an input face image such that the transformed
image can be successfully used for face recognition but not for gender
classification. In order to train this autoencoder, we propose a novel training
scheme, referred to as semi-adversarial training in this work. The training is
facilitated by attaching a semi-adversarial module consisting of a pseudo
gender classifier and a pseudo face matcher to the autoencoder. The objective
function utilized for training this network has three terms: one to ensure that
the perturbed image is a realistic face image; another to ensure that the
gender attributes of the face are confounded; and a third to ensure that
biometric recognition performance due to the perturbed image is not impacted.
Extensive experiments confirm the efficacy of the proposed architecture in
extending gender privacy to face images"
55,"Recommendation systems based on image recognition could prove a vital tool in
enhancing the experience of museum audiences. However, for practical systems
utilizing wearable cameras, a number of challenges exist which affect the
quality of image recognition. In this pilot study, we focus on recognition of
museum collections by using a wearable camera in three different museum spaces.
We discuss the application of wearable cameras, and the practical and technical
challenges in devising a robust system that can recognize artworks viewed by
the visitors to create a detailed record of their visit. Specifically, to
illustrate the impact of different kinds of museum spaces on image recognition,
we collect three training datasets of museum exhibits containing variety of
paintings, clocks, and sculptures. Subsequently, we equip selected visitors
with wearable cameras to capture artworks viewed by them as they stroll along
exhibitions. We use Convolutional Neural Networks (CNN) which are pre-trained
on the ImageNet dataset and fine-tuned on each of the training sets for the
purpose of artwork identification. In the testing stage, we use CNNs to
identify artworks captured by the visitors with a wearable camera. We analyze
the accuracy of their recognition and provide an insight into the applicability
of such a system to further engage audiences with museum exhibitions"
56,"The minutia descriptor which describes characteristics of minutia, plays a
major role in fingerprint recognition. Typically, fingerprint recognition
systems employ minutia descriptors to find potential correspondence between
minutiae, and they use similarity between two minutia descriptors to calculate
overall similarity between two fingerprint images. A good minutia descriptor
can improve recognition accuracy of fingerprint recognition system and largely
reduce comparing time. A good minutia descriptor should have high ability to
distinguish between different minutiae and at the same time should be robust in
difficult conditions including poor quality image and small size image. It also
should be effective in computational cost of similarity among descriptors. In
this paper, a robust minutia descriptor is constructed using Gabor wavelet and
linear discriminant analysis. This minutia descriptor has high distinguishing
ability, stability and simple comparing method. Experimental results on FVC2004
and FVC2006 databases show that the proposed minutia descriptor is very
effective in fingerprint recognition"
57,"In this paper, we introduce a large-scale, controlled, and multi-platform
object recognition dataset denoted as Challenging Unreal and Real Environments
for Object Recognition (CURE-OR). In this dataset, there are 1,000,000 images
of 100 objects with varying size, color, and texture that are positioned in
five different orientations and captured using five devices including a webcam,
a DSLR, and three smartphone cameras in real-world (real) and studio (unreal)
environments. The controlled challenging conditions include underexposure,
overexposure, blur, contrast, dirty lens, image noise, resizing, and loss of
color information. We utilize CURE-OR dataset to test recognition APIs-Amazon
Rekognition and Microsoft Azure Computer Vision- and show that their
performance significantly degrades under challenging conditions. Moreover, we
investigate the relationship between object recognition and image quality and
show that objective quality algorithms can estimate recognition performance
under certain photometric challenging conditions. The dataset is publicly
available at https://ghassanalregib.com/cure-or/.Comment: 8 pages, 7 figures, 4 table"
58,"The three-dimensional Anderson model is a well-studied model of disordered
electron systems that shows the delocalization--localization transition. As in
our previous papers on two- and three-dimensional (2D, 3D) quantum phase
transitions [J. Phys. Soc. Jpn. {\bf 85}, 123706 (2016), {\bf 86}, 044708
(2017)], we used an image recognition algorithm based on a multilayered
convolutional neural network. However, in contrast to previous papers in which
2D image recognition was used, we applied 3D image recognition to analyze
entire 3D wave functions. We show that a full phase diagram of the
disorder-energy plane is obtained once the 3D convolutional neural network has
been trained at the band center. We further demonstrate that the full phase
diagram for 3D quantum bond and site percolations can be drawn by training the
3D Anderson model at the band center.Comment: 11 pages, 5 figures. Published versio"
59,"For the automatic recognition of earth resources from ERTS-1 digital tapes, both multispectral and spatial pattern recognition techniques are important. Recognition of terrain types is based on spatial signatures that become evident by processing small portions of an image through selected algorithms. An investigation of spatial signatures that are applicable to ERTS-1 MSS images is described. Artifacts in the spatial signatures seem to be related to the multispectral scanner. A method for suppressing such artifacts is presented. Finally, results of terrain type recognition for one ERTS-1 image are presented"
60,"We consider dimensionality reduction methods for face recognition in a
supervised setting, using an image-as-matrix representation. A common procedure
is to project image matrices into a smaller space in which the recognition is
performed. These methods are often called ""two-dimensional"" in the literature
and there exist counterparts that use an image-as-vector representation. When
two face images are close to each other in the input space they may remain
close after projection - but this is not desirable in the situation when these
two images are from different classes, and this often affects the recognition
performance. We extend a previously developed `repulsion Laplacean' technique
based on adding terms to the objective function with the goal or creation a
repulsion energy between such images in the projected space. This scheme, which
relies on a repulsion graph, is generic and can be incorporated into various
two-dimensional methods. It can be regarded as a multilinear generalization of
the repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.
2392--2402]. Experimental results demonstrate that the proposed methodology
offers significant recognition improvement relative to the underlying
two-dimensional methods.Comment: 25 pages, 8 figures, 6 tables, unpublished manuscrip"
61,"The performance of the Fingerprint recognition system will be more accurate
with respect of enhancement for the fingerprint images. In this paper we
develop a novel method for Fingerprint image contrast enhancement technique
based on the discrete wavelet transform (DWT) and singular value decomposition
(SVD) has been proposed. This technique is compared with conventional image
equalization techniques such as standard general histogram equalization and
local histogram equalization. An automatic histogram threshold approach based
on a fuzziness measure is presented. Then, using an index of fuzziness, a
similarity process is started to find the threshold point. A significant
contrast between ridges and valleys of the best, medium and poor finger image
features to extract from finger images and get maximum recognition rate using
fuzzy measures. The experimental results show the recognition of superiority of
the proposed method to get maximum performance up gradation to the
implementation of this approach.Comment: Submitted to Journal of Computer Science and Engineering, see
  http://sites.google.com/site/jcseuk/volume-6-issue-1-marc"
62,"Enhancing low resolution images via super-resolution or image synthesis for
cross-resolution face recognition has been well studied. Several image
processing and machine learning paradigms have been explored for addressing the
same. In this research, we propose Synthesis via Deep Sparse Representation
algorithm for synthesizing a high resolution face image from a low resolution
input image. The proposed algorithm learns multi-level sparse representation
for both high and low resolution gallery images, along with an identity aware
dictionary and a transformation function between the two representations for
face identification scenarios. With low resolution test data as input, the high
resolution test image is synthesized using the identity aware dictionary and
transformation which is then used for face recognition. The performance of the
proposed SDSR algorithm is evaluated on four databases, including one real
world dataset. Experimental results and comparison with existing seven
algorithms demonstrate the efficacy of the proposed algorithm in terms of both
face identification and image quality measures"
63,"Although the image recognition has been a research topic for many years, many
researchers still have a keen interest in it[1]. In some papers[2][3][4],
however, there is a tendency to compare models only on one or two datasets,
either because of time restraints or because the model is tailored to a
specific task. Accordingly, it is hard to understand how well a certain model
generalizes across image recognition field[6]. In this paper, we compare four
neural networks on MNIST dataset[5] with different division. Among them, three
are Convolutional Neural Networks (CNN)[7], Deep Residual Network (ResNet)[2]
and Dense Convolutional Network (DenseNet)[3] respectively, and the other is
our improvement on CNN baseline through introducing Capsule Network
(CapsNet)[1] to image recognition area. We show that the previous models
despite do a quite good job in this area, our retrofitting can be applied to
get a better performance. The result obtained by CapsNet is an accuracy rate of
99.75\%, and it is the best result published so far. Another inspiring result
is that CapsNet only needs a small amount of data to get excellent performance.
Finally, we will apply CapsNet's ability to generalize in other image
recognition field in the future.Comment: TPW course essay. arXiv admin note: text overlap with
  arXiv:1709.04219 by other author"
64,"The recognition of materials and objects inside transparent containers using
computer vision has a wide range of applications, ranging from industrial
bottles filling to the automation of chemistry laboratory. One of the main
challenges in such recognition is the ability to distinguish between image
features resulting from the vessels surface and image features resulting from
the material inside the vessel. Reflections and the functional parts of a
vessels surface can create strong edges that can be mistakenly identified as
corresponding to the vessel contents, and cause recognition errors. The ability
to evaluate whether a specific edge in an image stems from the vessels surface
or from its contents can considerably improve the ability to identify materials
inside transparent vessels. This work will suggest a method for such
evaluation, based on the following two assumptions: 1) Areas of high curvature
on the vessel surface are likely to cause strong edges due to changes in
reflectivity, as is the appearance of functional parts (e.g. corks or valves).
2) Most transparent vessels (bottles, glasses) have high symmetry
(cylindrical). As a result the curvature angle of the vessels surface at each
point of the image is similar to the curvature angle of the contour line of the
vessel in the same row in the image. These assumptions, allow the
identification of image regions with strong edges corresponding to the vessel
surface reflections. Combining this method with existing image analysis methods
for detecting materials inside transparent containers allows considerable
improvement in accuracy"
65,"Convolutional Neural Networks (CNN) has achieved a great success in image
recognition task by automatically learning a hierarchical feature
representation from raw data. While the majority of Time-Series Classification
(TSC) literature is focused on 1D signals, this paper uses Recurrence Plots
(RP) to transform time-series into 2D texture images and then take advantage of
the deep CNN classifier. Image representation of time-series introduces
different feature types that are not available for 1D signals, and therefore
TSC can be treated as texture image recognition task. CNN model also allows
learning different levels of representations together with a classifier,
jointly and automatically. Therefore, using RP and CNN in a unified framework
is expected to boost the recognition rate of TSC. Experimental results on the
UCR time-series classification archive demonstrate competitive accuracy of the
proposed approach, compared not only to the existing deep architectures, but
also to the state-of-the art TSC algorithms.Comment: The 10th International Conference on Machine Vision (ICMV 2017"
66,"In this paper we propose an approach to lexicon-free recognition of text in
scene images. Our approach relies on a LSTM-based soft visual attention model
learned from convolutional features. A set of feature vectors are derived from
an intermediate convolutional layer corresponding to different areas of the
image. This permits encoding of spatial information into the image
representation. In this way, the framework is able to learn how to selectively
focus on different parts of the image. At every time step the recognizer emits
one character using a weighted combination of the convolutional feature vectors
according to the learned attention model. Training can be done end-to-end using
only word level annotations. In addition, we show that modifying the beam
search algorithm by integrating an explicit language model leads to
significantly better recognition results. We validate the performance of our
approach on standard SVT and ICDAR'03 scene text datasets, showing
state-of-the-art performance in unconstrained text recognition"
67,"Image feature representation plays an essential role in image recognition and
related tasks. The current state-of-the-art feature learning paradigm is
supervised learning from labeled data. However, this paradigm requires
large-scale category labels, which limits its applicability to domains where
labels are hard to obtain. In this paper, we propose a new data-driven feature
learning paradigm which does not rely on category labels. Instead, we learn
from user behavior data collected on social media. Concretely, we use the image
relationship discovered in the latent space from the user behavior data to
guide the image feature learning. We collect a large-scale image and user
behavior dataset from Behance.net. The dataset consists of 1.9 million images
and over 300 million view records from 1.9 million users. We validate our
feature learning paradigm on this dataset and find that the learned feature
significantly outperforms the state-of-the-art image features in learning
better image similarities. We also show that the learned feature performs
competitively on various recognition benchmarks"
68,"Recently, deep convolutional neural networks have shown good results for
image recognition. In this paper, we use convolutional neural networks with a
finder module, which discovers the important region for recognition and
extracts that region. We propose applying our method to the recognition of
protein crystals for X-ray structural analysis. In this analysis, it is
necessary to recognize states of protein crystallization from a large number of
images. There are several methods that realize protein crystallization
recognition by using convolutional neural networks. In each method, large-scale
data sets are required to recognize with high accuracy. In our data set, the
number of images is not good enough for training CNN. The amount of data for
CNN is a serious issue in various fields. Our method realizes high accuracy
recognition with few images by discovering the region where the crystallization
drop exists. We compared our crystallization image recognition method with a
high precision method using Inception-V3. We demonstrate that our method is
effective for crystallization images using several experiments. Our method
gained the AUC value that is about 5% higher than the compared method.Comment: 7 pages, 16 figure"
69,"In recent years, histopathology images have been increasingly used as a
diagnostic tool in the medical field. The process of accurately diagnosing a
biopsy sample requires significant expertise in the field, and as such can be
time-consuming and is prone to uncertainty and error. With the advent of
digital pathology, using image recognition systems to highlight problem areas
or locate similar images can aid pathologists in making quick and accurate
diagnoses. In this paper, we specifically consider the encoded local
projections (ELP) algorithm, which has previously shown some success as a tool
for classification and recognition of histopathology images. We build on the
success of the ELP algorithm as a means for image classification and
recognition by proposing a modified algorithm which captures the local
frequency information of the image. The proposed algorithm estimates local
frequencies by quantifying the changes in multiple projections in local windows
of greyscale images. By doing so we remove the need to store the full
projections, thus significantly reducing the histogram size, and decreasing
computation time for image retrieval and classification tasks. Furthermore, we
investigate the effectiveness of applying our method to histopathology images
which have been digitally separated into their hematoxylin and eosin stain
components. The proposed algorithm is tested on the publicly available invasive
ductal carcinoma (IDC) data set. The histograms are used to train an SVM to
classify the data. The experiments showed that the proposed method outperforms
the original ELP algorithm in image retrieval tasks. On classification tasks,
the results are found to be comparable to state-of-the-art deep learning
methods and better than many handcrafted features from the literature.Comment: Accepted for publication in the International Conference on Image
  Analysis and Recognition (ICIAR 2019"
70,"Recognizing a previously visited place, also known as place recognition (or
loop closure detection) is the key towards fully autonomous mobile robots and
self-driving vehicle navigation. Augmented with various Simultaneous
Localization and Mapping techniques (SLAM), loop closure detection allows for
incremental pose correction and can bolster efficient and accurate map
creation. However, repeated and similar scenes (perceptual aliasing) and long
term appearance changes (e.g. weather variations) are major challenges for
current place recognition algorithms. We introduce a new dataset Multisensory
Omnidirectional Long-term Place recognition (MOLP) comprising omnidirectional
intensity and disparity images. This dataset presents many of the challenges
faced by outdoor mobile robots and current place recognition algorithms. Using
MOLP dataset, we formulate the place recognition problem as a regularized
sparse convex optimization problem. We conclude that information extracted from
intensity image is superior to disparity image in isolating discriminative
features for successful long term place recognition. Furthermore, when these
discriminative features are extracted from an omnidirectional vision sensor, a
robust bidirectional loop closure detection approach is established, allowing
mobile robots to close the loop, regardless of the difference in the direction
when revisiting a place.Comment: 15 page"
71,"Spiking neural networks (SNNs) equipped with latency coding and spike-timing
dependent plasticity rules offer an alternative to solve the data and energy
bottlenecks of standard computer vision approaches: they can learn visual
features without supervision and can be implemented by ultra-low power hardware
architectures. However, their performance in image classification has never
been evaluated on recent image datasets. In this paper, we compare SNNs to
auto-encoders on three visual recognition datasets, and extend the use of SNNs
to color images. The analysis of the results helps us identify some bottlenecks
of SNNs: the limits of on-center/off-center coding, especially for color
images, and the ineffectiveness of current inhibition mechanisms. These issues
should be addressed to build effective SNNs for image recognition"
72,"Chinese herbs play a critical role in Traditional Chinese Medicine. Due to
different recognition granularity, they can be recognized accurately only by
professionals with much experience. It is expected that they can be recognized
automatically using new techniques like machine learning. However, there is no
Chinese herbal image dataset available. Simultaneously, there is no machine
learning method which can deal with Chinese herbal image recognition well.
Therefore, this paper begins with building a new standard Chinese-Herbs
dataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese
herbal recognition is proposed, where both novel competitive attention and
spatial collaborative attention are proposed and then applied. APN can
adaptively model Chinese herbal images with different feature scales. Finally,
a new framework for Chinese herbal recognition is proposed as a new application
of APN. Experiments are conducted on our constructed dataset and validate the
effectiveness of our methods.Comment: 14 pages, 8 figure"
73,"Zero-shot recognition aims to accurately recognize objects of unseen classes
by using a shared visual-semantic mapping between the image feature space and
the semantic embedding space. This mapping is learned on training data of seen
classes and is expected to have transfer ability to unseen classes. In this
paper, we tackle this problem by exploiting the intrinsic relationship between
the semantic space manifold and the transfer ability of visual-semantic
mapping. We formalize their connection and cast zero-shot recognition as a
joint optimization problem. Motivated by this, we propose a novel framework for
zero-shot recognition, which contains dual visual-semantic mapping paths. Our
analysis shows this framework can not only apply prior semantic knowledge to
infer underlying semantic manifold in the image feature space, but also
generate optimized semantic embedding space, which can enhance the transfer
ability of the visual-semantic mapping to unseen classes. The proposed method
is evaluated for zero-shot recognition on four benchmark datasets, achieving
outstanding results.Comment: Accepted as a full paper in IEEE Computer Vision and Pattern
  Recognition (CVPR) 201"
74,"Place recognition is an essential component of Simultaneous Localization And
Mapping (SLAM). Under severe appearance change, reliable place recognition is a
difficult perception task since the same place is perceptually very different
in the morning, at night, or over different seasons. This work addresses place
recognition as a domain translation task. Using a pair of coupled Generative
Adversarial Networks (GANs), we show that it is possible to generate the
appearance of one domain (such as summer) from another (such as winter) without
requiring image-to-image correspondences across the domains. Mapping between
domains is learned from sets of images in each domain without knowing the
instance-to-instance correspondence by enforcing a cyclic consistency
constraint. In the process, meaningful feature spaces are learned for each
domain, the distances in which can be used for the task of place recognition.
Experiments show that learned features correspond to visual similarity and can
be effectively used for place recognition across seasons.Comment: Accepted for publication in IEEE International Conference on Robotics
  and Automation (ICRA), 201"
75,"Recent results indicate that the generic descriptors extracted from the
convolutional neural networks are very powerful. This paper adds to the
mounting evidence that this is indeed the case. We report on a series of
experiments conducted for different recognition tasks using the publicly
available code and model of the \overfeat network which was trained to perform
object classification on ILSVRC13. We use features extracted from the \overfeat
network as a generic image representation to tackle the diverse range of
recognition tasks of object image classification, scene recognition, fine
grained recognition, attribute detection and image retrieval applied to a
diverse set of datasets. We selected these tasks and datasets as they gradually
move further away from the original task and data the \overfeat network was
trained to solve. Astonishingly, we report consistent superior results compared
to the highly tuned state-of-the-art systems in all the visual classification
tasks on various datasets. For instance retrieval it consistently outperforms
low memory footprint methods except for sculptures dataset. The results are
achieved using a linear SVM classifier (or $L2$ distance in case of retrieval)
applied to a feature representation of size 4096 extracted from a layer in the
net. The representations are further modified using simple augmentation
techniques e.g. jittering. The results strongly suggest that features obtained
from deep learning with convolutional nets should be the primary candidate in
most visual recognition tasks.Comment: version 3 revisions: 1)Added results using feature processing and
  data augmentation 2)Referring to most recent efforts of using CNN for
  different visual recognition tasks 3) updated text/captio"
76,"Which one comes first: segmentation or recognition? We

propose a probabilistic framework for carrying out the

two simultaneously. The framework combines an LDA

‘bag of visual words’ model for recognition, and a hybrid

parametric-nonparametric model for segmentation. If applied

to a collection of images, our framework can simultaneously

discover the segments of each image, and the correspondence

between such segments. Such segments may be

thought of as the ‘parts’ of corresponding objects that appear

in the image collection. Thus, the model may be used

for learning new categories, detecting/classifying objects,

and segmenting images"
77,"We describe in this report our audio scene recognition system submitted to
the DCASE 2016 challenge. Firstly, given the label set of the scenes, a label
tree is automatically constructed. This category taxonomy is then used in the
feature extraction step in which an audio scene instance is represented by a
label tree embedding image. Different convolutional neural networks, which are
tailored for the task at hand, are finally learned on top of the image features
for scene recognition. Our system reaches an overall recognition accuracy of
81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute
improvements of 8.7% and 6.1% on the development and test data, respectively.Comment: Task1 technical report for the DCASE2016 challenge. arXiv admin note:
  text overlap with arXiv:1606.0790"
78,"Automatic organization of personal photos is a problem with many real world
ap- plications, and can be divided into two main tasks: recognizing the event
type of the photo collection, and selecting interesting images from the
collection. In this paper, we attempt to simultaneously solve both tasks:
album-wise event recognition and image- wise importance prediction. We
collected an album dataset with both event type labels and image importance
labels, refined from an existing CUFED dataset. We propose a hybrid system
consisting of three parts: A siamese network-based event-specific image
importance prediction, a Convolutional Neural Network (CNN) that recognizes the
event type, and a Long Short-Term Memory (LSTM)-based sequence level event
recognizer. We propose an iterative updating procedure for event type and image
importance score prediction. We experimentally verified that image importance
score prediction and event type recognition can each help the performance of
the other.Comment: Accepted as oral in BMVC 201"
79,"For the efficient management of large image databases, the automated characterization of images and the usage of that characterization for searching and ordering tasks is highly desirable. The purpose of the project SEMACODE is to combine the still unsolved problem of content-oriented characterization of images with scale-invariant object recognition and modelbased compression methods. To achieve this goal, existing techniques as well as new concepts related to pattern matching, image encoding, and image compression are examined. The resulting methods are integrated in a common framework with the aid of a content-oriented conception. For the application, an image database at the library of the university of Frankfurt/Main (StUB; about 60000 images), the required operations are developed. The search and query interfaces are defined in close cooperation with the StUB project “Digitized Colonial Picture Library”. This report describes the fundamentals and first results of the image encoding and object recognition algorithms developed within the scope of the project"
80,"Convolutional Neural Networks (CNNs) require large image corpora to be
trained on classification tasks. The variation in image resolutions, sizes of
objects and patterns depicted, and image scales, hampers CNN training and
performance, because the task-relevant information varies over spatial scales.
Previous work attempting to deal with such scale variations focused on
encouraging scale-invariant CNN representations. However, scale-invariant
representations are incomplete representations of images, because images
contain scale-variant information as well. This paper addresses the combined
development of scale-invariant and scale-variant representations. We propose a
multi- scale CNN method to encourage the recognition of both types of features
and evaluate it on a challenging image classification task involving
task-relevant characteristics at multiple scales. The results show that our
multi-scale CNN outperforms single-scale CNN. This leads to the conclusion that
encouraging the combined development of a scale-invariant and scale-variant
representation in CNNs is beneficial to image recognition performance"
81,"The success of deep neural networks is mostly due their ability to learn
meaningful features from the data. Features learned in the hidden layers of
deep neural networks trained in computer vision tasks have been shown to be
similar to mid-level vision features. We leverage this fact in this work and
propose the visualization regularizer for image tasks. The proposed
regularization technique enforces smoothness of the features learned by hidden
nodes and turns out to be a special case of Tikhonov regularization. We achieve
higher classification accuracy as compared to existing regularizers such as the
L2 norm regularizer and dropout, on benchmark datasets without changing the
training computational complexity"
82,"Unlike natural images, medical images often have intrinsic characteristics
that can be leveraged for neural network learning. For example, images that
belong to different stages of a disease may continuously follow a certain
progression pattern. In this paper, we propose a novel method that leverages
disease progression learning for medical image recognition. In our method,
sequences of images ordered by disease stages are learned by a neural network
that consists of a shared vision model for feature extraction and a long
short-term memory network for the learning of stage sequences. Auxiliary vision
outputs are also included to capture stage features that tend to be discrete
along the disease progression. Our proposed method is evaluated on a public
diabetic retinopathy dataset, and achieves about 3.3% improvement in disease
staging accuracy, compared to the baseline method that does not use disease
progression learning"
83,"The multimedia content in the World Wide Web is rapidly growing and contains
valuable information for many applications in different domains. For this
reason, the Internet Archive initiative has been gathering billions of
time-versioned web pages since the mid-nineties. However, the huge amount of
data is rarely labeled with appropriate metadata and automatic approaches are
required to enable semantic search. Normally, the textual content of the
Internet Archive is used to extract entities and their possible relations
across domains such as politics and entertainment, whereas image and video
content is usually neglected. In this paper, we introduce a system for person
recognition in image content of web news stored in the Internet Archive. Thus,
the system complements entity recognition in text and allows researchers and
analysts to track media coverage and relations of persons more precisely. Based
on a deep learning face recognition approach, we suggest a system that
automatically detects persons of interest and gathers sample material, which is
subsequently used to identify them in the image data of the Internet Archive.
We evaluate the performance of the face recognition system on an appropriate
standard benchmark dataset and demonstrate the feasibility of the approach with
two use cases"
84,"The technology of image segmentation is widely used in medical image
processing, face recognition pedestrian detection, etc. The current image
segmentation techniques include region-based segmentation, edge detection
segmentation, segmentation based on clustering, segmentation based on
weakly-supervised learning in CNN, etc. This paper analyzes and summarizes
these algorithms of image segmentation, and compares the advantages and
disadvantages of different algorithms. Finally, we make a prediction of the
development trend of image segmentation with the combination of these
algorithms"
85,"In this paper we address the problem of offline Arabic handwriting word
recognition. Off-line recognition of handwritten words is a difficult task due
to the high variability and uncertainty of human writing. The majority of the
recent systems are constrained by the size of the lexicon to deal with and the
number of writers. In this paper, we propose an approach for multi-writers
Arabic handwritten words recognition using multiple Bayesian networks. First,
we cut the image in several blocks. For each block, we compute a vector of
descriptors. Then, we use K-means to cluster the low-level features including
Zernik and Hu moments. Finally, we apply four variants of Bayesian networks
classifiers (Na\""ive Bayes, Tree Augmented Na\""ive Bayes (TAN), Forest
Augmented Na\""ive Bayes (FAN) and DBN (dynamic bayesian network) to classify
the whole image of tunisian city name. The results demonstrate FAN and DBN
outperform good recognition ratesComment: arXiv admin note: substantial text overlap with arXiv:1204.167"
86,"Blur in facial images significantly impedes the efficiency of recognition
approaches. However, most existing blind deconvolution methods cannot generate
satisfactory results due to their dependence on strong edges, which are
sufficient in natural images but not in facial images. In this paper, we
represent point spread functions (PSFs) by the linear combination of a set of
pre-defined orthogonal PSFs, and similarly, an estimated intrinsic (EI) sharp
face image is represented by the linear combination of a set of pre-defined
orthogonal face images. In doing so, PSF and EI estimation is simplified to
discovering two sets of linear combination coefficients, which are
simultaneously found by our proposed coupled learning algorithm. To make our
method robust to different types of blurry face images, we generate several
candidate PSFs and EIs for a test image, and then, a non-blind deconvolution
method is adopted to generate more EIs by those candidate PSFs. Finally, we
deploy a blind image quality assessment metric to automatically select the
optimal EI. Thorough experiments on the facial recognition technology database,
extended Yale face database B, CMU pose, illumination, and expression (PIE)
database, and face recognition grand challenge database version 2.0 demonstrate
that the proposed approach effectively restores intrinsic sharp face images
and, consequently, improves the performance of face recognition"
87,"To address the sequential changes of images including poses, in this paper we
propose a recurrent regression neural network(RRNN) framework to unify two
classic tasks of cross-pose face recognition on still images and video-based
face recognition. To imitate the changes of images, we explicitly construct the
potential dependencies of sequential images so as to regularize the final
learning model. By performing progressive transforms for sequentially adjacent
images, RRNN can adaptively memorize and forget the information that benefits
for the final classification. For face recognition of still images, given any
one image with any one pose, we recurrently predict the images with its
sequential poses to expect to capture some useful information of others poses.
For video-based face recognition, the recurrent regression takes one entire
sequence rather than one image as its input. We verify RRNN in static face
dataset MultiPIE and face video dataset YouTube Celebrities(YTC). The
comprehensive experimental results demonstrate the effectiveness of the
proposed RRNN method"
88,"With the rapid development of digital imaging and communication technologies,
image set based face recognition (ISFR) is becoming increasingly important. One
key issue of ISFR is how to effectively and efficiently represent the query
face image set by using the gallery face image sets. The set-to-set distance
based methods ignore the relationship between gallery sets, while representing
the query set images individually over the gallery sets ignores the correlation
between query set images. In this paper, we propose a novel image set based
collaborative representation and classification method for ISFR. By modeling
the query set as a convex or regularized hull, we represent this hull
collaboratively over all the gallery sets. With the resolved representation
coefficients, the distance between the query set and each gallery set can then
be calculated for classification. The proposed model naturally and effectively
extends the image based collaborative representation to an image set based one,
and our extensive experiments on benchmark ISFR databases show the superiority
of the proposed method to state-of-the-art ISFR methods under different set
sizes in terms of both recognition rate and efficiency"
89,"System facilitates acquisition, digital processing, and recording of image data, as well as pattern recognition in an iterative mode"
90,"IPRS is a freely available software system which consists of about 250 library functions in C, and a set of application programs. It is designed to run under UNIX and comes with full source code, system manual pages, and a comprehensive user's and programmer's guide. It is intended for use by researchers in human vision, pattern recognition, image processing, machine vision and machine learning"
91,"Despite rapid advances in face recognition, there remains a clear gap between
the performance of still image-based face recognition and video-based face
recognition, due to the vast difference in visual quality between the domains
and the difficulty of curating diverse large-scale video datasets. This paper
addresses both of those challenges, through an image to video feature-level
domain adaptation approach, to learn discriminative video frame
representations. The framework utilizes large-scale unlabeled video data to
reduce the gap between different domains while transferring discriminative
knowledge from large-scale labeled still images. Given a face recognition
network that is pretrained in the image domain, the adaptation is achieved by
(i) distilling knowledge from the network to a video adaptation network through
feature matching, (ii) performing feature restoration through synthetic data
augmentation and (iii) learning a domain-invariant feature through a domain
adversarial discriminator. We further improve performance through a
discriminator-guided feature fusion that boosts high-quality frames while
eliminating those degraded by video domain-specific factors. Experiments on the
YouTube Faces and IJB-A datasets demonstrate that each module contributes to
our feature-level domain adaptation framework and substantially improves video
face recognition performance to achieve state-of-the-art accuracy. We
demonstrate qualitatively that the network learns to suppress diverse artifacts
in videos such as pose, illumination or occlusion without being explicitly
trained for them.Comment: accepted for publication at International Conference on Computer
  Vision (ICCV) 201"
92,"Image recognition using Deep Learning has been evolved for decades though
advances in the field through different settings is still a challenge. In this
paper, we present our findings in searching for better image classifiers in
offline and online environments. We resort to Convolutional Neural Network and
its variations of fully connected Multi-layer Perceptron. Though still
preliminary, these results are encouraging and may provide a better
understanding about the field and directions toward future works.Comment: 5 page"
93,"An approach to textures pattern recognition based on inverse resonance
filtration (IRF) is considered. A set of principal resonance harmonics of
textured image signal fluctuations eigen harmonic decomposition (EHD) is used
for the IRF design. It was shown that EHD is invariant to textured image linear
shift. The recognition of texture is made by transfer of its signal into
unstructured signal which simple statistical parameters can be used for texture
pattern recognition. Anomalous variations of this signal point on foreign
objects. Two methods of 2D EHD parameters estimation are considered with the
account of texture signal breaks presence. The first method is based on the
linear symmetry model that is not sensitive to signal phase jumps. The
condition of characteristic polynomial symmetry provides the model stationarity
and periodicity. Second method is based on the eigenvalues problem of matrices
pencil projection into principal vectors space of singular values decomposition
(SVD) of 2D correlation matrix. Two methods of classification of retrieval from
textured image foreign objects are offered.Comment: 8 pages, 3 figure"
94,"Neural networks for image recognition have evolved through extensive manual
design from simple chain-like models to structures with multiple wiring paths.
The success of ResNets and DenseNets is due in large part to their innovative
wiring plans. Now, neural architecture search (NAS) studies are exploring the
joint optimization of wiring and operation types, however, the space of
possible wirings is constrained and still driven by manual design despite being
searched. In this paper, we explore a more diverse set of connectivity patterns
through the lens of randomly wired neural networks. To do this, we first define
the concept of a stochastic network generator that encapsulates the entire
network generation process. Encapsulation provides a unified view of NAS and
randomly wired networks. Then, we use three classical random graph models to
generate randomly wired graphs for networks. The results are surprising:
several variants of these random generators yield network instances that have
competitive accuracy on the ImageNet benchmark. These results suggest that new
efforts focusing on designing better network generators may lead to new
breakthroughs by exploring less constrained search spaces with more room for
novel design.Comment: Technical repor"
95,"Examplers of a face are formed from multiple gallery images of a person and
are used in the process of classification of a test image. We incorporate such
examplers in forming a biologically inspired local binary decisions on
similarity based face recognition method. As opposed to single model approaches
such as face averages the exampler based approach results in higher recognition
accu- racies and stability. Using multiple training samples per person, the
method shows the following recognition accuracies: 99.0% on AR, 99.5% on FERET,
99.5% on ORL, 99.3% on EYALE, 100.0% on YALE and 100.0% on CALTECH face
databases. In addition to face recognition, the method also detects the natural
variability in the face images which can find application in automatic tagging
of face images"
96,"In this paper, we propose a non-frontal model based approach which ensures that a face recognition system always gets to compare images having similar view (or pose). This requires a virtual suspect reference set that consists of non-frontal suspect images having pose similar to the surveillance view trace image. We apply the 3D model reconstruction followed by image synthesis approach to the frontal view mug shot images in the suspect reference set in order to create such a virtual suspect reference set. This strategy not only ensures a stable 3D face model reconstruction because of the relatively good quality mug shot suspect images but also provides a practical solution for forensic cases where the trace is often of very low quality. For most face recognition algorithms, the relative pose difference between the test and reference image is one of the major causes of severe degradation in recognition performance. Moreover, given appropriate training, comparing a pair of non-frontal images is no more difficult that comparing frontal view images"
97,"We propose a very simple, efficient yet surprisingly effective feature
extraction method for face recognition (about 20 lines of Matlab code), which
is mainly inspired by spatial pyramid pooling in generic image classification.
We show that features formed by simply pooling local patches over a multi-level
pyramid, coupled with a linear classifier, can significantly outperform most
recent face recognition methods. The simplicity of our feature extraction
procedure is demonstrated by the fact that no learning is involved (except PCA
whitening). We show that, multi-level spatial pooling and dense extraction of
multi-scale patches play critical roles in face image classification. The
extracted facial features can capture strong structural information of
individual faces with no label information being used. We also find that,
pre-processing on local image patches such as contrast normalization can have
an important impact on the classification accuracy. In particular, on the
challenging face recognition datasets of FERET and LFW-a, our method improves
previous best results by more than 10% and 20%, respectively.Comment: 12 page"
98,"Many people desire to be informed about the nutritional specifics of the food they consume. Current popular dietary tracking methods are too slow and tedious for a lot of consumers due to requiring manual data entry for everything eaten. We propose a system that will take advantage of image recognition and the internal camera of Android phones to identify food based off of a picture of a user’s plate. Over the course the last year, we trained an object detection model with images of different types of food, built a mobile application around it, and tested their integration and performance. We believe that our program meets the requirements we set out for it at its conception and delivers a simple, fast, and efficient way of tracking one’s diet"
99,"With the development of convolutional neural networks (CNNs) in recent years,
the network structure has become more and more complex and varied, and has
achieved very good results in pattern recognition, image classification, object
detection and tracking. For CNNs used for image classification, in addition to
the network structure, more and more research is now focusing on the
improvement of the loss function, so as to enlarge the inter-class feature
differences, and reduce the intra-class feature variations as soon as possible.
Besides the traditional Softmax, typical loss functions include L-Softmax,
AM-Softmax, ArcFace, and Center loss, etc. Based on the concept of predefined
evenly-distributed class centroids (PEDCC) in CSAE network, this paper proposes
a PEDCC-based loss function called PEDCC-Loss, which can make the inter-class
distance maximal and intra-class distance small enough in hidden feature space.
Multiple experiments on image classification and face recognition have proved
that our method achieve the best recognition accuracy, and network training is
stable and easy to converge. Code is available in
https://github.com/ZLeopard/PEDCC-LossComment: 10 pages, 5 figure"
100,"Human observers can learn to recognize new categories of images from a
handful of examples, yet doing so with artificial ones remains an open
challenge. We hypothesize that data-efficient recognition is enabled by
representations which make the variability in natural signals more predictable.
We therefore revisit and improve Contrastive Predictive Coding, an unsupervised
objective for learning such representations. This new implementation produces
features which support state-of-the-art linear classification accuracy on the
ImageNet dataset. When used as input for non-linear classification with deep
neural networks, this representation allows us to use 2-5x less labels than
classifiers trained directly on image pixels. Finally, this unsupervised
representation substantially improves transfer learning to object detection on
the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet
classifiers"
101,"Computation capabilities of recent mobile devices enable natural feature
processing for Augmented Reality (AR). However, mobile AR applications are
still faced with scalability and performance challenges. In this paper, we
propose CloudAR, a mobile AR framework utilizing the advantages of cloud and
edge computing through recognition task offloading. We explore the design space
of cloud-based AR exhaustively and optimize the offloading pipeline to minimize
the time and energy consumption. We design an innovative tracking system for
mobile devices which provides lightweight tracking in 6 degree of freedom
(6DoF) and hides the offloading latency from users' perception. We also design
a multi-object image retrieval pipeline that executes fast and accurate image
recognition tasks on servers. In our evaluations, the mobile AR application
built with the CloudAR framework runs at 30 frames per second (FPS) on average
with precise tracking of only 1~2 pixel errors and image recognition of at
least 97% accuracy. Our results also show that CloudAR outperforms one of the
leading commercial AR framework in several performance metrics"
102,"License Plate Recognition plays an important role on the traffic monitoring
and parking management. Administration and restriction of those transportation
tools for their better service becomes very essential. In this paper, a fast
and real time method has an appropriate application to find plates that the
plat has tilt and the picture quality is poor. In the proposed method, at the
beginning, the image is converted into binary mode with use of adaptive
threshold. And with use of edge detection and morphology operation, plate
number location has been specified and if the plat has tilt; its tilt is
removed away. Then its characters are distinguished using image processing
techniques. Finally, K Nearest Neighbour (KNN) classifier was used for
character recognition. This method has been tested on available data set that
has different images of the background, considering distance, and angel of view
so that the correct extraction rate of plate reached at 98% and character
recognition rate achieved at 99.12%. Further we tested our character
recognition stage on Persian vehicle data set and we achieved 99% correct
recognition rate.Comment: 2013 First International Conference on computer, Information
  Technology and Digital Media. arXiv admin note: substantial text overlap with
  arXiv:1407.632"
103,"Recurrent Neural Networks (RNNs) have had considerable success in classifying
and predicting sequences. We demonstrate that RNNs can be effectively used in
order to encode sequences and provide effective representations. The
methodology we use is based on Fisher Vectors, where the RNNs are the
generative probabilistic models and the partial derivatives are computed using
backpropagation. State of the art results are obtained in two central but
distant tasks, which both rely on sequences: video action recognition and image
annotation. We also show a surprising transfer learning result from the task of
image annotation to the task of video action recognition"
104,"Learning to capture long-range relations is fundamental to image/video
recognition. Existing CNN models generally rely on increasing depth to model
such relations which is highly inefficient. In this work, we propose the
""double attention block"", a novel component that aggregates and propagates
informative global features from the entire spatio-temporal space of input
images/videos, enabling subsequent convolution layers to access features from
the entire space efficiently. The component is designed with a double attention
mechanism in two steps, where the first step gathers features from the entire
space into a compact set through second-order attention pooling and the second
step adaptively selects and distributes features to each location via another
attention. The proposed double attention block is easy to adopt and can be
plugged into existing deep neural networks conveniently. We conduct extensive
ablation studies and experiments on both image and video recognition tasks for
evaluating its performance. On the image recognition task, a ResNet-50 equipped
with our double attention blocks outperforms a much larger ResNet-152
architecture on ImageNet-1k dataset with over 40% less the number of parameters
and less FLOPs. On the action recognition task, our proposed model achieves the
state-of-the-art results on the Kinetics and UCF-101 datasets with
significantly higher efficiency than recent works.Comment: Accepted at NIPS 201"
105,"Hybrid approach has a special status among Face Recognition Systems as they
combine different recognition approaches in an either serial or parallel to
overcome the shortcomings of individual methods. This paper explores the area
of Hybrid Face Recognition using score based strategy as a combiner/fusion
process. In proposed approach, the recognition system operates in two modes:
training and classification. Training mode involves normalization of the face
images (training set), extracting appropriate features using Principle
Component Analysis (PCA) and Independent Component Analysis (ICA). The
extracted features are then trained in parallel using Back-propagation neural
networks (BPNNs) to partition the feature space in to different face classes.
In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face
image(s). The score based strategy which works as a combiner is applied to the
results of both PCA BPNN and ICA BPNN to classify given new face image(s)
according to face classes obtained during the training mode. The proposed
approach has been tested on ORL and other face databases; the experimented
results show that the proposed system has higher accuracy than face recognition
systems using single feature extractor"
106,"The use of analogies to physical phenomena is an exciting paradigm in computer vision that allows unorthodox approaches to feature extraction, creating new techniques with unique properties. A technique known as the ""image ray transform"" has been developed based upon an analogy to the propagation of light as rays. The transform analogises an image to a set of glass blocks with refractive index linked to pixel properties and then casts a large number of rays through the image. The course of these rays is accumulated into an output image. The technique can successfully extract tubular and circular features and we show successful circle detection, ear biometrics and retinal vessel extraction. The transform has also been extended through the use of multiple rays arranged as a beam to increase robustness to noise, and we show quantitative results for fully automatic ear recognition, achieving 95.2% rank one recognition across 63 subjects"
107,"In recent years, automatic human motion recognition has been widely researched within the computer vision and image processing communities. Here we propose a real-time embedded vision solution for human motion recognition implemented on a ubiquitous device. There are three main contributions in this paper. Firstly, we have developed a fast human motion recognition system with simple motion features and a linear Support Vector Machine(SVM) classifier. The method has been tested on a large, public human action dataset and achieved competitive performance for the temporal template (eg. ``motion history image"") class of approaches. Secondly, we have developed a reconfigurable, FPGA based video processing architecture. One advantage of this architecture is that the system processing performance can be reconfigured for a particular application, with the addition of new or replicated processing cores. Finally, we have successfully implemented a human motion recognition system on this reconfigurable architecture. With a small number of human actions (hand gestures), this stand-alone system is performing reliably, with an 80% average recognition rate using limited training data. This type of system has applications in security systems, man-machine communications and intelligent environments"
108,"Copyright @ 2008 Springer-Verlag.In recent years, automatic human motion recognition has been widely researched within the computer vision and image processing communities. Here we propose a real-time embedded vision solution for human motion recognition implemented on a ubiquitous device. There are three main contributions in this paper. Firstly, we have developed a fast human motion recognition system with simple motion features and a linear Support Vector Machine (SVM) classifier. The method has been tested on a large, public human action dataset and achieved competitive performance for the temporal template (eg. “motion history image”) class of approaches. Secondly, we have developed a reconfigurable, FPGA based video processing architecture. One advantage of this architecture is that the system processing performance can be reconfiured for a particular application, with the addition of new or replicated processing cores. Finally, we have successfully implemented a human motion recognition system on this reconfigurable architecture. With a small number of human actions (hand gestures), this stand-alone system is performing reliably, with an 80% average recognition rate using limited training data. This type of system has applications in security systems, man-machine communications and intelligent environments.DTI and Broadcom Ltd"
109,"In this paper, we focus on fine-grained recognition of vehicles mainly in
traffic surveillance applications. We propose an approach that is orthogonal to
recent advancements in fine-grained recognition (automatic part discovery and
bilinear pooling). In addition, in contrast to other methods focused on
fine-grained recognition of vehicles, we do not limit ourselves to a
frontal/rear viewpoint, but allow the vehicles to be seen from any viewpoint.
Our approach is based on 3-D bounding boxes built around the vehicles. The
bounding box can be automatically constructed from traffic surveillance data.
For scenarios where it is not possible to use precise construction, we propose
a method for an estimation of the 3-D bounding box. The 3-D bounding box is
used to normalize the image viewpoint by ""unpacking"" the image into a plane. We
also propose to randomly alter the color of the image and add a rectangle with
random noise to a random position in the image during the training of
convolutional neural networks (CNNs). We have collected a large fine-grained
vehicle data set BoxCars116k, with 116k images of vehicles from various
viewpoints taken by numerous surveillance cameras. We performed a number of
experiments, which show that our proposed method significantly improves CNN
classification accuracy (the accuracy is increased by up to 12% points and the
error is reduced by up to 50% compared with CNNs without the proposed
modifications). We also show that our method outperforms the state-of-the-art
methods for fine-grained recognition"
110,"Optical and optical electronic hybrid processing especially in the application area of image processing are emphasized. Real time pattern recognition processors for such airborne missions as target recognition, tracking, and terminal guidance are studied"
111,"Heterogeneous face recognition (HFR) refers to matching face images acquired
from different sources (i.e., different sensors or different wavelengths) for
identification. HFR plays an important role in both biometrics research and
industry. In spite of promising progresses achieved in recent years, HFR is
still a challenging problem due to the difficulty to represent two
heterogeneous images in a homogeneous manner. Existing HFR methods either
represent an image ignoring the spatial information, or rely on a
transformation procedure which complicates the recognition task. Considering
these problems, we propose a novel graphical representation based HFR method
(G-HFR) in this paper. Markov networks are employed to represent heterogeneous
image patches separately, which takes the spatial compatibility between
neighboring image patches into consideration. A coupled representation
similarity metric (CRSM) is designed to measure the similarity between obtained
graphical representations. Extensive experiments conducted on multiple HFR
scenarios (viewed sketch, forensic sketch, near infrared image, and thermal
infrared image) show that the proposed method outperforms state-of-the-art
methods.Comment: 13 pages, 10 figures, TPAMI 2016 accepte"
112,"There is much current interest in using multi-sensor airborne remote sensing
to monitor the structure and biodiversity of forests. This paper addresses the
application of non-parametric image registration techniques to precisely align
images obtained from multimodal imaging, which is critical for the successful
identification of individual trees using object recognition approaches.
Non-parametric image registration, in particular the technique of optimizing
one objective function containing data fidelity and regularization terms,
provides flexible algorithms for image registration. Using a survey of
woodlands in southern Spain as an example, we show that non-parametric image
registration can be successful at fusing datasets when there is little prior
knowledge about how the datasets are interrelated (i.e. in the absence of
ground control points). The validity of non-parametric registration methods in
airborne remote sensing is demonstrated by a series of experiments. Precise
data fusion is a prerequisite to accurate recognition of objects within
airborne imagery, so non-parametric image registration could make a valuable
contribution to the analysis pipeline.Comment: 11 pages, 5 figure"
113,"Deep convolutional neural networks have recently proven extremely competitive
in challenging image recognition tasks. This paper proposes the epitomic
convolution as a new building block for deep neural networks. An epitomic
convolution layer replaces a pair of consecutive convolution and max-pooling
layers found in standard deep convolutional neural networks. The main version
of the proposed model uses mini-epitomes in place of filters and computes
responses invariant to small translations by epitomic search instead of
max-pooling over image positions. The topographic version of the proposed model
uses large epitomes to learn filter maps organized in translational
topographies. We show that error back-propagation can successfully learn
multiple epitomic layers in a supervised fashion. The effectiveness of the
proposed method is assessed in image classification tasks on standard
benchmarks. Our experiments on Imagenet indicate improved recognition
performance compared to standard convolutional neural networks of similar
architecture. Our models pre-trained on Imagenet perform excellently on
Caltech-101. We also obtain competitive image classification results on the
small-image MNIST and CIFAR-10 datasets.Comment: 9 page"
114,"Naturally, with the mounting application of biometric systems, there arises a
difficulty in storing and handling those acquired biometric data. Fingerprint
recognition has been recognized as one of the most mature and established
technique among all the biometrics systems. In recent times, with fingerprint
recognition receiving increasingly more attention the amount of fingerprints
collected has been constantly creating enormous problems in storage and
transmission. Henceforth, the compression of fingerprints has emerged as an
indispensable step in automated fingerprint recognition systems. Several
researchers have presented approaches for fingerprint image compression. In
this paper, we propose a novel and efficient scheme for fingerprint image
compression. The presented scheme utilizes the Bezier curve representations for
effective compression of fingerprint images. Initially, the ridges present in
the fingerprint image are extracted along with their coordinate values using
the approach presented. Subsequently, the control points are determined for all
the ridges by visualizing each ridge as a Bezier curve. The control points of
all the ridges determined are stored and are used to represent the fingerprint
image. When needed, the fingerprint image is reconstructed from the stored
control points using Bezier curves. The quality of the reconstructed
fingerprint is determined by a formal evaluation. The proposed scheme achieves
considerable memory reduction in storing the fingerprint.Comment: 9 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis"
115,"This paper proposes a novel image set classification technique based on the
concept of linear regression. Unlike most other approaches, the proposed
technique does not involve any training or feature extraction. The gallery
image sets are represented as subspaces in a high dimensional space. Class
specific gallery subspaces are used to estimate regression models for each
image of the test image set. Images of the test set are then projected on the
gallery subspaces. Residuals, calculated using the Euclidean distance between
the original and the projected test images, are used as the distance metric.
Three different strategies are devised to decide on the final class of the test
image set. We performed extensive evaluations of the proposed technique under
the challenges of low resolution, noise and less gallery data for the tasks of
surveillance, video-based face recognition and object recognition. Experiments
show that the proposed technique achieves a better classification accuracy and
a faster execution time compared to existing techniques especially under the
challenging conditions of low resolution and small gallery and test data"
116,"This paper introduces self-taught object localization, a novel approach that
leverages deep convolutional networks trained for whole-image recognition to
localize objects in images without additional human supervision, i.e., without
using any ground-truth bounding boxes for training. The key idea is to analyze
the change in the recognition scores when artificially masking out different
regions of the image. The masking out of a region that includes the object
typically causes a significant drop in recognition score. This idea is embedded
into an agglomerative clustering technique that generates self-taught
localization hypotheses. Our object localization scheme outperforms existing
proposal methods in both precision and recall for small number of subwindow
proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the
state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that
the annotations automatically-generated by our method can be used to train
object detectors yielding recognition results remarkably close to those
obtained by training on manually-annotated bounding boxes.Comment: WACV 201"
117,"The main contribution of this paper is to present a novel method for automatic 3D face recognition based on sampling a 3D mesh structure in the presence of noise. A structured light method using line projection is employed where a 3D face is reconstructed from a single 2D shot. The process from image acquisition to recognition is described with focus on its real-time operation. Recognition results are presented and it is demonstrated that it can perform recognition in just over one second per subject in continuous operation mode and thus, suitable for real time operation"
118,"Many image segmentation techniques have been developed over the past two
decades for segmenting the images, which help for object recognition, occlusion
boundary estimation within motion or stereo systems, image compression, image
editing.
  In this, there is a combined approach for segmenting the image. By using
histogram equalization to the input image, from which it gives contrast
enhancement output image .After that by applying median filtering,which will
remove noise from contrast output image . At last I applied fuzzy c-mean
clustering algorithm to denoising output image, which give segmented output
image. In this way it produce better segmented image with less computation
time.Comment: 4 pages, 5 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT"
119,"This paper presents a Neural Aggregation Network (NAN) for video face
recognition. The network takes a face video or face image set of a person with
a variable number of face images as its input, and produces a compact,
fixed-dimension feature representation for recognition. The whole network is
composed of two modules. The feature embedding module is a deep Convolutional
Neural Network (CNN) which maps each face image to a feature vector. The
aggregation module consists of two attention blocks which adaptively aggregate
the feature vectors to form a single feature inside the convex hull spanned by
them. Due to the attention mechanism, the aggregation is invariant to the image
order. Our NAN is trained with a standard classification or verification loss
without any extra supervision signal, and we found that it automatically learns
to advocate high-quality face images while repelling low-quality ones such as
blurred, occluded and improperly exposed faces. The experiments on IJB-A,
YouTube Face, Celebrity-1000 video face recognition benchmarks show that it
consistently outperforms naive aggregation methods and achieves the
state-of-the-art accuracy.Comment: Post CVPR2017 version with minor typo fi"
120,"Tactile texture refers to the tangible feel of a surface and visual texture
refers to see the shape or contents of the image. In the image processing, the
texture can be defined as a function of spatial variation of the brightness
intensity of the pixels. Texture is the main term used to define objects or
concepts of a given image. Texture analysis plays an important role in computer
vision cases such as object recognition, surface defect detection, pattern
recognition, medical image analysis, etc. Since now many approaches have been
proposed to describe texture images accurately. Texture analysis methods
usually are classified into four categories: statistical methods, structural,
model-based and transform-based methods. This paper discusses the various
methods used for texture or analysis in details. New researches shows the power
of combinational methods for texture analysis, which can't be in specific
category. This paper provides a review on well known combinational methods in a
specific section with details. This paper counts advantages and disadvantages
of well-known texture image descriptors in the result part. Main focus in all
of the survived methods is on discrimination performance, computational
complexity and resistance to challenges such as noise, rotation, etc. A brief
review is also made on the common classifiers used for texture image
classification. Also, a survey on texture image benchmark datasets is included.Comment: 29 Pages, Keywords: Texture Image, Texture Analysis, Texture
  classification, Feature extraction, Image processing, Local Binary Patterns,
  Benchmark texture image dataset"
121,"Visual lip reading recognition is an essential stage in many multimedia systems such as “Audio Visual Speech

Recognition” [6], “Mobile Phone Visual System for deaf people”, “Sign Language Recognition System”, etc.

The use of lip visual features to help audio or hand recognition is appropriate because this information is robust

to acoustic noise. In this paper, we describe our work towards developing a robust technique for lip reading

classification that extracts the lips in a colour image by using EMPCA feature extraction and k-nearest-neighbor

classification. In order to reduce the dimensionality of the feature space the lip motion is characterized by three

templates that are modelled based on different mouth shapes: closed template, semi-closed template, and wideopen

template. Our goal is to classify each image sequence based on the distribution of the three templates and

group the words into different clusters. The words that form the database were grouped into three different

clusters as follows: group1(‘I’, ‘high’, ‘lie’, ‘hard’, ‘card’, ‘bye’), group2(‘you, ‘owe’, ‘word’), group3(‘bird’)"
122,"Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.Comment: Paper presented on the ""National Conference on Indian Language
  Computing"", Kochi, February 19-20, 2011. 6 pages, 5 figure"
123,"The past several years have seen both an explosion in the use of
Convolutional Neural Networks (CNNs) and the design of accelerators to make CNN
inference practical. In the architecture community, the lion share of effort
has targeted CNN inference for image recognition. The closely related problem
of video recognition has received far less attention as an accelerator target.
This is surprising, as video recognition is more computationally intensive than
image recognition, and video traffic is predicted to be the majority of
internet traffic in the coming years.
  This paper fills the gap between algorithmic and hardware advances for video
recognition by providing a design space exploration and flexible architecture
for accelerating 3D Convolutional Neural Networks (3D CNNs) - the core kernel
in modern video understanding. When compared to (2D) CNNs used for image
recognition, efficiently accelerating 3D CNNs poses a significant engineering
challenge due to their large (and variable over time) memory footprint and
higher dimensionality.
  To address these challenges, we design a novel accelerator, called Morph,
that can adaptively support different spatial and temporal tiling strategies
depending on the needs of each layer of each target 3D CNN. We codesign a
software infrastructure alongside the Morph hardware to find good-fit
parameters to control the hardware. Evaluated on state-of-the-art 3D CNNs,
Morph achieves up to 3.4x (2.5x average) reduction in energy consumption and
improves performance/watt by up to 5.1x (4x average) compared to a baseline 3D
CNN accelerator, with an area overhead of 5%. Morph further achieves a 15.9x
average energy reduction on 3D CNNs when compared to Eyeriss.Comment: Appears in the proceedings of the 51st Annual IEEE/ACM International
  Symposium on Microarchitecture (MICRO), 201"
124,"Face recognition from image or video is a popular topic in biometrics
research. Many public places usually have surveillance cameras for video
capture and these cameras have their significant value for security purpose. It
is widely acknowledged that the face recognition have played an important role
in surveillance system as it doesn't need the object's cooperation. The actual
advantages of face based identification over other biometrics are uniqueness
and acceptance. As human face is a dynamic object having high degree of
variability in its appearance, that makes face detection a difficult problem in
computer vision. In this field, accuracy and speed of identification is a main
issue.
  The goal of this paper is to evaluate various face detection and recognition
methods, provide complete solution for image based face detection and
recognition with higher accuracy, better response rate as an initial step for
video surveillance. Solution is proposed based on performed tests on various
face rich databases in terms of subjects, pose, emotions, race and light.Comment: 4 pages, 3 table, 4 figur"
125,"Recognition of grocery products in store shelves poses peculiar challenges.
Firstly, the task mandates the recognition of an extremely high number of
different items, in the order of several thousands for medium-small shops, with
many of them featuring small inter and intra class variability. Then, available
product databases usually include just one or a few studio-quality images per
product (referred to herein as reference images), whilst at test time
recognition is performed on pictures displaying a portion of a shelf containing
several products and taken in the store by cheap cameras (referred to as query
images). Moreover, as the items on sale in a store as well as their appearance
change frequently over time, a practical recognition system should handle
seamlessly new products/packages. Inspired by recent advances in object
detection and image retrieval, we propose to leverage on state of the art
object detectors based on deep learning to obtain an initial productagnostic
item detection. Then, we pursue product recognition through a similarity search
between global descriptors computed on reference and cropped query images. To
maximize performance, we learn an ad-hoc global descriptor by a CNN trained on
reference images based on an image embedding loss. Our system is
computationally expensive at training time but can perform recognition rapidly
and accurately at test time"
126,"Convolutional Neural Networks (CNN) for image recognition tasks are seeing
rapid advances in the available architectures and how networks are trained
based on large computational infrastructure and standard datasets with millions
of images. In contrast, performance and time constraints for example, of small
devices and free cloud GPUs necessitate efficient network training (i.e.,
highest accuracy in the shortest inference time possible), often on small
datasets. Here, we hypothesize that initially decreasing image size during
training makes the training process more efficient, because pre-shaping weights
with small images and later utilizing these weights with larger images reduces
initial network parameters and total inference time. We test this Efficient
Network TRaining (ENTR) Hypothesis by training pre-trained Residual Network
(ResNet) models (ResNet18, 34, & 50) on three small datasets (steel
microstructures, bee images, and geographic aerial images) with a free cloud
GPU. Based on three training regimes of i) not, ii) gradually or iii) in one
step increasing image size over the training process, we show that initially
reducing image size increases training efficiency consistently across datasets
and networks. We interpret these results mechanistically in the framework of
regularization theory. Support for the ENTR hypothesis is an important
contribution, because network efficiency improvements for image recognition
tasks are needed for practical applications. In the future, it will be exciting
to see how the ENTR hypothesis holds for large standard datasets like ImageNet
or CIFAR, to better understand the underlying mechanisms, and how these results
compare to other fields such as structural learning.Comment: 12 pages, 5 figures, 1 table +++ Keywords: Image recognition,
  Efficient Network Training hypothesis, image size increase, network
  efficiency, ResNet models, Google Colaboratory, free cloud GPU, material
  science, geoscience, environmental science, convolutional neural networks,
  regularizatio"
127,"Motivated by vision tasks such as robust face and object recognition, we
consider the following general problem: given a collection of low-dimensional
linear subspaces in a high-dimensional ambient (image) space and a query point
(image), efficiently determine the nearest subspace to the query in $\ell^1$
distance. We show in theory that Cauchy random embedding of the objects into
significantly-lower-dimensional spaces helps preserve the identity of the
nearest subspace with constant probability. This offers the possibility of
efficiently selecting several candidates for accurate search. We sketch
preliminary experiments on robust face and digit recognition to corroborate our
theory.Comment: To appear in NIPS workshop on big learning, 201"
128,"Conceptualizing away the sketch processing details in a user interface will
enable general users and domain experts to create more complex sketches. There
are many domains for which sketch recognition systems are being developed. But
they entail image-processing skill if they are to handle the details of each
domain, and also they are lengthy to build. The implemented system goal is to
enable user interface designers and domain experts who may not have proficiency
in sketch recognition to be able to construct these sketch systems. This sketch
recognition system takes in rough sketches from user drawn with the help of
mouse as its input. It then recognizes the sketch using segmentation and domain
classification, the properties of the user drawn sketch and segments are
searched heuristically in the domains and each figures of each domain, and
finally it shows its domain, the figure name and properties. It also draws the
sketch smoothly. The work is resulted through extensive research and study of
many existing image processing and pattern matching algorithms.Comment: 9 pages; International Journal of Advanced Computer Science and
  Applications, Special Issue on Image Processing and Analysis 1, 2011, 1-"
129,"Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.
However, when dealing with high dimensional inputs, the training of RNNs
becomes computational expensive due to the large number of model parameters.
This hinders RNNs from solving many important computer vision tasks, such as
Action Recognition in Videos and Image Captioning. To overcome this problem, we
propose a compact and flexible structure, namely Block-Term tensor
decomposition, which greatly reduces the parameters of RNNs and improves their
training efficiency. Compared with alternative low-rank approximations, such as
tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only
more concise (when using the same rank), but also able to attain a better
approximation to the original RNNs with much fewer parameters. On three
challenging tasks, including Action Recognition in Videos, Image Captioning and
Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of
both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes
17,388 times fewer parameters than the standard LSTM to achieve an accuracy
improvement over 15.6\% in the Action Recognition task on the UCF11 dataset.Comment: CVPR201"
130,"Objects recognition in image is one of the most difficult problems in
computer vision. It is also an important step for the implementation of several
existing applications that require high-level image interpretation. Therefore,
there is a growing interest in this research area during the last years. In
this paper, we present an algorithm for human detection and recognition in
real-time, from images taken by a CCD camera mounted on a car-like mobile
robot. The proposed technique is based on Histograms of Oriented Gradient (HOG)
and SVM classifier. The implementation of our detector has provided good
results, and can be used in robotics tasks"
131,"In this paper, we proposed a novel pipeline for image-level classification in
the hyperspectral images. By doing this, we show that the discriminative
spectral information at image-level features lead to significantly improved
performance in a face recognition task. We also explored the potential of
traditional feature descriptors in the hyperspectral images. From our
evaluations, we observe that SIFT features outperform the state-of-the-art
hyperspectral face recognition methods, and also the other descriptors. With
the increasing deployment of hyperspectral sensors in a multitude of
applications, we believe that our approach can effectively exploit the spectral
information in hyperspectral images, thus beneficial to more accurate
classification"
132,"Reliable facial expression recognition plays a critical role in human-machine
interactions. However, most of the facial expression analysis methodologies
proposed to date pay little or no attention to the protection of a user's
privacy. In this paper, we propose a Privacy-Preserving Representation-Learning
Variational Generative Adversarial Network (PPRL-VGAN) to learn an image
representation that is explicitly disentangled from the identity information.
At the same time, this representation is discriminative from the standpoint of
facial expression recognition and generative as it allows expression-equivalent
face image synthesis. We evaluate the proposed model on two public datasets
under various threat scenarios. Quantitative and qualitative results
demonstrate that our approach strikes a balance between the preservation of
privacy and data utility. We further demonstrate that our model can be
effectively applied to other tasks such as expression morphing and image
completion"
133,"This paper proposes a face recognition system that can be used to effectively match a face image scanned from an identity (ID) doc-ument against the face image stored in the biometric chip of such a document. The purpose of this specific face recognition algorithm is to aid the automatic detection of forged ID documents where the photography printed on the document’s surface has been altered or replaced. The proposed algorithm uses a novel combination of texture and shape features together with sub-space representation techniques. In addition, the robustness of the proposed algorithm when dealing with more general face recognition tasks has been proven with the Good, the Bad & the Ugly (GBU) dataset, one of the most challenging datasets containing frontal faces. The proposed algorithm has been complement-ed with a novel method that adopts two operating points to enhance the reliability of the algorithm’s final verification decision.Final Accepted Versio"
134,"Image processing and recognition technologies are required to solve various problems. We have already proposed the system which automatically constructs image processing
with Genetic Programming (GP), Automatic Construction of
Tree-structural Image Transformation (ACTIT). However, it is difficult to construct an accurate image processing for all training image sets if they have various characteristics. In this paper, we propose ACTIT-Boost which automatically constructs an accurate image processing by employing Adaptive Boosting (AdaBoost) to ACTIT. It learns training image sets and their areas which are difficultly approximated to target images in particular. We show experimentally that ACTIT-Boost is more effective in comparison with ordinary ACTIT"
135,"Majority of the face recognition algorithms use query faces captured from
uncontrolled, in the wild, environment. Often caused by the cameras limited
capabilities, it is common for these captured facial images to be blurred or
low resolution. Super resolution algorithms are therefore crucial in improving
the resolution of such images especially when the image size is small requiring
enlargement. This paper aims to demonstrate the effect of one of the
state-of-the-art algorithms in the field of image super resolution. To
demonstrate the functionality of the algorithm, various before and after 3D
face alignment cases are provided using the images from the Labeled Faces in
the Wild (lfw). Resulting images are subject to testing on a closed set face
recognition protocol using unsupervised algorithms with high dimension
extracted features. The inclusion of super resolution algorithm resulted in
significant improved recognition rate over recently reported results obtained
from unsupervised algorithms"
136,"In this article we propose a novel face recognition method based on Principal
Component Analysis (PCA) and Log-Gabor filters. The main advantages of the
proposed method are its simple implementation, training, and very high
recognition accuracy. For recognition experiments we used 5151 face images of
1311 persons from different sets of the FERET and AR databases that allow to
analyze how recognition accuracy is affected by the change of facial
expressions, illumination, and aging. Recognition experiments with the FERET
database (containing photographs of 1196 persons) showed that our method can
achieve maximal 97-98% first one recognition rate and 0.3-0.4% Equal Error
Rate. The experiments also showed that the accuracy of our method is less
affected by eye location errors and used image normalization method than of
traditional PCA -based recognition method.Comment: Unpublished manuscript. March 2005. 23 pages, 7 figures, 5 table"
137,"In this paper, we introduce a simple but quite effective recognition
framework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN.
The framework consists of two parallel CNNs, a discriminator and an extra
classifier which takes integrated features from parallel networks and gives
final prediction. The discriminator is core which drives parallel networks to
focus on different regions and learn different representations. The
corresponding training strategy is introduced to ensures utilization of
discriminator. We validate D-PCN with several CNN models on benchmark datasets:
CIFAR-100, and ImageNet, D-PCN enhances all models. In particular it yields
state of the art performance on CIFAR-100 compared with related works. We also
conduct visualization experiment on fine-grained Stanford Dogs dataset to
verify our motivation. Additionally, we apply D-PCN for segmentation on PASCAL
VOC 2012 and also find promotion.Comment: Accepted by ACCV 201"
138,"Face recognition approaches that are based on deep convolutional neural
networks (CNN) have been dominating the field. The performance improvements
they have provided in the so called in-the-wild datasets are significant,
however, their performance under image quality degradations have not been
assessed, yet. This is particularly important, since in real-world face
recognition applications, images may contain various kinds of degradations due
to motion blur, noise, compression artifacts, color distortions, and occlusion.
In this work, we have addressed this problem and analyzed the influence of
these image degradations on the performance of deep CNN-based face recognition
approaches using the standard LFW closed-set identification protocol. We have
evaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and
GoogLeNet. Results have indicated that blur, noise, and occlusion cause a
significant decrease in performance, while deep CNN models are found to be
robust to distortions, such as color distortions and change in color balance.Comment: 8 pages, 3 figure"
139,"Many recent news headlines have labeled face recognition technology as biased
or racist. We report on a methodical investigation into differences in face
recognition accuracy between African-American and Caucasian image cohorts of
the MORPH dataset. We find that, for all four matchers considered, the impostor
and the genuine distributions are statistically significantly different between
cohorts. For a fixed decision threshold, the African-American image cohort has
a higher false match rate and a lower false non-match rate. ROC curves compare
verification rates at the same false match rate, but the different cohorts
achieve the same false match rate at different thresholds. This means that ROC
comparisons are not relevant to operational scenarios that use a fixed decision
threshold. We show that, for the ResNet matcher, the two cohorts have
approximately equal separation of impostor and genuine distributions. Using
ICAO compliance as a standard of image quality, we find that the initial image
cohorts have unequal rates of good quality images. The ICAO-compliant subsets
of the original image cohorts show improved accuracy, with the main effect
being to reducing the low-similarity tail of the genuine distributions.Comment: Paper will appear in the BEFA workshop at CVPR 201"
140,"Statistical pattern recognition methods based on the Coherence Length Diagram
(CLD) have been proposed for medical image analyses, such as quantitative
characterisation of human skin textures, and for polarized light microscopy of
liquid crystal textures. Further investigations are made on image maps
originated from such diagram and some examples related to irregularity of
microstructures are shown"
141,"With the increasing number of computer graphics, image processing, and pattern recognition applications, economical storage, efficient representation and manipulation, and powerful and flexible query languages for retrieval of image data are of paramount importance. These and related issues pertinent to image data bases are examined"
142,"While recent deep neural network models have achieved promising results on
the image captioning task, they rely largely on the availability of corpora
with paired image and sentence captions to describe objects in context. In this
work, we propose the Deep Compositional Captioner (DCC) to address the task of
generating descriptions of novel objects which are not present in paired
image-sentence datasets. Our method achieves this by leveraging large object
recognition datasets and external text corpora and by transferring knowledge
between semantically similar concepts. Current deep caption models can only
describe objects contained in paired image-sentence corpora, despite the fact
that they are pre-trained with large object recognition datasets, namely
ImageNet. In contrast, our model can compose sentences that describe novel
objects and their interactions with other objects. We demonstrate our model's
ability to describe novel concepts by empirically evaluating its performance on
MSCOCO and show qualitative results on ImageNet images of objects for which no
paired image-caption data exist. Further, we extend our approach to generate
descriptions of objects in video clips. Our results show that DCC has distinct
advantages over existing image and video captioning approaches for generating
descriptions of new objects in context"
143,"Image data has a great potential of helping post-earthquake visual
inspections of civil engineering structures due to the ease of data acquisition
and the advantages in capturing visual information. A variety of techniques
have been applied to detect damages automatically from a close-up image of a
structural component. However, the application of the automatic damage
detection methods become increasingly difficult when the image includes
multiple components from different structures. To reduce the inaccurate false
positive alarms, critical structural components need to be recognized first,
and the damage alarms need to be cleaned using the component recognition
results. To achieve the goal, this study aims at recognizing and extracting
bridge components from images of urban scenes. The bridge component recognition
begins with pixel-wise classifications of an image into 10 scene classes. Then,
the original image and the scene classification results are combined to
classify the image pixels into five component classes. The multi-scale
convolutional neural networks (multi-scale CNNs) are used to perform pixel-wise
classification, and the classification results are post-processed by averaging
within superpixels and smoothing by conditional random fields (CRFs). The
performance of the bridge component extraction is tested in terms of accuracy
and consistency"
144,"Recently regression analysis becomes a popular tool for face recognition. The
existing regression methods all use the one-dimensional pixel-based error
model, which characterizes the representation error pixel by pixel individually
and thus neglects the whole structure of the error image. We observe that
occlusion and illumination changes generally lead to a low-rank error image. To
make use of this low-rank structural information, this paper presents a
two-dimensional image matrix based error model, i.e. matrix regression, for
face representation and classification. Our model uses the minimal nuclear norm
of representation error image as a criterion, and the alternating direction
method of multipliers method to calculate the regression coefficients. Compared
with the current regression methods, the proposed Nuclear Norm based Matrix
Regression (NMR) model is more robust for alleviating the effect of
illumination, and more intuitive and powerful for removing the structural noise
caused by occlusion. We experiment using four popular face image databases, the
Extended Yale B database, the AR database, the Multi-PIE and the FRGC database.
Experimental results demonstrate the performance advantage of NMR over the
state-of-the-art regression based face recognition methods.Comment: 30 page"
145,"Recently, it has been shown that performance of a face recognition system depends on the quality of both face images participating in the recognition process: the reference and the test image. In the context of forensic face recognition, this observation has two implications: a) the quality of the trace (extracted from CCTV footage) constrains the performance achievable using a particular face recognition system; b) the quality of the suspect reference set (to which the trace is matched against) can be judiciously chosen to approach optimal recognition performance under such a constraint. Motivated by these recent findings, we propose a framework for forensic face recognition that is based on calibrating the recognition performance for the quality of pairs of images. The application of this framework to several mock-up forensic cases, created entirely from the MultiPIE dataset, shows that optimal recognition performance, under such a constraint, can be achieved by matching the quality (pose, illumination, and, imaging device) of the reference set to that of the trace. This improvement in recognition performance helps reduce the rate of misleading interpretation of the evidence"
146,"Action recognition from still images is an important task of computer vision
applications such as image annotation, robotic navigation, video surveillance
and several others. Existing approaches mainly rely on either bag-of-feature
representations or articulated body-part models. However, the relationship
between the action and the image segments is still substantially unexplored.
For this reason, in this paper we propose to approach action recognition by
leveraging an intermediate layer of ""superpixels"" whose latent classes can act
as attributes of the action. In the proposed approach, the action class is
predicted by a structural model(learnt by Latent Structural SVM) based on
measurements from the image superpixels and their latent classes. Experimental
results over the challenging Stanford 40 Actions dataset report a significant
average accuracy of 74.06% for the positive class and 88.50% for the negative
class, giving evidence to the performance of the proposed approach.Comment: To appear in the Proceedings of the IEEE International Conference on
  Image Processing. Copyright 2015 IEEE. Please be aware of your obligations
  with respect to copyrighted materia"
147,"In the recent time deep learning has achieved huge popularity due to its
performance in various machine learning algorithms. Deep learning as
hierarchical or structured learning attempts to model high level abstractions
in data by using a group of processing layers. The foundation of deep learning
architectures is inspired by the understanding of information processing and
neural responses in human brain. The architectures are created by stacking
multiple linear or non-linear operations. The article mainly focuses on the
state-of-art deep learning models and various real world applications specific
training methods. Selecting optimal architecture for specific problem is a
challenging task, at a closing stage of the article we proposed optimal
approach to deep convolutional architecture for the application of image
recognition"
148,"We present a new circuit for non-Boolean recognition of binary images.
Employing all-spin logic (ASL) devices, we design logic comparators and
non-Boolean decision blocks for compact and efficient computation. By
manipulation of fan-in number in different stages of the circuit, the structure
can be extended for larger training sets or larger images. Operating based on
the mainly similarity idea, the system is capable of constructing a mean image
and compare it with a separate input image within a short decision time. Taking
advantage of the non-volatility of ASL devices, the proposed circuit is capable
of hybrid memory/logic operation. Compared with existing CMOS pattern
recognition circuits, this work achieves a smaller footprint, lower power
consumption, faster decision time and a lower operational voltage. To the best
of our knowledge, this is the first fully spin-based complete pattern
recognition circuit demonstrated using spintronic devices.Comment: This article is accepted to appear in IEEE Transactions on
  Nanotechnolog"
149,"Emotion is a key element in user-generated videos. However, it is difficult
to understand emotions conveyed in such videos due to the complex and
unstructured nature of user-generated content and the sparsity of video frames
expressing emotion. In this paper, for the first time, we study the problem of
transferring knowledge from heterogeneous external sources, including image and
textual data, to facilitate three related tasks in understanding video emotion:
emotion recognition, emotion attribution and emotion-oriented summarization.
Specifically, our framework (1) learns a video encoding from an auxiliary
emotional image dataset in order to improve supervised video emotion
recognition, and (2) transfers knowledge from an auxiliary textual corpora for
zero-shot recognition of emotion classes unseen during training. The proposed
technique for knowledge transfer facilitates novel applications of emotion
attribution and emotion-oriented summarization. A comprehensive set of
experiments on multiple datasets demonstrate the effectiveness of our
framework.Comment: 13 pages, 11 figures. Published at the IEEE Transactions on Affective
  Computin"
150,"In recent years, the widespread use of deep neural networks (DNNs) has
facilitated great improvements in performance for computer vision tasks like
image classification and object recognition. In most realistic computer vision
applications, an input image undergoes some form of image distortion such as
blur and additive noise during image acquisition or transmission. Deep networks
trained on pristine images perform poorly when tested on such distortions. In
this paper, we evaluate the effect of image distortions like Gaussian blur and
additive noise on the activations of pre-trained convolutional filters. We
propose a metric to identify the most noise susceptible convolutional filters
and rank them in order of the highest gain in classification accuracy upon
correction. In our proposed approach called DeepCorrect, we apply small stacks
of convolutional layers with residual connections, at the output of these
ranked filters and train them to correct the worst distortion affected filter
activations, whilst leaving the rest of the pre-trained filter outputs in the
network unchanged. Performance results show that applying DeepCorrect models
for common vision tasks like image classification (ImageNet), object
recognition (Caltech-101, Caltech-256) and scene classification (SUN-397),
significantly improves the robustness of DNNs against distorted images and
outperforms other alternative approaches..Comment: Accepted to IEEE Transactions on Image Processing, April 2019. For
  associated code, see https://github.com/tsborkar/DeepCorrec"
151,"Motion blur, out of focus, insufficient spatial resolution, lossy compression
and many other factors can all cause an image to have poor quality. However,
image quality is a largely ignored issue in traditional pattern recognition
literature. In this paper, we use face detection and recognition as case
studies to show that image quality is an essential factor which will affect the
performances of traditional algorithms. We demonstrated that it is not the
image quality itself that is the most important, but rather the quality of the
images in the training set should have similar quality as those in the testing
set. To handle real-world application scenarios where images with different
kinds and severities of degradation can be presented to the system, we have
developed a quality classified image analysis framework to deal with images of
mixed qualities adaptively. We use deep neural networks first to classify
images based on their quality classes and then design a separate face detector
and recognizer for images in each quality class. We will present experimental
results to show that our quality classified framework can accurately classify
images based on the type and severity of image degradations and can
significantly boost the performances of state-of-the-art face detector and
recognizer in dealing with image datasets containing mixed quality images.Comment: 6 page"
152,"Vehicle tracking is an integral part of intelligent traffic management
systems. Previous implementations of vehicle tracking used Global Positioning
System(GPS) based systems that gave location of the vehicle of an individual on
their smartphones.The proposed system uses a novel approach to vehicle tracking
using Vehicle License plate detection and recognition (VLPR) technique, which
can be integrated on a large scale with traffic management systems. Initial
methods of implementing VLPR used simple image processing techniques which were
quite experimental and heuristic. With the onset of Deep learning and Computer
Vision, one can create robust VLPR systems that can produce results close to
human efficiency. Previous implementations, based on deep learning, made use of
object detection and support vector machines for detection and a heuristic
image processing based approach for recognition. The proposed system makes use
of scene text detection model architecture for License plate detection and for
recognition it uses the Optical character recognition engine (OCR) Tesseract.
The proposed system obtained extraordinary results when it was tested on a
highway video using NVIDIA Ge-force RTX 2080ti GPU, results were obtained at a
speed of 30 frames per second with accuracy close to human.Comment: 6 pages, 13 figure"
153,"The recognition of 3-D objects from sequences of their 2-D views is modeled by a neural architecture, called VIEWNET that uses View Information Encoded With NETworks. VIEWNET illustrates how several types of noise and varialbility in image data can be progressively removed while incornplcte image features are restored and invariant features are discovered using an appropriately designed cascade of processing stages. VIEWNET first processes 2-D views of 3-D objects using the CORT-X 2 filter, which discounts the illuminant, regularizes and completes figural boundaries, and removes noise from the images. Boundary regularization and cornpletion are achieved by the same mechanisms that suppress image noise. A log-polar transform is taken with respect to the centroid of the resulting figure and then re-centered to achieve 2-D scale and rotation invariance. The invariant images are coarse coded to further reduce noise, reduce foreshortening effects, and increase generalization. These compressed codes are input into a supervised learning system based on the fuzzy ARTMAP algorithm. Recognition categories of 2-D views are learned before evidence from sequences of 2-D view categories is accumulated to improve object recognition. Recognition is studied with noisy and clean images using slow and fast learning. VIEWNET is demonstrated on an MIT Lincoln Laboratory database of 2-D views of jet aircraft with and without additive noise. A recognition rate of 90% is achieved with one 2-D view category and of 98.5% correct with three 2-D view categories.National Science Foundation (IRI 90-24877); Office of Naval Research (N00014-91-J-1309, N00014-91-J-4100, N00014-92-J-0499); Air Force Office of Scientific Research (F9620-92-J-0499, 90-0083"
154,"Thermal infrared (IR) images represent the heat patterns emitted from hot
object and they do not consider the energies reflected from an object. Objects
living or non-living emit different amounts of IR energy according to their
body temperature and characteristics. Humans are homoeothermic and hence
capable of maintaining constant temperature under different surrounding
temperature. Face recognition from thermal (IR) images should focus on changes
of temperature on facial blood vessels. These temperature changes can be
regarded as texture features of images and wavelet transform is a very good
tool to analyze multi-scale and multi-directional texture. Wavelet transform is
also used for image dimensionality reduction, by removing redundancies and
preserving original features of the image. The sizes of the facial images are
normally large. So, the wavelet transform is used before image similarity is
measured. Therefore this paper describes an efficient approach of human face
recognition based on wavelet transform from thermal IR images. The system
consists of three steps. At the very first step, human thermal IR face image is
preprocessed and the face region is only cropped from the entire image.
Secondly, Haar wavelet is used to extract low frequency band from the cropped
face region. Lastly, the image classification between the training images and
the test images is done, which is based on low-frequency components. The
proposed approach is tested on a number of human thermal infrared face images
created at our own laboratory and Terravic Facial IR Database. Experimental
results indicated that the thermal infra red face images can be recognized by
the proposed system effectively. The maximum success of 95% recognition has
been achieved.Comment: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.100"
155,"In this paper, we propose a new unsupervised feature learning framework,
namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer
architecture for visual object recognition tasks. The main innovation of the
framework is that it connects the sparse-encoders from different layers by a
sparse-to-dense module. The sparse-to-dense module is a composition of a local
spatial pooling step and a low-dimensional embedding process, which takes
advantage of the spatial smoothness information in the image. As a result, the
new method is able to learn several levels of sparse representation of the
image which capture features at a variety of abstraction levels and
simultaneously preserve the spatial smoothness between the neighboring image
patches. Combining the feature representations from multiple layers, DeepSC
achieves the state-of-the-art performance on multiple object recognition tasks.Comment: 9 pages, submitted to ICL"
156,"We propose a novel approach to template based face recognition. Our dual goal
is to both increase recognition accuracy and reduce the computational and
storage costs of template matching. To do this, we leverage on an approach
which was proven effective in many other domains, but, to our knowledge, never
fully explored for face images: average pooling of face photos. We show how
(and why!) the space of a template's images can be partitioned and then pooled
based on image quality and head pose and the effect this has on accuracy and
template size. We perform extensive tests on the IJB-A and Janus CS2 template
based face identification and verification benchmarks. These show that not only
does our approach outperform published state of the art despite requiring far
fewer cross template comparisons, but also, surprisingly, that image pooling
performs on par with deep feature pooling.Comment: Appeared in the IEEE Computer Society Workshop on Biometrics, IEEE
  Conf. on Computer Vision and Pattern Recognition (CVPR), June, 201"
157,"Inspired by recent successes in neural machine translation and image caption
generation, we present an attention based encoder decoder model (AED) to
recognize Vietnamese Handwritten Text. The model composes of two parts: a
DenseNet for extracting invariant features, and a Long Short-Term Memory
network (LSTM) with an attention model incorporated for generating output text
(LSTM decoder), which are connected from the CNN part to the attention model.
The input of the CNN part is a handwritten text image and the target of the
LSTM decoder is the corresponding text of the input image. Our model is trained
end-to-end to predict the text from a given input image since all the parts are
differential components. In the experiment section, we evaluate our proposed
AED model on the VNOnDB-Word and VNOnDB-Line datasets to verify its efficiency.
The experiential results show that our model achieves 12.30% of word error rate
without using any language model. This result is competitive with the
handwriting recognition system provided by Google in the Vietnamese Online
Handwritten Text Recognition competition"
158,"This paper highlights the factors to consider in entering into sports agreements including the tax treatment applied to image rights and the recent update and commentary of the Inland Revenue (HRMC) to their capital gains tax manual on the taxation of image rights.  The commentary of HRMC on image rights is yet another indication that, although traditional common law remedies will evolve further in the light of the growing commercialisation of image rights, a formal legislative or jurisprudential recognition of personality rights remains unlikely in the near future in the UK"
159,"A major focus of current research on place recognition is visual localization
for autonomous driving. In this scenario, as cameras will be operating
continuously, it is realistic to expect videos as an input to visual
localization algorithms, as opposed to the single-image querying approach used
in other place recognition works. In this paper, we show that exploiting
temporal continuity in the testing sequence significantly improves visual
localization - qualitatively and quantitatively. Although intuitive, this idea
has not been fully explored in recent works. Our main contribution is a novel
Monte Carlo-based visual localization technique that can efficiently reason
over the image sequence. Also, we propose an image retrieval pipeline which
relies on local features and an encoding technique to represent an image as a
single vector. The experimental results show that our proposed method achieves
better results than state-of-the-art approaches for the task on visual
localization under significant appearance change. Our synthetic dataset and
source code are publicly made available.Comment: Best paper award at DICTA 201"
160,"This paper proposes a novel deep architecture to address multi-label image
recognition, a fundamental and practical task towards general visual
understanding. Current solutions for this task usually rely on an extra step of
extracting hypothesis regions (i.e., region proposals), resulting in redundant
computation and sub-optimal performance. In this work, we achieve the
interpretable and contextualized multi-label image classification by developing
a recurrent memorized-attention module. This module consists of two alternately
performed components: i) a spatial transformer layer to locate attentional
regions from the convolutional feature maps in a region-proposal-free way and
ii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict
semantic labeling scores on the located regions while capturing the global
dependencies of these regions. The LSTM also output the parameters for
computing the spatial transformer. On large-scale benchmarks of multi-label
image classification (e.g., MS-COCO and PASCAL VOC 07), our approach
demonstrates superior performances over other existing state-of-the-arts in
both accuracy and efficiency.Comment: Accepted at ICCV 201"
161,"Background and Object: In China, body constitution is highly related to
physiological and pathological functions of human body and determines the
tendency of the disease, which is of great importance for treatment in clinical
medicine. Tongue diagnosis, as a key part of Traditional Chinese Medicine
inspection, is an important way to recognize the type of constitution.In order
to deploy tongue image constitution recognition system on non-invasive mobile
device to achieve fast, efficient and accurate constitution recognition, an
efficient method is required to deal with the challenge of this kind of complex
environment. Methods: In this work, we perform the tongue area detection,
tongue area calibration and constitution classification using methods which are
based on deep convolutional neural network. Subject to the variation of
inconstant environmental condition, the distribution of the picture is uneven,
which has a bad effect on classification performance. To solve this problem, we
propose a method based on the complexity of individual instances to divide
dataset into two subsets and classify them separately, which is capable of
improving classification accuracy. To evaluate the performance of our proposed
method, we conduct experiments on three sizes of tongue datasets, in which deep
convolutional neural network method and traditional digital image analysis
method are respectively applied to extract features for tongue images. The
proposed method is combined with the base classifier Softmax, SVM, and
DecisionTree respectively. Results: As the experiments results shown, our
proposed method improves the classification accuracy by 1.135% on average and
achieves 59.99% constitution classification accuracy. Conclusions: Experimental
results on three datasets show that our proposed method can effectively improve
the classification accuracy of tongue constitution recognition"
162,"In this paper, we discuss the adaptation of our decentralized place
recognition method described in [1] to full image descriptors. As we had shown,
the key to making a scalable decentralized visual place recognition lies in
exploting deterministic key assignment in a distributed key-value map. Through
this, it is possible to reduce bandwidth by up to a factor of n, the robot
count, by casting visual place recognition to a key-value lookup problem. In
[1], we exploited this for the bag-of-words method [3], [4]. Our method of
casting bag-of-words, however, results in a complex decentralized system, which
has inherently worse recall than its centralized counterpart. In this paper, we
instead start from the recent full-image description method NetVLAD [5]. As we
show, casting this to a key-value lookup problem can be achieved with k-means
clustering, and results in a much simpler system than [1]. The resulting system
still has some flaws, albeit of a completely different nature: it suffers when
the environment seen during deployment lies in a different distribution in
feature space than the environment seen during training.Comment: 3 pages, 4 figures. This is a self-published paper that accompanies
  our original work [1] as well as the ICRA 2017 Workshop on Multi-robot
  Perception-Driven Control and Planning [2"
163,"Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a
software package for the training, classifying, and feature extraction of
images. The UCF Sports Action dataset is a widely used machine learning dataset
that has 200 videos taken in 720x480 resolution of 9 different sporting
activities: diving, golf, swinging, kicking, lifting, horseback riding,
running, skateboarding, swinging (various gymnastics), and walking. In this
report we report on a caffe feature extraction pipeline of images taken from
the videos of the UCF Sports Action dataset. A similar test was performed on
overfeat, and results were inferior to caffe. This study is intended to explore
the architecture and hyper parameters needed for effective static analysis of
action in videos and classification over a variety of image datasets.Comment: 12 pages. 11 tables. 0 Images. Written Summer 201"
164,"In this paper, we are interested in the few-shot learning problem. In
particular, we focus on a challenging scenario where the number of categories
is large and the number of examples per novel category is very limited, e.g. 1,
2, or 3. Motivated by the close relationship between the parameters and the
activations in a neural network associated with the same category, we propose a
novel method that can adapt a pre-trained neural network to novel categories by
directly predicting the parameters from the activations. Zero training is
required in adaptation to novel categories, and fast inference is realized by a
single forward pass. We evaluate our method by doing few-shot image recognition
on the ImageNet dataset, which achieves the state-of-the-art classification
accuracy on novel categories by a significant margin while keeping comparable
performance on the large-scale categories. We also test our method on the
MiniImageNet dataset and it strongly outperforms the previous state-of-the-art
methods"
165,"We propose a novel approach based on deep Convolutional Neural Networks (CNN)
to recognize human actions in still images by predicting the future motion, and
detecting the shape and location of the salient parts of the image. We make the
following major contributions to this important area of research: (i) We use
the predicted future motion in the static image (Walker et al., 2015) as a
means of compensating for the missing temporal information, while using the
saliency map to represent the the spatial information in the form of location
and shape of what is predicted as significant. (ii) We cast action
classification in static images as a domain adaptation problem by transfer
learning. We first map the input static image to a new domain that we refer to
as the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tune
the layers of a deep CNN model trained on classifying the ImageNet dataset to
perform action classification in the POF-SM domain. (iii) We tested our method
on the popular Willow dataset. But unlike existing methods, we also tested on a
more realistic and challenging dataset of over 2M still images that we
collected and labeled by taking random frames from the UCF-101 video dataset.
We call our dataset the UCF Still Image dataset or UCFSI-101 in short. Our
results outperform the state of the art"
166,"Convolutional neural networks (CNNs) have rapidly risen in popularity for
many machine learning applications, particularly in the field of image
recognition. Much of the benefit generated from these networks comes from their
ability to extract features from the data in a hierarchical manner. These
features are extracted using various transformational layers, notably the
convolutional layer which gives the model its name. In this work, we introduce
a new type of transformational layer called a quantum convolution, or
quanvolutional layer. Quanvolutional layers operate on input data by locally
transforming the data using a number of random quantum circuits, in a way that
is similar to the transformations performed by random convolutional filter
layers. Provided these quantum transformations produce meaningful features for
classification purposes, then the overall algorithm could be quite useful for
near term quantum computing, because it requires small quantum circuits with
little to no error correction. In this work, we empirically evaluated the
potential benefit of these quantum transformations by comparing three types of
models built on the MNIST dataset: CNNs, quantum convolutional neural networks
(QNNs), and CNNs with additional non-linearities introduced. Our results showed
that the QNN models had both higher test set accuracy as well as faster
training compared to the purely classical CNNs.Comment: 7 pages, 3 figure"
167,"In the paper a piecewise constant image approximations of sequential number
of pixel clusters or segments are treated. A majorizing of optimal
approximation sequence by hierarchical sequence of image approximations is
studied. Transition from pixel clustering to image segmentation by reducing of
segment numbers in clusters is provided. Algorithms are proved by elementary
formulas.Comment: 5 pages, 3 figures, 4 formulas, submitted to the 12 International
  Conference on Pattern Recognition and Information Processing May 28-30, 2014,
  Minsk, Belaru"
168,"This paper addresses the problem of automated vehicle tracking and
recognition from aerial image sequences. Motivated by its successes in the
existing literature focus on the use of linear appearance subspaces to describe
multi-view object appearance and highlight the challenges involved in their
application as a part of a practical system. A working solution which includes
steps for data extraction and normalization is described. In experiments on
real-world data the proposed methodology achieved promising results with a high
correct recognition rate and few, meaningful errors (type II errors whereby
genuinely similar targets are sometimes being confused with one another).
Directions for future research and possible improvements of the proposed method
are discussed"
169,"A conceptually simple way to classify images is to directly compare test-set
data and training-set data. The accuracy of this approach is limited by the
method of comparison used, and by the extent to which the training-set data
cover configuration space. Here we show that this coverage can be substantially
increased using simple strategies of coarse graining (replacing groups of
images by their centroids) and stochastic sampling (using distinct sets of
centroids in combination). We use the MNIST and Fashion-MNIST data sets to show
that coarse graining can be used to convert a subset of training images into
many fewer image centroids, with no loss of accuracy of classification of
test-set images by direct (nearest-neighbor) classification. Distinct batches
of centroids can be used in combination as a means of stochastically sampling
configuration space, and can classify test-set data more accurately than can
the unaltered training set. The approach works most naturally with multiple
processors in parallel"
170,"International Conference Image Analysis and Recognition (ICIAR 2018, Póvoa de Varzim, Portugal"
171,"Recognizing multiple labels of images is a fundamental but challenging task
in computer vision, and remarkable progress has been attained by localizing
semantic-aware image regions and predicting their labels with deep
convolutional neural networks. The step of hypothesis regions (region
proposals) localization in these existing multi-label image recognition
pipelines, however, usually takes redundant computation cost, e.g., generating
hundreds of meaningless proposals with non-discriminative information and
extracting their features, and the spatial contextual dependency modeling among
the localized regions are often ignored or over-simplified. To resolve these
issues, this paper proposes a recurrent attention reinforcement learning
framework to iteratively discover a sequence of attentional and informative
regions that are related to different semantic objects and further predict
label scores conditioned on these regions. Besides, our method explicitly
models long-term dependencies among these attentional regions that help to
capture semantic label co-occurrence and thus facilitate multi-label
recognition. Extensive experiments and comparisons on two large-scale
benchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior
performance over existing state-of-the-art methods in both performance and
efficiency as well as explicitly identifying image-level semantic labels to
specific object regions.Comment: Accepted at AAAI 201"
172,"Depth is one of the keys that make neural networks succeed in the task of
large-scale image recognition. The state-of-the-art network architectures
usually increase the depths by cascading convolutional layers or building
blocks. In this paper, we present an alternative method to increase the depth.
Our method is by introducing computation orderings to the channels within
convolutional layers or blocks, based on which we gradually compute the outputs
in a channel-wise manner. The added orderings not only increase the depths and
the learning capacities of the networks without any additional computation
costs, but also eliminate the overlap singularities so that the networks are
able to converge faster and perform better. Experiments show that the networks
based on our method achieve the state-of-the-art performances on CIFAR and
ImageNet datasets"
173,"It is of great importance to preserve locality and similarity information in
semi-supervised learning (SSL) based applications. Graph based SSL and manifold
regularization based SSL including Laplacian regularization (LapR) and
Hypergraph Laplacian regularization (HLapR) are representative SSL methods and
have achieved prominent performance by exploiting the relationship of sample
distribution. However, it is still a great challenge to exactly explore and
exploit the local structure of the data distribution. In this paper, we present
an effect and effective approximation algorithm of Hypergraph p-Laplacian and
then propose Hypergraph p-Laplacian regularization (HpLapR) to preserve the
geometry of the probability distribution. In particular, p-Laplacian is a
nonlinear generalization of the standard graph Laplacian and Hypergraph is a
generalization of a standard graph. Therefore, the proposed HpLapR provides
more potential to exploiting the local structure preserving. We apply HpLapR to
logistic regression and conduct the implementations for remote sensing image
recognition. We compare the proposed HpLapR to several popular manifold
regularization based SSL methods including LapR, HLapR and HpLapR on UC-Merced
dataset. The experimental results demonstrate the superiority of the proposed
HpLapR.Comment: 9 pages, 6 figure"
174,"Recognition of human emotions from the imaging templates is useful in a wide
variety of human-computer interaction and intelligent systems applications.
However, the automatic recognition of facial expressions using image template
matching techniques suffer from the natural variability with facial features
and recording conditions. In spite of the progress achieved in facial emotion
recognition in recent years, the effective and computationally simple feature
selection and classification technique for emotion recognition is still an open
problem. In this paper, we propose an efficient and straightforward facial
emotion recognition algorithm to reduce the problem of inter-class pixel
mismatch during classification. The proposed method includes the application of
pixel normalization to remove intensity offsets followed-up with a Min-Max
metric in a nearest neighbor classifier that is capable of suppressing feature
outliers. The results indicate an improvement of recognition performance from
92.85% to 98.57% for the proposed Min-Max classification method when tested on
JAFFE database. The proposed emotion recognition technique outperforms the
existing template matching methods"
175,"We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge"
176,"Recently, manifold regularized semi-supervised learning (MRSSL) received
considerable attention because it successfully exploits the geometry of the
intrinsic data probability distribution including both labeled and unlabeled
samples to leverage the performance of a learning model. As a natural nonlinear
generalization of graph Laplacian, p-Laplacian has been proved having the rich
theoretical foundations to better preserve the local structure. However, it is
difficult to determine the fitting graph p-Lapalcian i.e. the parameter which
is a critical factor for the performance of graph p-Laplacian. Therefore, we
develop an ensemble p-Laplacian regularization (EpLapR) to fully approximate
the intrinsic manifold of the data distribution. EpLapR incorporates multiple
graphs into a regularization term in order to sufficiently explore the
complementation of graph p-Laplacian. Specifically, we construct a fused graph
by introducing an optimization approach to assign suitable weights on different
p-value graphs. And then, we conduct semi-supervised learning framework on the
fused graph. Extensive experiments on UC-Merced data set demonstrate the
effectiveness and efficiency of the proposed method.Comment: 13 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1806.0810"
177,"Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection.Comment: 5 pages, 1 figur"
178,"An automated data reduction system for the analysis of interference fringe patterns obtained using the particle image velocimetry technique is described. This system is based on digital image processing techniques that have provided the flexibility and speed needed to obtain more complete automation of the data reduction process. As approached here, this process includes scanning/searching for data on the photographic record, recognition of fringe patterns of sufficient quality, and, finally, analysis of these fringes to determine a local measure of the velocity magnitude and direction. The fringe analysis as well as the fringe image recognition are based on full frame autocorrelation techniques using parallel processing capabilities"
179,"Human doing actions will result in WiFi distortion, which is widely explored
for action recognition, such as the elderly fallen detection, hand sign
language recognition, and keystroke estimation. As our best survey, past work
recognizes human action by categorizing one complete distortion series into one
action, which we term as series-level action recognition. In this paper, we
introduce a much more fine-grained and challenging action recognition task into
WiFi sensing domain, i.e., sample-level action recognition. In this task, every
WiFi distortion sample in the whole series should be categorized into one
action, which is a critical technique in precise action localization,
continuous action segmentation, and real-time action recognition. To achieve
WiFi-based sample-level action recognition, we fully analyze approaches in
image-based semantic segmentation as well as in video-based frame-level action
recognition, then propose a simple yet efficient deep convolutional neural
network, i.e., Temporal Unet. Experimental results show that Temporal Unet
achieves this novel task well. Codes have been made publicly available at
https://github.com/geekfeiw/WiSLAR.Comment: 14 pages, 14 figures, 1 tabl"
180,"This paper is an overview of Image Processing and Analysis using Scilab, a
free prototyping environment for numerical calculations similar to Matlab. We
demonstrate the capabilities of SIP -- the Scilab Image Processing Toolbox --
which extends Scilab with many functions to read and write images in over 100
major file formats, including PNG, JPEG, BMP, and TIFF. It also provides
routines for image filtering, edge detection, blurring, segmentation, shape
analysis, and image recognition. Basic directions to install Scilab and SIP are
given, and also a mini-tutorial on Scilab. Three practical examples of image
analysis are presented, in increasing degrees of complexity, showing how
advanced image analysis techniques seems uncomplicated in this environment.Comment: 16 pages, 6 figure"
181,"Spatial Pyramid Matching (SPM) and its variants have achieved a lot of
success in image classification. The main difference among them is their
encoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of
Vector Quantization (VQ) into the framework of SPM. Although the methods
achieve a higher recognition rate than the traditional SPM, they consume more
time to encode the local descriptors extracted from the image. In this paper,
we propose using Low Rank Representation (LRR) to encode the descriptors under
the framework of SPM. Different from SC, LRR considers the group effect among
data points instead of sparsity. Benefiting from this property, the proposed
method (i.e., LrrSPM) can offer a better performance. To further improve the
generalizability and robustness, we reformulate the rank-minimization problem
as a truncated projection problem. Extensive experimental studies show that
LrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving
competitive recognition rates on nine image data sets.Comment: accepted into knowledge based systems, 201"
182,"Gabor functions have wide-spread applications in image processing and
computer vision. In this paper, we prove that 2D Gabor functions are
translation-invariant positive-definite kernels and propose a novel formulation
for the problem of image representation with Gabor functions based on infinite
kernel learning regression. Using this formulation, we obtain a support vector
expansion of an image based on a mixture of Gabor functions. The problem with
this representation is that all Gabor functions are present at all support
vector pixels. Applying LASSO to this support vector expansion, we obtain a
sparse representation in which each Gabor function is positioned at a very
small set of pixels. As an application, we introduce a method for learning a
dataset-specific set of Gabor filters that can be used subsequently for feature
extraction. Our experiments show that use of the learned Gabor filters improves
the recognition accuracy of a recently introduced face recognition algorithm"
183,"Deep convolutional neural networks (CNNs) have been immensely successful in
many high-level computer vision tasks given large labeled datasets. However,
for video semantic object segmentation, a domain where labels are scarce,
effectively exploiting the representation power of CNN with limited training
data remains a challenge. Simply borrowing the existing pretrained CNN image
recognition model for video segmentation task can severely hurt performance. We
propose a semi-supervised approach to adapting CNN image recognition model
trained from labeled image data to the target domain exploiting both semantic
evidence learned from CNN, and the intrinsic structures of video data. By
explicitly modeling and compensating for the domain shift from the source
domain to the target domain, this proposed approach underpins a robust semantic
object segmentation method against the changes in appearance, shape and
occlusion in natural videos. We present extensive experiments on challenging
datasets that demonstrate the superior performance of our approach compared
with the state-of-the-art methods"
184,"Deep neural networks (DNNs) have demonstrated success for many supervised
learning tasks, ranging from voice recognition, object detection, to image
classification. However, their increasing complexity might yield poor
generalization error that make them hard to be deployed on edge devices.
Quantization is an effective approach to compress DNNs in order to meet these
constraints. Using a quasiconvex base function in order to construct a binary
quantizer helps training binary neural networks (BNNs) and adding noise to the
input data or using a concrete regularization function helps to improve
generalization error. Here we introduce foothill function, an infinitely
differentiable quasiconvex function. This regularizer is flexible enough to
deform towards $L_1$ and $L_2$ penalties. Foothill can be used as a binary
quantizer, as a regularizer, or as a loss. In particular, we show this
regularizer reduces the accuracy gap between BNNs and their full-precision
counterpart for image classification on ImageNet.Comment: Accepted in 16th International Conference of Image Analysis and
  Recognition (ICIAR 2019"
185,"We present a baseline convolutional neural network (CNN) structure and image
preprocessing methodology to improve facial expression recognition algorithm
using CNN. To analyze the most efficient network structure, we investigated
four network structures that are known to show good performance in facial
expression recognition. Moreover, we also investigated the effect of input
image preprocessing methods. Five types of data input (raw, histogram
equalization, isotropic smoothing, diffusion-based normalization, difference of
Gaussian) were tested, and the accuracy was compared. We trained 20 different
CNN models (4 networks x 5 data input types) and verified the performance of
each network with test images from five different databases. The experiment
result showed that a three-layer structure consisting of a simple convolutional
and a max pooling layer with histogram equalization image input was the most
efficient. We describe the detailed training procedure and analyze the result
of the test accuracy based on considerable observation.Comment: 6 pages, RO-MAN2016 Conferenc"
186,"Many previous methods have showed the importance of considering semantically
relevant objects for performing event recognition, yet none of the methods have
exploited the power of deep convolutional neural networks to directly integrate
relevant object information into a unified network. We present a novel unified
deep CNN architecture which integrates architecturally different, yet
semantically-related object detection networks to enhance the performance of
the event recognition task. Our architecture allows the sharing of the
convolutional layers and a fully connected layer which effectively integrates
event recognition, rigid object detection and non-rigid object detection.Comment: submitted to IEEE International Conference on Image Processing 201"
187,"We present a new action recognition deep neural network which adaptively
learns the best action velocities in addition to the classification. While deep
neural networks have reached maturity for image understanding tasks, we are
still exploring network topologies and features to handle the richer
environment of video clips. Here, we tackle the problem of multiple velocities
in action recognition, and provide state-of-the-art results for gesture
recognition, on known and new collected datasets. We further provide the
training steps for our semi-supervised network, suited to learn from huge
unlabeled datasets with only a fraction of labeled examples"
188,"This paper addresses the task of zero-shot image classification. The key
contribution of the proposed approach is to control the semantic embedding of
images -- one of the main ingredients of zero-shot learning -- by formulating
it as a metric learning problem. The optimized empirical criterion associates
two types of sub-task constraints: metric discriminating capacity and accurate
attribute prediction. This results in a novel expression of zero-shot learning
not requiring the notion of class in the training phase: only pairs of
image/attributes, augmented with a consistency indicator, are given as ground
truth. At test time, the learned model can predict the consistency of a test
image with a given set of attributes , allowing flexible ways to produce
recognition inferences. Despite its simplicity, the proposed approach gives
state-of-the-art results on four challenging datasets used for zero-shot
recognition evaluation.Comment: in ECCV 2016, Oct 2016, amsterdam, Netherlands. 201"
189,"This work has been done as part of the EU VICAR (IST) project and the EU SCOFI project (IAP). The aim of the first project was to develop a real time video indexing classification annotation and retrieval system. For our systems, we have adapted the approach of Picard and Minka [3], who categorized elements of a scene automatically with so-called ’stuff’ categories (e.g., grass, sky, sand, stone). Campbell et al. [1] use similar concepts to describe certain parts of an image, which they named “labeled image regions”. However, they did not use these elements to classify the topic of the scene. Subsequently, we developed a generic approach for the recognition of visual scenes, where an alphabet of basic visual elements (or “typed patches”) is used to classify the topic of a scene. We define a new image element: a patch, which is a group of adjacent pixels within an image, described by a specific local pixel distribution, brightness, and color. In contrast with pixels, a patch as a whole can incorporate semantics. A patch is described by a HSI color histogram with 16 bins and by three texture features (i.e., the variance and two values based on the two eigen values of the covariance matrix of the Intensity values of a mask ran over the image. For more details on the features used we refer to Israel et al. [2]. We aimed at describing each image as a vector with a fixed size and with information about the position of patches that is not strict (strict position would limit generalization). Therefore, a fixed grid is placed over the image and each grid cell is segmented into patches, which are then categorized by a patch classifier. For each grid cell a frequency vector of its classified patches is calculated. These vectors are concate- nated. The resulting vector describes the complete image. Several grids were applied and several patch sizes with the grid cells were tested. Grid size of 3x2 combined with patches of size 16x16 provided the best system performance. For the two classification phases of our system, back-propagation networks were trained: (i) classification of the patches and (ii) classification of the image vector, as a whole. The system was tested on the classification of eight categories of scenes from the Corel database: interiors, city/street, forest, agriculture/countryside, desert, sea, portrait, and crowds. Each of these categories were relevant for the VICAR project. Based upon their relevance for these eight categories of scenes, we choose nine categories for the classification of the patches: building, crowd, grass, road, sand, skin, sky, tree, and water. This approach was found to be successful (for classification of the patches 87.5% correct, and classification of the scenes 73.8% correct). An advantage of our method is its low computational complexity. Moreover, the classified patches themselves are intermediate image representations and can be used for image classification, image segmentation as well as for image matching. A disadvantage is that the patches with which the classifiers were trained had to be manually classified. To solve this drawback, we currently develop algorithms for automatic extraction of relevant patch types. Within the IST project VICAR, a video indexing system was built for the Netherlands Institute for Sound and Vision1, consisting of four independent mod- ules: car recognition, face recognition, movement recognition (of people) and scene recognition. The latter module was based upon the afore mentioned approach. Within the IAP project SCOFI, a real time Internet pornography filter was built, based upon this approach. The system is currently running on several schools in Europe. Within the SCOFI filtering system, our image classification system (with a performance of 92% correct) works together with a text classi- fication system that includes a proxy server (FilterX, developed by Demokritos, Greece) to classify web-pages. Its total performance is 0% overblocking and 1% underblocking"
190,"Single-sample face recognition is one of the most challenging problems in
face recognition. We propose a novel algorithm to address this problem based on
a sparse representation based classification (SRC) framework. The new algorithm
is robust to image misalignment and pixel corruption, and is able to reduce
required gallery images to one sample per class. To compensate for the missing
illumination information traditionally provided by multiple gallery images, a
sparse illumination learning and transfer (SILT) technique is introduced. The
illumination in SILT is learned by fitting illumination examples of auxiliary
face images from one or more additional subjects with a sparsely-used
illumination dictionary. By enforcing a sparse representation of the query
image in the illumination dictionary, the SILT can effectively recover and
transfer the illumination and pose information from the alignment stage to the
recognition stage. Our extensive experiments have demonstrated that the new
algorithms significantly outperform the state of the art in the single-sample
regime and with less restrictions. In particular, the single-sample face
alignment accuracy is comparable to that of the well-known Deformable SRC
algorithm using multiple gallery images per class. Furthermore, the face
recognition accuracy exceeds those of the SRC and Extended SRC algorithms using
hand labeled alignment initialization"
191,"This paper addresses the problem of learning word image representations:
given the cropped image of a word, we are interested in finding a descriptive,
robust, and compact fixed-length representation. Machine learning techniques
can then be supplied with these representations to produce models useful for
word retrieval or recognition tasks. Although many works have focused on the
machine learning aspect once a global representation has been produced, little
work has been devoted to the construction of those base image representations:
most works use standard coding and aggregation techniques directly on top of
standard computer vision features such as SIFT or HOG.
  We propose to learn local mid-level features suitable for building word image
representations. These features are learnt by leveraging character bounding box
annotations on a small set of training images. However, contrary to other
approaches that use character bounding box information, our approach does not
rely on detecting the individual characters explicitly at testing time. Our
local mid-level features can then be aggregated to produce a global word image
signature. When pairing these features with the recent word attributes
framework of Almaz\'an et al., we obtain results comparable with or better than
the state-of-the-art on matching and recognition tasks using global descriptors
of only 96 dimensions"
192,"We present an attention-based model for end-to-end handwriting recognition.
Our system does not require any segmentation of the input paragraph. The model
is inspired by the differentiable attention models presented recently for
speech recognition, image captioning or translation. The main difference is the
covert and overt attention, implemented as a multi-dimensional LSTM network.
Our principal contribution towards handwriting recognition lies in the
automatic transcription without a prior segmentation into lines, which was
crucial in previous approaches. To the best of our knowledge this is the first
successful attempt of end-to-end multi-line handwriting recognition. We carried
out experiments on the well-known IAM Database. The results are encouraging and
bring hope to perform full paragraph transcription in the near future"
193,"This paper demonstrates two different fusion techniques at two different
levels of a human face recognition process. The first one is called data fusion
at lower level and the second one is the decision fusion towards the end of the
recognition process. At first a data fusion is applied on visual and
corresponding thermal images to generate fused image. Data fusion is
implemented in the wavelet domain after decomposing the images through
Daubechies wavelet coefficients (db2). During the data fusion maximum of
approximate and other three details coefficients are merged together. After
that Principle Component Analysis (PCA) is applied over the fused coefficients
and finally two different artificial neural networks namely Multilayer
Perceptron(MLP) and Radial Basis Function(RBF) networks have been used
separately to classify the images. After that, for decision fusion based
decisions from both the classifiers are combined together using Bayesian
formulation. For experiments, IRIS thermal/visible Face Database has been used.
Experimental results show that the performance of multiple classifier system
along with decision fusion works well over the single classifier system.Comment: Keywords: Thermal Image, Visual Image, Fused Image, Data Fusion,
  Wavelet decomposition, Decision Fusion, Classificatio"
194,"In order to establish face recognition system in rehabilitation nursing
robots beds and achieve real-time monitor the patient on the bed. We propose a
face recognition method based on partial matching Hu moments which apply for
rehabilitation nursing robots beds. Firstly we using Haar classifier to detect
human faces automatically in dynamic video frames. Secondly we using Otsu
threshold method to extract facial features (eyebrows, eyes, mouth) in the face
image and its Hu moments. Finally, we using Hu moment feature set to achieve
the automatic face recognition. Experimental results show that this method can
efficiently identify face in a dynamic video and it has high practical value
(the accuracy rate is 91% and the average recognition time is 4.3s)"
195,"A sample-relaxed two-dimensional color principal component analysis
(SR-2DCPCA) approach is presented for face recognition and image reconstruction
based on quaternion models. A relaxation vector is automatically generated
according to the variances of training color face images with the same label. A
sample-relaxed, low-dimensional covariance matrix is constructed based on all
the training samples relaxed by a relaxation vector, and its eigenvectors
corresponding to the $r$ largest eigenvalues are defined as the optimal
projection. The SR-2DCPCA aims to enlarge the global variance rather than to
maximize the variance of the projected training samples. The numerical results
based on real face data sets validate that SR-2DCPCA has a higher recognition
rate than state-of-the-art methods and is efficient in image reconstruction.Comment: 18 pages, 7 figure"
196,"In this paper, we propose a micro hand gesture recognition system and methods
using ultrasonic active sensing. This system uses micro dynamic hand gestures
for recognition to achieve human-computer interaction (HCI). The implemented
system, called hand-ultrasonic gesture (HUG), consists of ultrasonic active
sensing, pulsed radar signal processing, and time-sequence pattern recognition
by machine learning. We adopt lower frequency (300 kHz) ultrasonic active
sensing to obtain high resolution range-Doppler image features. Using high
quality sequential range-Doppler features, we propose a state-transition-based
hidden Markov model for gesture recognition. This method achieves a recognition
accuracy of nearly 90\% by using symbolized range-Doppler features and
significantly reduces the computational complexity and power consumption.
Furthermore, to achieve higher classification accuracy, we utilize an
end-to-end neural network model and obtain a recognition accuracy of 96.32\%.
In addition to offline analysis, a real-time prototype is released to verify
our method's potential for application in the real world"
197,"Feature descriptors involved in image processing are generally manually
chosen and high dimensional in nature. Selecting the most important features is
a very crucial task for systems like facial expression recognition. This paper
investigates the performance of deep autoencoders for feature selection and
dimension reduction for facial expression recognition on multiple levels of
hidden layers. The features extracted from the stacked autoencoder outperformed
when compared to other state-of-the-art feature selection and dimension
reduction techniques"
198,"Advances in image restoration and enhancement techniques have led to
discussion about how such algorithmscan be applied as a pre-processing step to
improve automatic visual recognition. In principle, techniques like deblurring
and super-resolution should yield improvements by de-emphasizing noise and
increasing signal in an input image. But the historically divergent goals of
the computational photography and visual recognition communities have created a
significant need for more work in this direction. To facilitate new research,
we introduce a new benchmark dataset called UG^2, which contains three
difficult real-world scenarios: uncontrolled videos taken by UAVs and manned
gliders, as well as controlled videos taken on the ground. Over 160,000
annotated frames forhundreds of ImageNet classes are available, which are used
for baseline experiments that assess the impact of known and unknown image
artifacts and other conditions on common deep learning-based object
classification approaches. Further, current image restoration and enhancement
techniques are evaluated by determining whether or not theyimprove baseline
classification performance. Results showthat there is plenty of room for
algorithmic innovation, making this dataset a useful tool going forward.Comment: Supplemental material: https://goo.gl/vVM1xe, Dataset:
  https://goo.gl/AjA6En, CVPR 2018 Prize Challenge: ug2challenge.or"
199,"Subspace-based holistic registration is introduced as an alternative to landmark-based face registration, which has a poor performance on low-resolution images, as obtained in camera surveillance applications. The proposed registration method finds the alignment by maximizing the similarity score between a probe and a gallery image. We use a novel probabilistic framework for both user-independent as well as user-specific face registration. The similarity is calculated using the probability that the face image is correctly aligned in a face subspace, but additionally we take the probability into account that the face is misaligned based on the residual error in the dimensions perpendicular to the face subspace. We perform extensive experiments on the FRGCv2 database to evaluate the impact that the face registration methods have on face recognition. Subspace-based holistic registration on low-resolution images can improve face recognition in comparison with landmark-based registration on high-resolution images. The performance of the tested face recognition methods after subspace-based holistic registration on a low-resolution version of the FRGC database is similar to that after manual registration"
200,"Real-world image recognition is often challenged by the variability of visual
styles including object textures, lighting conditions, filter effects, etc.
Although these variations have been deemed to be implicitly handled by more
training data and deeper networks, recent advances in image style transfer
suggest that it is also possible to explicitly manipulate the style
information. Extending this idea to general visual recognition problems, we
present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary
styles from images. Considering certain style features play an essential role
in discriminative tasks, BIN learns to selectively normalize only disturbing
styles while preserving useful styles. The proposed normalization module is
easily incorporated into existing network architectures such as Residual
Networks, and surprisingly improves the recognition performance in various
scenarios. Furthermore, experiments verify that BIN effectively adapts to
completely different tasks like object classification and style transfer, by
controlling the trade-off between preserving and removing style variations. BIN
can be implemented with only a few lines of code using popular deep learning
frameworks"
201,"Freehand sketches often contain sparse visual detail. In spite of the
sparsity, they are easily and consistently recognized by humans across
cultures, languages and age groups. Therefore, analyzing such sparse sketches
can aid our understanding of the neuro-cognitive processes involved in visual
representation and recognition. In the recent past, Convolutional Neural
Networks (CNNs) have emerged as a powerful framework for feature representation
and recognition for a variety of image domains. However, the domain of sketch
images has not been explored. This paper introduces a freehand sketch
recognition framework based on ""deep"" features extracted from CNNs. We use two
popular CNNs for our experiments -- Imagenet CNN and a modified version of
LeNet CNN. We evaluate our recognition framework on a publicly available
benchmark database containing thousands of freehand sketches depicting everyday
objects. Our results are an improvement over the existing state-of-the-art
accuracies by 3% - 11%. The effectiveness and relative compactness of our deep
features also make them an ideal candidate for related problems such as
sketch-based image retrieval. In addition, we provide a preliminary glimpse of
how such features can help identify crucial attributes (e.g. object-parts) of
the sketched objects.Comment: Submitted to ICIP-2015, 5 pages. Removed an erroneous claim regarding
  a work cited in the pape"
202,"The heat transfer performance of Plate Fin Heat Sink (PFHS) has been
investigated experimentally and extensively. Commonly, the objective function
of the PFHS design is based on the responses of simulations. Compared with
existing studies, the purpose of this study is to transfer from analysis-based
model to image-based one for heat sink designs. Compared with the popular
objective function based on maximum, mean, variance values etc., more
information should be involved in image-based and thus a more objective model
should be constructed. It means that the sequential optimization should be
based on images instead of responses and more reasonable solutions should be
obtained. Therefore, an image-based reconstruction model of a heat transfer
process for a 3D-PFHS is established. Unlike image recognition, such procedure
cannot be implemented by existing recognition algorithms (e.g. Convolutional
Neural Network) directly. Therefore, a Reconstructive Neural Network (ReConNN),
integrated supervised learning and unsupervised learning techniques, is
suggested and improved to achieve higher accuracy. According to the
experimental results, the heat transfer process can be observed more detailed
and clearly, and the reconstructed results are meaningful for the further
optimizations"
203,"Automatic facial emotion recognition is a challenging task that has gained
significant scientific interest over the past few years, but the problem of
emotion recognition for a group of people has been less extensively studied.
However, it is slowly gaining popularity due to the massive amount of data
available on social networking sites containing images of groups of people
participating in various social events. Group emotion recognition is a
challenging problem due to obstructions like head and body pose variations,
occlusions, variable lighting conditions, variance of actors, varied indoor and
outdoor settings and image quality. The objective of this task is to classify a
group's perceived emotion as Positive, Neutral or Negative. In this report, we
describe our solution which is a hybrid machine learning system that
incorporates deep neural networks and Bayesian classifiers. Deep Convolutional
Neural Networks (CNNs) work from bottom to top, analysing facial expressions
expressed by individual faces extracted from the image. The Bayesian network
works from top to bottom, inferring the global emotion for the image, by
integrating the visual features of the contents of the image obtained through a
scene descriptor. In the final pipeline, the group emotion category predicted
by an ensemble of CNNs in the bottom-up module is passed as input to the
Bayesian Network in the top-down module and an overall prediction for the image
is obtained. Experimental results show that the stated system achieves 65.27%
accuracy on the validation set which is in line with state-of-the-art results.
As an outcome of this project, a Progressive Web Application and an
accompanying Android app with a simple and intuitive user interface are
presented, allowing users to test out the system with their own pictures"
204,"This paper presents face recognition using maximum a posteriori (MAP)
discriminant on YCbCr color space. The YCbCr color space is considered in order
to cover the skin information of face image on the recognition process. The
proposed method is employed to improve the recognition rate and equal error
rate (EER) of the gray scale based face recognition. In this case, the face
features vector consisting of small part of dominant frequency elements which
is extracted by non-blocking DCT is implemented as dimensional reduction of the
raw face images. The matching process between the query face features and the
trained face features is performed using maximum a posteriori (MAP)
discriminant. From the experimental results on data from four face databases
containing 2268 images with 196 classes show that the face recognition YCbCr
color space provide better recognition rate and lesser EER than those of gray
scale based face recognition which improve the first rank of grayscale based
method result by about 4%. However, it requires three times more computation
time than that of grayscale based method"
205,"Various factors, such as identities, views (poses), and illuminations, are
coupled in face images. Disentangling the identity and view representations is
a major challenge in face recognition. Existing face recognition systems either
use handcrafted features or learn features discriminatively to improve
recognition accuracy. This is different from the behavior of human brain.
Intriguingly, even without accessing 3D data, human not only can recognize face
identity, but can also imagine face images of a person under different
viewpoints given a single 2D image, making face perception in the brain robust
to view changes. In this sense, human brain has learned and encoded 3D face
models from 2D images. To take into account this instinct, this paper proposes
a novel deep neural net, named multi-view perceptron (MVP), which can untangle
the identity and view features, and infer a full spectrum of multi-view images
in the meanwhile, given a single 2D face image. The identity features of MVP
achieve superior performance on the MultiPIE dataset. MVP is also capable to
interpolate and predict images under viewpoints that are unobserved in the
training data"
206,"We propose a novel couple mappings method for low resolution face recognition
using deep convolutional neural networks (DCNNs). The proposed architecture
consists of two branches of DCNNs to map the high and low resolution face
images into a common space with nonlinear transformations. The branch
corresponding to transformation of high resolution images consists of 14 layers
and the other branch which maps the low resolution face images to the common
space includes a 5-layer super-resolution network connected to a 14-layer
network. The distance between the features of corresponding high and low
resolution images are backpropagated to train the networks. Our proposed method
is evaluated on FERET data set and compared with state-of-the-art competing
methods. Our extensive experimental results show that the proposed method
significantly improves the recognition performance especially for very low
resolution probe face images (11.4% improvement in recognition accuracy).
Furthermore, it can reconstruct a high resolution image from its corresponding
low resolution probe image which is comparable with state-of-the-art
super-resolution methods in terms of visual quality.Comment: 11 pages, 8 figure"
207,"Existing methods to recognize actions in static images take the images at
their face value, learning the appearances---objects, scenes, and body
poses---that distinguish each action class. However, such models are deprived
of the rich dynamic structure and motions that also define human activity. We
propose an approach that hallucinates the unobserved future motion implied by a
single snapshot to help static-image action recognition. The key idea is to
learn a prior over short-term dynamics from thousands of unlabeled videos,
infer the anticipated optical flow on novel static images, and then train
discriminative models that exploit both streams of information. Our main
contributions are twofold. First, we devise an encoder-decoder convolutional
neural network and a novel optical flow encoding that can translate a static
image into an accurate flow map. Second, we show the power of hallucinated flow
for recognition, successfully transferring the learned motion into a standard
two-stream network for activity recognition. On seven datasets, we demonstrate
the power of the approach. It not only achieves state-of-the-art accuracy for
dense optical flow prediction, but also consistently enhances recognition of
actions and dynamic scenes.Comment: Published in CVPR 2018, project page:
  http://vision.cs.utexas.edu/projects/im2flow"
208,"Given a food image, can a fine-grained object recognition engine tell ""which
restaurant which dish"" the food belongs to? Such ultra-fine grained image
recognition is the key for many applications like search by images, but it is
very challenging because it needs to discern subtle difference between classes
while dealing with the scarcity of training data. Fortunately, the ultra-fine
granularity naturally brings rich relationships among object classes. This
paper proposes a novel approach to exploit the rich relationships through
bipartite-graph labels (BGL). We show how to model BGL in an overall
convolutional neural networks and the resulting system can be optimized through
back-propagation. We also show that it is computationally efficient in
inference thanks to the bipartite structure. To facilitate the study, we
construct a new food benchmark dataset, which consists of 37,885 food images
collected from 6 restaurants and totally 975 menus. Experimental results on
this new food and three other datasets demonstrates BGL advances previous works
in fine-grained object recognition. An online demo is available at
http://www.f-zhou.com/fg_demo/"
209,"We present a new approach for recognition of complex graphic symbols in
technical documents. Graphic symbol recognition is a well known challenge in
the field of document image analysis and is at heart of most graphic
recognition systems. Our method uses structural approach for symbol
representation and statistical classifier for symbol recognition. In our system
we represent symbols by their graph based signatures: a graphic symbol is
vectorized and is converted to an attributed relational graph, which is used
for computing a feature vector for the symbol. This signature corresponds to
geometry and topology of the symbol. We learn a Bayesian network to encode
joint probability distribution of symbol signatures and use it in a supervised
learning scenario for graphic symbol recognition. We have evaluated our method
on synthetically deformed and degraded images of pre-segmented 2D architectural
and electronic symbols from GREC databases and have obtained encouraging
recognition rates.Comment: 5 pages, 8 figures, Tenth International Conference on Document
  Analysis and Recognition (ICDAR), IEEE Computer Society, 2009, volume 10,
  1325-132"
210,"For pattern recognition like image recognition, it has become clear that each
machine-learning dictionary data actually became data in probability space
belonging to Euclidean space. However, the distances in the Euclidean space and
the distances in the probability space are separated and ununified when machine
learning is introduced in the pattern recognition. There is still a problem
that it is impossible to directly calculate an accurate matching relation
between the sampling data of the read image and the learned dictionary data. In
this research, we focused on the reason why the distance is changed and the
extent of change when passing through the probability space from the original
Euclidean distance among data belonging to multiple probability spaces
containing Euclidean space. By finding the reason of the cause of the distance
error and finding the formula expressing the error quantitatively, a possible
distance formula to unify Euclidean space and probability space is found. Based
on the results of this research, the relationship between machine-learning
dictionary data and sampling data was clearly understood for pattern
recognition. As a result, the calculation of collation among data and
machine-learning to compete mutually between data are cleared, and complicated
calculations became unnecessary. Finally, using actual pattern recognition
data, experimental demonstration of a possible distance formula to unify
Euclidean space and probability space discovered by this research was carried
out, and the effectiveness of the result was confirmed"
211,"Understanding how cities visually differ from each others is interesting for
planners, residents, and historians. We investigate the interpretation of deep
features learned by convolutional neural networks (CNNs) for city recognition.
Given a trained city recognition network, we first generate weighted masks
using the known Grad-CAM technique and to select the most discriminate regions
in the image. Since the image classification label is the city name, it
contains no information of objects that are class-discriminate, we investigate
the interpretability of deep representations with two methods. (i) Unsupervised
method is used to cluster the objects appearing in the visual explanations.
(ii) A pretrained semantic segmentation model is used to label objects in pixel
level, and then we introduce statistical measures to quantitatively evaluate
the interpretability of discriminate objects. The influence of network
architectures and random initializations in training, is studied on the
interpretability of CNN features for city recognition. The results suggest that
network architectures would affect the interpretability of learned visual
representations greater than different initializations.Comment: CVPR-19 workshop on Explainable A"
212,"For image recognition, an extensive number of methods have been proposed to
overcome the high-dimensionality problem of feature vectors being used. These
methods vary from unsupervised to supervised, and from statistics to
graph-theory based. In this paper, the most popular and the state-of-the-art
methods for dimensionality reduction are firstly reviewed, and then a new and
more efficient manifold-learning method, named Soft Locality Preserving Map
(SLPM), is presented. Furthermore, feature generation and sample selection are
proposed to achieve better manifold learning. SLPM is a graph-based
subspace-learning method, with the use of k-neighbourhood information and the
class information. The key feature of SLPM is that it aims to control the level
of spread of the different classes, because the spread of the classes in the
underlying manifold is closely connected to the generalizability of the learned
subspace. Our proposed manifold-learning method can be applied to various
pattern recognition applications, and we evaluate its performances on facial
expression recognition. Experiments on databases, such as the Bahcesehir
University Multilingual Affective Face Database (BAUM-2), the Extended
Cohn-Kanade (CK+) Database, the Japanese Female Facial Expression (JAFFE)
Database, and the Taiwanese Facial Expression Image Database (TFEID), show that
SLPM can effectively reduce the dimensionality of the feature vectors and
enhance the discriminative power of the extracted features for expression
recognition. Furthermore, the proposed feature-generation method can improve
the generalizability of the underlying manifolds for facial expression
recognition"
213,"This work studies the representational mapping across multimodal data such
that given a piece of the raw data in one modality the corresponding semantic
description in terms of the raw data in another modality is immediately
obtained. Such a representational mapping can be found in a wide spectrum of
real-world applications including image/video retrieval, object recognition,
action/behavior recognition, and event understanding and prediction. To that
end, we introduce a simplified training objective for learning multimodal
embeddings using the skip-gram architecture by introducing convolutional
""pseudowords:"" embeddings composed of the additive combination of distributed
word representations and image features from convolutional neural networks
projected into the multimodal space. We present extensive results of the
representational properties of these embeddings on various word similarity
benchmarks to show the promise of this approach"
214,"The arrangement of products in store shelves is carefully planned to maximize
sales and keep customers happy. However, verifying compliance of real shelves
to the ideal layout is a costly task routinely performed by the store
personnel. In this paper, we propose a computer vision pipeline to recognize
products on shelves and verify compliance to the planned layout. We deploy
local invariant features together with a novel formulation of the product
recognition problem as a sub-graph isomorphism between the items appearing in
the given image and the ideal layout. This allows for auto-localizing the given
image within the aisle or store and improving recognition dramatically.Comment: Slightly extended version of the paper accepted at ICIAP 2017. More
  information @project_page -->
  http://vision.disi.unibo.it/index.php?option=com_content&view=article&id=111&catid=7"
215,"Práce se zabývá způsoby rozpoznávání obrazu a moţnostmi jejich vyhodnocování. Součástí je návrh a implementace metody porovnání vlivu předzpracování na vybrané algoritmy rozpoznávání obrazu. Metoda byla testována a posouzena pro algoritmy lineární klasifikace, k-nearest neighbors, AdaBoost a SVM.This work deals with pattern recognition methods and possibilities of their evaluation. It involves design and implementation of the method comparing the influence of preprocessing to pattern recognition algorithms. The method was tested and evaluated for linear classification algorithm, k-Nearest Neighbors, AdaBoost and SVM."
216,"Front end of data collection and loading into database manually may cause
potential errors in data sets and a very time consuming process. Scanning of a
data document in the form of an image and recognition of corresponding
information in that image can be considered as a possible solution of this
challenge. This paper presents an automated solution for the problem of data
cleansing and recognition of user written data to transform into standard
printed format with the help of artificial neural networks. Three different
neural models namely direct, correlation based and hierarchical have been
developed to handle this issue. In a very hostile input environment, the
solution is developed to justify the proposed logic.Comment: 11 pages, 13 figures, International Journal on Computational Sciences
  & Applications (IJCSA) Vol.3, No.6, December 201"
217,"This paper studies complexity of recognition of classes of bounded
configurations by a generalization of conventional cellular automata (CA) --
finite dynamic cellular automata (FDCA). Inspired by the CA-based models of
biological and computer vision, this study attempts to derive the properties of
a complexity measure and of the classes of input configurations that make it
beneficial to realize the recognition via a two-layered automaton as compared
to a one-layered automaton. A formalized model of an image pattern recognition
task is utilized to demonstrate that the derived conditions can be satisfied
for a non-empty set of practical problems.Comment: 11 pages, 1 figur"
218,"AbstractFor the purpose of information management on postmark according to the date, the paper put forward a method of postmark date recognition based on machine vision, which could meet the demands of personal postmark collectors. On the basis of the relative theories of machine vision, image processing and pattern recognition, the overall process is introduced in the paper from postmark image acquisition to date recognition. Firstly, threshold method is used to generate binary image from smoothed postmark image. So region of date numbers could be extracted from binary image according to different region features. Then regions of date numbers which are connected or broken could be processed through mathematical morphology of binary image. Individual regions of date numbers are obtained for recognition. Finally, classification and pattern recognition based on support vector machine make date numbers classified and date recognition is implemented correctly"
219,"Image based rendering is a fundamental problem in computer vision and
graphics. Modern techniques often rely on depth image for the 3D construction.
However for most of the existing depth cameras, the large and unpredictable
noises can be problematic, which can cause noticeable artifacts in the rendered
results. In this paper, we proposed an efficacious method for depth image noise
removal that can be applied for most RGBD systems. The proposed solution will
benefit many subsequent vision problems such as 3D reconstruction, novel view
rendering, object recognition. Our experimental results demonstrate the
efficacy and accuracy"
220,"Recently, two-dimensional canonical correlation analysis (2DCCA) has been
successfully applied for image feature extraction. The method instead of
concatenating the columns of the images to the one-dimensional vectors,
directly works with two-dimensional image matrices. Although 2DCCA works well
in different recognition tasks, it lacks a probabilistic interpretation. In
this paper, we present a probabilistic framework for 2DCCA called probabilistic
2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the
parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to 2DCCA.
For real data, three subsets of AR face database and also the UMIST face
database confirm the robustness of the proposed algorithm in face recognition
tasks with different illumination conditions, facial expressions, poses and
occlusions"
221,"This paper presents an automated algorithm to determine DNA fragment size from atomic force microscope images and to extract the molecular profiles. The sizing of DNA fragments is a widely used procedure for investigating the physical properties of individual or protein-bound DNA molecules. Several atomic force microscope (AFM) real and computer-generated images were tested for different pixel and fragment sizes and for different background noises. The automated approach minimizes processing time with respect to manual and semi-automated DNA sizing. Moreover, the DNA molecule profile recognition can be used to perform further structural analysis. For computer-generated images, the root mean square error incurred by the automated algorithm in the length estimation is 0.6% for a 7.8 nm image pixel size and 0.34% for a 3.9 nm image pixel size. For AFM real images we obtain a distribution of lengths with a standard deviation of 2.3% of mean and a measured average length very close to the real one, with an error around 0.33%"
222,"Optical Character Recognition (OCR) is one of the important fields in image
processing and pattern recognition domain. Handwritten character recognition
has always been a challenging task. Only a little work can be traced towards
the recognition of handwritten characters for the south Indian languages.
Kannada is one such south Indian language which is also one of the official
language of India. Accurate recognition of Kannada characters is a challenging
task because of the high degree of similarity between the characters. Hence,
good quality features are to be extracted and better classifiers are needed to
improve the accuracy of the OCR for Kannada characters. This paper explores the
effectiveness of feature extraction method like run length count (RLC) and
directional chain code (DCC) for the recognition of handwritten Kannada
numerals. In this paper, a classifier fusion method is implemented to improve
the recognition rate. For the classifier fusion, we have considered K-nearest
neighbour (KNN) and Linear classifier (LC). The novelty of this method is to
achieve better accuracy with few features using classifier fusion approach.
Proposed method achieves an average recognition rate of 96%.Comment: 6 pages having 3 tables and 9 figures. Published in ICECT 2012
  conferenc"
223,"This is the author's accepted manuscript. The final published article is available from the link below. Copyright @ 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works.In this paper, we present a novel and efficient gait recognition system. The proposed system uses two novel gait representations, i.e., the shifted energy image and the gait structural profile, which have increased robustness to some classes of structural variations. Furthermore, we introduce a novel method for the simulation of walking conditions and the generation of artificial subjects that are used for the application of linear discriminant analysis. In the decision stage, the two representations are fused. Thorough experimental evaluation, conducted using one traditional and two new databases, demonstrates the advantages of the proposed system in comparison with current state-of-the-art systems"
224,"In this paper, we propose a novel image set representation and classification
method by maximizing the margin of image sets. The margin of an image set is
defined as the difference of the distance to its nearest image set from
different classes and the distance to its nearest image set of the same class.
By modeling the image sets by using both their image samples and their affine
hull models, and maximizing the margins of the images sets, the image set
representation parameter learning problem is formulated as an minimization
problem, which is further optimized by an expectation -maximization (EM)
strategy with accelerated proximal gradient (APG) optimization in an iterative
algorithm. To classify a given test image set, we assign it to the class which
could provide the largest margin. Experiments on two applications of
video-sequence-based face recognition demonstrate that the proposed method
significantly outperforms state-of-the-art image set classification methods in
terms of both effectiveness and efficiency"
225,"Skin recognition is used in many applications ranging from algorithms for
face detection, hand gesture analysis, and to objectionable image filtering. In
this work a skin recognition system was developed and tested. While many skin
segmentation algorithms relay on skin color, our work relies on both skin color
and texture features (features derives from the GLCM) to give a better and more
efficient recognition accuracy of skin textures. We used feed forward neural
networks to classify input textures images to be skin or non skin textures. The
system gave very encouraging results during the neural network generalization
face.Comment: 4 pages, 6 figures, conference ACIT 2008, Tunisi"
226,"Face verification and recognition problems have seen rapid progress in recent
years, however recognition from small size images remains a challenging task
that is inherently intertwined with the task of face super-resolution. Tackling
this problem using multiple frames is an attractive idea, yet requires solving
the alignment problem that is also challenging for low-resolution faces. Here
we present a holistic system for multi-frame recognition, alignment, and
superresolution of faces. Our neural network architecture restores the central
frame of each input sequence additionally taking into account a number of
adjacent frames and making use of sub-pixel movements. We present our results
using the popular dataset for video face recognition (YouTube Faces). We show a
notable improvement of identification score compared to several baselines
including the one based on single-image super-resolution"
227,"The sparse, hierarchical, and modular processing of natural signals is
related to the ability of humans to recognize objects with high accuracy. In
this study, we report a sparse feature processing and encoding method, which
improved the recognition performance of an automated object recognition system.
Randomly distributed localized gradient enhanced features were selected before
employing aggregate functions for representation, where we used a modular and
hierarchical approach to detect the object features. These object features were
combined with a minimum distance classifier, thereby obtaining object
recognition system accuracies of 93% using the Amsterdam library of object
images (ALOI) database, 92% using the Columbia object image library (COIL)-100
database, and 69% using the PASCAL visual object challenge 2007 database. The
object recognition performance was shown to be robust to variations in noise,
object scaling, and object shifts. Finally, a comparison with eight existing
object recognition methods indicated that our new method improved the
recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%
with the PASCAL visual object challenge 2007 database.Comment: Pages 1"
228,"Human motion recognition is one of the most important branches of
human-centered research activities. In recent years, motion recognition based
on RGB-D data has attracted much attention. Along with the development in
artificial intelligence, deep learning techniques have gained remarkable
success in computer vision. In particular, convolutional neural networks (CNN)
have achieved great success for image-based tasks, and recurrent neural
networks (RNN) are renowned for sequence-based problems. Specifically, deep
learning methods based on the CNN and RNN architectures have been adopted for
motion recognition using RGB-D data. In this paper, a detailed overview of
recent advances in RGB-D-based motion recognition is presented. The reviewed
methods are broadly categorized into four groups, depending on the modality
adopted for recognition: RGB-based, depth-based, skeleton-based and
RGB+D-based. As a survey focused on the application of deep learning to
RGB-D-based motion recognition, we explicitly discuss the advantages and
limitations of existing techniques. Particularly, we highlighted the methods of
encoding spatial-temporal-structural information inherent in video sequence,
and discuss potential directions for future research"
229,"The basic scheme for automated holographic analysis involves an optical system for reconstruction of the three dimensional real image of the droplet field, a spatial scanning system to transport a digitizing X-y image sensor through the real image, and processing algorithms for droplet recognition which establish the droplet sizes and positions. The hardware for system demonstrated includes the expanded and collimated beam from a 5 mW helium-neon laser for hologram reconstruction, an imaging lens for magnification of the real image field, and a video camera and digitizer providing 512-by-512 pixel resolution with 8-bit digitization. A mechanical stage is used to scan the hologram in three dimensional space, maintaining constant image magnification. A test droplet hologram is used for development and testing of the image processing algorithms"
230,"Active Appearance Model (AAM) is a commonly used method for facial image
analysis with applications in face identification and facial expression
recognition. This paper proposes a new approach based on image alignment for
AAM fitting called bidirectional warping. Previous approaches warp either the
input image or the appearance template. We propose to warp both the input
image, using incremental update by an affine transformation, and the appearance
template, using an inverse compositional approach. Our experimental results on
Multi-PIE face database show that the bidirectional approach outperforms
state-of-the-art inverse compositional fitting approaches in extracting
landmark points of faces with shape and pose variations"
231,"Image super-resolution is a process to enhance image resolution. It is widely
used in medical imaging, satellite imaging, target recognition, etc. In this
paper, we conduct continuous modeling and assume that the unknown image
intensity function is defined on a continuous domain and belongs to a space
with a redundant basis. We propose a new iterative model for single image
super-resolution based on an observation: an image is consisted of smooth
components and non-smooth components, and we use two classes of approximated
Heaviside functions (AHFs) to represent them respectively. Due to sparsity of
the non-smooth components, a $L_{1}$ model is employed. In addition, we apply
the proposed iterative model to image patches to reduce computation and
storage. Comparisons with some existing competitive methods show the
effectiveness of the proposed method"
232,"Approaches to goal recognition have progressively relaxed the requirements
about the amount of domain knowledge and available observations, yielding
accurate and efficient algorithms capable of recognizing goals. However, to
recognize goals in raw data, recent approaches require either human engineered
domain knowledge, or samples of behavior that account for almost all actions
being observed to infer possible goals. This is clearly too strong a
requirement for real-world applications of goal recognition, and we develop an
approach that leverages advances in recurrent neural networks to perform goal
recognition as a classification task, using encoded plan traces for training.
We empirically evaluate our approach against the state-of-the-art in goal
recognition with image-based domains, and discuss under which conditions our
approach is superior to previous ones.Comment: Added/Fixed some reference"
233,This work describes the process of face synthesis by image morphing from less expensive 3D sensors such as KINECT that are prone to sensor noise. Its main aim is to create a useful face database for future face recognition studies.Peer reviewe
234,"Deep neural networks have recently achieved tremendous success in image
classification. Recent studies have however shown that they are easily misled
into incorrect classification decisions by adversarial examples. Adversaries
can even craft attacks by querying the model in black-box settings, where no
information about the model is released except its final decision. Such
decision-based attacks usually require lots of queries, while real-world image
recognition systems might actually restrict the number of queries. In this
paper, we propose qFool, a novel decision-based attack algorithm that can
generate adversarial examples using a small number of queries. The qFool method
can drastically reduce the number of queries compared to previous
decision-based attacks while reaching the same quality of adversarial examples.
We also enhance our method by constraining adversarial perturbations in
low-frequency subspace, which can make qFool even more computationally
efficient. Altogether, we manage to fool commercial image recognition systems
with a small number of queries, which demonstrates the actual effectiveness of
our new algorithm in practice"
235,"High resolution satellite image sequences are multidimensional signals
composed of spatio-temporal patterns associated to numerous and various
phenomena. Bayesian methods have been previously proposed in (Heas and Datcu,
2005) to code the information contained in satellite image sequences in a graph
representation using Bayesian methods. Based on such a representation, this
paper further presents a supervised learning methodology of semantics
associated to spatio-temporal patterns occurring in satellite image sequences.
It enables the recognition and the probabilistic retrieval of similar events.
Indeed, graphs are attached to statistical models for spatio-temporal
processes, which at their turn describe physical changes in the observed scene.
Therefore, we adjust a parametric model evaluating similarity types between
graph patterns in order to represent user-specific semantics attached to
spatio-temporal phenomena. The learning step is performed by the incremental
definition of similarity types via user-provided spatio-temporal pattern
examples attached to positive or/and negative semantics. From these examples,
probabilities are inferred using a Bayesian network and a Dirichlet model. This
enables to links user interest to a specific similarity model between graph
patterns. According to the current state of learning, semantic posterior
probabilities are updated for all possible graph patterns so that similar
spatio-temporal phenomena can be recognized and retrieved from the image
sequence. Few experiments performed on a multi-spectral SPOT image sequence
illustrate the proposed spatio-temporal recognition method"
236,"The goal of the present study is to explore the application of deep
convolutional network features to emotion recognition. Results indicate that
they perform similarly to other published models at a best recognition rate of
94.4%, and do so with a single still image rather than a video stream. An
implementation of an affective feedback game is also described, where a
classifier using these features tracks the facial expressions of a player in
real-time.Comment: 6 pages, 8 figures, IEEE styl"
237,"Although deep convolutional neural networks (CNNs) have achieved great
success in computer vision tasks, its real-world application is still impeded
by its voracious demand of computational resources. Current works mostly seek
to compress the network by reducing its parameters or parameter-incurred
computation, neglecting the influence of the input image on the system
complexity. Based on the fact that input images of a CNN contain substantial
redundancy, in this paper, we propose a unified framework, dubbed as ThumbNet,
to simultaneously accelerate and compress CNN models by enabling them to infer
on one thumbnail image. We provide three effective strategies to train
ThumbNet. In doing so, ThumbNet learns an inference network that performs
equally well on small images as the original-input network on large images.
With ThumbNet, not only do we obtain the thumbnail-input inference network that
can drastically reduce computation and memory requirements, but also we obtain
an image downscaler that can generate thumbnail images for generic
classification tasks. Extensive experiments show the effectiveness of ThumbNet,
and demonstrate that the thumbnail-input inference network learned by ThumbNet
can adequately retain the accuracy of the original-input network even when the
input images are downscaled 16 times"
238,"Image Processing, Optimization and Prediction of an Image play a key role in
Computer Science. Image processing provides a way to analyze and identify an
image .Many areas like medical image processing, Satellite images, natural
images and artificial images requires lots of analysis and research on
optimization. In Image Optimization and Prediction we are combining the
features of Query Optimization, Image Processing and Prediction . Image
optimization is used in Pattern analysis, object recognition, in medical Image
processing to predict the type of diseases, in satellite images for predicting
weather forecast, availability of water or mineral etc. Image Processing,
Optimization and analysis is a wide open area for research .Lots of research
has been conducted in the area of Image analysis and many techniques are
available for image analysis but, a single technique is not yet identified for
image analysis and prediction .our research is focused on identifying a global
technique for image analysis and Prediction.Comment: Pages: 08 Figures: 02, Proceedings of International Conferences
  CAAM-09 BITS, Durg, India, 10 Jan 200"
239,"The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise.

This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques"
240,"The topic of semantic segmentation has witnessed considerable progress due to
the powerful features learned by convolutional neural networks (CNNs). The
current leading approaches for semantic segmentation exploit shape information
by extracting CNN features from masked image regions. This strategy introduces
artificial boundaries on the images and may impact the quality of the extracted
features. Besides, the operations on the raw image domain require to compute
thousands of networks on a single image, which is time-consuming. In this
paper, we propose to exploit shape information via masking convolutional
features. The proposal segments (e.g., super-pixels) are treated as masks on
the convolutional feature maps. The CNN features of segments are directly
masked out from these maps and used to train classifiers for recognition. We
further propose a joint method to handle objects and ""stuff"" (e.g., grass, sky,
water) in the same framework. State-of-the-art results are demonstrated on
benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling
computational speed.Comment: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  201"
241,"Printed text recognition is an important problem for industrial OCR systems.
Printed text is constructed in a standard procedural fashion in most settings.
We develop a mathematical model for this process that can be applied to the
backward inference problem of text recognition from an image. Through ablation
experiments we show that this model is realistic and that a multi-task
objective setting can help to stabilize estimation of its free parameters,
enabling use of conventional deep learning methods. Furthermore, by directly
modeling the geometric perturbations of text synthesis we show that our model
can help recover missing characters from incomplete text regions, the bane of
multicomponent OCR systems, enabling recognition even when the detection
returns incomplete information"
242,"The problem of domain generalization is to take knowledge acquired from a
number of related domains where training data is available, and to then
successfully apply it to previously unseen domains. We propose a new feature
learning algorithm, Multi-Task Autoencoder (MTAE), that provides good
generalization performance for cross-domain object recognition.
  Our algorithm extends the standard denoising autoencoder framework by
substituting artificially induced corruption with naturally occurring
inter-domain variability in the appearance of objects. Instead of
reconstructing images from noisy versions, MTAE learns to transform the
original image into analogs in multiple related domains. It thereby learns
features that are robust to variations across domains. The learnt features are
then used as inputs to a classifier.
  We evaluated the performance of the algorithm on benchmark image recognition
datasets, where the task is to learn features from multiple datasets and to
then predict the image label from unseen datasets. We found that (denoising)
MTAE outperforms alternative autoencoder-based models as well as the current
state-of-the-art algorithms for domain generalization.Comment: accepted in ICCV 201"
243,"Deep learning using convolutional neural networks (CNNs) is quickly becoming
the state-of-the-art for challenging computer vision applications. However,
deep learning's power consumption and bandwidth requirements currently limit
its application in embedded and mobile systems with tight energy budgets. In
this paper, we explore the energy savings of optically computing the first
layer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs),
custom CMOS diffractive image sensors which act similar to Gabor filter banks
in the V1 layer of the human visual cortex. ASPs replace both image sensing and
the first layer of a conventional CNN by directly performing optical edge
filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our
experimental results (both on synthetic data and a hardware prototype) for a
variety of vision tasks such as digit recognition, object recognition, and face
identification demonstrate using ASPs while achieving similar performance
compared to traditional deep learning pipelines.Comment: Presented in CVPR 2016 (oral), 10 pages, 12 figures. This new version
  corrects the comparison between imaging power for ASPs and a regular image
  senso"
244,"Recognition across domains has recently become an active topic in the
research community. However, it has been largely overlooked in the problem of
recognition in new unseen domains. Under this condition, the delivered deep
network models are unable to be updated, adapted, or fine-tuned. Therefore,
recent deep learning techniques, such as domain adaptation, feature
transferring, and fine-tuning, cannot be applied. This paper presents a novel
approach to the problem of domain generalization in the context of deep
learning. The proposed method is evaluated on different datasets in various
problems, i.e. (i) digit recognition on MNIST, SVHN, and MNIST-M, (ii) face
recognition on Extended Yale-B, CMU-PIE and CMU-MPIE, and (iii) pedestrian
recognition on RGB and Thermal image datasets. The experimental results show
that our proposed method consistently improves performance accuracy. It can
also be easily incorporated with any other CNN frameworks within an end-to-end
deep network design for object detection and recognition problems to improve
their performance.Comment: Accepted to Computer and Robot Vision 2020. arXiv admin note:
  substantial text overlap with arXiv:1812.0340"
245,"In a real-world setting, visual recognition systems can be brought to make
predictions for images belonging to previously unknown class labels. In order
to make semantically meaningful predictions for such inputs, we propose a
two-step approach that utilizes information from knowledge graphs. First, a
knowledge-graph representation is learned to embed a large set of entities into
a semantic space. Second, an image representation is learned to embed images
into the same space. Under this setup, we are able to predict structured
properties in the form of relationship triples for any open-world image. This
is true even when a set of labels has been omitted from the training protocols
of both the knowledge graph and image embeddings. Furthermore, we append this
learning framework with appropriate smoothness constraints and show how prior
knowledge can be incorporated into the model. Both these improvements combined
increase performance for visual recognition by a factor of six compared to our
baseline. Finally, we propose a new, extended dataset which we use for
experiments"
246,"Fingerprint recognition has drawn a lot of attention during last decades.
Different features and algorithms have been used for fingerprint recognition in
the past. In this paper, a powerful image representation called scattering
transform/network, is used for recognition. Scattering network is a
convolutional network where its architecture and filters are predefined wavelet
transforms. The first layer of scattering representation is similar to sift
descriptors and the higher layers capture higher frequency content of the
signal. After extraction of scattering features, their dimensionality is
reduced by applying principal component analysis (PCA). At the end, multi-class
SVM is used to perform template matching for the recognition task. The proposed
scheme is tested on a well-known fingerprint database and has shown promising
results with the best accuracy rate of 98\%.Comment: IEEE Signal Processing in Medicine and Biology Symposium, 201"
247,"Biometric time and attendance system is one of the most successful
applications of biometric technology. One of the main advantage of a biometric
time and attendance system is it avoids ""buddy-punching"". Buddy punching was a
major loophole which will be exploiting in the traditional time attendance
systems. Fingerprint recognition is an established field today, but still
identifying individual from a set of enrolled fingerprints is a time taking
process. Most fingerprint-based biometric systems store the minutiae template
of a user in the database. It has been traditionally assumed that the minutiae
template of a user does not reveal any information about the original
fingerprint. This belief has now been shown to be false; several algorithms
have been proposed that can reconstruct fingerprint images from minutiae
templates. In this paper, a novel fingerprint reconstruction algorithm is
proposed to reconstruct the phase image, which is then converted into the
grayscale image. The proposed reconstruction algorithm reconstructs the phase
image from minutiae. The proposed reconstruction algorithm is used to automate
the whole process of taking attendance, manually which is a laborious and
troublesome work and waste a lot of time, with its managing and maintaining the
records for a period of time is also a burdensome task. The proposed
reconstruction algorithm has been evaluated with respect to the success rates
of type-I attack (match the reconstructed fingerprint against the original
fingerprint) and type-II attack (match the reconstructed fingerprint against
different impressions of the original fingerprint) using a commercial
fingerprint recognition system. Given the reconstructed image from our
algorithm, we show that both types of attacks can be effectively launched
against a fingerprint recognition system.Comment: 6pages,5figure"
248,"Attention-based learning for fine-grained image recognition remains a
challenging task, where most of the existing methods treat each object part in
isolation, while neglecting the correlations among them. In addition, the
multi-stage or multi-scale mechanisms involved make the existing methods less
efficient and hard to be trained end-to-end. In this paper, we propose a novel
attention-based convolutional neural network (CNN) which regulates multiple
object parts among different input images. Our method first learns multiple
attention region features of each input image through the one-squeeze
multi-excitation (OSME) module, and then apply the multi-attention multi-class
constraint (MAMC) in a metric learning framework. For each anchor feature, the
MAMC functions by pulling same-attention same-class features closer, while
pushing different-attention or different-class features away. Our method can be
easily trained end-to-end, and is highly efficient which requires only one
training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog
species dataset that surpasses similar existing datasets by category coverage,
data volume and annotation quality. This dataset will be released upon
acceptance to facilitate the research of fine-grained image recognition.
Extensive experiments are conducted to show the substantial improvements of our
method on four benchmark datasets"
249,"Gesture recognition is a hot topic in computer vision and pattern
recognition, which plays a vitally important role in natural human-computer
interface. Although great progress has been made recently, fast and robust hand
gesture recognition remains an open problem, since the existing methods have
not well balanced the performance and the efficiency simultaneously. To bridge
it, this work combines image entropy and density clustering to exploit the key
frames from hand gesture video for further feature extraction, which can
improve the efficiency of recognition. Moreover, a feature fusion strategy is
also proposed to further improve feature representation, which elevates the
performance of recognition. To validate our approach in a ""wild"" environment,
we also introduce two new datasets called HandGesture and Action3D datasets.
Experiments consistently demonstrate that our strategy achieves competitive
results on Northwestern University, Cambridge, HandGesture and Action3D hand
gesture datasets. Our code and datasets will release at
https://github.com/Ha0Tang/HandGestureRecognition.Comment: 11 pages, 3 figures, accepted to NeuroComputin"
250,"AbstractIn order to get better image processing and target recognition, this paper presents a fuzzy automata system to target recognition. The system first performs image processing, and then accomplishes the target recognition. The system consists of four parts: image preprocessing, feature extraction, target matching and experiment. Compared with existing approaches, this paper uses both global features and local features of the target image, and carries out target recognition by using a fuzzy automata system. Simulation results show that the correct recognition rate based on the fuzzy automata system for target recognition is higher at 94.59%, an improvement on an average of 29.24%, compared to other existing approaches. Finally, some directions for future research are described"
251,"In this work we present a framework for the recognition of natural scene
text. Our framework does not require any human-labelled data, and performs word
recognition on the whole image holistically, departing from the character based
recognition systems of the past. The deep neural network models at the centre
of this framework are trained solely on data produced by a synthetic text
generation engine -- synthetic data that is highly realistic and sufficient to
replace real data, giving us infinite amounts of training data. This excess of
data exposes new possibilities for word recognition models, and here we
consider three models, each one ""reading"" words in a different way: via 90k-way
dictionary encoding, character sequence encoding, and bag-of-N-grams encoding.
In the scenarios of language based and completely unconstrained text
recognition we greatly improve upon state-of-the-art performance on standard
datasets, using our fast, simple machinery and requiring zero data-acquisition
costs"
252,"Winograd convolution is widely used in deep neural networks (DNNs). Existing
work for DNNs considers only the subset Winograd algorithms that are equivalent
to Toom-Cook convolution. We investigate a wider range of Winograd algorithms
for DNNs and show that these additional algorithms can significantly improve
floating point (FP) accuracy in many cases. We present results for three FP
formats: fp32, fp16 and bf16 (a truncated form of fp32) using 2000 inputs from
the ImageNet dataset. We found that in fp16 this approach gives us up to 6.5
times better image recognition accuracy in one important case while maintaining
the same number of elementwise multiplication operations in the innermost loop.
In bf16 the convolution can be computed using 5% fewer innermost loop
multiplications than with currently used Winograd algorithms while keeping the
accuracy of image recognition the same as for direct convolution method"
253,"The complementary DNA (cDNA) sequence is considered to be the magic biometric
technique for personal identification. In this paper, we present a new method
for cDNA recognition based on the artificial neural network (ANN). Microarray
imaging is used for the concurrent identification of thousands of genes. We
have segmented the location of the spots in a cDNA microarray. Thus, a precise
localization and segmenting of a spot are essential to obtain a more accurate
intensity measurement, leading to a more precise expression measurement of a
gene. The segmented cDNA microarray image is resized and it is used as an input
for the proposed artificial neural network. For matching and recognition, we
have trained the artificial neural network. Recognition results are given for
the galleries of cDNA sequences . The numerical results show that, the proposed
matching technique is an effective in the cDNA sequences process. We also
compare our results with previous results and find out that, the proposed
technique is an effective matching performance.Comment: 17 pages, 7 figures and 23 Reference"
254,"Thanks to recent advances in deep neural networks (DNNs), face recognition
systems have become highly accurate in classifying a large number of face
images. However, recent studies have found that DNNs could be vulnerable to
adversarial examples, raising concerns about the robustness of such systems.
Adversarial examples that are not restricted to small perturbations could be
more serious since conventional certified defenses might be ineffective against
them. To shed light on the vulnerability to such adversarial examples, we
propose a flexible and efficient method for generating unrestricted adversarial
examples using image translation techniques. Our method enables us to translate
a source image into any desired facial appearance with large perturbations to
deceive target face recognition systems. Our experimental results indicate that
our method achieved about $90$ and $80\%$ attack success rates under white- and
black-box settings, respectively, and that the translated images are
perceptually realistic and maintain the identifiability of the individual while
the perturbations are large enough to bypass certified defenses.Comment: Kazuya Kakizaki and Kosuke Yoshida share equal contributions.
  Accepted at AAAI Workshop on Artificial Intelligence Safety (2020"
255,"In this paper, we introduce a new and challenging large-scale food image
dataset called ""ChineseFoodNet"", which aims to automatically recognizing
pictured Chinese dishes. Most of the existing food image datasets collected
food images either from recipe pictures or selfie. In our dataset, images of
each food category of our dataset consists of not only web recipe and menu
pictures but photos taken from real dishes, recipe and menu as well.
ChineseFoodNet contains over 180,000 food photos of 208 categories, with each
category covering a large variations in presentations of same Chinese food. We
present our efforts to build this large-scale image dataset, including food
category selection, data collection, and data clean and label, in particular
how to use machine learning methods to reduce manual labeling work that is an
expensive process. We share a detailed benchmark of several state-of-the-art
deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose
a novel two-step data fusion approach referred as ""TastyNet"", which combines
prediction results from different CNNs with voting method. Our proposed
approach achieves top-1 accuracies of 81.43% on the validation set and 81.55%
on the test set, respectively. The latest dataset is public available for
research and can be achieved at https://sites.google.com/view/chinesefoodnet.Comment: 8 pages, 5 figure, 2 table"
256,"In this paper, we address the problem of landmark-based visual place
recognition. In the state-of-the-art method, accurate object proposal
algorithms are first leveraged for generating a set of local regions containing
particular landmarks with high confidence. Then, these candidate regions are
represented by deep features and pairwise matching is performed in an
exhaustive manner for the similarity measure. Despite its success, conventional
object proposal methods usually produce massive landmark-dependent image
patches exhibiting significant distribution variance in scale and overlap. As a
result, the inconsistency in landmark distributions tends to produce biased
similarity between pairwise images yielding the suboptimal performance. In
order to gain an insight into the landmark-based place recognition scheme, we
conduct a comprehensive study in which the influence of landmark scales and the
proportion of overlap on the recognition performance is explored. More
specifically, we thoroughly study the exhaustive search based landmark matching
mechanism, and thus derive three-fold important observations in terms of the
beneficial effect of specific landmark generation strategies. Inspired by the
above observations, a simple yet effective dense sampling based scheme is
presented for accurate place recognition in this paper. Different from the
conventional object proposal strategy, we generate local landmarks of multiple
scales with uniform distribution from entire image by dense sampling, and
subsequently perform multi-scale fusion on the densely sampled landmarks for
similarity measure. The experimental results on three challenging datasets
demonstrate that the recognition performance can be significantly improved by
our efficient method in which the landmarks are appropriately produced for
accurate pairwise matching"
257,"At the present time, hand gestures recognition system could be used as a more
expected and useable approach for human computer interaction. Automatic hand
gesture recognition system provides us a new tactic for interactive with the
virtual environment. In this paper, a face and hand gesture recognition system
which is able to control computer media player is offered. Hand gesture and
human face are the key element to interact with the smart system. We used the
face recognition scheme for viewer verification and the hand gesture
recognition in mechanism of computer media player, for instance, volume
down/up, next music and etc. In the proposed technique, first, the hand gesture
and face location is extracted from the main image by combination of skin and
cascade detector and then is sent to recognition stage. In recognition stage,
first, the threshold condition is inspected then the extracted face and gesture
will be recognized. In the result stage, the proposed technique is applied on
the video dataset and the high precision ratio acquired. Additional the
recommended hand gesture recognition method is applied on static American Sign
Language (ASL) database and the correctness rate achieved nearby 99.40%. also
the planned method could be used in gesture based computer games and virtual
reality"
258,"ColorCheckers are reference standards that professional photographers and
filmmakers use to ensure predictable results under every lighting condition.
The objective of this work is to propose a new fast and robust method for
automatic ColorChecker detection. The process is divided into two steps: (1)
ColorCheckers localization and (2) ColorChecker patches recognition. For the
ColorChecker localization, we trained a detection convolutional neural network
using synthetic images. The synthetic images are created with the 3D models of
the ColorChecker and different background images. The output of the neural
networks are the bounding box of each possible ColorChecker candidates in the
input image. Each bounding box defines a cropped image which is evaluated by a
recognition system, and each image is canonized with regards to color and
dimensions. Subsequently, all possible color patches are extracted and grouped
with respect to the center's distance. Each group is evaluated as a candidate
for a ColorChecker part, and its position in the scene is estimated. Finally, a
cost function is applied to evaluate the accuracy of the estimation. The method
is tested using real and synthetic images. The proposed method is fast, robust
to overlaps and invariant to affine projections. The algorithm also performs
well in case of multiple ColorCheckers detection.Comment: Submitted to Image and Vision Computin"
259,"Images may have elements containing text and a bounding box associated with
them, for example, text identified via optical character recognition on a
computer screen image, or a natural image with labeled objects. We present an
end-to-end trainable architecture to incorporate the information from these
elements and the image to segment/identify the part of the image a natural
language expression is referring to. We calculate an embedding for each element
and then project it onto the corresponding location (i.e., the associated
bounding box) of the image feature map. We show that this architecture gives an
improvement in resolving referring expressions, over only using the image, and
other methods that incorporate the element information. We demonstrate
experimental results on the referring expression datasets based on COCO, and on
a webpage image referring expression dataset that we developed.Comment: Accepted into IEEE SLT Worksho"
260,"When we are faced with challenging image classification tasks, we often
explain our reasoning by dissecting the image, and pointing out prototypical
aspects of one class or another. The mounting evidence for each of the classes
helps us make our final decision. In this work, we introduce a deep network
architecture -- prototypical part network (ProtoPNet), that reasons in a
similar way: the network dissects the image by finding prototypical parts, and
combines evidence from the prototypes to make a final classification. The model
thus reasons in a way that is qualitatively similar to the way ornithologists,
physicians, and others would explain to people on how to solve challenging
image classification tasks. The network uses only image-level labels for
training without any annotations for parts of images. We demonstrate our method
on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show
that ProtoPNet can achieve comparable accuracy with its analogous
non-interpretable counterpart, and when several ProtoPNets are combined into a
larger network, it can achieve an accuracy that is on par with some of the
best-performing deep models. Moreover, ProtoPNet provides a level of
interpretability that is absent in other interpretable deep models.Comment: Chaofan Chen and Oscar Li contributed equally to this work. This work
  has been accepted for spotlight presentation (top 3% of papers) at NeurIPS
  201"
261,"Generative Adversarial Networks (GAN) have attracted much research attention
recently, leading to impressive results for natural image generation. However,
to date little success was observed in using GAN generated images for improving
classification tasks. Here we attempt to explore, in the context of car license
plate recognition, whether it is possible to generate synthetic training data
using GAN to improve recognition accuracy. With a carefully-designed pipeline,
we show that the answer is affirmative. First, a large-scale image set is
generated using the generator of GAN, without manual annotation. Then, these
images are fed to a deep convolutional neural network (DCNN) followed by a
bidirectional recurrent neural network (BRNN) with long short-term memory
(LSTM), which performs the feature learning and sequence labelling. Finally,
the pre-trained model is fine-tuned on real images. Our experimental results on
a few data sets demonstrate the effectiveness of using GAN images: an
improvement of 7.5% over a strong baseline with moderate-sized real data being
available. We show that the proposed framework achieves competitive recognition
accuracy on challenging test datasets. We also leverage the depthwise separate
convolution to construct a lightweight convolutional RNN, which is about half
size and 2x faster on CPU. Combining this framework and the proposed pipeline,
we make progress in performing accurate recognition on mobile and embedded
devices"
262,"Recently, Convolutional Neural Networks (ConvNets) have shown promising
performances in many computer vision tasks, especially image-based recognition.
How to effectively use ConvNets for video-based recognition is still an open
problem. In this paper, we propose a compact, effective yet simple method to
encode spatio-temporal information carried in $3D$ skeleton sequences into
multiple $2D$ images, referred to as Joint Trajectory Maps (JTM), and ConvNets
are adopted to exploit the discriminative features for real-time human action
recognition. The proposed method has been evaluated on three public benchmarks,
i.e., MSRC-12 Kinect gesture dataset (MSRC-12), G3D dataset and UTD multimodal
human action dataset (UTD-MHAD) and achieved the state-of-the-art results"
263,"Supervised deep learning relies on the assumption that enough training data
is available, which presents a problem for its application to several fields,
like medical imaging. On the example of a binary image classification task
(breast cancer recognition), we show that pretraining a generative model for
meaningful image augmentation helps enhance the performance of the resulting
classifier. By augmenting the data, performance on downstream classification
tasks could be improved even with a relatively small training set. We show that
this ""adversarial augmentation"" yields promising results compared to classical
image augmentation on the example of breast cancer classification"
264,"Automatically creating the description of an image using any natural
languages sentence like English is a very challenging task. It requires
expertise of both image processing as well as natural language processing. This
paper discuss about different available models for image captioning task. We
have also discussed about how the advancement in the task of object recognition
and machine translation has greatly improved the performance of image
captioning model in recent years. In addition to that we have discussed how
this model can be implemented. In the end, we have also evaluated the
performance of model using standard evaluation matrices.Comment: Pre-print version of paper accepted at 2017 International Conference
  on Innovations in information Embedded and Communication Systems (ICIIECS"
265,"The recognition ability of human beings is developed in a progressive way.
Usually, children learn to discriminate various objects from coarse to
fine-grained with limited supervision. Inspired by this learning process, we
propose a simple yet effective model for the Few-Shot Fine-Grained (FSFG)
recognition, which tries to tackle the challenging fine-grained recognition
task using meta-learning. The proposed method, named Pairwise Alignment
Bilinear Network (PABN), is an end-to-end deep neural network. Unlike
traditional deep bilinear networks for fine-grained classification, which adopt
the self-bilinear pooling to capture the subtle features of images, the
proposed model uses a novel pairwise bilinear pooling to compare the nuanced
differences between base images and query images for learning a deep distance
metric. In order to match base image features with query image features, we
design feature alignment losses before the proposed pairwise bilinear pooling.
Experiment results on four fine-grained classification datasets and one generic
few-shot dataset demonstrate that the proposed model outperforms both the
state-ofthe-art few-shot fine-grained and general few-shot methods.Comment: ICME 2019 Ora"
266,"In this work we present an end-to-end system for text spotting -- localising
and recognising text in natural scene images -- and text based image retrieval.
This system is based on a region proposal mechanism for detection and deep
convolutional neural networks for recognition. Our pipeline uses a novel
combination of complementary proposal generation techniques to ensure high
recall, and a fast subsequent filtering stage for improving precision. For the
recognition and ranking of proposals, we train very large convolutional neural
networks to perform word recognition on the whole proposal region at the same
time, departing from the character classifier based systems of the past. These
networks are trained solely on data produced by a synthetic text generation
engine, requiring no human labelled data.
  Analysing the stages of our pipeline, we show state-of-the-art performance
throughout. We perform rigorous experiments across a number of standard
end-to-end text spotting benchmarks and text-based image retrieval datasets,
showing a large improvement over all previous methods. Finally, we demonstrate
a real-world application of our text spotting system to allow thousands of
hours of news footage to be instantly searchable via a text query"
267,"Building robust recognizers for Arabic has always been challenging. We
demonstrate the effectiveness of an end-to-end trainable CNN-RNN hybrid
architecture in recognizing Arabic text in videos and natural scenes. We
outperform previous state-of-the-art on two publicly available video text
datasets - ALIF and ACTIV. For the scene text recognition task, we introduce a
new Arabic scene text dataset and establish baseline results. For scripts like
Arabic, a major challenge in developing robust recognizers is the lack of large
quantity of annotated data. We overcome this by synthesising millions of Arabic
text images from a large vocabulary of Arabic words and phrases. Our
implementation is built on top of the model introduced here [37] which is
proven quite effective for English scene text recognition. The model follows a
segmentation-free, sequence to sequence transcription approach. The network
transcribes a sequence of convolutional features from the input image to a
sequence of target labels. This does away with the need for segmenting input
image into constituent characters/glyphs, which is often difficult for Arabic
script. Further, the ability of RNNs to model contextual dependencies yields
superior recognition results.Comment: 5 page"
268,"Digital images nowadays have various styles of appearance, in the aspects of
color tones, contrast, vignetting, and etc. These 'picture styles' are directly
related to the scene radiance, image pipeline of the camera, and post
processing functions. Due to the complexity and nonlinearity of these causes,
popular gradient-based image descriptors won't be invariant to different
picture styles, which will decline the performance of object recognition. Given
that images shared online or created by individual users are taken with a wide
range of devices and may be processed by various post processing functions, to
find a robust object recognition system is useful and challenging. In this
paper, we present the first study on the influence of picture styles for object
recognition, and propose an adaptive approach based on the kernel view of
gradient descriptors and multiple kernel learning, without estimating or
specifying the styles of images used in training and testing. We conduct
experiments on Domain Adaptation data set and Oxford Flower data set. The
experiments also include several variants of the flower data set by processing
the images with popular photo effects. The results demonstrate that our
proposed method improve from standard descriptors in all cases.Comment: 8 page"
269,"We evaluate a new approach to face recognition using a variety of surface representations of three-dimensional facial structure. Applying principal component analysis (PCA), we show that high levels of recognition accuracy can be achieved on a large database of 3D face models, captured under conditions that present typical difficulties to more conventional two-dimensional approaches. Applying a ran-c of image processing, techniques we identify the most effective surface representation for use in such application areas as security surveillance, data compression and archive searching"
270,"This paper presents an automated system for human face recognition in a real
time background world for a large homemade dataset of persons face. The task is
very difficult as the real time background subtraction in an image is still a
challenge. Addition to this there is a huge variation in human face image in
terms of size, pose and expression. The system proposed collapses most of this
variance. To detect real time human face AdaBoost with Haar cascade is used and
a simple fast PCA and LDA is used to recognize the faces detected. The matched
face is then used to mark attendance in the laboratory, in our case. This
biometric system is a real time attendance system based on the human face
recognition with a simple and fast algorithms and gaining a high accuracy
rate..Comment: 14 pages; ISSN : 0975-900X (Online), 0976-2191 (Print"
271,"We introduce a new light-field dataset of materials, and take advantage of
the recent success of deep learning to perform material recognition on the 4D
light-field. Our dataset contains 12 material categories, each with 100 images
taken with a Lytro Illum, from which we extract about 30,000 patches in total.
To the best of our knowledge, this is the first mid-size dataset for
light-field images. Our main goal is to investigate whether the additional
information in a light-field (such as multiple sub-aperture views and
view-dependent reflectance effects) can aid material recognition. Since
recognition networks have not been trained on 4D images before, we propose and
compare several novel CNN architectures to train on light-field images. In our
experiments, the best performing CNN architecture achieves a 7% boost compared
with 2D image classification (70% to 77%). These results constitute important
baselines that can spur further research in the use of CNNs for light-field
applications. Upon publication, our dataset also enables other novel
applications of light-fields, including object detection, image segmentation
and view interpolation.Comment: European Conference on Computer Vision (ECCV) 201"
272,"In this paper a DWT based steganography in frequency domain, termed as ATFDWT
has been proposed. Here, the cover image is transformed into the time domain
signal through DWT, resulting four sub-image components as 'Low resolution',
'Horizontal orientation', 'Vertical orientation' and 'Diagonal orientation'.
The secret message/image bits stream in varying positions are embedded in
'Vertical orientation sub-image' followed by reverse transformation to generate
embedded/encrypted image. The decoding is done through the reverse procedure.
The experimental results against statistical and visual attack has been
computed and compared with the existing technique like IAFDDFTT[1], in terms of
Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR), Standard
Deviation(SD) and Image Fidelity(IF) analysis, which shows better performances
in ATFDWT.Comment: 18 Page Journal in 'Advances in Modelling Signal Processing and
  Pattern Recognition' (AMSE), vol-54, Issue 2, 2001"
273,"Text data present in multimedia contain useful information for automatic
annotation, indexing. Extracted information used for recognition of the overlay
or scene text from a given video or image. The Extracted text can be used for
retrieving the videos and images. In this paper, firstly, we are discussed the
different techniques for text extraction from images and videos. Secondly, we
are reviewed the techniques for indexing and retrieval of image and videos by
using extracted text.Comment: 12 page"
274,"Convolutional neural networks (CNNs) have obtained astounding successes for
important pattern recognition tasks, but they suffer from high computational
complexity and the lack of interpretability. The recent Tsetlin Machine (TM)
attempts to address this lack by using easy-to-interpret conjunctive clauses in
propositional logic to solve complex pattern recognition problems. The TM
provides competitive accuracy in several benchmarks, while keeping the
important property of interpretability. It further facilitates hardware-near
implementation since inputs, patterns, and outputs are expressed as bits, while
recognition and learning rely on straightforward bit manipulation. In this
paper, we exploit the TM paradigm by introducing the Convolutional Tsetlin
Machine (CTM), as an interpretable alternative to CNNs. Whereas the TM
categorizes an image by employing each clause once to the whole image, the CTM
uses each clause as a convolution filter. That is, a clause is evaluated
multiple times, once per image patch taking part in the convolution. To make
the clauses location-aware, each patch is further augmented with its
coordinates within the image. The output of a convolution clause is obtained
simply by ORing the outcome of evaluating the clause on each patch. In the
learning phase of the TM, clauses that evaluate to 1 are contrasted against the
input. For the CTM, we instead contrast against one of the patches, randomly
selected among the patches that made the clause evaluate to 1. Accordingly, the
standard Type I and Type II feedback of the classic TM can be employed
directly, without further modification. The CTM obtains a peak test accuracy of
99.4% on MNIST, 96.31% on Kuzushiji-MNIST, 91.5% on Fashion-MNIST, and 100.0%
on the 2D Noisy XOR Problem, which is competitive with results reported for
simple 4-layer CNNs, BinaryConnect, Logistic Circuits and an FPGA-accelerated
Binary CNN.Comment: 14 pages, 5 figure"
275,"This disclosure describes techniques that, with user permission, utilize user actions as implicit feedback for image recognition. With user permission and express consent, post-recognition user actions are utilized to adjust image recognition model(s). An image recognition request and one or more image(s) are received. Image recognition techniques are applied to the received image(s) and results are provided to the user. Based on actions performed by the user post-recognition, various factors are inferred and utilized to train image recognition model(s) to improve the performance of the model for future searches. With permission, user affinity model(s) are trained that can provide customization of future search results for the particular user"
276,"Material attributes have been shown to provide a discriminative intermediate
representation for recognizing materials, especially for the challenging task
of recognition from local material appearance (i.e., regardless of object and
scene context). In the past, however, material attributes have been recognized
separately preceding category recognition. In contrast, neuroscience studies on
material perception and computer vision research on object and place
recognition have shown that attributes are produced as a by-product during the
category recognition process. Does the same hold true for material attribute
and category recognition? In this paper, we introduce a novel material category
recognition network architecture to show that perceptual attributes can, in
fact, be automatically discovered inside a local material recognition
framework. The novel material-attribute-category convolutional neural network
(MAC-CNN) produces perceptual material attributes from the intermediate pooling
layers of an end-to-end trained category recognition network using an auxiliary
loss function that encodes human material perception. To train this model, we
introduce a novel large-scale database of local material appearance organized
under a canonical material category taxonomy and careful image patch extraction
that avoids unwanted object and scene context. We show that the discovered
attributes correspond well with semantically-meaningful visual material traits
via Boolean algebra, and enable recognition of previously unseen material
categories given only a few examples. These results have strong implications in
how perceptually meaningful attributes can be learned in other recognition
tasks"
277,"In this paper we borrow concepts from Information Theory and Statistical
Mechanics to perform a pattern recognition procedure on a set of x-ray hazelnut
images. We identify two relevant statistical scales, whose ratio affects the
performance of a machine learning algorithm based on statistical observables,
and discuss the dependence of such scales on the image resolution. Finally, by
averaging the performance of a Support Vector Machines algorithm over a set of
training samples, we numerically verify the predicted onset of an optimal scale
of resolution, at which the pattern recognition is favoured"
278,"Drones are conventionally controlled using joysticks, remote controllers,
mobile applications, and embedded computers. A few significant issues with
these approaches are that drone control is limited by the range of
electromagnetic radiation and susceptible to interference noise. In this study
we propose the use of hand gestures as a method to control drones. We
investigate the use of computer vision methods to develop an intuitive way of
agent-less communication between a drone and its operator. Computer
vision-based methods rely on the ability of a drone's camera to capture
surrounding images and use pattern recognition to translate images to
meaningful and/or actionable information. The proposed framework involves a few
key parts toward an ultimate action to be taken. They are: image segregation
from the video streams of front camera, creating a robust and reliable image
recognition based on segregated images, and finally conversion of classified
gestures into actionable drone movement, such as takeoff, landing, hovering and
so forth. A set of five gestures are studied in this work. Haar feature-based
AdaBoost classifier is employed for gesture recognition. We also envisage
safety of the operator and drone's action calculating the distance based on
computer vision for this task. A series of experiments are conducted to measure
gesture recognition accuracies considering the major scene variabilities,
illumination, background, and distance. Classification accuracies show that
well-lit, clear background, and within 3 ft gestures are recognized correctly
over 90%. Limitations of current framework and feasible solutions for better
gesture recognition are discussed, too. The software library we developed, and
hand gesture data sets are open-sourced at project website.Comment: ICDIS 201"
279,"Mount Tai has abundant sunshine, abundant rainfall and favorable climatic
conditions, forming dense vegetation with various kinds of trees. In order to
make it easier for tourists to understand each tree and experience the culture
of Mount Tai, this paper develops an App for tree recognition of Mount Tai
based on convolution neural network (CNN), taking advantage of CNN efficient
image recognition ability and easy-to-carry characteristics of Android mobile
phone. The APP can accurately identify several common trees in Mount Tai, and
give a brief introduction for tourists"
280,"Single-view place recognition, that we can define as finding an image that
corresponds to the same place as a given query image, is a key capability for
autonomous navigation and mapping. Although there has been a considerable
amount of research in the topic, the high degree of image variability (with
viewpoint, illumination or occlusions for example) makes it a research
challenge.
  One of the particular challenges, that we address in this work, is weather
variation. Seasonal changes can produce drastic appearance changes, that
classic low-level features do not model properly. Our contributions in this
paper are twofold. First we pre-process and propose a partition for the
Nordland dataset, frequently used for place recognition research without
consensus on the partitions. And second, we evaluate several neural network
architectures such as pre-trained, siamese and triplet for this problem. Our
best results outperform the state of the art of the field. A video showing our
results can be found in https://youtu.be/VrlxsYZoHDM. The partitioned version
of the Nordland dataset at http://webdiis.unizar.es/~jmfacil/pr-nordland/.Comment: Accepted at 10th Planning, Perception and Navigation for Intelligent
  Vehicles (PPNIV'18), Workshop at IROS 201"
281,"This article describes the use of clustering for face recognition image. Clustering was performed using a recurrent neural network used at two stages of the recognition process. The algorithm includes the recognition process itself perform clustering pixel brightness image, calculating image information close proximity and clustering measures to in order to obtain the cluster containing the original similar images"
282,"In spite of the advances in pattern recognition technology, Handwritten
Bangla Character Recognition (HBCR) (such as alpha-numeric and special
characters) remains largely unsolved due to the presence of many perplexing
characters and excessive cursive in Bangla handwriting. Even the best existing
recognizers do not lead to satisfactory performance for practical applications.
To improve the performance of Handwritten Bangla Digit Recognition (HBDR), we
herein present a new approach based on deep neural networks which have recently
shown excellent performance in many pattern recognition and machine learning
applications, but has not been throughly attempted for HBDR. We introduce
Bangla digit recognition techniques based on Deep Belief Network (DBN),
Convolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and
Gaussian filters, and CNN with dropout and Gabor filters. These networks have
the advantage of extracting and using feature information, improving the
recognition of two dimensional shapes with a high degree of invariance to
translation, scaling and other pattern distortions. We systematically evaluated
the performance of our method on publicly available Bangla numeral image
database named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition
rate using the proposed method: CNN with Gabor features and dropout, which
outperforms the state-of-the-art algorithms for HDBR.Comment: 12 pages, 10 figures, 3 table"
283,"Department of Computer Science and EngineeringThe goal of semantic image segmentation is to partition the pixels of an image into semantically meaningful parts and classifying those parts according to a predefined label set. Although object recognition

models achieved remarkable performance recently and they even surpass human???s ability to recognize

objects, but semantic segmentation models are still behind. One of the reason that makes semantic

segmentation relatively a hard problem is the image understanding at pixel level by considering global

context as oppose to object recognition. One other challenge is transferring the knowledge of an object

recognition model for the task of semantic segmentation. In this thesis, we are delineating some of the

main challenges we faced approaching semantic image segmentation with machine learning algorithms.

Our main focus was how we can use deep learning algorithms for this task since they require the

least amount of feature engineering and also it was shown that such models can be applied to large scale

datasets and exhibit remarkable performance. More precisely, we worked on a variation of convolutional

neural networks (CNN) suitable for the semantic segmentation task. We proposed a model called deep

fully residual convolutional networks (DFRCN) to tackle this problem. Utilizing residual learning makes

training of deep models feasible which ultimately leads to having a rich powerful visual representation.

Our model also benefits from skip-connections which ease the propagation of information from the

encoder module to the decoder module. This would enable our model to have less parameters in the

decoder module while it also achieves better performance. We also benchmarked the effective variation

of the proposed model on a semantic segmentation benchmark.

We first make a thorough review of current high-performance models and the problems one might

face when trying to replicate such models which mainly arose from the lack of sufficient provided

information. Then, we describe our own novel method which we called deep fully residual convolutional

network (DFRCN). We showed that our method exhibits state of the art performance on a challenging

benchmark for aerial image segmentation.clos"
284,"Hand gestures recognition (HGR) is one of the main areas of research for the
engineers, scientists and bioinformatics. HGR is the natural way of Human
Machine interaction and today many researchers in the academia and industry are
working on different application to make interactions more easy, natural and
convenient without wearing any extra device. HGR can be applied from games
control to vision enabled robot control, from virtual reality to smart home
systems. In this paper we are discussing work done in the area of hand gesture
recognition where focus is on the intelligent approaches including soft
computing based methods like artificial neural network, fuzzy logic, genetic
algorithms etc. The methods in the preprocessing of image for segmentation and
hand image construction also taken into study. Most researchers used fingertips
for hand detection in appearance based modeling. Finally the comparison of
results given by different researchers is also presented"
285,"In this paper, we propose a multi-level texture encoding and representation
network (MuLTER) for texture-related applications. Based on a multi-level
pooling architecture, the MuLTER network simultaneously leverages low- and
high-level features to maintain both texture details and spatial information.
Such a pooling architecture involves few extra parameters and keeps feature
dimensions fixed despite of the changes of image sizes. In comparison with
state-of-the-art texture descriptors, the MuLTER network yields higher
recognition accuracy on typical texture datasets such as MINC-2500 and
GTOS-mobile with a discriminative and compact representation. In addition, we
analyze the impact of combining features from different levels, which supports
our claim that the fusion of multi-level features efficiently enhances
recognition performance. Our source code will be published on GitHub
(https://github.com/olivesgatech).Comment: Proceedings of IEEE International Conference on Image Processing
  (ICIP), Sep. 201"
286,"Identifying prescription medications is a frequent task for patients and
medical professionals; however, this is an error-prone task as many pills have
similar appearances (e.g. white round pills), which increases the risk of
medication errors. In this paper, we introduce ePillID, the largest public
benchmark on pill image recognition, composed of 13k images representing 9804
appearance classes (two sides for 4902 pill types). For most of the appearance
classes, there exists only one reference image, making it a challenging
low-shot recognition setting. We present our experimental setup and evaluation
results of various baseline models on the benchmark. The best baseline using a
multi-head metric-learning approach with bilinear features performed remarkably
well; however, our error analysis suggests that they still fail to distinguish
particularly confusing classes. The code and data are available at
https://github.com/usuyama/ePillID-benchmark.Comment: CVPR 2020 VL3. Project Page:
  https://github.com/usuyama/ePillID-benchmar"
287,"Optical Character Recognition (OCR) has been a topic of interest for many
years. It is defined as the process of digitizing a document image into its
constituent characters. Despite decades of intense research, developing OCR
with capabilities comparable to that of human still remains an open challenge.
Due to this challenging nature, researchers from industry and academic circles
have directed their attentions towards Optical Character Recognition. Over the
last few years, the number of academic laboratories and companies involved in
research on Character Recognition has increased dramatically. This research
aims at summarizing the research so far done in the field of OCR. It provides
an overview of different aspects of OCR and discusses corresponding proposals
aimed at resolving issues of OCR"
288,"In this paper, the author presents a work on i) range data and ii) stereo-vision system based disparity map profiling that are used as signatures for 3D face recognition. The signatures capture the intensity variations along a line at sample points on a face in any particular direction. The directional signatures and some of their combinations are compared to study the variability in recognition performances. Two 3D face image datasets namely, a local student database captured with a stereo vision system and the FRGC v1 range dataset are used for performance evaluation"
289,"In this paper, we introduce our submissions for the tasks of trimmed activity
recognition (Kinetics) and trimmed event recognition (Moments in Time) for
Activitynet Challenge 2018. In the two tasks, non-local neural networks and
temporal segment networks are implemented as our base models. Multi-modal cues
such as RGB image, optical flow and acoustic signal have also been used in our
method. We also propose new non-local-based models for further improvement on
the recognition accuracy. The final submissions after ensembling the models
achieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics
validation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT
validation set.Comment: 4 pages, 3 figures, CVPR worksho"
290,"Recent studies have demonstrated the power of recurrent neural networks for
machine translation, image captioning and speech recognition. For the task of
capturing temporal structure in video, however, there still remain numerous
open research questions. Current research suggests using a simple temporal
feature pooling strategy to take into account the temporal aspect of video. We
demonstrate that this method is not sufficient for gesture recognition, where
temporal information is more discriminative compared to general video
classification tasks. We explore deep architectures for gesture recognition in
video and propose a new end-to-end trainable neural network architecture
incorporating temporal convolutions and bidirectional recurrence. Our main
contributions are twofold; first, we show that recurrence is crucial for this
task; second, we show that adding temporal convolutions leads to significant
improvements. We evaluate the different approaches on the Montalbano gesture
recognition dataset, where we achieve state-of-the-art results"
291,"IQ tests are an accepted method for assessing human intelligence. The tests
consist of several parts that must be solved under a time constraint. Of all
the tested abilities, pattern recognition has been found to have the highest
correlation with general intelligence. This is primarily because pattern
recognition is the ability to find order in a noisy environment, a necessary
skill for intelligent agents. In this paper, we propose a convolutional neural
network (CNN) model for solving geometric pattern recognition problems. The CNN
receives as input multiple ordered input images and outputs the next image
according to the pattern. Our CNN is able to solve problems involving rotation,
reflection, color, size and shape patterns and score within the top 5% of human
performance"
292,"Gender is an important demographic attribute of people. This paper provides a
survey of human gender recognition in computer vision. A review of approaches
exploiting information from face and whole body (either from a still image or
gait sequence) is presented. We highlight the challenges faced and survey the
representative methods of these approaches. Based on the results, good
performance have been achieved for datasets captured under controlled
environments, but there is still much work that can be done to improve the
robustness of gender recognition under real-life environments.Comment: 30 page"
293,"Artificial neural networks have been successfully applied to a variety of
machine learning tasks, including image recognition, semantic segmentation, and
machine translation. However, few studies fully investigated ensembles of
artificial neural networks. In this work, we investigated multiple widely used
ensemble methods, including unweighted averaging, majority voting, the Bayes
Optimal Classifier, and the (discrete) Super Learner, for image recognition
tasks, with deep neural networks as candidate algorithms. We designed several
experiments, with the candidate algorithms being the same network structure
with different model checkpoints within a single training process, networks
with same structure but trained multiple times stochastically, and networks
with different structure. In addition, we further studied the over-confidence
phenomenon of the neural networks, as well as its impact on the ensemble
methods. Across all of our experiments, the Super Learner achieved best
performance among all the ensemble methods in this study"
294,"Road sign recognition is one of the core technologies in Intelligent
Transport Systems. In the current study, a robust and real-time method is
presented to identify and detect the roads speed signs in road image in
different situations. In our proposed method, first, the connected components
are created in the main image using the edge detection and mathematical
morphology and the location of the road signs extracted by the geometric and
color data; then the letters are segmented and recognized by Multiclass Support
Vector Machine (SVMs) classifiers. Regarding that the geometric and color
features ate properly used in detection the location of the road signs, so it
is not sensitive to the distance and noise and has higher speed and efficiency.
In the result part, the proposed approach is applied on Iranian road speed sign
database and the detection and recognition accuracy rate achieved 98.66% and
100% respectively"
295,"The question of object–picture recognition has received relatively little attention in both human and comparative psychology; a paradoxical situation given the important use of image technology (e.g. slides, digitised pictures) made by neuroscientists in their experimental investigation of visual cognition. The  present review examines the relevant literature pertaining to the question of the
correspondence between and:or equivalence of real objects and their pictorial representations in animals and humans. Two classes of reactions towards pictures will be considered in turn: acquired responses in picture recognition experiments and spontaneous responses to pictures of biologically relevant objects (e.g. prey or conspecifics). Our survey will lead to the conclusion that humans show evidence of picture recognition from an early age; this recognition is, however, facilitated by prior exposure to pictures. This same exposure or training effect appears also to be necessary in nonhuman primates as well as in other mammals and in birds. Other factors are also identified as playing a role in the acquired responses to pictures: familiarity with and nature of the stimulus objects, presence of motion in the image, etc. Spontaneous and adapted reactions to pictures are a wide phenomenon present in different phyla including invertebrates but in most instances, this phenomenon is more likely to express confusion between objects and pictures than discrimination and active correspondence between the two. Finally, given the nature of a picture (e.g. bi-dimensionality, reduction of cues related to depth), it is suggested that object–picture recognition be envisioned in various levels, with true equivalence being a limited case, rarely observed in the behaviour of animals and even humans"
296,"Inspired by recent work on neural network image generation which rely on
backpropagation towards the network inputs, we present a proof-of-concept
system for speech texture synthesis and voice conversion based on two
mechanisms: approximate inversion of the representation learned by a speech
recognition neural network, and on matching statistics of neuron activations
between different source and target utterances. Similar to image texture
synthesis and neural style transfer, the system works by optimizing a cost
function with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able to extract
speaker characteristics from very limited amounts of target speaker data, as
little as a few seconds, and can be used to generate realistic speech babble or
reconstruct an utterance in a different voice.Comment: Accepted to ICASSP 201"
297,"The science of hiding secret information in another message is known as
Steganography; hence the presence of secret information is concealed. It is the
method of hiding cognitive content in same or another media to avoid
recognition by the intruders. This paper introduces new method wherein
irreversible steganography is used to hide an image in the same medium so that
the secret data is masked. The secret image is known as payload and the carrier
is known as cover image. X-OR operation is used amongst mid level bit planes of
carrier image and high level bit planes of data image to generate new low level
bit planes of the stego image. Recovery process includes the X-ORing of low
level bit planes and mid level bit planes of the stego image. Based on the
result of the recovery, subsequent data image is generated. A RGB color image
is used as carrier and the data image is a grayscale image of dimensions less
than or equal to the dimensions of the carrier image. The proposed method
greatly increases the embedding capacity without significantly decreasing the
PSNR value"
298,"State-of-the-art image-set matching techniques typically implicitly model
each image-set with a Gaussian distribution. Here, we propose to go beyond
these representations and model image-sets as probability distribution
functions (PDFs) using kernel density estimators. To compare and match
image-sets, we exploit Csiszar f-divergences, which bear strong connections to
the geodesic distance defined on the space of PDFs, i.e., the statistical
manifold. Furthermore, we introduce valid positive definite kernels on the
statistical manifolds, which let us make use of more powerful classification
schemes to match image-sets. Finally, we introduce a supervised dimensionality
reduction technique that learns a latent space where f-divergences reflect the
class labels of the data. Our experiments on diverse problems, such as
video-based face recognition and dynamic texture classification, evidence the
benefits of our approach over the state-of-the-art image-set matching methods"
299,"Several recent approaches showed how the representations learned by
Convolutional Neural Networks can be repurposed for novel tasks. Most commonly
it has been shown that the activation features of the last fully connected
layers (fc7 or fc6) of the network, followed by a linear classifier outperform
the state-of-the-art on several recognition challenge datasets. Instead of
recognition, this paper focuses on the image retrieval problem and proposes a
examines alternative pooling strategies derived for CNN features. The presented
scheme uses the features maps from an earlier layer 5 of the CNN architecture,
which has been shown to preserve coarse spatial information and is semantically
meaningful. We examine several pooling strategies and demonstrate superior
performance on the image retrieval task (INRIA Holidays) at the fraction of the
computational cost, while using a relatively small memory requirements. In
addition to retrieval, we see similar efficiency gains on the SUN397 scene
categorization dataset, demonstrating wide applicability of this simple
strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from
different geographical locations in the world for image retrieval that stresses
more dramatic changes in appearance and viewpoint"
