,full_text
0,"On the Application of Reservoir Computing Networks for Noisy Image
Recognition
Azarakhsh Jalalvanda, Kris Demuyncka, Wesley De Nevea,b, Jean-Pierre Martensa
aData Science Lab, Ghent University–iMinds, B–9000 Ghent, Belgium
bImage and Video Systems Lab, KAIST, Daejeon, South Korea
Abstract
Reservoir Computing Networks (RCNs) are a special type of single layer recurrent neural networks, in which the in-
put and the recurrent connections are randomly generated and only the output weights are trained. Besides the ability
to process temporal information, the key points of RCN are easy training and robustness against noise. Recently,
we introduced a simple strategy to tune the parameters of RCNs. Evaluation in the domain of noise robust speech
recognition proved that this method was effective. The aim of this work is to extend that study to the field of image
processing, by showing that the proposed parameter tuning procedure is equally valid in the field of image processing
and conforming that RCNs are apt at temporal modeling and are robust with respect to noise. In particular, we inves-
tigate the potential of RCNs in achieving competitive performance on the well-known MNIST dataset by following
the aforementioned parameter optimizing strategy. Moreover, we achieve good noise robust recognition by utilizing
such a network to denoise images and supplying them to a recognizer that is solely trained on clean images. The
experiments demonstrate that the proposed RCN-based handwritten digit recognizer achieves an error rate of 0.81
percent on the clean test data of the MNIST benchmark and that the proposed RCN-based denoiser can effectively
reduce the error rate on the various types of noise.
Keywords:
Reservoir computing networks, recurrent neural networks, text recognition, image classification, image denoising
1. Introduction
Thanks to the advances in the structure of the neural
networks since the early 80’s, such as introducing the
concept of Deep Neural Networks (DNNs) [1, 2] and
Convolutional Neural Network (CNN) [3] along with
the more powerful computational hardware, image pro-
cessing have become more elegant than ever. For in-
stance, in recent devices the traditional keyboards are
being replaced with modern interfaces such as touch-
screens that input text via handwriting (e.g., TV touch-
pad remotes). Due to the different handwriting styles,
there is a lot of variability in the images of the same
character, making automatic handwriting recognition
(HWR) a challenging task.
The presence of background noise is another source
of variability the HWR system may have to deal with.
In fact, in applications such as address recognition on
Email address: Azarakhsh.Jalalvand@UGent.be
(Azarakhsh Jalalvand)
parcels or full text recognition from digital scans of old
manuscripts or typed documents, noise corrupted im-
ages such as the one depicted in Figure 1 are the norm.
Figure 1: Part of the military newspaper “The Stars and Stripes” pub-
lished in 1944.
In this paper, we show that reservoir computing net-
works (RCNs) have great potential for achieving good
performance in HWR from noise corrupted images. We
demonstrate this on the MNIST [3] dataset, a handwrit-
ten digit recognition task (HDR) used by many research
groups to benchmark their technologies.
More than two decades ago, Multilayer Perceptrons
(MLPs) [4] were among the first classifiers that were
Preprint submitted to Elsevier November 29, 2017
tested on MNIST. In [3], an MLP with two computa-
tional layers of neurons was reported to reach a digit er-
ror rate (DER) of 2.95%, and a later study [5] reported a
DER of 1.60%. Employing MLPs with more layers was
long time believed to yield no significant improvement.
However, new training methods, permitting a better ex-
ploitation of multiple hidden layers, were recently dis-
covered and gave rise to the emergence of Deep Neural
Networks [6]. Differences in the details lead to DNNs
of various types. Two of them, Deep Belief Networks
(DBNs) and Deep Boltzmann Machines (DBMs) have
also been tested on MNIST (see Table 1). Roughly
speaking, they achieve a DER of about 1%.
It is, however, generally acknowledged that conven-
tional and modern neural networks such as DNNs per-
form well, but that they are still hard to train: it takes a
lot of time and the hyperparameters of the training pro-
cess must be set properly. More recent approaches such
as dropout [7] and maxout [8] are examples of efforts
to both facilitate improved training and improved effec-
tiveness of the models.
Nonetheless, long before deep neural networks be-
came successful, significant improvement over a stan-
dard 2-layer MLP was achieved by means of a Convolu-
tional Neural Network (CNN) [3] that acts like a feature
extractor. In fact, one of the main points of criticism
raised against an MLP was that its hidden neurons see
the whole image and are therefore bound to overlook
the local topological relations between adjacent pixels
undoubtedly present in sub-regions of the image [3].
Hence, the idea was to scan the image, to filter the pix-
els appearing in the emerging sub-regions by means of
trainable filters and to down-sample the filtered outputs
so as to create a rich and compact feature representation
that constitutes a more suitable input to the MLP-based
classifier. The first results obtained with the CNN ap-
proach were already mentioned in [3]. With a DER of
0.95%, CNN-based systems can still be considered as
state-of-the-art for HDR and the CNN-based features
are being used in many complex visual understanding
models [19].
Obviously, there is no reason why the concepts of a
CNN and a DNN could not be combined. Deep Convo-
lutional Neural Networks are the exponent of that idea,
leading to a DER of 0.83%.
Another idea that induced a significant boost in HDR,
was to enrich the original training dataset with new
images, obtained by deforming the raw training im-
ages. In [5], for instance, elastic deformations were
applied to the raw images achieving a convincing drop
in DER from 1.6% to 0.7%. Since then, basically all
novel methods have shown to benefit from such an en-
richment of the training set (see right column of Ta-
ble 1). By also introducing separate DNNs for different
digit widths (6 classes), it was even possible to achieve
human-competitive performance (0.23%) [18].
In spite of the spectacular performances achieved
in clean conditions, all aforementioned approaches fail
dramatically when recognizing digits from noisy sam-
ples. In [12], for instance, it was shown that a DBN
trained on clean samples, fails completely when recog-
nizing noisy samples. The DER raises to 33.8% when
the digits are partially masked by square blocks and to
66.1% when the digits are surrounded by a black bor-
der (see Figure 2). Consequently, new research has
been directed towards improving the robustness of HDR
against the presence of noise.
In general, one can roughly distinguish three ap-
proaches: (1) add noise to the training examples and
perform a so-called multi-conditional training of the
neural network, (2) make the classifier intrinsically
more robust against the effects induced by noise, for ex-
ample, by using a sparsely connected DBN rather than a
conventional fully connected one [12], and (3) remove a
large part of the noise from the input image before pre-
senting it to the classifier. In [12], it was argued that
due to the noise, a lot of neurons are driven into satura-
tion and are therefore not contributing to the recognition
anymore. By training it on noisy images, the standard
DBN could be made much more effective. The DER
could be reduced from 33.8% to 8.7% for the case of
block noise and from 66.1% to 1.9% for the case of bor-
der noise.
In some other work [20, 21], a stacked sparse DBN-
based denoising auto-encoder (SSDA) is trained to de-
noise the images. In such a system, one SSDA per noise
type was trained and the denoised image is obtained as
a linear combination of the individual SSDA outputs.
Feeding these images to a DBN trained on clean sam-
ples induced a dramatic improvement. The average er-
ror rate was reduced from 34.3% (an average over five
noise types) to 2.4%. Examples of the noise types are
depicted in Figure 2 and Table 5 lists the improvements
per noise type. As the combination weights are deter-
mined by a weight prediction module, the latter sys-
tem was called an adaptive multi-column SSDA (AMC-
SSDA) system.
Besides the noise-robustness, the incapability of pro-
cessing temporal information is another challenge in
expanding the application of these techniques to pro-
cess sequential data such as continuous text and videos.
Long short-term memory systems (LSTM) and some
more complex CNNs have been proposed and used to
address this weakness with the purpose of motion pic-
2
Table 1: Reference results on MNIST using the original training set and using an expanded version of the training set (for example, by applying
deformation). The presented DERs are accompanied by a reference to the paper introducing the technique that was used.
System DER% DER%(Original training set) (Enriched training set)
LSTM[9, 10] 1.80 0.32
2-layer MLP [5] 1.60 0.70
MLP + dropout [11] 1.05 -
DBN [12] 1.03 -
DBM [13] 0.95 -
CNN [3] 0.95 0.80
MLP + maxout + dropout [8] 0.94 -
ELM [14] 0.86 -
DCN [15, 16] 0.83 0.35
DBM + dropout [7] 0.79 -
Large CNN [17] 0.60 0.39
CNN + maxout + dropout [8] 0.45 -
Multi-CNN [18] - 0.23
Figure 2: From left to right, a clean MNIST sample and its corre-
sponding noisy versions: salt & pepper, border, Gaussian, block, and
speckle, respectively.
ture classification [9, 10, 22].
In this paper, the focus is on reservoir computing net-
works (RCNs) [23], which are a special type of recur-
rent neural networks. As was shown in [24, 25], RCNs
in combination with the proposed simple but effective
training procedure, can provide adequate solutions in
the field of speech recognition and noise robust speech
processing. The aim of this paper is to investigate if
the same training procedure is applicable in the domain
of image recognition and if the strong points of RCNs
such as good temporal modeling and noise robustness
transfer to the new domain as well.
Although developed in parallel, on the conceptual
level, an RCN can be considered as an extension of the
Extreme Learning Machine (ELM) proposed in [26].
An ELM is a two-layer MLP with a randomly ini-
tialized and afterwards fixed (i.e. non-trained) hidden
layer of non-linear neurons followed by an output layer
of linear neurons whose weights are determined so
as to minimize the mean squared difference between
the computed and the desired outputs. Under these
constraints, there exists a closed-form solution for the
weights which can be obtained by inverting a squared
matrix and performing some additional matrix multipli-
cations.
ELMs have been proven to be effective, efficient and
robust algorithms for pattern classification. In the past
years, several versions of ELMs have been introduced
to tackle the different challenges in the field of ma-
chine learning. For instance, due to the difficulty in
obtaining the labeled data, Huang et, al. [27] proposed
a semi-supervised ELM for classification and an unsu-
pervised ELM for clustering. Other solutions to the
dilemma of insufficient labeled data are domain adap-
tation and transfer learning [28]. In this respect, Zhang
and Zhang [29] extended ELMs to handle domain adap-
tation problems for improving the transferring capabil-
ity of ELM between multiple domains with very few
labeled guide instances in target domain, and over-
come the generalization disadvantages of ELM in multi-
domains application.
An RCN differs from an ELM in the sense that its first
layer, called the reservoir, consists of recurrently con-
nected non-linear neurons with randomly initilialized
and fixed (non-trained) coefficients. Similar to ELMs,
the “andom and fixed” first layer feeds into an an output
layer of linear neurons with trained coefficients. Since
this layer reads from the reservoir to construct its output,
it is called the ‘read out’ layer.
In our previous study, we conceived a straight-
forward strategy to design an RCN [25]: one only needs
to specify the bandwidths of the RCN inputs and out-
puts, after which the method automatically produces
good values for all the hyperparameters of the reser-
voir component of the RCN. The size of the reservoir
remains a free parameter, and its optimal value depends
3
on the number of available training examples and the
envisioned compromise between accuracy and compu-
tational cost. This method sped up the design of the
robust spoken digit recognizer significantly. The afore-
mentioned observations motivated us to experimentally
investigate whether the devised techniques could also be
successfully applied in a domain different from speech,
namely the visual domain, and in particular handwriting
recognition.
A summary of our results with RCNs on handwrit-
ing recognition appeared in [30]. This paper extends
that contribution by (1) providing an empirical anal-
ysis on how to setup RCN architectures for perform-
ing HDR, (2) introducing a multi-column RCN-based
model to process the noisy images, (3) comparing RCNs
and ELMs with regard to their robustness against over-
fitting, and (4) having a short look at the recognition of
connected digits. The remainder of this paper is orga-
nized as follows. Section 2 briefly recalls the general
principles underlying RCNs. Section 3 introduces the
various ways in which the two dimensional image can
be fed to a temporal model such as an RCN. Next, we
describe our experimental study of these architectures
for the recognition of clean handwritten digits (Sec-
tions 4 and 5). In the second part of the paper, we focus
on the noise-robustness of the proposed RCN architec-
tures (Section 6). The paper ends with a summary and
conclusions, as well as some ideas for future research.
2. Reservoir Computing Network (RCN)
In its simplest form, an RCN is a neural network with
two particular computational layers: (1) a hidden layer
(pool) of recurrently interconnected non-linear neurons,
called reservoir, driven by inputs and by delayed feed-
backs of its outputs and (2) an output layer of linear
neurons, called readouts, driven by the hidden neuron
outputs (Figure 3). A fundamental point is that the input
weights and the recurrent weights are initialized from
random distributions, and only the output weights are
optimized (trained) for solving the targeted problem.
If Ut, Rt and Yt represent the reservoir inputs, the
reservoir outputs and the readouts at time t, the RCN
equations can be written as follows [23]:
Rt = (1− λ)Rt−1 + λ fres(WinUt +WrecRt−1) (1)
Yt =W
outRt (2)
with 0 < λ ≤ 1, with fres being the non-linear ac-
tivation function of the reservoir neurons (hyperbolic
tangent in this work) and withWin,Wrec andWout
being the input, recurrent and output weight matrices,
510 510 
32 32 
32 340 
386 693 
Decoder 
Speech 
U 
Language 
Model 
Pronunciation 
Dictionary 
Acoustic 
Models 
Back-end component 
Front-end 
component 
Yt 
 …
 
P(Ut|qt) 
F 
... 
Zero Nine 
I1 
Po 
I2 
# 
 …
 
Speech 
U Transcribed 
Speech 
Front-end 
component 
Back-end 
component 
𝐰  
𝐰  
S 
S 
Speech 
Text Text 
Synthesis 
Human 
Meaning 
Speech 
Generation Understanding 
Recognition 
Machine 
q1 q2 q3 q4 q5 
 …
 
Input 
layer Reservoir 
Readout 
layer 
random input weights 
random recurrent weights 
trained output weights 
Ut Yt W
in Wout 
readout vector 
output weight matrix 
input weight matrix 
reservoir state vector 
input feature vector 
Wrec 
recurrent weight matrix 
 …
 
Rt 
Ut 
Yt 
Win 
Wout 
Wrec 
Rt 
Transcribed 
Speech 
Figure 3: A basic RCN consists of a reservoir and a readout layer.
The reservoir is composed of interconnected non-linear neurons with
fixed random weights. The readout layer consists of linear neurons
with trained weights.
respectively. The constant λ is called the leak rate be-
cause (if one makes abstraction of fres) Equation (1)
represents a leaky integration of the neuron activation.
Each individual input is normalized so that it has a
zero mean and unit variance over the training examples.
The initialized input and recurrent weights to the reser-
voir nodes are assigned from a normal distribution and
they are characterized by four parameters [25]:
αU the largest singular value ofWin,
ρ (known as spectral radius) the maximal absolute
eigenvalue ofWrec,
Kin the number of inputs driving each reservoir neuron,
and
Krec the number of delayed reservoir outputs driving
each reservoir neuron.
The first two parameters control the absolute and the rel-
ative importance of the inputs and the delayed reservoir
outputs in the reservoir neuron activation. The latter two
control the sparsity of the input and the recurrent weight
matrices.
The output weights are such that they minimize the
mean squared error (MSE) between the computed read-
outs Yt and the desired readouts Dt over the training
examples. If an RCN is trained for recognition, the de-
sired output Dt is a unit vector with one non-zero entry
encoding the desired class at time t. If it is trained for
feature denoising, Dt is the desired clean feature vector
at time t. In both cases, the output weights emerge as the
solution of an over-determined set of linear equations.
3. RCN-based architectures for image processing
In many neural network-based image processing sys-
tems, the input is a pixel array of the whole im-
age [21, 31]. However, in order to exploit the dynamical
system properties of an RCN, we need to create a se-
quential input stream. This can be achieved by scanning
4
the image column-wise (horizontal scanning) or row-
wise (horizontal scanning).
The readouts of the RCN that will be encompassed
in the recognizer correspond to the ten digits and to the
white space which is present in each digit image. By in-
troducing this white space and by envisioning an image
as a digit surrounded by white space, we can achieve
that the digit readouts will mainly react to features that
are typical for the digit they represent.
3.1. Basic architecture
A trivial procedure leading to the desired input stream
is horizontal scanning: the image is scanned column-
wise from left to right and the subsequent columns
(called frames) form the input vector sequence (see Fig-
ure 4).
The digit scores are obtained by accumulating the
digit readouts across time (the Σ component) and a
winner-take-all algorithm returns the winner digit with
the highest readout activity.
One could also benefit from bi-directional process-
ing [25], which means that each RCN contains two
reservoirs: the forward reservoir that processes the in-
puts U1→T whereas the backward reservoir that pro-
cesses the inputs UT→1. The outputs of the latter reser-
voir are then time reversed before combining them with
the outputs of the forward reservoir. A deep (cascade)
RCN is obtained by stacking multiple RCNs, as de-
picted in Figure 4. Each layer of the deep RCN is a
basic bi-directional RCN.
510 510 
32 32 
32 340 
386 693 
# 
+ 
wh wv 
(a)  H-V-Inp (c)  H-V-Res (b)   H-V-wscr 
Σ 
R
C
N
 
UH UV 
forward 
backward 
R
ev
 
R
ev
 
Layer 1 U1T Y11T 
U1 UT … 
# 3 3 … 
Σ 
Scores 
Winner-
Take-All 
3 
5
50
MFCC MFCC MVN MVN MVN AFE AFE
Simple LIN LIN LIN LIN LIN LIN
1 1 1 2 3 3 3
4000 4000 4000 4000 8000 8000 8000
Uni Uni Uni Uni Uni Uni Bi
W
E
R
%
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
R
C
N
 
Σ 
3 
Noisy 
image 
3 
Clean 
image 
DAE 1 
U
H
 
DAE 2 
U
H
 
DAE M 
U
H
 
…
 
DAE 
3 3 
forward 
backward 
R
ev
 
R
ev
 
Layer L YL1T 
Figure 4: Architecture of a deep RCN-based digit recognizer leverag-
ing bi-directional processing in each layer.
The layers are trained one after the other using the
same desired outputs in every layer. The argument for
cascading layers is that the new layer can correct some
of the mistakes made by the preceding layers because it
offers additional temporal modeling capacity and a new
inner space in which to perform the classification.
3.2. More complex architectures
Given that it is also suitable for continuous HWR,
horizontal image scanning seems to be an obvious
choice. However, for isolated digit recognition, one can
also consider vertical scanning, as well as a combined
scanning approach. The ones we propose here are de-
picted in Figure 5.
Combination of input features
A simple combination strategy is to supply the RCN
with the concatenation of one row and one column at
each time step (see Figure 5(a)). Obviously, this ap-
510 510 
32 32 
32 340 
386 693 
# 
+ 
wh wv 
(a)  H-V-Inp (c)  H-V-Res (b)   H-V-wscr 
Σ 
R
C
N
 
UH UV 
forward 
backward 
R
ev
 
R
ev
 
Layer 1 U1T Y11T 
U1 UT … 
# 3 3 … 
Σ 
Scores 
Winner-
Take-All 
3 
5
50
MFCC MFCC MVN MVN MVN AFE AFE
Simple LIN LIN LIN LIN LIN LIN
1 1 1 2 3 3 3
4000 4000 4000 4000 8000 8000 8000
Uni Uni Uni Uni Uni Uni Bi
W
E
R
%
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
R
C
N
 
Σ 
3 
Noisy 
image 
3 
Clean 
image 
DAE 1 
U
H
 
DAE 2 
U
H
 
DAE M 
U
H
 
…
 
DAE 
3 3 
forward 
backward 
R
ev
 
R
ev
 
Layer L YL1T 
Figure 5: Different ways of combining horizontal (H) and vertical
scanning (V ) in a system: (a) supply the RCN with one row and one
column of the image, (b) compute a weighted sum of the digit scores
(accumulations over time) emerging from an H-RCN and a V-RCN
and (c) supply the H-RCN and V-RCN outputs to another RCN and
accumulate the scores of the readouts of this RCN.
proach presumes a square image, leading to an identical
number of frames per scanning direction.
Weighted sum of scores
Another strategy is to make two independent parallel
systems: one using horizontal (H-RCN) and one using
vertical scanning (V-RCN). The digit scores can then be
obtained as a linear combination of the scores emerging
from the two sub-systems (see Figure 5(b)). The ad-
vantage of this approach is that it can also be applied to
rectangular images.
Combination of readouts
The third option is to supply the combined readouts
of the V-RCN and the H-RCN to the final digit recog-
nition RCN (see Figure 5(c)). Obviously, this approach
again presumes a square image.
4. Experimental setup
In this section, we present the experimental frame-
work that was set-up in order to assess the potential of
the proposed system configurations.
5
4.1. MNIST corpus
The MNIST corpus [3] consists of clean handwrit-
ten isolated digit samples, grouped into two datasets:
a training set consisting of 60K samples and a test set
consisting of 10K samples. Each sample is represented
by a 28×28 gray-scale encoded pixel array. The origi-
nal pixel codes (between 0 and 255) are interpreted as
real numbers between 0 and 1. Many studies sub-divide
the development set into a training set of 50,000 images
and a validation set of 10,000 images. We report the
digit error rate (DER%) on the validation or test set as
the recognition performance measure.
Some studies extend the training dataset by deform-
ing the original training images and by considering the
deformed images as extra training examples, but here
we refrain from doing so because our main objective
is to show that an RCN-based system has potential to
become an alternative to other state-of-the-art systems
and it is difficult to make a fair comparison of results
obtained with an extended training set without knowing
exactly which deformations were applied in the systems
one wants to compare with.
In order to conduct experiments on noise robustness,
we construct a multi-condition dataset by dividing the
dataset into six equally large parts. One part is left un-
altered and serves as a clean dataset. The images of
the other five parts are corrupted with noise, one noise
type per part. The following noise types are chosen to
be representatives of the challenges in the common im-
age processing procedures [12, 21]. We consider Salt
& Pepper, for which a certain amount of the pixels in
the image are either black or white. This noise can, e.g.,
appear in charge coupled device (CCD) sensor outputs
or in the transmission of the image. Principal sources of
Gaussian noise in digital images arise during acquisi-
tion e.g. sensor noise caused by poor illumination, high
temperature, transmission e.g. electronic circuit noise,
etc. Speckle is a granular noise that inherently exists in
and degrades the quality of the active radar, synthetic
aperture radar (SAR), medical ultrasound and optical
coherence tomography images. Apart from these tech-
nical noise types, usually the objects in an image are
partially blocked by other elements or the objects are
bordered in a frame (e.g., house numbers).
4.2. Front-end
The front-end scans the image either horizontally (H)
or vertically (V) and per scanning step t, the column
vector (if H) or the row vector (if V) is a 28-dimensional
vector Xt. However, it is common in neural networks
to obtain Ut by extending Xt with its first and second
derivatives in the scanning direction, or by stacking the
vectors Xt−k, .., Xt+k. Both approaches have the ad-
vantage of providing the system with a glimpse of the
future. In our experiments, we use frame stacking with
k = 2.
4.3. RCN hyperparameters
The creation of a suitable RCN was studied in detail
in [25]. In summary, the theory presented there leads to
the following conclusions:
1. The input and recurrent weight matrices (Win and
Wrec) can be very sparse. In particular, 5 to 10
elements per node are enough, regardless of the
reservoir size and the input feature vector size.
2. The spectral radius, ρ, must be tuned to the band-
width (normalized frequency, F ) of the input acti-
vations of the reservoir (interpreted as time series):
ln(ρ) =
−F
0.35
(3)
3. The leak rate λ must be tuned to Tmin, the min-
imum time (in scan steps) the reservoir output is
expected to remain constant:
ln(1− λ) = −1
Tmin
(4)
4. The square of αU follows from a function of the
other parameters. This function is proportional to
an auxiliary parameter Vopt, defined as the optimal
reservoir output variance.
In [25] we argue that Vopt may be independent of the
task, but since the objective of that work was only
speech recognition, we did not present any experimen-
tal evidence for this argument yet. Here, we will show
that Vopt = 0.035 which was found optimal for spo-
ken digit recognition, is also valid for handwritten digit
recognition.
4.4. Design of the reservoir in the first layer
The first step consists of determining the bandwidth
of the input activations. In order to do so, it suffices to
consider an arbitrary small reservoir (500 nodes) with
memory-less neurons (no leaky integration) and no re-
current connection, to record a few hundred input acti-
vation patterns of 28 samples long (which is the number
of scan steps), to determine the periodogram (the square
of the magnitude of the DFT) of each recorded pattern
and to calculate the envisioned power spectrum as the
mean of these periodograms. To facilitate our experi-
ment, we fed the reservoir with normalized pixels with
a unit variance.
6
Figure 6 (left) shows the estimated power spectrum,
|B(f)|2, and its bandwidth, F = 0.15. For this value it
follows from Equation 3 that ρ = exp(−0.15/0.35) =
0.65. Note that there is little difference in bandwidth
between horizontal and vertical scanning.
5
1 2 3 4
0.08
0.09
0.1
N layers
N
F
Gaussian
(0.42)
S&P
(0.60)
Speckle
(0.44)
Block
(0.42)
Border
(0.83)
Avg.
(0.48)
0
0.05
0.1
0.15
0.2
N
F
Combined-DAE
Mixed-DAE
Ideal-DAE
0 0.2 0.4
0
0.1
0.2
0.3
F
Normalized freqency (f )
Po
w
er
Sp
ec
tr
um
(a) Optimizing
0 0.2 0.4 0.6 0.8 1
10
20
30
40
Input scale (αU )
D
E
R
%
(b) Optimizing
Figure 6: (left) Power spectrum of the input activation and (right)
DER as a function of the input scaling factor αU for a system encom-
passing a single layer RCN with 500 nodes.
Next, we have to identify the minimum number of
scan steps a readout is supposed to remain constant.
Given that white spaces are often not more than 4 pix-
els wide and that the core of a digit like ’1’ may be as
narrow as that as well, we select Tmin = 4 as a realistic
value. For this value, Equation 4 leads to λ = 0.22.
The third step consists of finding the proper input
scale factor αU . The equation for solving αU as a
function of the other reservoir parameters can be found
in [25].
Here we verify whether this function leads to a good
result in the simple case of a recognizer built with the
reservoir we used for measuring the power spectrum of
the input activation. For this reservoir, the function re-
duces to
α2U K
in VU φb(F ) = Vopt = 0.035 (5)
with Kin = 5, VU = 1 and
φb(F ) =
∫ F
−F |B(f)|2 df∫ 0.5
−0.5 |B(f)|2 df
= 0.85 (6)
The result is αU = 0.28. In order to verify whether this
is a suitable value, we reused the same reservoir in com-
bination with a large number of input scaling factors.
Figure 6 (right) shows the DER of the digit recognizer
on a validation set as a function of this factor. It appears
that 0.28 is in the middle of the optimal range from 0.2
to 0.5.
We also verified whether (ρ, λ) = (0.65, 0.22) were
appropriate values. Again, we considered a reservoir
with 500 nodes. Since it was already shown in [25] that
λ and ρ can be optimized independently of each other,
we made two lines of search: one along ρ (while λ = 1)
and one along λ with the optimum ρ from the previous
sweep. According to Figure 7, the values originating
from the theory are close to the actual optimum points.
For the higher layers, the inputs are always close to
the desired outputs. Therefore, after measuring the av-
erage bandwidth of the outputs of the first layer, we set
ρ = 0.4 and αU = 0.6 for all further layers. Since the
desired outputs are the same as the first layer, the same
λ = 0.22 was set for the higher layers. A control ex-
periment with a two-layer system confirmed that these
settings are appropriate.
6
0 0.2 0.4 0.6 0.8 1
0
10
20
ρ
D
E
R
%
(c) Optimizing
0 0.2 0.4 0.6 0.8 1
0
10
20
λ
D
E
R
%
(d) Optimizing
−0.4 −0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.40
50
100
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Se
en
U
ns
ee
n
Cross correlation
Ref
P1
P2
P3
P3′
P3′′
Figure 7: Control experiments to optimize ρ (left) and λ (right).
5. Experimental results
In this section, we assess the performance of our sys-
tems as a function of the reservoir size (the number of
neurons in the reservoir), the depth of the RCN (the
number of layers) and the direction of scanning in the
front-end. Unless stated otherwise, all RCNs are bi-
directional and an RCN with a reservoir of size N ac-
tually ncompasses two independent reservoirs of size
N/2 working in parallel. The number of trainable pa-
rameters of such an RCN is 11× (N + 1), in which the
extra 1 represents a bias for each readout node.
5.1. Deep versus wide
First, we compare single- and multi-layer RCNs with
horizontal image scanning. In multi-layer RCN, the
reservoir size is kept the same in each layer. The results
depicted in Figure 8 support the following conclusions:
• Any single-layer system can be improved by
adding extra layers and the relative reduction of the
DER that can be attributed to adding a second layer
is about 25%, irrespective of the reservoir size.
• The positive impact of adding layers decreases
quickly with the depth of the RCN. In general,
there is no point in creating systems with more than
three layers.
7
• Even though a multi-layer system does not yield
a much lower DER than a single-layer system en-
compassing the same number of trainable parame-
ters, the former is easier and faster to design. In
fact, the memory load and the training time are
roughly proportional to the square of the reser-
voir size, meaning that for the training of one-layer
RCN with a 32K reservoir, one needs four times
more memory and two times more time than for
the training of a two-layer RCN with a 16K reser-
voir in each layer.
2
Clean 20 15 10 5 0 -5
100
101
102
SNR (dB)
W
E
R
(%
)
1layer-4k
2layers-4k
2layers-4k+250A
1 2 3 4 5 6 7
1
2
4
8
Number of layers
D
E
R
%
1K 2K 4K
8K 16K 32K
104 105 106
1
2
4
8
Number of free parameters
D
E
R
%
1K 2K 4K
8K 16K 32K
Figure 8: DER (in %) on the validation set as a function of the reser-
voir size and the number of layers (top) and the same results, but as a
function of the number of trainable parameters (bottom).
5.2. Scanning directions
In a second experiment, we assess the impact of the
image scanning direction and the scanning combination
strategy on the system performance.
3
1 2 3 4 5 6 7
1
2
4
8
16
Number of layers
D
E
R
%
8k-V 8k-H
4k-V 4k-H
1k-V 1k-H
5 10 15 20 25
0
0.5
1
Vertical scanning frames (rows)
Y
5 10 15 20 25
0
0.5
1
Horizontal scanning frames (columns)
Y
Figure 9: Comparing the effect of horizontal image scanning (H) with
the vertical procedure (V) for three multi-layer RC-based systems
having 1K, 4K, and 8K nodes per layer.
Table 2: Comparing different input scanning options.
H V H-V-inp H-V-wscr H-V-res
DER% 1.52 1.39 1.48 1.30 1.18
Figure 9 clearly shows that in the case of a single-
layer system, horizontal scanning outperforms vertical
scanning; whereas for the deep systems, vertical scan-
ning tends to produce slightly better results. The differ-
ences may be due to the fact that most digits occupy a
smaller part of the image in horizontal than in vertical
direction, as illustrated in Figure 10. This means on the
one hand that a single column carries more information
about the digit on average than a single row, which is
beneficial for horizontal scanning. On the other hand,
this means that the digit score in vertical scanning sys-
tems is based on more frames; what should normally
favor this scanning direction. Apparently, the first phe-
nomenon is more dominant than the second one in a
one-layer system, whereas the second phenomenon is
more dominant in the multi-layer systems.
Figure 10: The readouts of reservoirs working with horizontal (top)
and vertical (bottom) scanning for a sample of digit 1. The strong
black line is the readout of digit 1, the dashed line is the readout of
white space class.
Table 2 lists the results of five systems: (1) H: one
2-layer system with 5K reservoirs and horizontal scan-
ning, (2) V: one 2-layer system with 5K reservoirs and
vertical scanning, (3) H-V-inp: one 2-layer system with
5K reservoirs driven by the concatenation of the two
scanning directions, (4) H-V-wscr: two 2-layer systems
with 2.5K reservoirs (one per scanning direction) whose
digit scores are linearly combined, and (5) H-V-res: two
2-layer systems with 2K reservoirs (one per scanning
direction) followed by a single-layer system with a 2K
reservoir.
As shown by our results, system H-V-res, clearly out-
performs both single scanning systems. This indicates
that the H and the V readouts for a frame together form
8
Table 3: Reference results on MNIST using the original training set.
The presented DERs are accompanied by a reference to the paper in-
troducing the technique that was used.
Model DER%
MLP [5] 1.60
MLP + dropout [11] 1.05
DBN [12] 1.03
DBM [13] 0.95
CNN [3] 0.95
MLP + maxout + dropout [8] 0.94
ELM [14] 0.86
RCN (This work) 0.81
DBM + dropout [7] 0.79
CNN + maxout + dropout [8] 0.45
a richer feature space for the final classification of the
frames.
5.3. Final result
Based on the above findings, we designed a system
of type H-V-res that consists of two 2-layer systems
comprising a 16K reservoir in each layer, followed by
a single layer RCN encompassing a 16K reservoir. This
system has 880K trainable parameters and it achieves
a DER of 0.81% on the MNIST test set (see Table 3),
showing that it is competitive with formerly reported
systems working with the same inputs and being trained
on the same training samples. As depicted in Figure 11
the most errors occurs for digit ‘9’ being recognized as
‘4’ and ‘7’ being recognized as ‘2’.
0 1 2 3 4 5 6 7 8 9 Sum
0 0 0 0 0 0 1 1 0 0 0 2
1 0 0 1 1 1 0 2 1 0 0 6
2 0 0 0 0 0 0 0 3 2 0 5
3 0 0 3 0 0 3 0 2 2 0 10
4 0 0 0 0 0 0 3 0 1 3 7
5 1 0 0 3 0 0 1 0 0 0 5
6 2 1 0 0 1 3 0 0 1 0 8
7 0 2 8 1 1 0 0 0 0 2 14
8 2 0 1 0 0 0 0 0 0 1 4
9 3 1 1 3 4 3 0 3 2 0 20
8 4 14 8 7 10 7 9 8 6 81
Recognized as
D
ig
it
Sum
Figure 11: The confusion matrix for the performance of the RCN-
based classifier on the MNIST dataset. The most errors occurs for
digits ‘9’ and ‘7’, with ‘7’ being recognized as ‘2’.
6. Noise robustness
In this section, we study the noise robustness of our
RCN systems. First, we consider systems that recognize
digits from raw noisy images and later, we consider sys-
tems that recognize digits from denoised images.
6.1. Recognition of noisy images
According to [25, 26, 32], the non-trained random
weights, in combination with the MSE optimization cri-
terion, reduces the chance of overfitting the training data
in RCNs and ELMs and hence, let the system general-
ize better to noisy data than a system with fully trained
parameters. In what follows, we distinguish two ex-
perimental settings: one in which the system is trained
on clean images only (clean training) and one in which
the system is trained on a mix of clean samples and
samples corrupted by the five noise types that are also
present in the test set (multi-conditional training). We
present DERs for each of the six subsets of the multi-
conditional test set. Note that multi-conditional training
is bound to yield an optimistic result as all noise types
encountered in the test data were also present during
system training. Nevertheless, we followed this recipe
to have comparable results with other references.
Preliminary results
With regard to the effect of number of trainable pa-
rameters on the robustness of the RCN, we considered
the single layer reservoir that scans the image horizon-
tally (the H system) and the number of trainable pa-
rameters is controlled by changing the size of the reser-
voir. The experiments were conducted for both clean
and multi-conditional training. Figure 12 shows the per-
formance of RCNs of 500 to 16K nodes, that are tested
on clean and a mixture of noisy samples. From these ex-
periences one can conclude that a glimpse of overfitting
is visible only for very large RCNs that are trained on
clean and tested on noisy samples. This phenomenon
was evident in our previous study on speech recogni-
tion in [25] only in the extremely mismatched condi-
tions (i.e. clean training and testing on very noisy sam-
ples). According to multi-conditional training results,
as well as trained-tested on clean samples, no overfit-
ting occurs in case of matched conditions. A conclusion
that is inline with other studies on RCNs.
Apart from the aforementioned common character-
istics of RCNs and ELMs, RCNs benefit from the re-
current connections that add the ability of processing
temporal information in a non-linear way. In order to
study the effect of these connections to the robustness
of the RCNs, we repeated the same experiments with
9
9−1 −0.5 0 0.5 1−1
−0.5
0
0.5
1
0 0.2 0.4 0.6 0.8 1
10
20
30
40
Sweeping range
D
E
R
%
Sweeping αU
(g) Optimizing
0 0.2 0.4 0.6 0.8 1
0
10
20
Sweeping range
D
E
R
%
Sweeping ρ
Sweeping λ
(h) Optimizing
5K 10K 15K
0
20
40
RCN size
D
E
R
%
RCN-Clean RCN-Noisy ELM-Clean ELM-Noisy
5K 10K 15K
0
20
40
RCN size
D
E
R
%
Figure 12: The effect of increasing the size of the hidden layer on the
performance of single layer RCN and ELM using clean training (left)
as well as multi-conditional training (right). The condition of the test
samples (Clean or Noisy) is determined in the legend.
Table 4: Comparing the performance of a single layer 16K-node RCN
with an equivalent ELM.
System Clean training Multi-cond. training
Clean Noisy Clean Noisy
RCN 2.18 33.78 3.80 8.65
ELM 2.54 31.08 4.52 9.58
an equivalent ELM. That is actually the same RCN but
without the recurrent connections. The performance of
this system is depicted in Figure 12. Also the exact er-
ror rates of the 16K-node systems are listed in Table 4.
These experiments show that both ELM and RCN fol-
low the same trend of behavior with two main excep-
tions: (1) trained on clean and tested on noisy samples,
the ELM suffers less from the overfitting and (2) RCN
outperforms ELM in the matched conditions. There-
fore, it can be confirmed that the robustness against
overfitting mainly is caused by the random weights and
the linear training approach, while the recurrent connec-
tions allows the RCN to focus on the less apparent rela-
tions between features and classes, and hence help more
for matched conditions where such fine differences are
more meaningful.
RCN-based noisy image recognizer
The results of our experiments using different RCN-
based architectures (explained in Section 3) are listed in
Table 5. For comparison with state-of-the-art, the table
also includes the results for DBN systems we could find
in the literature. In the clean training case, the presence
of noise induces a dramatic increase of the DER in all
systems. None of the systems stands out on all condi-
tions. The DBN system wins in three of the six condi-
tions, the RCN in the other three, be it that on average
the DBN system yields the lowest DER. It is fair to say
that RCNs degenerate at more or less the same pace as
DBNs when the mismatch between the training and the
test conditions increases. We interpret this as a positive
result because deep neural networks are acknowledged
for their good noise robustness and because the research
on RCNs is still in its initial phase.
In the multi-conditional training case, the effect of the
noise is much more moderate. The H-V-res system now
yields an average error rate of only 3.54% and it out-
performs the DBN systems in all conditions for which
a comparison is possible. Combining two scanning di-
rections seems to help significantly as long as there is
no big mismatch between the training and the test con-
ditions (that means clean test for clean training and all
tests for multi-conditional training).
None of the tested systems stands out on all noise
types, but on average, the H-V-inp architecture is the
winner because there is no noise type for which it com-
pletely breaks down. This system performs exception-
ally well for the Border noise. The DBN-based system
on the other hand is the winner for three of the five noise
types.
In order to further investigate the difference between
the performances of H-V-inp, H and V, we should take a
look at Figure 13 which shows the readouts of the three
systems for a noisy sample of digit 7 surrounded by a
border. This figure shows that in the case of H and V, the
winner is mainly determined by the digit readout that
reaches the highest value at the beginning and the end
of the scan, where only the black border is observed.
In the case of horizontal scanning, this seems to be ‘1’
which often comprise a more or less vertical line that
bares a lot of resemblance with the black border. For
the H-V-inp system, none of the digit readouts seems to
match the black border and the correct winner is more
likely to pop-up.
To confirm our hypothesis we investigated in more
detail the confusions between the recognized and the
correct digit in the case border noise is present. Ta-
ble 6 shows the digits that were recognized in case of
a wrong decision. For instance, it is indicated that with
the H-RCN system, 8501 of the validation samples have
been wrongly classified as digit 1.
Apparently, the H-system is strongly biased towards
the digits exhibiting a strong vertical line (‘1’ and ‘4’)
whilst system V is biased towards digits with horizon-
tal lines on top, bottom or center (‘2’, ‘4’,‘5’ and ‘7’).
A simple solution to reduce the effect of the false pos-
itive reaction to a border, without seriously degrading
the performance on the other noise types, is to bound
the readouts to an acceptable interval such as (-0.05,
0.25). By doing so, the DER for Border noise can be
10
Table 5: DER (in %) per noise type for the cases of clean and multi-conditional training. The systems DBN-2010 and DBN-2013 refer to [12]
and [21], respectively. (V) and (H) respectively refer to the RCNs that are supplied with vertical and horizontal scanning of the image as the
input. Also see Section 3 for the combined approaches. The last row shows the DER of a multi-column RCN-based recognizer comprising twelve
sub-systems each trained on one noise condition and one direction.
System Clean Gaussian S & P Speckle Block Border Average
C
le
an
DBN-2010 1.03 - - - 33.78 66.14 -
DBN-2013 1.09 29.17 18.63 8.11 25.72 90.05 28.80
V 1.11 57.04 56.27 72.96 24.97 85.49 49.64
H 1.28 31.43 40.91 45.91 25.41 60.99 34.32
H-V-inp 1.18 29.46 40.94 30.70 22.12 16.97 23.56
H-V-wscr 0.89 32.12 38.47 48.50 21.79 64.94 34.45
H-V-res 0.81 32.10 38.91 49.32 21.85 79.34 37.06
M
ul
ti
DBN-2010 1.68 - - - 8.72 1.95 -
V 1.88 4.73 6.06 7.38 9.50 2.45 5.33
H 2.28 4.12 5.17 5.65 9.10 2.42 4.79
H-V-inp 2.28 4.20 5.22 5.23 8.96 2.63 4.75
H-V-wscr 1.65 3.12 3.90 4.47 7.20 1.93 3.71
H-V-res 1.50 3.08 3.75 4.32 6.82 1.75 3.54
MC-RCN 2.82 4.54 6.07 6.22 9.82 3.23 5.45
Table 6: Distribution of the wrong decisions in case of border noise.
0 1 2 3 4 5 6 7 8 9 Sum
H 0 8501 4 0 33 0 0 0 11 1 8549
V 0 0 5180 13 69 788 0 50 0 0 6099
H-V-inp 2 1533 2 1 98 17 0 41 2 1 1697
4
5 10 15 20 25
−2
−1
0
1
2
Horizontal scanning frames (winner is 1)
Y
5 10 15 20 25
−2
−1
0
1
2
Vertical scanning frames (winner is 2)
Y
5 10 15 20 25
−2
−1
0
1
2
Combined scanning frames (winner is 7)
Y
1 2 4 8 16 32
0.01
0.05
0.1
0.5
Reservoir size (×1000)
N
F
Clean (0) Gaussian (0.42) S&P (0.60) Speckle (0.44)
Block (0.42) Border (0.83) Avg. (0.48)
Figure 13: The readouts of the H, V, and H-V-inp recognizers trained
on the clean dataset and fed with an image of digit 7 corrupted with
border noise. The solid black, dashed black, and dotted gray lines
belong to the readouts of digit 7, the winner digit and white space,
respectively.
decreased to 64.93%, 34.79%, and 15.42% for the V,
H, and H-V-inp systems, respectively. This, in its turn,
leads to average DERs of 44.26%, 27.61%, and 22.09%,
respectively.
For multi-conditional training, only two results are
reported for the DBN system, but the figures in Table 6
do not contradict the conclusion that RCN-based sys-
tems are even more robust to noise than DBN systems
which are generally acknowledged for their good ro-
bustness in comparison to other techniques.
Multi-Column RCN
For completeness, we also trained a two-stage multi-
column RCN (MC-RCN) recognizer. The first stage en-
compasses twelve 2-layer RCNs with a 3K reservoir per
layer, one RCN per noise type (6 noise types) and per
scanning direction (2 directions). The outputs of these
twelve RCNs drive a 2-layer RCN with a 4K reservoir
per layer. The reservoir sizes were chosen such that the
system has the same number of trainable parameters as
the H-V-res system. Apparently, the MC-RCN does not
even outperform the much simpler H and V systems (see
Table 5). Our hypothesis is that the reservoirs in the
first stage are too small to permit an accurate classifica-
tion. This was confirmed by an experiment in which the
reservoir size was increased to 8K (leading to 2M train-
11
able parameters) and in which the error rate dropped to
3.41%. Increasing the size of the H-V-res system on the
other hand did not cause any significant improvement
(error rate of 3.49%). This latter results proves that the
MC-RCN, in spite of its much larger complexity, will in
the end not significantly outperform the much simpler
H-V-res architecture.
6.2. Recognizing connected digits
As described in Section 3, the capability of process-
ing temporal information makes it possible to recognize
the digit by scanning the image. Consequently, one can
train an RCN by scanning the isolated digits horizon-
tally and operate this system on the connected samples
without any extra pre-processing (e.g., digit segmenta-
tion). This is a noticeable discrepancy between RCNs
and many other conventional neural networks. Fig-
ure 14 depicts the output of a multi-conditionally trained
RCN with horizontal scanning (the H system in Table 5)
which has been supplied with a concatenation of multi-
ple noisy digits.
Figure 14: The readouts of a multi-conditionally trained RCN with
horizontal scanning supplied with a concatenation of multiple noisy
digits.
6.3. Removing the noise in the front-end
Multi-condition training is an approach to reduce the
mismatch between training and testing. One could also
reduce the noise in the front-end. In this phase, we pro-
pose an RCN-based denoising Auto-Encoder (DAE) to
accomplish this.
For fixing the hyper-parameters of the DAE reser-
voirs, we follow the same strategy as before, but this
time under the assumption that the dynamics of the tar-
geted outputs are identical to the dynamics of the inputs.
Moreover, we established that bi-directional processing
is also helpful for this task but that it suffices to stack
three (instead of five) successive frames in the DAE in-
put. Since the output of the DAE is a denoised version
of the input feature vector, the number of trainable pa-
rameters of such an RCN-based DAE of the size N is
28 × (N + 1), with 28 being the number of pixels per
column/row.
We introduce two DAE architectures: (1) Mixed-
DAE: a single noise-independent DAE that is trained
to remove any kind of noise that appeared in the train-
ing data and (2) Combined-DAE: a committee of five
noise-specific DAEs (one for each noise type as shown
in Figure 15), followed by a noise-independent DAE
which is driven by the concatenation of the outputs of
the former five DAEs.
In order to quantify the amount of noise in the im-
age, we define Noise Fraction (NF) as a function of
the Pearson correlation coefficient (PCC), with NF =
1−PCC2. The values of NF are between 0 and 1, with
NF = 0 denoting a clean image.
510 510 
32 32 
32 340 
386 693 
# 
+ 
wh wv 
(a)  H-V-Inp (c)  H-V-Res (b)   H-V-wscr 
Σ 
R
C
N
 
UH UV 
forward 
backward 
R
ev
 
R
ev
 
Layer 1 U1T Y11T 
U1 UT … 
# 3 3 … 
Σ 
Scores 
Winner-
Take-All 
3 
5
50
MFCC MFCC MVN MVN MVN AFE AFE
Simple LIN LIN LIN LIN LIN LIN
1 1 1 2 3 3 3
4000 4000 4000 4000 8000 8000 8000
Uni Uni Uni Uni Uni Uni Bi
W
E
R
%
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
Σ 
R
C
N
 
UH UV 
R
C
N
 
R
C
N
 
Σ 
3 
Noisy 
image 
3 
Clean 
image 
DAE 1 
U
H
 
DAE 2 
U
H
 
DAE M 
U
H
 
…
 
DAE 
3 3 
forward 
backward 
R
ev
 
R
ev
 
Layer L YL1T 
Figure 15: Architecture of the combined DAE: the outputs of a com-
mittee of noise-specific DAEs (M different noise types) drive a noise-
independent DAE.
Figure 16(a) shows the mean NF on the validation
set as a function of the reservoir size and the noise type
obtained after denoising the image by means of a single-
layer Mixed-DAE using horizontal scanning.
• With a reservoir of size 4K, the mean NF is already
smaller than 0.2 for all noise types. The mean NF
is in all cases significantly smaller than the mean
NF of the raw noisy images (this mean ranged be-
tween 0.4 and 0.83 depending on the noise type).
• The noise reduction improves very gradually as the
reservoir size increases. There is no clear bend in
the curve for any of the noise types.
• Border noise, the most problematic noise type in
the previous experiments, is easy to remove almost
completely. This follows from the fact that it is
very easy to establish where it occurs and which
clean pixel value the DAE has to predict there.
Therefore, it is not surprising to find that the NF
after denoising of an image corrupted by border
noise is even lower than that of a clean image after
denoising (there, the DAE output depends on the
location of the digit in the image).
12
• Speckle noise is the only noise type for which the
NF is almost independent of the size of the DAE.
The effect of adding layers to the average NF of a
single-layer RCN with 32K reservoir is depicted in Fig-
ure 16(b). Adding a second layer clearly induces an
additional gain whilst further layers are not beneficial
anymore.
4
5 10 15 20 25
−2
−1
0
1
2
Horizontal scanning frames (winner is 1)
Y
5 10 15 20 25
−2
−1
0
1
2
Vertical scanning frames (winner is 2)
Y
5 10 15 20 25
−2
−1
0
1
2
Combined scanning frames (winner is 7)
Y
1 2 4 8 16 32
0.01
0.05
0.1
0.5
Reservoir size (×1000)
N
F
Clean (0) Gaussian (0.42) S&P (0.60) Speckle (0.44)
Block (0.42) Border (0.83) Avg. (0.48)
(a) Optimizing the reservoir size for a single layer DAE. The
figures between brackets in the legend represent the NF of the
original noisy images.
5
1 2 3 4
0.08
0.09
0.1
N layers
N
F
Gaussian
(0.42)
S&P
(0.60)
Speckle
(0.44)
Block
(0.42)
Border
(0.83)
Avg.
(0.48)
0
0.05
0.1
0.15
0.2
N
F
Combined-DAE
Mixed-DAE
Ideal-DAE
0 0.2 0.4
0
0.1
0.2
0.3
F
Normalized freqency (f )
Po
w
er
Sp
ec
tr
um
(a) Optimizing
0 0.2 0.4 0.6 0.8 1
10
20
30
40
Input scale (αU )
D
E
R
%
(b) Optimizing
(b) Optimizing the number of layers for a DAE with 32K neu-
rons per layer.
Figure 16: Optimizing the reservoir size and the number of layers for
an RCN-based DAE.
Without reporting the results in detail, we mention
that neither changing the scanning direction nor com-
bining two scanning directions in an H-V-res like sys-
tem leads to any significant improvement. Because the
aim of denoising is to find and remove the noise pat-
terns and the noise types encountered in this work are
direction-independent.
Based on the above findings, we also considered a
2-layer Mixed-DAE with 32K reservoirs in each layer
as the reference (1.8M trainable parameters) against
which we will compare the Combined-DAE. To make
Combined-DAE equally complex as the Mixed-DAE (in
terms of trainable parameters), the former is composed
of five 2-layer noise-specific DAEs with 6K reservoirs
per layer and a single-layer noise-independent DAE
with a 4K reservoir.
In a control experiment, we also consider an ideal sit-
uation by having pre-knowledge about the noise type
of the input image. Therefore, we feed the image
only to one particular DAE from the first stage of the
Combined-DAE that has been trained for the same noise
type. This so-called ideal DAE is denoted as Ideal-
DAE.
Figure 17 summarizes the results obtained with these
three DAEs and supports the following conclusions: (1)
For Gaussian and S&P noise types, the Combined-DAE
achieves a noise reduction that is nearly identical to that
of the Ideal-DAE, but on three other types, the Mixed-
DAE is better than the Combined-DAE. (2) On aver-
age, there is little difference between the simple Mixed-
DAE and the much more complex Combined-DAE. Fig-
5
1 2 3 4
0.08
0.09
0.1
N layers
N
F
Gaussian
(0.42)
S&P
(0.60)
Speckle
(0.44)
Block
(0.42)
Border
(0.83)
Avg.
(0.48)
0
0.05
0.1
0.15
0.2
N
F
Combined-DAE
Mixed-DAE
Ideal-DAE
0 0.2 0.4
0
0.1
0.2
0.3
F
Normalized freqency (f )
Po
w
er
Sp
ec
tr
um
(a) Optimizing
0 0.2 0.4 0.6 0.8 1
10
20
30
40
Input scale (αU )
D
E
R
%
(b) Optimizing
Figure 17: The noise fraction (NF) for the output of the mixed, the
combined and an ‘ideal’ DAE that has prior knowledge of the noise
type. The NF of the raw noisy images are mentioned between brack-
ets.
ure 18 shows the performance of Mixed-DAE on de-
noising some examples.
Figure 18: One clean and five noise corrupted samples of digit 9 (top)
and the corresponding outputs of the Mixed-DAE.
6.4. Recognition for denoised images
In order to evaluate the influence of the RCN-
based DAE on the recognition, we test the cascade of
the Mixed-DAE and the H-V-res system we formerly
trained on clean images. The results obtained with this
cascade are listed in Table 7. The table also includes
the performance of the adaptive multi-column stacked
sparse denoising auto-encoder (AMC-SSDA) reported
in [21] and the RBM-based denoiser reported in [12]. It
is clear that the Mixed-DAE introduces a dramatic gain
in noise robustness of the H-V-res system at the cost
of only a minor loss of accuracy in the case of clean
13
Table 7: The influence of adding an RCN-based DAE in front of the classifier on the performance of the RCN-based recognizer (as DER%) on the
noisy version of the MNIST dataset. The systems DBN-2010 and DBN-2013 refer to [12] and [21], respectively.
DAE RBM-based AMC-SSDA - Mixed-DAE Mixed-DAE
Classifier DBN-2010 DBN-2013 H-V-res H-V-res H-V-res (RT)
Clean 1.24 1.5 0.81 1.03 1.22
Gaussian - 1.47 32.1 1.33 1.57
S & P - 2.22 38.91 1.86 2.17
Speckle - 2.09 49.32 2.41 2.19
Block 19.09 5.18 21.85 4.95 3.94
Border 1.29 1.15 79.34 0.89 1.25
Average - 2.27 37.06 2.08 2.06
images. Furthermore, the H-V-res system with Mixed-
DAE outperforms the AMC-SSDA system in five of the
six conditions.
In theory, the just tested configuration is sub-optimal
because it implies a mismatch between training and test-
ing. Therefore, we also trained an H-V-res system on
denoised training images (called H-V-res (RT)). How-
ever, to our surprise, the figures in Table 7 show no sig-
nificant improvement over the sub-optimal system. Ap-
parently, there is no need to retrain the recognizer every
time the DAE is improved (e.g., by taking a new noise
type into account).
The results obtained for the H-V-res system embed-
ding a mixed DAE show that image denoising in combi-
nation with clean training is more effective than multi-
condition training, even though the latter is over op-
timistic because it is tested on noise types that were
present during training. This is a remarkable result since
a limited study in [12] involving border noise and block
noise came to the opposite conclusion for a system en-
compassing sparse DBNs. In that study, a clean trained
DBN, a multi-conditionally trained DBN, and a clean
trained DBN supplied with the denoised images led to
the DERs of 22.7%, 4.6% and 6.4%, respectively.
7. Conclusions, discussion and future work
The aim of this work was to investigate the poten-
tial of reservoir computing networks (RCNs) in the
context of image processing, with a particular focus
on handwritten digit recognition and image denoising.
Key properties of RCNs that were observed on other
task such as (noisy) speech recognition and that were
validated in this paper on the (noisy) MNIST image
dataset are: RNCs generalize well to unseen conditions
and they are easy to train, especially so in combina-
tion with our training procedure [25] which provides
near-optimal values for almost all hyperparamters with
very little effort. The temporal processing capabilities
of RCNs could also be readily used to coax the sys-
tem in recognizing digits strings instead of single dig-
its. The results obtained on the MNIST dataset do show
that a large enough RCN recognizer can surpass con-
ventional neural network-based recognizers in matched
conditions. Moreover, we established that an RCN can
be highly effective in denoising an image and that the
combination of a denoiser and a recognizer outperforms
a similar combination created by means of conventional
deep neural network technology.
However, results on MNIST should not be general-
ized to image processing in general. As can be seen
in Table 1, having intermediate representations of var-
ious complexity as obtained with deep convolutional
nets, only provides small gains on MNIST. On other im-
age recognition task such as CIFAR-10 [33], intermedi-
ate representations obtained/learned via 2-dimensional
(2D) convolution seem to be crucial in obtaining state-
of-the-art results [34]. Hence, we do expect that on
tasks such as CIFAR-10, RCNs will need good inter-
mediate representations based on some form of con-
volution as well in order to obtain competitive results.
A straightforward approach would be to plug in fea-
tures obtained with CNNs. However, this would re-
quire training a CNN before training the RCN, loos-
ing one of the key benefits of RCNs, namely the abil-
ity for quick experimentation without needing to fine-
tune a lot of hyperparameters. Alternative approaches
that use bottom-up generated convolutional features ob-
tained via Restricted Boltzmann Machines or k-means
clustering [35], or via other unsupervised learning ap-
proaches, are in this aspect more appealing. A ma-
jor, and as of yet unanswered question in this regard
is if supervised learning is a pre-requisite for obtain-
ing good intermediate representations. Yet another ap-
14
proach would be to make 2-dimensional convolution an
inherent property of the RCN. In its current form, RCNs
perform a form of 1-dimensional convolution by means
of their recurrent nodes. Extending this to 2 dimensions
would make RCNs more suitable for processing images,
and 3 dimensions could enable fast learning for video
processing. Combining bi-directional (horizontal and
vertical) scanning as done in this paper and in [36] does
provide some of the benefits of 2D convolution, but we
expect that such setup will prove to be sub-optimal for
larger image sizes. Extensions that better mimic the lo-
cal properties of 2D convolution are however non-trivial
and, to our best knowledge, no such extensions have
been proposed yet.
Nevertheless, considering the strong points and the
performance of RCN, we believe that, even in their
current form, RCNs are good candidates to be merged
to the conventional DNN-based image and video pro-
cessing systems. For the more challenging image and
video task, some feature engineering will be needed, but
we expect that their fast and robust training combined
with the easily pre-computed hyperparameters will still
makes RCNs good candidates for quick prototyping and
even for final systems.
Acknowledgment
The research activities described in this paper were
funded by Ghent University–iMinds, the Institute for
the Promotion of Innovation by Science and Technology
in Flanders (IWT), the Fund for Scientific Research-
Flanders (FWO-Flanders), and the European Union.
References
[1] K. Fukushima, Neocognitron: A self-organizing neural network
model for a mechanism of pattern recognition unaffected by
shift in position, Biological Cybernetics 36 (4) (1980) 193–202.
URL http://dx.doi.org/10.1007/BF00344251
[2] J. Weng, N. Ahuja, T. S. Huang, Cresceptron: a self-organizing
neural network which grows adaptively, in: Proc. IJCNN, 1992,
pp. 576–581.
[3] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based
learning applied to document recognition, Proc. IEEE 86 (11)
(1998) 2278–2324.
[4] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning in-
ternal representations by error propagation, Parallel Distributed
Processing: Explorations in the Microstructure of Cognition 1
(1986) 318–362.
[5] P. Simard, D. Steinkraus, J. C. Platt, Best practices for convolu-
tional neural networks applied to visual document analysis, in:
Proc. DAR, 2003, pp. 958–963.
[6] J. Schmidhuber, Deep learning in neural networks: An
overview, Neural Networks 61 (2015) 85 – 117. doi:http:
//dx.doi.org/10.1016/j.neunet.2014.09.003.
URL http://www.sciencedirect.com/science/
article/pii/S0893608014002135
[7] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,
R. Salakhutdinov, Improving neural networks by preventing co-
adaptation of feature detectors, CoRR.
URL http://arxiv.org/abs/1207.0580
[8] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C. Courville,
Y. Bengio, Maxout networks, in: Proc. ICML, 2013, pp. 1319–
1327.
[9] M. Arjovsky, A. Shah, Y. Bengio, Unitary evolution recurrent
neural networks, CoRR abs/1511.06464.
URL http://arxiv.org/abs/1511.06464
[10] N. Kalchbrenner, I. Danihelka, A. Graves, Grid long short-term
memory, CoRR abs/1507.01526.
URL http://arxiv.org/abs/1507.01526
[11] N. Srivastava, Improving neural networks with dropout, Mas-
ter’s thesis, U. Toronto (2013).
[12] Y. Tang, C. Eliasmith, Deep networks for robust visual recogni-
tion, in: Proc. ICML, 2010, pp. 1055–1062.
[13] R. Salakhutdinov, G. E. Hinton, Deep boltzmann machines, in:
AISTATS, 2009, pp. 448–455.
[14] J. Tang, C. Deng, G.-B. Huang, Extreme learning ma-
chine for multilayer perceptron, IEEE Trans. Neural Networks
and Learning Systems (2015) 1–1doi:10.1109/TNNLS.
2015.2424995.
[15] D. Yu, L. Deng, Deep convex net: A scalable architecture for
speech pattern classification., in: Proc. Interspeech, 2011, pp.
2285–2288.
[16] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella,
J. Schmidhuber, Flexible, high performance convolutional neu-
ral networks for image classification, in: Proc. IJCAI, 2011, pp.
1237–1242.
[17] M. Ranzato, C. S. Poultney, S. Chopra, Y. LeCun, Efficient
learning of sparse representations with an energy-based model.,
in: Proc. NIPS, MIT Press, 2006, pp. 1137–1144.
[18] D. C. Ciresan, U. Meier, J. Schmidhuber, Multi-column deep
neural networks for image classification, in: Proc. CVPR, 2012,
pp. 3642–3649.
[19] L. Zhang, D. Zhang, Visual understanding via multi-feature
shared learning with global consistency, IEEE Trans. Multi-
media 18 (2) (2016) 247–259. doi:10.1109/TMM.2015.
2510509.
[20] J. Xie, L. Xu, E. Chen, Image denoising and inpainting with
deep neural networks, in: Proc. NIPS, 2012, pp. 350–358.
[21] F. Agostinelli, M. Anderson, H. Lee, Adaptive multi-column
deep neural networks with application to robust image denois-
ing, Proc. NIPS 26 (2013) 1–9.
[22] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
L. Fei-Fei, Large-scale video classification with convolutional
neural networks, in: Proc. CVPR, 2014.
[23] H. Jaeger, The ‘‘echo state’’ approach to analysing and train-
ing recurrent neural networks - with an erratum note, Tech. rep.,
GMD Report 148, German National Research Center for Infor-
mation Technology (2001).
URL http://goo.gl/gJDEZJ
[24] A. Jalalvand, K. Demuynck, J.-P. Martens, Feature enhancement
with a reservoir-based denoising auto encoder, in: International
Symposium on Signal Processing and Information Technology,
Proceedings, Proc. ISSPIT, 2013, p. 6.
[25] A. Jalalvand, F. Triefenbach, K. Demuynck, J.-P. Martens, Ro-
bust continuous digit recognition using reservoir computing,
Computer Speech and Language 30 (1) (2015) 135 – 158.
[26] G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine:
theory and applications, Neurocomputing 70 (2006) 489–501.
[27] G. Huang, S. Song, J. N. D. Gupta, C. Wu, Semi-supervised
15
and unsupervised extreme learning machines, IEEE Trans. Cy-
bernetics 44 (12) (2014) 2405–2417. doi:10.1109/TCYB.
2014.2307349.
[28] L. Zhang, W. Zuo, D. Zhang, LSDT: latent sparse
domain transfer learning for visual adaptation, IEEE
Trans. Image Processing 25 (3) (2016) 1177–1191.
doi:10.1109/TIP.2016.2516952.
URL http://dx.doi.org/10.1109/TIP.2016.
2516952
[29] L. Zhang, D. Zhang, Domain adaptation extreme learning ma-
chines for drift compensation in e-nose systems, IEEE Trans.
Instrumentation and Measurement 64 (2015) 1790–1801.
[30] A. Jalalvand, W. De Neve, J.-P. Martens, R. Van de Walle, To-
wards using reservoir computing networks for noise-robust im-
age recognition, in: Proc. IJCNN, 2016, pp. 1666–1672.
[31] D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber,
Handwritten digit recognition with a committee of deep neural
nets on GPUs, Neural Networks Tricks of the Trade.
[32] G.-B. Huang, X. Ding, H. Zhou, Optimization method based ex-
treme learning machine for classification, Neural Computation
74 (13) (2010) 155–163.
[33] A. Krizhevsky, Learning multiple layers of features from tiny
images, Tech. rep., University of Toronto (2009).
[34] http://rodrigob.github.io/are we there yet/build/, accessed:
2016-09-15.
[35] A. Coates, H. Lee, A. Y. Ng, An analysis of single-layer net-
works in unsupervised feature learning, in: Proc. Int. Conf. on
Artificial Intelligence and Statistics (AISTATS), 2011, pp. 215–
223.
[36] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville,
Y. Bengio, ReNet: A recurrent neural network based alterna-
tive to convolutional networks, arXiv-CS.CV.
URL http://arxiv.org/abs/1505.00393
1
Azarakhsh Jalalvand obtained the M.Eng. de-
gree in Artificial Intelligence and Robotics from
Iran University of Science and Technology
(2009), and the PhD degree in Computer Engi-
neering from Ghent University, Ghent, Belgium
(2015). The focus of his PhD research was on
the noise-corrupted speech and image signal
processing using artificial neural networks. He
is currently a post-doctoral researcher in the
Data Science Lab, Ghent University. His prin-
cipal research interests are noise-robust signal
processing, machine learning and time-series data analysis.
Kris Demuynck received the master’s degree
in Electrical Engineering and the PhD degree
in Applied Sciences from the Katholieke Univer-
siteit Leuven, in 1993 and 2001, respectively.
Between 2001 and 2012 he worked as a Post-
doctoral Researcher at the Katholieke Univer-
siteit Leuven on various national and interna-
tional projects. In 2012 he joined the Data Sci-
ence Lab at Ghent University, where he works as
part-time professor and senior researcher. His
principal research interests are large vocabulary
continuous speech recognition (LVCSR), machine learning and search
algorithms.
Wesley De Neve received the M.Sc. degree
in Computer Science and the Ph.D. degree in
Computer Science Engineering from Ghent Uni-
versity, Ghent, Belgium, in 2002 and 2007, re-
spectively. He is currently working as a senior re-
searcher for both the Data Science Lab at Ghent
University - iMinds in Belgium and the Image
and Video Systems Lab at the Korea Advanced
Institute of Science and Technology (KAIST) in
South Korea. His current research interests are
natural language understanding, visual content
analysis and machine learning.
Jean-Pierre Martens graduated in 1974 as a
M.Sc. in Electrical Engineering and obtained his
PhD in 1982. Since 1974, he has been working
at University Gent, first as a researcher, then
as an associate professor, and since 1996 as
a full professor. He has carried out research in
psycho-acoustics, auditory model-based speech
analysis, speech recognition and music signal
analysis. He is a member of ASA, IEEE, INNS
and ISCA.
16
"
1,"In: Proc. of the 26th Int. Conference on Artificial Neural Networks (ICANN), Alghero, Italy, September 11-14, 2017
The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-68600-4_1
Semi-Supervised Phoneme Recognition with
Recurrent Ladder Networks
Marian Tietz, Tayfun Alpay, Johannes Twiefel, and Stefan Wermter
Knowledge Technology Institute, Department of Informatics, Universita¨t Hamburg,
Vogt-Ko¨lln-Str. 30, 22527 Hamburg, Germany
{tietz,alpay,twiefel,wermter}@informatik.uni-hamburg.de
http://www.informatik.uni-hamburg.de/WTM/
Abstract. Ladder networks are a notable new concept in the field of
semi-supervised learning by showing state-of-the-art results in image
recognition tasks while being compatible with many existing neural archi-
tectures. We present the recurrent ladder network, a novel modification of
the ladder network, for semi-supervised learning of recurrent neural net-
works which we evaluate with a phoneme recognition task on the TIMIT
corpus. Our results show that the model is able to consistently outper-
form the baseline and achieve fully-supervised baseline performance with
only 75% of all labels which demonstrates that the model is capable of
using unsupervised data as an effective regulariser.
Keywords: semi-supervised learning, recurrent neural networks, ladder
networks, phoneme recognition
1 Introduction
There is no doubt that the recent success of deep learning is tied to the rising
availability of labelled data. While tasks such as image or text classification have
greatly benefited from this availability, there are still a number of domains, e.g.
speech recognition, where the majority of the research community has no free
access to large amounts of labelled data. One promising approach towards this
problem is semi-supervised learning where models trained with labelled data can
be further improved by training with unlabelled data.
Recent methods, such as graph-supported training [10], sparse autoencoders
([4]; SSSAE) and especially the Ladder Network (LN) [11], a stacked Denoising
Autoencoder (DAE) with shortcut connections, show promising results for semi-
supervised training of feed-forward neural networks. The LN has been shown to
deliver state-of-the-art results in semi-supervised image classification while still
being compatible with many existing feed-forward neural networks [11].
However, this novel architecture has not yet been explored on more com-
plex sequential tasks, such as speech recognition, where Recurrent Neural Net-
work (RNN) architectures, like Gated Recurrent Units (GRU; [1]), are the cur-
rent state of the art. We therefore propose a novel Recurrent Ladder Network
ar
X
iv
:1
70
6.
02
12
4v
2 
 [c
s.C
L]
  1
8 S
ep
 20
17
2 Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks
(RLN) architecture and evaluate it on the TIMIT phoneme recognition bench-
mark [5]. We introduce a novel recurrent layer for the LN decoder in order to find
better-suited abstractions for semi-supervised learning and test two noise injec-
tion schemes tailored to support recurrent dynamics to increase the regularising
nature of the RLN. Our results show that after hyper-parameter optimization
the model is able to significantly outperform the baseline in all experiments
using unsupervised data as a regulariser and achieves fully-supervised baseline
performance while training only on 75% of the labelled data.
2 The Ladder Network Architecture
The basic idea of the LN architecture [11], depicted in Fig. 1, is to make au-
toencoders more expressive by adding shortcut connections from the encoder to
the decoder. Each decoder layer is then able to combine the preactivation of the
encoder layer with the reconstruction of the previous decoder layer by means of
a combinator function g(·, ·). Therefore, the encoder does not have to carry all
reconstruction information since the shortcuts can compensate for it. Since the
shortcuts allow perfect reconstruction by simply copying the encoder input to
the decoder output, Gaussian noise N (0, σ2) is added to prevent the direct usage
of these short-circuits and enforce learning in the intermediate layers, i.e. we use
a denoising autoencoder. To ensure that the noise can be removed, the decoder’s
(noisy) reconstruction zˆ(l) is compared to the encoder’s (clean) preactivation z(l)
and added to the unsupervised objective function:
CDAE =
n∑
l
λlC
(l)
d with C
(l)
d = ‖ z(l) − zˆ(l)‖2 , (1)
where n is the total amount of layers, z(l) is the preactivation vector of the l-th
encoder layer without noise and zˆ(l) the l-th decoder layer reconstruction from
noisy input. The hyper-parameter λi controls the targeted similarity between
the encoder and decoder layers and prevents short-circuits by punishing direct
copies of the noisy data by weighting the difference between the layers. For semi-
supervised learning the encoder path is also used for the supervised task, i.e. its
output is evaluated with a supervised objective function Csup and combined with
the unsupervised objective function CDAE: Csemsup = Csup +CDAE. When using
the encoder in a supervised task the shortcuts help with reconstruction as the
needed information may also be retrieved over the shortcuts [11].
The combinator function g(·, ·) models p(z(l) | z(l+1)) and is responsible for
creating the reconstruction of the l-th layer zˆ(l) with the help of the recon-
struction of the previous layer zˆ(l+1) and the shortcut value of the l-th layer
z˜(l), i.e., zˆ(l) = g(z˜(l), zˆ(l+1)). The function may attempt to remove the noise
from z˜(l) with the help of the previous reconstruction, infer the inverse mapping
zˆ(l+1) → zˆ(l) or do a combination of both.
Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks 3
g
g
g
xˆ
f
f
y˜
x
N (0, σ2)
N (0, σ2)
N (0, σ2)
f
f
y
x
C
(2)
d
C
(1)
d
C
(0)
d
z˜(0)
z˜(1)
z˜(2) zˆ(2)
zˆ(1)
zˆ(0) z(0)
z(1)
z(2)
Fig. 1. Illustration of the non-recurrent LN architecture with one hidden and one
output layer. The encoder and decoder paths are highlighted in green and yellow,
respectively.
3 Recurrent Ladder Networks
In this section, we will elaborate our modelling choices for the RLN. In order
to extend the original LN to support recurrence in the encoder, both the noise
injection scheme and the decoder have to be adapted since recurrent layers use
additional context layers. Overall, we are proposing two noise injection methods
and two decoder variants (see Fig. 2). Our supervised baseline model will be the
encoder of the RLN since it encodes the task closely to the full RLN but has
no means of using unsupervised data. The resulting six model combinations are
No-Decoder with Feed-Forward Noise (ND-FFN), No-Decoder with Recurrent
Noise (ND-RN), Recurrent Decoder with Feed-Forward Noise (RD-FFN) and
Recurrent Noise (RD-RN) as well as a Feed-Forward Decoder with Feed-Forward
Noise (FFD-FFN) and Recurrent Noise (FFD-RN).
3.1 Noise Injection
In the feed-forward case, noise is applied directly to the preactivations so that
the output of the layer and the shortcut are affected, i.e. z˜ = W x˜ + n with
n ∼ N (0, σ2). This would, however, introduce noise into the context memory of
recurrent layers even after receiving the noisy output from the previous layer,
effectively amplifying the noise even further. Therefore, we apply noise only to
the preactivation and the shortcut without direct perturbation of the context
memory. A hidden layer ht and its noisy counterpart h˜t are therefore updated
as follows:
ht = f(zt) = f(W x˜t + Uht−1), (2)
h˜t = f(z˜t) = f(zt + n), (3)
4 Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks
where f(·) is the activation function, xt the input, W the input weight matrix,
and U the hidden-to-hidden weights, updated at each time step t.
This noise injection method will be referred to as recurrent noise from here
on. Another method of noise injection that we tested, referred to as feed-forward
noise, is to not inject additional noise at the recurrent layer, i.e. feed-forward
layers will be injected with noise but recurrent layers will not.
3.2 Recurrent Decoder
The decoder path in an autoencoder models the inverse information flow of the
encoder path. We propose two modelling options for the decoder path in an
RLN. The first (Fig. 2c) is a recurrent layer with g(·, ·) as activation function:
u
(l)
t = V zˆ
(l+1)
t +Ozˆ
(l)
t−1, (4)
zˆ
(l)
t = g(z˜
(l)
t ,u
(l)
t ), (5)
where V are the input weights, O the hidden-to-hidden weights, u
(l)
t the pre-
activation of the recurrent decoder and z˜
(l)
t the noisy preactivation of the l-th
encoder layer at time-step t from the shortcut. The second modelling option is
to simply use a feed-forward network (Fig. 2d) in the decoder [11].
Batch normalisation is heavily used in the LN both for normalisation of
the layer-wise reconstruction cost and for normalisation of layer activations. It
was considered problematic with recurrent networks until the introduction of
recurrent batch normalisation [2]. Since it potentially requires tuning of another
hyper-parameter we decided to model the RLN without batch normalisation
with the exception of the layer-wise reconstruction cost function C
(l)
d which is
computed exactly as described by Rasmus et al. [11].
4 Experiments
We evaluate the RLN on the TIMIT phoneme recognition benchmark [5], a
widely used test corpus which allows comparing our architecture to previous
approaches. The audio samples of the corpus are reduced in dimensionality by
using libROSA1 to compute 13 Mel Frequency Cepstral Components (MFCC) [3]
and their first and second derivative with 20ms frames and 10ms frame skip,
similar to related work [4]. The 39-dimensional feature vectors are normalised
to have zero mean and unit variance. We grouped easily confused phonemes of
the English phoneme alphabet as described by Halberstadt [8] resulting in 39
phoneme classes to predict.
We use Connectionist Temporal Classification (CTC) [6] for the supervised
cost Csup to solve the problem of label alignment. Phoneme Error Rate (PER)
is used for evaluation and computed using the Levenshtein distance of all label
sequences to the predictions, normalised to the total length of all label sequences.
1 https://librosa.github.io
Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks 5
+
f
·
h˜
(l)
t
h˜
(l)
t−1
h˜
(l+1)
t
z˜
(l)
t
(a) FFN
+
f N
f
·
h˜
(l)
t
h˜
(l)
t−1
h˜
(l+1)
t
z˜
(l)
t
(b) RN
+
g
·
zˆ
(l)
t−1
zˆ
(l)
t
zˆ
(l+1)
t
z˜
(l)
t
(c) RD
g
·
zˆ
(l)
t
zˆ
(l+1)
t
z˜
(l)
t
(d) FFD
Fig. 2. Overview of (a) feed-forward noise (FFN) and (b) recurrent noise (RN) in-
jection schemes for the encoders (green) introduced in subsection 3.1 as well as (c)
recurrent decoder (RD) and (d) feed-forward decoder (FFD) layouts (yellow) intro-
duced in subsection 3.2. Combining all encoder and decoder layouts gives a total of six
model variants including the two no-decoder (ND) baselines ND-RN and ND-FFN.
The predictions are obtained by using best path decoding [6], i.e. choosing the
phoneme class with the highest probability at each time step.
To build the supervised and unsupervised training sets we keep all input data
for unsupervised training and reduce the supervised set by drawing samples from
the full dataset until the least represented phonemes are drawn a minimum num-
ber of times to prevent under-representing a class while keeping the distribution
intact. We cycle the supervised dataset to match sizes with the unsupervised
set, similar to the implementation by Rasmus et al. [11].
4.1 Training Procedure
All networks have been trained using Adam [9] with a learning rate of 0.002 for
at least 100 epochs until the validation error stopped improving. The models
are four-layer networks consisting of one GRU layer with 192 units with tanh(·)
activation and one feed-forward output layer with softmax activation, as well as
the inverse layers in the decoder. The noisy softmax output is used to classify
phonemes during training for additional regularisation. Since the performance
of the encoder is likely to correlate with RLN performance, hyper-parameters,
including layer sizes and learning rate, were determined empirically by a grid
search using the encoder described in section 3, i.e. an RLN with λi = 0, which
also serves as the baseline. DAE cost weights (λ0, λ1, λ2) = (1000, 10, 0.1) and
the MLP combinator g(·, ·) were both adopted from Rasmus et al. [11].
We test the semi-supervised learning capabilities of the six RLN variants
from section 3, by varying the labels for the supervised part of the architecture
in steps of 25% (940), 50% (1856), 75% (2754), and 100% (3696) of labelled
6 Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks
sequences while the unsupervised part of the model always receives all available
unlabelled data.
5 Results & Discussion
An overview of our results can be seen in Fig. 3 where the different modelling
choices are directly compared against each other. The overall best results after
hyper-parameter optimization for each supervised data split, as well as results
of other approaches, are shown in Table 1.
0.30
0.35
0.40
0.45
0.50
PE
R
No Dec. | Feed-Forw. Noise (ND-FFN) Rec. Dec. | Feed-Forw. Noise (RD-FFN) Feed-Forw. Dec. | Feed-Forw. Noise (FFD-FFN)
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
noise σ
0.30
0.35
0.40
0.45
0.50
PE
R
No Dec. | Rec. Noise (ND-RN)
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
noise σ
Rec. Dec. | Rec. Noise (RD-RN)
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
noise σ
Feed-Forw. Dec. | Rec. Noise (FFD-RN)
Supervised label ratio 100% 75% 50% 25%
Fig. 3. Comparison of PER achieved by the RLN variants with varying amount of
labelled data (25%-100%) and noise standard deviation σ. Each data point represents
the mean, the whiskers cover a 95% confidence interval. Higher σ are needed for fewer
labels to prevent overfitting.
As can be seen in Table 1, the RLN consistently outperforms the baseline
configuration, even in fully-supervised training and is able to achieve the same
performance as the baseline with 25% less labelled data which shows that the
RLN complements the encoder well and demonstrates the compatibility of the
LN with existing models. On average, the RD models perform better than the
FFD models for most σ, more so with fewer labels, suggesting that the recurrent
decoder is better at filtering noise. This also explains why the RD models work
better with higher σ compared to FFD.
The noise injection method and the chosen σ greatly impact the overall
performance. The performance curves are roughly concave and shift towards
stronger noise with less available labels because the network overfits easily with
fewer labels which is prevented by the higher noise. Performance degrades for
higher σ because the network needs to be trained significantly longer to remove
the noise which the chosen training parameters do not allow.
Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks 7
Recurrent noise injection was expected to achieve better regularisation due to
the additional noise at the recurrent layer but does not. By observing the encoder
layers we found that their outputs often differed significantly which causes un-
recoverable perturbations in the recurrent layers when applying equally strong
noise to all layers instead of noise relative to each layer’s output. Employing
batch normalisation might solve this, as hypothesised in related work [12]: nor-
malising the preactivation of each layer to unit variance before adding noise
makes the change in variance relative to the preactivation, therefore coupling
noise and layer activation strength with the benefit of reducing the search space
for σ significantly. We predict that this will lead to an increase in performance
when using fewer labels.
Even though our best results for the RLN are slightly lower ranked when
compared with related approaches, our model has significantly fewer parameters
(e.g. differing by a factor of 160 when compared to SSSAE [4]). We therefore
hypothesise that an increase of parameters and more complex layer architec-
tures will result in even better performance. This is indicated by our best RLN
achieving similar results (31.66% PER, 175k parameters) as the Bi-directional
Long Short-Term Memory (BLSTM) ([6]; 31.25% PER, 114k parameters) while
using only half of the labels.
Table 1. Best results in phoneme error rate (PER), achieved by the proposed RLN
modelling options: No decoder (ND, baseline), recurrent decoder (RD), feedforward
decoder (FFD), feedforward noise (FFN), and recurrent noise (RN). †: linear inter-
polation between 10% and 30% labels. ††: Graves et al. [7] have shown significantly
improved results with more parameters (17.7% PER, 4.3m param.).
labels ND-FFN RD-FFN FFD-FFN ND-RN RD-RN FFD-RN SSSAE [4] BLSTM [6]
25% 40.65 36.40 37.13 39.90 38.82 36.41 31.0† -
50% 34.22 31.66 32.06 34.07 33.07 33.39 - -
75% 30.96 29.16 30.31 31.17 30.84 30.42 - -
100% 29.11 28.02 28.08 29.26 29.67 29.26 - 31.25††
param 0.134m 0.177m 0.159m 0.134m 0.177m 0.159m 28.7m 0.114m
6 Conclusion
We have shown that the recurrent ladder network is able to perform as good as
similarly parametrised BLSTM models while using only 50% of the labelled data,
demonstrating the RLN’s ability to effectively regularise itself using unsupervised
training data. Current state-of-the-art methods performed better overall but this
does not come as a surprise given that these models use up to 160 times more
parameters. We argue that this gap could potentially be closed by scaling up
our models, as demonstrated for BLSTM models by Graves et al. [7].
8 Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks
The proposed recurrent decoder proved to be better at denoising than the
feed-forward decoder. Additionally, we found that recurrent noise injection does
not perform as expected and we hypothesise that it needs the help of normali-
sation (e.g. batch normalisation) to work efficiently.
In the future, we would also like to take advantage of the semi-supervised
learning abilities of the RLN in conjunction with more complex recurrent mod-
els such as bidirectional and attention-based RNNs to utilise unlabelled data
even more effectively and explore how the learning framework scales with more
complex temporal dynamics in more challenging tasks such as end-to-end speech
recognition or question answering.
Acknowledgments. The authors gratefully acknowledge partial support from
the German Research Foundation DFG under project CML (TRR 169), the
European Union under project SECURE (No 642667), and the Hamburg Lan-
desforschungsfo¨rderungsprojekt CROSS.
References
1. Cho, K., Van Merrie¨nboer, B., Gu¨lc¸ehre, C¸., Bahdanau, D., Bougares, F., Schwenk,
H., Bengio, Y.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)
2. Cooijmans, T., Ballas, N., Laurent, C., Gu¨lc¸ehre, C¸., Courville, A.: Recurrent batch
normalization. arXiv preprint arXiv:1603.09025 (2016)
3. Davis, S., Mermelstein, P.: Comparison of parametric representations for mono-
syllabic word recognition in continuously spoken sentences. IEEE Transactions on
Acoustics, Speech, and Signal Processing 28(4), 357–366 (1980)
4. Dhaka, A. K., Salvi, G.: Semi-supervised learning with sparse autoencoders in phone
classification. arXiv preprint arXiv:1610.00520 (2016)
5. Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., Pallett, D. S.: DARPA
TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-
1.1. NASA STI/Recon Technical Report N 93 (1993)
6. Graves, A., Ferna´ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks.
In: Proceedings of ICML-2006, pp. 369–376 (2006)
7. Graves, A., Mohamed, A., Hinton, G. E.: Speech recognition with deep recurrent
neural networks. In: Proceedings of ICASSP-2013, pp. 6645–6649 (2013)
8. Halberstadt, A. K.: Heterogeneous acoustic measurements and multiple classifiers
for speech recognition. Ph.D. thesis, Massachusetts Institute of Technology (1998)
9. Kingma, D. P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
10. Liu, Y., Kirchhoff, K.: Graph-based semi-supervised learning for phone and seg-
ment classification. In: Proceedings of INTERSPEECH-2013, pp. 1840–1843 (2013)
11. Rasmus, A., Berglund, M., Honkala, M., Valpola, H., Raiko, T.: Semi-supervised
learning with ladder networks. In: Proceedings of NIPS-2015, pp. 3532–3540 (2015)
12. Zhang, Y., Lee, K., Lee, H.: Augmenting supervised neural networks with unsuper-
vised objectives for large-scale image classification. In: Proceedings of ICML-2016,
pp. 612–621 (2016)
"
2,"Lightweight Classification of IoT Malware Basedon Image Recognition1st Jiawei SuKyushu UniversityJapanjiawei.su@inf.kyushu-u.ac.jp2nd Danilo Vasconcellos VargasKyushu UniversityJapanvargas@inf.kyushu-u.ac.jp3rd Sanjiva PrasadIndian Institute of Technology DelhiIndiaSanjiva.Prasad@cse.iitd.ac.in4th Daniele SgandurraRoyal Holloway University of LondonUKdaniele.sgandurra@rhul.ac.uk5th Yaokai FengKyushu UniversityJapanfengyk@ait.kyushu-u.ac.jp6th Kouichi SakuraiKyushu UniversityJapansakurai@csce.kyushu-u.ac.jpAbstract—The Internet of Things (IoT) is an extension of thetraditional Internet, which allows a very large number of smartdevices, such as home appliances, network cameras, sensors andcontrollers to connect to one another to share information andimprove user experiences. IoT devices are micro-computers fordomain-specific computations rather than traditional function-specific embedded devices. This opens the possibility of seeingmany kinds of existing attacks, traditionally targeted at theInternet, also directed at IoT devices. As shown by recentevents, such as the Mirai and Brickerbot botnets, DDoS attackshave become very common in IoT environments as these lackbasic security monitoring and protection mechanisms. In thispaper, we propose a novel light-weight approach for detectingDDos malware in IoT environments. We extract the malwareimages (i.e., a one-channel gray-scale image converted from amalware binary) and utilize a light-weight convolutional neuralnetwork for classifying their families. The experimental resultsshow that the proposed system can achieve 94.0% accuracy forthe classification of goodware and DDoS malware, and 81.8%accuracy for the classification of goodware and two main malwarefamilies.Index Terms—IoT cyber-security, Malware image classifica-tion, Convolutional Neural NetworkI. INTRODUCTIONNowadays the notion of the “Internet” has extended fromthe connection between personal computers to networks toa much larger range of devices. Traditional micro devices,such as many kinds of sensors and controllers, are typicallyonly able to perform domain-specific tasks based on pre-defined rules. By substituting these function-specific deviceswith CPU-controlled ones and connection-enabled micro-computers, these “things” become smarter due to the strongercomputational capability and the information sharing throughthe interconnection among them via the Internet. Therefore,these things can deal with much more complicated tasks thanbefore and by enabling Cloud services users can easily receivedata reported by the things and control them.Despite these advantages, becoming smarter means alsobecoming more vulnerable, with more chances for potentialadversaries to threaten these things. Yet general IoT systemsare still far from being properly secured due to the difficultyof creating unified standards for the various types of IoThardware and software platforms. In addition, even if smartercompared with current personal computers, IoT devices stilllack of sufficient computational resources to be able to use ex-isting PC security solutions. However, Cloud services providea way for developing security protection for IoT devices, e.g.,for malware detection [18], [19].In this paper, we consider a solution to protect the local IoTdevices from being abused for DDoS attacks based on botnetsof IoT devices, which is currently a common attack methodagainst IoT networks. To accomplish this, we first classifythe IoT DDoS malware samples recently collected in thewild on two major families, namely Mirai and Linux.Gafgyt.We then propose a lightweight solution for detecting andclassifying IoT DDoS malware and benign application locallyon the IoT devices by converting the program binaries togray-scale images, and by feeding these images to a smallsize convolutional neural network for detecting malware. Inthis way, resource-constrained IoT devices can afford thecomputation needed for running the proposed detection systemlocally. Experimental results show that the proposed systemcan achieve 94.0% accuracy for classifying goodware andDDoS malware, and 81.8% accuracy for the classification ofgoodware and two main malware families.The main contributions of this research are the followingones:• this is the first classification system tested on real IoTmalware samples - previous works have used regular ormobile malware samples instead, due to the difficulty inobtaining IoT malware samples [2], [11], [13]. Specifi-cally, there is currently no publicly available IoT malwaredataset and the first IoT honeypot for collecting samplesof IoT threats was released relatively recently [1];• the proposed IoT malware classification system can bedeployed on real IoT devices. We show in detail the feasi-bility of using lightweight image classifier for recognizingIoT malware through malware images. Malware imageclassification has been proposed for classifying regularmalware [4]; however, IoT malware is functionally differ-ent. For example, many IoT malware may try to kill othermalware to guarantee enough computational resource forthemselves;• according to the experimental results, we prove that theproposed system can reliably classify goodware and IoTDDoS malware;• to the best of our knowledge, there is currently noreference to describe the time complexity of convolu-tional neural networks (CNNs). However, the proposedCNN-based approach is empirically considered to belightweight since it does not need to maintain any trainingdata for classification, which is unlike several other typesof common classifiers for malware classification suchas Support Vector Machine and K nearest neighbours.The computation of CNN for classification is rathersimple which only involves summation and activation.In addition, the proposed system is based on a twolayer shallow network which is much more efficient thancommon deep learning models.The paper is structured as follows. In Sect. II we discussrelated work. Sect. III explains procedures for extractingIoT DDoS malware images and implementing a small sizeconvolutional neural network for classification. In Sect. IVthe detection results in two different scenarios are listed andin Sect. 5 the limitation of the proposed method is discussed.In Sect. VI the achievement of this research is summarizedand future work is discussed.II. RELATED WORKSEven if IoT security is an important topic, few defensivesolutions exist in the literature [12]. Only recently, the firsthoneypot specifically for collecting IoT malware has beenestablished by Pa et al. [1]. Their honeypot systems simulated8 different CPU architectures and are built for observingattacks coming through the Telnet protocol. Initially theycollected 43 distinct malware samples which are mostly DDoSattack malware. Their results show that the DDoS attackis the most common security threat in current IoT networkenvironments. These authors kindly shared their observed dataset with us which we have used in this research for evaluatingour proposal.To the best of our knowledge, while most other worksfocus on Android malware detection [24], [25], the “Cloudeye”[2] is in practice the only current work specific for IoTmalware detection. The system is a signature matching-basedmalware detection solution. IoT clients are only responsiblefor preliminary scanning the software locally, and then sendinghashed abstracts of suspicious files to Cloud servers for deepanalysis, therefore guaranteeing data privacy and low-costcommunications. However, in IoT environments the inherentweakness of signature matching-based detection still exists:for example, the proposed system is not able to deal with newvariants of existing samples.Apart from signature matching, machine learning-basedmalware detection has been proved as effective in variousscenarios [3], [14]–[16], [22], [23]. In IoT environments,machine learning methods are expected to be suitable toobecause of the availability of Cloud services. In fact, in apossible scenario, the training can be performed on Cloudserver, while resource-constrained IoT devices can receivethe trained classifiers from the servers and run the algorithmlocally. In fact, several machine learning classifiers are heavyat training but efficient during test phase.Classifying malware images has been proved as an effectiveway for recognizing common PC malware [9], [26]. It is essen-tially a method for comparing two malware binaries. Nataraj etal. first utilize malware images for classifying regular Internetmalware with k-nearest neighbors [4]. However, the systemrequires pre-processing of filtering to extract the image textureas features for classification, which might not fit the resource-constrained IoT environments. Similaly, the artificial neuralnetwork (ANN) malware classification proposed by Makandar[28] might also be hard for IoT devices to handle since theheavy computational cost of multiple fully connected layersin ANN for classification. Yue utilized convolutional neuralnetwork for malware family classification [5]. In this research,we use malware images for IoT malware classification andshow it is a feasible approach.III. METHODOLOGYIn this Section, we describe the methodology of feedingmalware images as features to a small two-layer convolutionalneural network for detection.A. Lightweight IoT DDoS Malware FilterFor the scenario of detecting IoT DDoS malware detectionlocally, as previously pointed out, the main difficulty ofdeploying malware filters lies in the fact that the computationalresources available on IoT devices is limited. A direct solutionunder such a condition is relying on the security protectionservices provided by powerful remote servers such as IoTCloud servers. These servers are usually well guarded suchthat a central node failure (e.g., taken down by attacks)rarely occurs. Another advantage is that the threat databasesmaintained on these servers are much more comprehensiveand can be updated more rapidly than on IoT devices. Forthese reasons, in our proposed system, firstly a lightweightmalware classification system can be responsible for recog-nizing suspicious programs and behaviors locally. Note that atthis stage, the main goal is to provide a score whether a filemight be suspicious or not, i.e. it needs further analysis. Insuch a case, the system delivers the files or the correspondingabstracts to a remote Cloud server for deeper analysis. TheCloud side can update and distribute new trained detectors tothe clients periodically. In the following, we discuss the localmalware filter on the client side. We assume that a set of Cloudservers are able to analyze malware samples and retrain theclassifiers using standard machine learning algorithms. Theproposal system structure is shown in Fig.1.Fig. 1. The light-weight malware detection scheme proposed.B. IoT DDoS Malware FamiliesAccording to the recent observation and preliminary anal-ysis [1], the IoT DDoS malware are functionally similarto existing DDoS malware on PC platforms. However, IoTDDoS malware also contain some new features that are rarelyobserved before. For example, some samples try to kill othersamples of competitive families to get more system resourcesfor themselves, due to the limited computational capability ofIoT devices. In addition, IoT malware often targets a specificclass of devices, such as Internet cameras, DVR and so on.Finally, IoT malware can be also compatible with differentprocessor architectures, ensuring the maximum possible suc-cessful infections.C. Malware Image ClassificationAn interesting and novel way of conducting malware classi-fication is to analyze malware images. In particular, a malwarebinary can be reformatted as an 8-bit sequence and then beconverted to a gray-scale image which has one channel andpixel values from 0 to 255 [4]. The resulting image can thenbe fed into machine learning image classifiers for classifica-tion. Many machine learning classifiers are essentially muchmore efficient than signature-matching systems, which is themost common used malware detection method. In a matchingsignatures system, the signature database is typically largein size as it has to contain information for each malwaresample and all of its possible variants. In the case of machinelearning, little information has to be kept for classification.For example, k-means clustering needs only the informationof centroids and radii for classification once trained. Supportvector machine merely keeps a small set of training data (i.e.,the support vectors) in the test phase. In addition, machinelearning methods overcome signature matching on detectingzero-day attacks. Even if machine learning based methods canhave higher false-positives, however, in our case of buildingpreliminary malware filter, the false-positives can be less ex-pensive than false-negatives since the latter will make the IoTdevice exposure to maliciousness directly. Converting malwarebinaries to the corresponding images only requires to obtainingthe input vectors of the convolutional neural network, i.e., 8-bitvectors. Such convention is straight-forwards that requires onlyre-organize the binaries but no further pre-processing (i.e., thereal image is even not necessary but only the correspondingvector that represents the image is needed as input).D. Neural Network for Malware DetectionConvolutional neural networks have been proved to havebetter performance for image recognition than many otherkinds of classifiers. A convolutional neural network has twoimportant characteristics that make it fit the scenario of pre-liminary filtering malware on local IoT devices:• automatic feature extraction: many previous workshave focused on extracting effective features for malwaredetection. However, most of them are only effective underspecific scenarios, and this might lead to poor scalability.Neural network can automatically extract higher levelfeatures from the input raw features. That is, the networkcan learn deep non-linear features that can be hardlydiscovered and understood by human-beings. These aresometimes actually counter-intuitive, but indeed effective.• test phase efficiency: the training progress of a con-volutional neural network requires heavy computationand, for instance, high-end graphic cards are necessaryfor accelerating training large networks. However, oncetrained, the network itself is rather lightweight and canbe run with tiny computational resources, since onlythe trained parameters and information of network struc-ture are kept [29], [30]. In contrast, another supervisedlightweight classifier, the one-class support vector ma-chine (OCSVM), though simpler than normal two-classSVM, still needs to keep a certain amount of training datawhen running the classification, while a convolutionalneural network does not need to keep any. In practice,the training can be handled by the Cloud servers andonly the trained network is sent to IoT nodes. On thelocal IoT side, the convolutional neural network can berun for detecting malware images.IV. EXPERIMENT AND RESULTSA. Preparing the DatasetIn this Section we evaluate the efficacy and efficiency ofthe proposed method on an IoT malware dataset collectedby IoTPOT [1], the first honeypot for collecting IoT threatsamples. The malware samples are labelled using VirusTotal[8] with the majority rule. The dataset originally contains 500malware samples, where most of them are classified into fourbig families: Linux.Gafgyt.1, Linux.Gafgyt (other variants ofLinux.Gafgyt family except Linux.Gafgyt.1), Mirai [10] andTrojan.Linux.Fgt. The rest of the samples belong to relativelyrare families such as Tsunami, Hajime, LightAidra. Then were-organize the samples into two big categories: Mirai family,which contains Mirai and Trojan.Linux.Fgt, and Linux.Gafgytfamily which contains Linux.Gafgyt.1 and other variants. Inparticular, Mirai has been shown to have similar features toTrojan.Linux.Fgt [17].On the other side, the benign binary samples are collectedfrom Ubuntu 16.04.3 system files. The number of samples arebalanced for each family by randomly removing the samplesthat belong to classes that are too big. After the preprocessingphase, we analyzed 365 samples where each class has the sameFig. 2. Images of GoodwareFig. 3. Malware Image Examples of the Linux.Gafgyt Family. Note that theseare the raw malware images (the same below) whose size will be unified forputting in CNN.number of samples. Among them, we utilize 45 samples (eachclass has 15 samples) for testing, and the rest for training.According to the discussion above, the system proposed isonly responsible for preliminary detection. That is, the goal isto identify whether a sample is benign or belongs to one ofthe big malware families: Mirai and Linux.Gafgyt, but thereis no need to understand exactly which kind of variant it is.B. Obtaining the Malware ImagesOnce the raw data-set is ready, we convert each sample tothe corresponding malware gray-scale image by following thesame procedures implemented in [4]. In particular, a malwarebinary can be reformatted to a sequence whose elements are8-bit strings. Then each string can be converted to a decimalnumber which can be seen as the value of a one-channel pixel,which is in the range between 0 and 255. Therefore the entiresequence represents a gray-scale image. We rescale the imagesto the size of 64x64 such that they can be used as input toa convolutional neural network. Some examples of malwareand benign-ware images are shown by Fig. 2, 3 and 4. Bycomparison, the structural difference between malware andgoodware images can be identified. For example, it can beseen that malware images always are more dense. In particular,the majority of the Mirai malware images have a dense centralcode payload. On the other hand, the image of goodwares tendto have larger header parts than malwares.Fig. 4. Malware Image examples of the Mirai Familyconvolution layer(kernel=3, stride=1, depth=32)max pooling layer(kernel=2, stride=2)convolution layer(kernel=3, stride = 1, depth=72)max pooling layer(kernel=2, stride=2)fully connected layer(size=256)softmax classifierTABLE ISTRUCTURE OF THE IMPLEMENTED CNN.C. Convolutional Neural Network ConfigurationTo be lightweight, we have implemented a small, two layershallow convolutional neural network, compared with commonimage recognition models, such as ImageNet [20] and VGG[7]. The network structure is shown in Table. I. The networkis trained with 5000 iterations with a training batch size of 32and learning rate 0.0001.D. ResultsThe classification results are shown in Tables III andII for the cases of two (benign and malicious) and three-class (benign and two malware families: Mirai and gafgyt)classification. We have performed a 5-fold hold-out validation,namely the experiments were conducted five times which eachtime with a completely different training/test data combination(i.e., there are no shared test samples between any two of fivetest data sets).According to the results of two-class classification, we findthe proposed system can predict the existence of maliciousnesswith about 94.0% accuracy on the average. The accuracy ofthree-class classification is relatively lower. Specifically, thereare 6.67% malicious samples are misclassified as benign whichXXXXXXXTruePredict Benign Gafgyt MiraiBenign 94.67% 2.67% 2.67%Gafgyt 6.67% 72.00% 21.33%Mirai 0% 21.33% 78.67%TABLE IICONFUSION MATRIX FOR 3-CLASS CLASSIFICATIONXXXXXXXTruePredict Benign MaliciousBenign 94.67% 5.33%Malicious 6.67% 93.33%TABLE IIICONFUSION MATRIX FOR 2-CLASS CLASSIFICATIONall belong to Gafgyt family while there is no misclassificationof Mirai family to benign. This indicates the Gafgyt hasmore similar binaries to benign goodware. On the other side,the probability of misclassifying a Mirai sample to Gafgytclass is the same as probability of misclassifying the latterto the former. Generally, the difference between benign andmalicious samples is more recognizable than the differencebetween two malware families. Comparing with misclassi-fication between benign and malicious samples (i.e., two-class classification), the system is more likely to misclassifythe samples of two malware families in the case of three-class classification. This indicates the similarity between thesetwo families. Specifically, samples of two families might beobfuscated in similar ways, or/and share a part of the maliciousfunctions. In fact, the basic botnet functions of different DDoSmalware are similar, and mainly include receiving instructionsfrom the control server and spreading the infection. Similar tothe local malware filter proposed in this paper, the IoT malwareitself also has to be lightweight such that their functionshave to be relatively direct and simple since there is littlespace to add more complex functions according to the limitedcomputational resources.Our accuracy results compete with similar previous works[3], [5]. In specific, Yue [5] also utilized convolutional neuralnetworks and malware images for classifying several PCmalware families. However the results are carried out by usingmuch bigger and complex network structures (i.e. Very deepnetworks (VGG) which contain more than 10 layers while oursonly has two layers). Similarly, a very complex preprocessprocedure is needed in [5] which involves initial featureselection and random projection while our proposal directlyuses raw features for classification. According to the accuracyresults, the proposed system can be utilized as a regularmalware detector, or a first-layer malware classifier. That is,it can conduct a precise classification to identify benign andmaliciousness but may misclassify the exact identity of aspecific malware sample, which needs the aid of Cloud toconduct precise classification. A comparison of correspondingexperimental accuracy and settings is shown by Table.4.V. LIMITATIONDespite the advantages of being fast and lightweight, theproposed detection method is vulnerable to complex codeobfuscation that entirely changes the structure of a binary –this issue is common in image-based detectors [31]–[33]. Theproblem can be partially mitigated by using more complexstatic features, such as OpCode sequences and API calls [34],even though obfuscation on these features is also possible.However, the usage of obfuscation techniques on IoT mal-ware is not widespread nowadays, therefore it is difficult toevaluate whether in the near future IoT malware will be obfus-cated using similar techniques used in traditional malware, alsoconsidering how the limited computational resources of IoTdevices influences the implementation of obfuscation methods.VI. CONCLUSION AND FUTURE WORKIn this paper we have proposed a lightweight malware imageclassification scheme for detecting IoT DDoS malware onlocal IoT devices, and shown its effectivess. The malwaredetector is based on convolutional neural networks and canbe tuned to be more efficient by using various techniques ofreducing network size. For example, removing the neuronsand links that are not critical in the network can reduce thenumber of parameters needed for classification [21]. Suchfurther improvements can make the proposed system imple-mentable on IoT devices with even less computation resources.In addition, new malware image extraction methods can beproposed to obtain more representative features of malwarefor classification.VII. ACKNOWLEDGMENTThis research was partially supported by Collaboration Hubsfor International Program (CHIRP) of SICORP, Japan Scienceand Technology Agency (JST), and Project of security in theIoT space funding by Department of Science and Technology(DST), India.REFERENCES[1] Pa, Y.M.P., Suzuki, S., Yoshioka, K., Matsumoto, T., Kasama, T. andRossow, C., 2015. IoTPOT: analysing the rise of IoT compromises.EMU, 9, p.1.[2] Sun, H., Wang, X., Buyya, R. and Su, J., 2017. CloudEyes: Cloudbased malware detection with reversible sketch for resource constrainedinternet of things (IoT) devices. Software: Practice and Experience,47(3), pp.421-441.[3] Dahl, G.E., Stokes, J.W., Deng, L. and Yu, D., 2013, May. Large-scalemalware classification using random projections and neural networks.In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEEInternational Conference on (pp. 3422-3426). IEEE.[4] Nataraj, L., Karthikeyan, S., Jacob, G. and Manjunath, B.S., 2011,July. Malware images: visualization and automatic classification. InProceedings of the 8th international symposium on visualization forcyber security (p. 4). ACM.[5] Yue, S., 2017. Imbalanced Malware Images Classification: a CNN basedApproach. arXiv preprint arXiv:1708.08042.[6] LeCun, Y., 2015. LeNet-5, convolutional neural networks. URL:http://yann. lecun. com/exdb/lenet.[7] Simonyan, K. and Zisserman, A., 2014. Very deep convolu-tional networks for large-scale image recognition. arXiv preprintarXiv:1409.1556.[8] Https://www.virustotal.com/`````````MetricsSystems Our method ANN with random projection [5] Weighted loss [3]Accuracy 94.0% 99.5% 96.9%Classifier CNN ANN CNN(VGG-s)Num of Layers 2 2 5Num of nodes 104 1536 1888Fully connect layer 256 2048 4096X2Preprocess Or-organizing binary N-gram binary, Random projection Or-organizing binaryInput dimension 64X64 scalar matrix 179 thousand binaries UnknownTABLE IVCOMPARING PROPOSED SYSTEM WITH TWO PREVIOUS RELATED WORKS. IN PARTICULAR, THE NUMBER OF HIDDEN LAYERS, NUMBER OF NEURONSAND THE NUMBER OF NODES IN FULLY CONNECT LAYERS ARE SHOWN BY “NUM OF LAYERS”,“NUM OF NODES”,“FULLY CONNECT LAYER”RESPECTIVELY. IT CAN BE SEEN THAT THE PROPOSED SYSTEM IS MORE LIGHTWEIGHT THAN REFERENCES DUE TO THE SMALLER SIZE OF NETWORKMODEL AND LOWER DIMENSIONS OF INPUT, AS WELL AS SIMPLER PREPROCESSING.[9] Wagner, M., Fischer, F., Luh, R., Haberson, A., Rind, A., Keim, D.A.,Aigner, W., Borgo, R., Ganovelli, F. and Viola, I., 2015. A surveyof visualization systems for malware analysis. In EG Conference onVisualization (EuroVis)-STARs (pp. 105-125).[10] Kolias, C., Kambourakis, G., Stavrou, A. and Voas, J., 2017. DDoS inthe ioT: Mirai and other botnets. Computer, 50(7), pp.80-84.[11] Alam, M.S. and Vuong, S.T., 2013, August. Random forest clas-sification for detecting android malware. In Green Computing andCommunications (GreenCom), 2013 IEEE and Internet of Things(iThings/CPSCom), IEEE International Conference on and IEEE Cyber,Physical and Social Computing (pp. 663-669). IEEE.[12] Zhang, Z.K., Cho, M.C.Y., Wang, C.W., Hsu, C.W., Chen, C.K. andShieh, S., 2014, November. IoT security: ongoing challenges and re-search opportunities. In Service-Oriented Computing and Applications(SOCA), 2014 IEEE 7th International Conference on (pp. 230-234).IEEE.[13] Ham, H.S., Kim, H.H., Kim, M.S. and Choi, M.J., 2014. Linear SVM-based android malware detection for reliable IoT services. Journal ofApplied Mathematics, 2014.[14] Firdausi, I., Erwin, A. and Nugroho, A.S., 2010, December. Analysis ofmachine learning techniques used in behavior-based malware detection.In Advances in Computing, Control and Telecommunication Technolo-gies (ACT), 2010 Second International Conference on (pp. 201-203).IEEE.[15] Ahmed, F., Hameed, H., Shafiq, M.Z. and Farooq, M., 2009, November.Using spatio-temporal information in API calls with machine learningalgorithms for malware detection. In Proceedings of the 2nd ACMworkshop on Security and artificial intelligence (pp. 55-62). ACM.[16] Shamili, A.S., Bauckhage, C. and Alpcan, T., 2010, August. Malwaredetection on mobile devices using distributed machine learning. InPattern Recognition (ICPR), 2010 20th International Conference on (pp.4348-4351). IEEE.[17] Hallman, R., Bryan, J., Palavicini, G., Divita, J. and Romero-Mariona,J., 2017. IoDDoS The Internet of Distributed Denial of Sevice Attacks.[18] Burguera, I., Zurutuza, U. and Nadjm-Tehrani, S., 2011, October.Crowdroid: behavior-based malware detection system for android. InProceedings of the 1st ACM workshop on Security and privacy insmartphones and mobile devices (pp. 15-26). ACM.[19] Masud, M.M., Al-Khateeb, T.M., Hamlen, K.W., Gao, J., Khan, L., Han,J. and Thuraisingham, B., 2011. Cloud-based malware detection forevolving data streams. ACM transactions on management informationsystems (TMIS), 2(3), p.16.[20] Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classifi-cation with deep convolutional neural networks. In Advances in neuralinformation processing systems (pp. 1097-1105).[21] Pan, W., Dong, H. and Guo, Y., 2016. DropNeuron: Simplifying theStructure of Deep Neural Networks. arXiv preprint arXiv:1606.07326.[22] Shabtai, A., Moskovitch, R., Elovici, Y. and Glezer, C., 2009. Detectionof malicious code by applying machine learning classifiers on staticfeatures: A state-of-the-art survey. information security technical report,14(1), pp.16-29.[23] Siddiqui, M., Wang, M.C. and Lee, J., 2008, March. A survey ofdata mining techniques for malware detection using file features. InProceedings of the 46th annual southeast regional conference on xx(pp. 509-510). ACM.[24] Schmidt, A.D., Bye, R., Schmidt, H.G., Clausen, J., Kiraz, O., Yuksel,K.A., Camtepe, S.A. and Albayrak, S., 2009, June. Static analysis ofexecutables for collaborative malware detection on android. In Commu-nications, 2009. ICC’09. IEEE International Conference on (pp. 1-5).IEEE.[25] Felt, A.P., Finifter, M., Chin, E., Hanna, S. and Wagner, D., 2011,October. A survey of mobile malware in the wild. In Proceedings ofthe 1st ACM workshop on Security and privacy in smartphones andmobile devices (pp. 3-14). ACM.[26] Makandar, A. and Patrot, A., 2018. Trojan Malware Image PatternClassification. In Proceedings of International Conference on Cognitionand Recognition (pp. 253-262). Springer, Singapore.[27] Makandar, A. and Patrot, A., 2015. Overview of malware analysis anddetection. In IJCA proceedings on national conference on knowledge,innovation in technology and engineering, NCKITE (Vol. 1, pp. 35-40).[28] Makandar, A. and Patrot, A., 2015, December. Malware analysis andclassification using Artificial Neural Network. In Trends in Automation,Communications and Computing Technology (I-TACT-15), 2015 Inter-national Conference on (Vol. 1, pp. 1-6). IEEE.[29] Wang, S.C., 2003. Artificial neural network. In Interdisciplinary com-puting in java programming (pp. 81-100). Springer, Boston, MA.[30] Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classifi-cation with deep convolutional neural networks. In Advances in neuralinformation processing systems (pp. 1097-1105).[31] Wu, Y. and Yap, R.H., 2012, July. Experiments with malware visu-alization. In International Conference on Detection of Intrusions andMalware, and Vulnerability Assessment (pp. 123-133). Springer, Berlin,Heidelberg.[32] Kancherla, K. and Mukkamala, S., 2013, April. Image visualizationbased malware detection. In Computational Intelligence in Cyber Se-curity (CICS), 2013 IEEE Symposium on (pp. 40-44). IEEE.[33] S.Z.M. and Maarof, M.A., 2014, August. Malware behavior image formalware variant identification. In Biometrics and Security Technologies(ISBAST), 2014 International Symposium on (pp. 238-243). IEEE.[34] Wagner, M., Fischer, F., Luh, R., Haberson, A., Rind, A., Keim, D.A.,Aigner, W., Borgo, R., Ganovelli, F. and Viola, I., 2015. A surveyof visualization systems for malware analysis. In EG Conference onVisualization (EuroVis)-STARs (pp. 105-125)."
3,"Photonic Reservoir Computing: 
a New Approach to Optical Information Processing 
Kristof Vandoorne1*, Martin Fiers1, David Verstraeten2, Benjamin Schrauwen2,  
Joni Dambre2 and Peter Bienstman1* 
1 Photonics Research Group, Department of Information Technology, Ghent University – IMEC,  
Sint-Pietersnieuwstraat 41, 9000 Gent, Belgium 
2 PARIS, Department of Electronics and Information Systems, Ghent University,  
Sint-Pietersnieuwstraat 41, 9000 Gent, Belgium 
* Tel: +32 9 264 3447, Fax: +32 9 264 3593, e-mail: Kristof.Vandoorne@intec.UGent.be 
ABSTRACT 
Despite ever increasing computational power, recognition and classification problems remain challenging to 
solve. Recently advances have been made by the introduction of the new concept of reservoir computing. This is 
a methodology coming from the field of machine learning and neural networks and has been successfully used in 
several pattern classification problems, like speech and image recognition. The implementations have so far been 
in software, limiting their speed and power efficiency. Photonics could be an excellent platform for a hardware 
implementation of this concept because of its inherent parallelism and unique nonlinear behaviour.  
We propose using a network of coupled Semiconductor Optical Amplifiers (SOA) and show in simulation that 
it could be used as a reservoir by comparing it on a benchmark speech recognition task to conventional software 
implementations. In spite of several differences, they perform as good as or better than conventional 
implementations. Moreover, a photonic implementation offers the promise of massively parallel information 
processing with low power and high speed. 
We will also address the role phase plays on the reservoir performance. 
Keywords: photonic reservoir computing, integrated optics, semiconductor optical amplifiers, nonlinear optics, 
optical neural networks, speech recognition 
1. INTRODUCTION 
Although computers and algorithms are becoming ever stronger and more powerful, there are problems that are 
not easily solved with an algorithmic approach. Speech and image recognition are among them, while humans 
seem to have a natural ability for such tasks. This has inspired the fields of machine learning and Artificial 
Neural Networks (ANN) where people try to build systems suited for this class of problems and where they often 
take models of the human brain as an inspiration. ANNs consist of a network of nodes and interconnections 
between these nodes, just as neurons in our brain are connected to many other neurons. The nodes itself perform 
a function which can be as easy as applying a tangent hyperbolic or as complex as a model approximating the 
behaviour of biological neurons.  
What sets ANNs apart from algorithms is the fact that they are, just as humans, trained to perform a task. The 
training is done through learning by example. In a simplified manner this training takes the following form. We 
want the ANN to distinguish between different classes of data (e.g. different words in speech). Examples are 
obtained and ideally sufficient examples of every class are used. Every example consists of a certain input and a 
desired output that is expected from the ANN. In our case the learning will be supervised, which means that an 
external observer is involved in judging the performance of the output of the ANN. Part of the examples is fed to 
the ANN and the output is monitored. Ideally the output should be the same as the desired output, and therefore 
the interconnection strength between the nodes (or interconnection weight) is changed until the output of the 
ANN is as close as possible to the desired output. The second part of the examples, which differ from the 
examples used for training, is then used to evaluate the trained ANN. If the training was done well, then the 
ANN should perform as good on the unseen data of the test set. In this case we say that the ANN generalizes 
well, which means that it has learned to distinguish between the different classes that the examples are samples 
from and not just between the examples itself. 
Feed-forward Neural Networks, which have no feedback and are structured in layers, have been studied for a 
long time and are used for a number of applications. The lack of feedback makes that there is just a mapping 
from input to output and several well established training rules for the adaptation of the weights exist. The 
drawback is that they lack memory needed for many real world applications which have temporal behaviour 
such as speech recognition. Recurrent Neural Networks (RNN) do have feedback connections but their use 
remained problematic for a long time due to reasons as slow or non-convergence during training.  
Reservoir Computing (RC) is a new training concept for RNNs, introduced a few years ago, that combined the 
advantages of both recurrent and feed forward neural networks [1,2]. In this framework a RNN is used but left 
untrained and we will call it the ‘reservoir’. The state of all the nodes of the RNN is then fed into a linear 
readout, which can then be trained with well established methods. Here as well, training means adapting certain 
connections strengths, but only within the readout, not within the reservoir.  The interesting properties of RNNs 
and its associated memory are maintained, while the training remains easy because linear methods can be used.  
Although the reservoir itself remains untrained, this does not imply that any reservoir will do. Rather, from 
experiments and experience it turns out that most reservoirs perform differently in different dynamical regimes, 
but best on the edge of stability, i.e. the region in between stable and unstable to chaotic behaviour. This region 
is determined by the total amount of gain and loss in the network. A measure often used is the spectral radius, 
the largest eigenvalue of the interconnection matrix. At zero input it is an indication of the stability of the 
network. If its value is larger than one, the network will be unstable. Intuitively this can be understood because 
over time this connection matrix will be applied again and again. If one of its eigenvalues is larger than one, then 
there is gain in the overall network, which will eventually result in unstable behaviour. In classical reservoirs 
based on tangent hyperbolic functions, the spectral radius is often used as a measure to create a network that is 
on the edge of stability for good performance, i.e. with a spectral radius just below one. It is important to 
mention that this measure is just an indication since it is only valid for linear functions and for zero input. For 
non-zero input and nonlinear functions stable behaviour can also happen for spectral radii larger than one. 
2. PHOTONIC RESERVOIR COMPUTING 
The idea of RC is actually very broadly applicable and many different types of reservoirs are currently being 
investigated. One way of interpreting the reservoir and the readout function is to view the reservoir as a special 
and advanced kind of pre-processing or filtering of the input before the readout function. In this view, the 
reservoir essentially mixes the inputs together, so that the interesting features are more easily extracted by the 
readout. A nonlinear mixing often seems to offer advantages over linear mixing when dealing with more 
complicated problems such as speech recognition. 
Most implementations so far have been software based, hence the pursuit of finding a suitable hardware 
platform for performing the reservoir calculation. This transition offers the potential for huge power 
consumption savings and speed enhancement. What makes a hardware implementation even more attractive is 
the fact that the computation in RC happens through the transient states and changing dynamical behaviour. This 
is in sharp contrast with digital computation where the state is only important after the transients have died out.  
Photonics seems like a very interesting candidate of building a reservoir, because it has a range of different 
nonlinear interactions working on different timescales. It also offers the promise of being more power efficient. 
There remain, however, many challenges as well. If you encode the information in changing power levels, then it 
becomes difficult to have negative weights and to subtract signals. A topology made on a 2D chip, which is the 
case for most photonic chips nowadays, limits the freedom in connectivity that exists in software 
implementations, since one would like to minimise the number of crossings. When using a coherent light source, 
the amplitude and accompanying phase start to play a role as well, whereas traditional RC is only amplitude 
based. In this paper we will show through simulation results that despite these limitations, photonic reservoirs 
can perform quite well on benchmark problems. 
3. SIMULATION RESULTS 
In this section the speech recognition task that we used as a benchmark problem will be described, as well as the 
model we used to simulate a photonic reservoir. In our case the reservoir consists of a network of coupled 
Semiconductor Optical Amplifiers (SOAs).  
3.1 Speech Recognition 
Speech recognition is a very difficult problem to solve and methods based on ANNs have been among the state 
of the art for a long time. Reservoir Computing with classical neural networks has been employed with success 
for speech recognition. The speech recognition that we have used in this paper is about digit recognition, zero to 
nine, uttered by 5 female speakers ten times. The dataset and simulation framework for classical reservoirs is 
publicly available and can be found here: http://snn.elis.ugent.be/rctoolbox. As is standard for speech 
recognition, some pre-processing of the raw speech signal is performed before it is fed into the reservoir. Often 
these methods involve a transformation to the frequency domain and highlighting certain frequencies typical for 
our ear by using some kind of ear model. The model used for the results in this paper was the Lyon ear model  
[3].  
Audio signals are rather slow and in our simulation we fed the speech signal at much higher speeds to the 
photonic reservoir, at timescales typical for the delays in a network of SOAs. One of the reasons is that the 
relation between the time scales of the reservoir and those of the input is important [4]. This made the duration 
of an average sample in the order of a few hundred ps. So although we use this task to demonstrate the potential 
of photonic reservoir computing, we don’t propose to use photonic reservoir as a platform for standard real-time, 
slow audio signals.  
3.2 Simulation model 
SOAs with their saturation of the gain and output power are the optical device closest to the tangent hyperbolic 
functions used in many ANN implementations. That is the reason we chose them as a first medium to verify the 
usefulness of photonic reservoirs. The SOA model we used is one proposed by Agrawal [5]. It captures the most 
important features such as gain saturation, carrier lifetime and phase shift depending on the gain. Spectral hole 
burning, cross gain and phase modulation were not considered, since the operation is set to be at one wavelength. 
 A time step based rate equation model, solved by a fourth order Runge-Kutta, of a network of coupled SOAs 
was then plugged into the freely available RC toolbox mentioned earlier. This toolbox offers a number of 
reservoir simulations and benchmark problems that can be simulated and tested. Furthermore almost every 
aspect of it can be changed to make it suitable to specific experiments.  
  
 
 
Figure 1: the topology used for the SOA simulations 
 
3.3 Results 
In these experiments the input consists of 77 channels at every time step. These channels are the result of the 
pre-processing of the speech data. Therefore we have chosen are network large enough. All the experiments 
were done with a network of 81 nodes. An example of the topology used can be seen in Fig. 1. There a network 
of 3 by 3 SOAs is shown. The connections are made in such a way that they don’t cross. The information flows 
from the top left to the bottom right SOAs with nearest neighbour connections. The feedback is assured by 
having as many feedback connections on the edges of the network as possible without having to use crossings. 
The topology we used was a 9 by 9 network but the construction method was the same. 
In the experiment we varied two variables: the phase change and attenuation in every connection. Although in 
practice these wouldn’t be the variables that are swept, they are orthogonal and therefore provide an interesting 
insight in the behaviour of the network. In reality the input current of the SOAs and the wavelength of the light 
can be used, but since the input current of the SOAs influences the gain, which in turns influences the amount of 
phase change inside an SOA, these variables are not orthogonal.  
The total amount of gain and loss in the network is also calculated by means of the spectral radius, mentioned 
earlier. Since coherent light is used, the signals can be represented by complex amplitudes. It is therefore 
important that the weights in the interconnection matrix, used for the calculation of the spectral radius, are the 
weights for these complex signals and not for their intensity. In this way interference effect are accounted for. 
An example of a result of such an experiment is shown in Fig. 2. Here a clear transition at a spectral radius 
around 1 can be seen. Above 1 the results become suddenly a lot worse and this is due to the fact that instability 
kicks in because there is gain in the network. This regime actually corresponds with one where some SOAs 
would become lasers. Our simulation model is probably not suited for addressing a network of coupled lasers, so 
it is not possible to state with certainty that this regime is very bad for RC. Maybe some kind of emergent 
behaviour, useful for RC, could arise from coupled lasers, but this is the topic of ongoing investigation.  
Another consequence of using coherent light is that the delay in the connections changes the phase of the light 
according to the wavelength, length and the effective index of the connection. All the connections were 
considered equally long in this experiment, which means that the phase change in every connection can be 
changed at the same rate, for example by changing the wavelength. Phase is important, since it determines the 
interference of the light when it combines in front of every SOA. Looking at Fig. 2, it becomes clear that the 
network performs better for some phase changes or interference. The optimal result for this task was a Word 
Error Rate (WER), which is the percentage of the words incorrectly recognized, of about 1%.  This is 
comparable to the results achieved by classical tangent hyperbolic networks, although the performance of both 
software and photonic reservoirs could be improved when a low-pass filter is added to all of the nodes.  
From Fig. 2 it becomes apparent that the phase is very important to the performance of the reservoir. Phase is 
much more sensitive than gain, so it is important go have an optimal region that is as vast as possible for phase. 
This can be understood because for a small optimal region, a small phase change can be enough to lose your 
optimal performance. A lot of ongoing investigation is about finding networks with large optimal areas (making 
them phase ‘independent’). An easy way of doing this is working with incoherent light, but the results are typical 
a lot worse for incoherent networks. 
 
 
 
Figure 2: Simulation result for a network of coupled SOAs for speech recognition. The x-axis shows the spectral 
radius, the y-axis the phase change in every connection. The darker the colour, the better the performance. 
4. CONCLUSIONS 
In this paper we have investigated a network of coupled SOAs as a reservoir for RC by means of evaluating this 
kind of reservoir on a benchmark speech recognition task. It turns out that SOA reservoirs can be used to solve 
such kind of complex problems, despite the limitations imposed by a practical implementation. In the future, a 
practical demonstration of a chip of SOAs used as a photonic reservoir will be further pursued. 
ACKNOWLEDGEMENTS 
K. Vandoorne acknowledges the Special Research Fund (BOF) of Ghent University for a specialization grant. 
This work has been carried out in the framework of the IAP project Photonics@be of the Belgian Science Policy. 
REFERENCES 
[1] H. Jaeger,  H. Haas: Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless 
communication, Science, vol. 304, pp. 78-80, Apr 2 2004. 
[2] W. Maass, T. Natschlager, H. Markram: Real-time computing without stable states: A new framework for 
neural computation based on perturbations, Neural Computation, vol. 14, pp. 2531-2560, Nov 2002. 
[3] R. Lyon: A computational model of filtering, detection and compression in the cochlea, in Proc. IEEE 
ICASSP, Paris, May 1982, pp. 1282-1285.  
[4] D. Verstraeten, PhD thesis: Reservoir Computing: Computation with Dynamical Systems, in PARIS, Dept. 
of Electronics and Information Systems Ghent: Ghent University, 2009. 
[5] G. P. Agrawal, N. A. Olsson,: Self-Phase Modulation and Spectral Broadening of Optical Pulses in 
Semiconductor-Laser Amplifiers, IEEE Journal of Quantum Electronics, vol. 25, pp. 2297-2306, Nov 
1989. 
"
4,"Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2017.DOI
Benchmark Analysis of Representative
Deep Neural Network Architectures
SIMONE BIANCO1, REMI CADENE2, LUIGI CELONA1, AND PAOLO NAPOLETANO1.
1University of Milano-Bicocca, Department of Informatics, Systems and Communication, viale Sarca, 336, 20126 Milano, Italy
2Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
Corresponding author: Luigi Celona (e-mail: luigi.celona@disco.unimib.it).
 https://github.com/CeLuigi/models-comparison.pytorch
ABSTRACT This work presents an in-depth analysis of the majority of the deep neural networks (DNNs)
proposed in the state of the art for image recognition. For each DNN multiple performance indices are
observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and
inference time. The behavior of such performance indices and some combinations of them are analyzed and
discussed. To measure the indices we experiment the use of DNNs on two different computer architectures,
a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson
TX1 board. This experimentation allows a direct comparison between DNNs running on machines with
very different computational capacity. This study is useful for researchers to have a complete view of what
solutions have been explored so far and in which research directions are worth exploring in the future;
and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical
deployments and applications. To complete this work, all the DNNs, as well as the software used for the
analysis, are available online.
INDEX TERMS Deep neural networks, Convolutional neural networks, Image recognition.
I. INTRODUCTION
Deep neural networks (DNNs) have achieved remarkable
results in many computer vision tasks [1]. AlexNet [2], that is
the first DNN presented in the literature in 2012, drastically
increased the recognition accuracy (about 10% higher) with
respect to traditional methods on the 1000-class ImageNet
Large-Scale Visual Recognition Competition (ImageNet-1k)
[3]. Since then, literature has worked both in designing more
accurate networks as well as in designing more efficient
networks from a computational-cost point of view.
Although there is a lot of literature discussing new archi-
tectures from the point of view of the layers composition and
recognition performance, there are few papers that analyze
the aspects related to the computational cost (memory usage,
inference time, etc.), and more importantly how computa-
tional cost impacts on the recognition accuracy.
Canziani et al. [4] in the first half of 2016 proposed
a comprehensive analysis of some DNN architectures by
performing experiments on an embedded system based on a
NVIDIA Jetson TX1 board. They measured accuracy, power
consumption, memory footprint, number of parameters and
operations count, and more importantly they analyzed the re-
lationship between these performance indices. It is a valuable
work, but it has been focused on a limited number (i.e. 14) of
DNNs and more importantly the experimentation has been
carried out only on the NVIDIA Jetson TX1 board. In [5],
speed/accuracy trade-off of modern DNN-based detection
systems has been explored by re-implementing a set of meta-
architectures inspired by well-known detection networks in
the state of the art. Results include performance comparisons
between an Intel Xeon CPU and a NVIDIA Titan X GPU.
The aim of this work is to provide a more comprehen-
sive and complete analysis of existing DNNs for image
recognition and most importantly to provide an analysis on
two hardware platforms with a very different computational
capacity: a workstation equipped with a NVIDIA Titan X
Pascal (often referred to as Titan Xp) and an embedded
system based on a NVIDIA Jetson TX1. To this aim we
analyze and compare more than 40 state-of-the-art DNN
architectures in terms of computational cost and accuracy. In
particular we experiment the selected DNN architectures on
the ImageNet-1k challenge and we measure: accuracy rate,
model complexity, memory usage, computational complex-
ity, and inference time. Further, we analyze relationships be-
VOLUME 4, 2018 ©2018 IEEE 1
ar
X
iv
:1
81
0.
00
73
6v
2 
 [c
s.C
V]
  1
9 O
ct 
20
18
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
tween these performance indices that provide insights for: 1)
understanding what solutions have been explored so far and
in what direction it would be appropriate to go in the future;
2) selecting the DNN architecture that better fits the resource
constraints of practical deployments and applications.
The most important findings are that: i) the recognition
accuracy does not increase as the number of operations
increases; ii) there is not a linear relationship between model
complexity and accuracy; iii) the desired throughput places
an upper bound to the achievable accuracy; iv) not all the
DNN models use their parameters with the same level of
efficiency; v) almost all models are capable of super real-time
performance on a high-end GPU, while just some of them can
guarantee it on an embedded system; vi) even DNNs with
a very low level model complexity have a minimum GPU
memory footprint of about 0.6GB.
The rest of paper is organized as follows: in Section II
hardware and software used for experiments are detailed;
in Section III the considered DNN architectures are briefly
introduced; in Section IV the measured performance indices
are described; finally, in Section V obtained results are
reported and analyzed, and Section VI presents our final
considerations.
II. BENCHMARKING
We implement the benchmark framework for DNNs compar-
ison in Python. The PyTorch package [6] is used for neural
networks processing with cuDNN-v5.1 and CUDA-v9.0 as
back-end. All the code for the estimation of the adopted
performance indices, as well as all the considered DNN
models are made publicly available [7].
We run all the experiments on a workstation and on an
embedded system:
1) The workstation is equipped with an Intel Core I7-7700
CPU @ 3.60GHZ, 16GB DDR4 RAM 2400 MHz,
NVIDIA Titan X Pascal GPU with 3840 CUDA cores
(top-of-the-line consumer GPU). The operating system
is Ubuntu 16.04.
2) The embedded system is a NVIDIA Jetson TX1 board
with 64-bit ARM®A57 CPU @ 2GHz, 4GB LPDDR4
1600MHz, NVIDIA Maxwell GPU with 256 CUDA
cores. The board includes the JetPack-2.3 SDK.
The use of these two different systems allows to highlight
how critical the computational resources can be depending
on the DNN model adopted especially in terms of memory
usage and inference time.
III. ARCHITECTURES
In this section we briefly describe the analyzed architectures.
We select different architectures, some of which have been
designed to be more performing in terms of effectiveness,
while others have been designed to be more efficient and
therefore more suitable for embedded vision applications.
In some cases there is a number following the name of the
architecture. Such a number depicts the number of layers that
contains parameters to be learned (i.e. convolutional or fully
connected layers).
We consider the following architectures: AlexNet [2]; the
family of VGG architectures [8] (VGG-11, -13, -16, and -
19) without and with the use of Batch Normalization (BN)
layers [9]; BN-Inception [9]; GoogLeNet [10]; SqueezeNet-
v1.0 and -v1.1 [11]; ResNet-18, -34, -50, -101, and -152
[12]; Inception-v3 [13]; Inception-v4 and Inception-ResNet-
v2 [14]; DenseNet-121, -169, and -201 with growth rate
corresponding to 32, and DenseNet-161 with growth rate
equal to 48 [15]; ResNeXt-101 (32x4d), and ResNeXt-101
(64x4d), where the numbers inside the brackets denote re-
spectively the number of groups per convolutional layer
and the bottleneck width [16]; Xception [17]; DualPathNet-
68, -98, and -131, [18]; SE-ResNet-50, SENet-154, SE-
ResNet-101, SE-ResNet-152, SE-ResNeXt-50 (32x4d), SE-
ResNeXt-101 (32x4d) [19]; NASNet-A-Large, and NASNet-
A-Mobile, whose architecture is directly learned [20].
Furthermore, we also consider the following efficientcy-
oriented models: MobileNet-v1 [21], MobileNet-v2 [22], and
ShuffleNet [23].
IV. PERFORMANCE INDICES
In order to perform a direct and fair comparison, we exactly
reproduce the same sampling policies: we directly collect
models trained using the PyTorch framework [6], or we
collect models trained with other deep learning frameworks
and then we convert them in PyTorch.
All the pre-trained models expect input images normalized
in the same way, i.e. mini-batches of RGB images with shape
3×H ×W , where H and W are expected to be:
- 331 pixels for the NASNet-A-Large model;
- 229 pixels for InceptionResNet-v2, Inception-v3,
Inception-v4, and Xception models;
- 224 pixels for all the other models considered.
We consider multiple performance indices useful for a
comprehensive benchmark of DNN models. Specifically, we
measure: accuracy rate, model complexity, memory usage,
computational complexity, and inference time.
A. ACCURACY RATE
We estimate Top-1 and Top-5 accuracy on the ImageNet-1k
validation set for image classification task. The predictions
are computed by evaluating the central crop only. Slightly
better performances can be achieved by considering the aver-
age prediction coming from multiple crops (four corners plus
central crop and their horizontal flips).
B. MODEL COMPLEXITY
We analyze model complexity by counting the total amount
of learnable parameters. Specifically, we collect the size
of the parameter file in terms of MB for the considered
models. This information is very useful for understanding the
minimum amount of GPU memory required for each model.
2 VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
(a) (b)
FIGURE 1: Ball chart reporting the Top-1 and Top-5 accuracy vs. computational complexity. Top-1 and Top-5 accuracy using
only the center crop versus floating-point operations (FLOPs) required for a single forward pass are reported. The size of each
ball corresponds to the model complexity. (a) Top-1; (b) Top-5.
C. MEMORY USAGE
We evaluate the total memory consumption, which includes
all the memory that is allocated, i.e. the memory allocated for
the network model and the memory required while process-
ing the batch. We measure memory usage for different batch
sizes: 1, 2, 4, 8, 16, 32, and 64.
D. COMPUTATIONAL COMPLEXITY
We measure the computational cost of each DNN model
considered using the floating-point operations (FLOPs) in
the number of multiply-adds as in [16]. More in detail, the
multiply-adds are counted as two FLOPs because, in many
recent models, convolutions are bias-free and it makes sense
to count multiply and add as separate FLOPs.
E. INFERENCE TIME
We report inference time per image for each DNN model
for both the NVIDIA Titan X Pascal GPU and the NVIDIA
Jetson TX1. We measure inference time in terms of mil-
liseconds and by considering the same batch sizes described
in Section IV-C. For statistical validation the reported time
corresponds to the average over 10 runs.
V. RESULTS
A. ACCURACY-RATE VS COMPUTATIONAL
COMPLEXITY VS MODEL COMPLEXITY
The ball charts reported in Figures 1 (a) and (b) show Top-
1 and Top-5 accuracy on the ImageNet-1k validation set
with respect to the computational complexity of the con-
sidered architectures for a single forward pass measured
for both the workstation and the embedded board, The ball
size corresponds to the model complexity. From the plots
it can be seen that the DNN model reaching the highest
Top-1 and Top-5 accuracy is the NASNet-A-Large that is
also the one having the highest computational complexity.
Among the models having the lowest computational com-
plexity instead (i.e. lower than 5 G-FLOPs), SE-ResNeXt-
50 (32x4d) is the one reaching the highest Top-1 and Top-
5 accuracy showing at the same time a low level of model
complexity, with approximately 2.76 M-params. Overall, it
seems that there is no relationship between computational
complexity and recognition accuracy, for instance SENet-154
needs about 3× the number of operations that are needed
by SE-ResNeXt-101(32x4d) while having almost the same
accuracy. Moreover, it seems that there is no relationship
also between model complexity and recognition accuracy:
for instance VGG-13 has a much higher level of model
complexity (size of the ball) than ResNet-18 while having
almost the same accuracy.
B. ACCURACY-RATE VS LEARNING POWER
It is known that DNNs are inefficient in the use of their
full learning power (measured as the number of parameters
with respect to the degrees of freedom). Although many
papers exist that exploit this feature to produce compressed
DNN models with the same accuracy of the original models
[24] we want here to measure how efficiently each model
uses its parameters. We follow [4] and measure it as Top-1
VOLUME 4, 2018 3
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
(a) (b)
FIGURE 2: Top-1 accuracy density (a) and Top-1 accuracy vs. Top-1 accuracy density (b). The accuracy density measures how
efficiently each model uses its parameters.
accuracy density, i.e. Top-1 accuracy divided by the number
of parameters. The higher is this value and the higher is
the efficiency. The plot is reported in Figure 2(a), where it
can be seen that the models that use their parameters most
efficiently are the SqueezeNets, ShuffleNet, the MobileNets
and NASNet-A-Mobile. To focus to the density information,
we plot the Top-1 accuracy with respect to the Top-1 accuracy
density (see Figure 2(b)), that permits to find more easily
the desired trade-off. In this way it is possible to easily see
that among the most efficient models, NASNet-A-Mobile
and MobileNet-v2 are the two providing a much higher Top-
1 accuracy. Among the models having the highest Top-1
accuracy (i.e. higher than 80%) we can observe how the
models using their parameters more efficiently are Inception-
v4 and SE-ResNeXt-101 (32x4d).
C. INFERENCE TIME
Average per image inference time over 10 runs for all the
DNN models considered are reported in Tables 1(a) and (b)
for batch size equal to 1, 2, 4, 8, 16, 32, and 64 on both
the Titan Xp and the Jetson. Inference time is measured
in milliseconds and the entries in Tables 1(a) and (b) are
color coded to easily convert them in frames per second
(FPS). From the table it is possible to see that all the DNN
models considered are able to achieve super real-time perfor-
mances on the Titan Xp with the only exception of SENet-
154, when a batch size of 1 is considered. On the Jetson
instead, only a few models are able to achieve super real-time
performances when a batch size of 1 is considered, namely:
the SqueezeNets, the MobileNets, ResNet-18, GoogLeNet,
and AlexNet. Missing measurements are due to the lack
of enough system memory required to process the larger
batches.
D. ACCURACY-RATE VS INFERENCE TIME
In Figure 3(a) and (b) we report the plots of the top-1
accuracy with respect to the number of images processed
per second (i.e. the number of inferences per second) with
a batch size of 1 on both the Titan Xp and the Jetson TX1.
On each plot the linear upper bound is also reported; the two
have almost the same intercept (≈ 83.3 for Titan Xp and
≈ 83.0 for the Jetson TX1), but the first has a slope that
is almost 8.3× smaller than the second one (−0.0244 vs.
−0.2025); these bounds show that the Titan Xp guarantees
a lower decay of the maximum accuracy achievable when a
larger throughput is needed. Note that this bound appears a
curve instead of line in the plots because of the logarithmic
scale of the images per second axis. From the Titan Xp
plot it is possible to see that if one targets a throughput of
more than 250 FPS, the model giving the highest accuracy
is ResNet-34, with 73.27% Top-1 accuracy; with a target of
more than 125 FPS the model giving the highest accuracy
is Xception, with 78,79% Top-1 accuracy; with a target of
more than 62.5 FPS the model giving the highest accuracy is
SE-ResNeXt-50 (32x4d), with 79,11% Top-1 accuracy; with
a target of more than 30 FPS the model giving the highest
accuracy is NASNet-A-Large, with 82,50% Top-1 accuracy.
This analysis shows how even the most accurate model in
the state of the art, i.e. NASNet-A-Large, is able to provide
super real-time performance (30.96 FPS) on the Titan Xp.
Considering the Jetson TX1 plot it is possible to see that if
one targets super real-time performance, the model giving the
4 VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
highest accuracy is MobileNet-v2, with a Top-1 accuracy of
71.81%; if one targets a Top-1 accuracy larger than 75%, the
maximum throughput is achieved by ResNet-50 (18,83 FPS);
targeting a Top-1 accuracy larger than 80%, the maximum
throughput is achieved by SE-ResNeXt-101 (32x4d) (7,16
FPS); targeting the highest Top-1 accuracy in the state of the
art the throughput achievable is 2,29 FPS.
E. MEMORY USAGE
In Table 2 we analyze the memory consumption for all the
DNN models considered for different batch sizes on the Titan
Xp. From the memory footprints reported it can be seen that
when a batch size of 1 is considered, most models require
less than 1GB of memory, with the exception of NASNet-A-
Large, the SE-ResNets, the SE-ResNeXTs, SENet-154, the
VGGs and Xception. However none of them requires more
than 1.5GB for a batch size of 1.
F. MEMORY USAGE VS MODEL COMPLEXITY
In Figure 4 we analyze the relationship between the initial
static allocation of the model parameters (i.e. the model
complexity) and the total memory utilization for a batch size
of 1 on the Titan Xp. We can see that the relationship is linear,
and follows two different lines with approximately the same
slope (i.e. 1.10 and 1.15) and different intercepts (i.e. 910
and 639 respectively). This means that the model complexity
can be used to reliably estimate the total memory utilization.
From the plots we can observe that families of models belong
to the same line, as for example the VGGs, the SE-ResNets
ans SqueezeNets lie on the line with higher intercept, while
the ResNets, DualPathNets, DenseNets, Inception nets and
MobileNets line on the line with the lower intercept. In par-
ticular we can observe how models having the smallest com-
plexity (i.e. SqueezeNet-v1.0 and SqueezeNet-v1.1 both with
5MB) have a 943MB and 921MB memory footprint, while
models having slightly higher complexity (i.e. MobileNet-
v1 and MobileNet-v2 with respectively 17MB and 14MB)
have a much smaller memory footprint, equal to 650MB and
648MB respectively.
G. BEST DNN AT GIVEN CONSTRAINTS
Table 3 shows the best DNN architectures in terms of
recognition accuracy when specific hardware resources are
given as computational constraints. This analysis is done for
both the Titan Xp and Jetson TX1. We define the following
constraints:
- Memory usage: high (≤1.4GB), medium (≤1.0GB) and
low (≤0.7GB);
- Computational time: half real-time (@15FPS), real-time
(@30FPS), super real-time (@60FPS);
A Titan Xp, with a low memory usage as constraint, achieves
a recognition accuracy of at most 75.95% by using the
DPN-68 network independently of the computational time.
Having more resources, for instance medium and high mem-
ory usage, Titan Xp achieves a recognition accuracy of at
most 79.11% by using the SE-ResNeXt-50 (32x4d) with a
super real-time throughput. Having no requirements in terms
of memory usage, the Jetson TX1 achieves a recognition
accuracy of at most 69.52% by using the MobileNet-v1,
which guarantees a super real-time throughput. To have a
DNN running on a Jetson that is comparable in terms of
recognition accuracy to the best DNNs running on the Titan
Xp, a memory size of at least 1GB is needed. In this case the
most performing is the ResNet-50, able to guarantee an half
real-time throughput, with a recognition accuracy of 76.01%.
VI. CONCLUSION
The design of Deep neural networks (DNNs) with increas-
ing complexity able to improve the performance of the
ImageNet-1k competition plays a central rule in advancing
the state-of-the-art also on other vision tasks. In this article
we present a comparison between different DNNs in order to
provide an immediate and comprehensive tool for guiding in
the selection of the appropriate architecture responding to re-
source constraints in practical deployments and applications.
Specifically, we analyze more than 40 state-of-the-art DNN
architectures trained on ImageNet-1k in terms of accuracy,
number of parameters, memory usage, computational com-
plexity, and inference time.
The key findings of this paper are the following:
- the recognition accuracy does not increase as the num-
ber of operations increases: in fact, there are some archi-
tectures that with a relatively low number of operations,
such as the SE-ResNeXt-50 (32x4d), achieve very high
accuracy (see Figures 1a and b). This finding is indepen-
dent on the computer architecture experimented;
- there is not a linear relationship between model com-
plexity and accuracy (see Figures 1a and b);
- not all the DNN models use their parameters with the
same level of efficiency (see Figures 2a and b);
- the desired throughput (expressed for example as the
number of inferences per second) places an upper bound
to the achievable accuracy (see Figures 3a and b);
- model complexity can be used to reliably estimate the
total memory utilization (see Figure 4);
- almost all models are capable of real-time or super real-
time performance on a high-end GPU, while just a few
of them can guarantee them on an embedded system
(see Tables 1a and b);
- even DNNs with a very low level model complexity
have a minimum GPU memory footprint of about 0.6GB
(see Table 2).
All the DNNs considered, as well as the software used
for the analysis, are available online [7]. We plan to add to
the repository interactive plots that allow other researchers to
better explore the results of this study, and more importantly
to effortlessly add new entries.
ACKNOWLEDGMENTS
We gratefully acknowledge the support of NVIDIA Corpo-
ration with the donation of the Titan X Pascal GPU used for
VOLUME 4, 2018 5
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
TABLE 1: Inference time vs. batch size. Inference time per image is estimated across different batch sizes for the Titan Xp
(left), and Jetson TX1 (right). Missing data are due to the lack of enough system memory required to process the larger batches.
DNN 1 2 4 8 16 32 64
AlexNet 1.28 0.70 0.48 0.27 0.18 0.14 0.15
BN-Inception 5.79 3.00 1.64 1.10 0.87 0.77 0.71
CaffeResNet-101 8.20 4.82 3.32 2.54 2.27 2.16 2.08
DenseNet-121 (k=32) 8.93 4.41 2.64 1.96 1.64 1.44 1.39
DenseNet-169 (k=32) 13.03 6.72 3.97 2.73 2.14 1.87 1.75
DenseNet-201 (k=32) 17.15 9.25 5.36 3.66 2.84 2.41 2.27
DenseNet-161 (k=48) 15.50 9.10 5.89 4.45 3.66 3.43 3.24
DPN-68 10.68 5.36 3.24 2.47 1.80 1.59 1.52
DPN-98 22.31 13.84 8.97 6.77 5.59 4.96 4.72
DPN-131 29.70 18.29 11.96 9.12 7.57 6.72 6.37
FBResNet-152 14.55 7.79 5.15 4.31 3.96 3.76 3.65
GoogLeNet 4.54 2.44 1.65 1.06 0.86 0.76 0.72
Inception-ResNet-v2 25.94 14.36 8.82 6.43 5.19 4.88 4.59
Inception-v3 10.10 5.70 3.65 2.54 2.05 1.89 1.80
Inception-v4 18.96 10.61 6.53 4.85 4.10 3.77 3.61
MobileNet-v1 2.45 0.89 0.68 0.60 0.55 0.53 0.53
MobileNet-v2 3.34 1.63 0.95 0.78 0.72 0.63 0.61
NASNet-A-Large 32.30 23.00 19.75 18.49 18.11 17.73 17.77
NASNet-A-Mobile 22.36 11.44 5.60 2.81 1.61 1.75 1.51
ResNet-101 8.90 5.16 3.32 2.69 2.42 2.29 2.21
ResNet-152 14.31 7.36 4.68 3.83 3.50 3.30 3.17
ResNet-18 1.79 1.01 0.70 0.56 0.51 0.41 0.38
ResNet-34 3.11 1.80 1.20 0.96 0.82 0.71 0.67
ResNet-50 5.10 2.87 1.99 1.65 1.49 1.37 1.34
ResNeXt-101 (32x4d) 17.05 9.02 6.27 4.62 3.71 3.25 3.11
ResNeXt-101 (64x4d) 21.05 15.54 10.39 7.80 6.39 5.62 5.29
SE-ResNet-101 15.10 9.26 6.17 4.72 4.03 3.62 3.42
SE-ResNet-152 23.43 13.08 8.74 6.55 5.51 5.06 4.85
SE-ResNet-50 8.32 5.16 3.36 2.62 2.22 2.01 2.06
SE-ResNeXt-101 (32x4d) 24.96 13.86 9.16 6.55 5.29 4.53 4.29
SE-ResNeXt-50 (32x4d) 12.06 7.41 5.12 3.64 2.97 3.01 2.56
SENet-154 53.80 30.30 19.32 13.27 10.45 9.41 8.91
ShuffleNet 5.40 2.67 1.37 0.82 0.66 0.59 0.56
SqueezeNet-v1.0 1.53 0.84 0.66 0.59 0.54 0.52 0.53
SqueezeNet-v1.1 1.60 0.77 0.44 0.37 0.32 0.31 0.30
VGG-11 3.57 4.40 2.89 1.56 1.19 1.10 1.13
VGG-11_BN 3.49 4.60 2.99 1.71 1.33 1.24 1.27
VGG-13 3.88 5.03 3.44 2.25 1.83 1.75 1.79
VGG-13_BN 4.40 5.37 3.71 2.42 2.05 1.97 2.00
VGG-16 5.17 5.91 4.01 2.84 2.20 2.12 2.15
VGG-16_BN 5.04 5.95 4.27 3.06 2.45 2.36 2.41
VGG-19 5.50 6.26 4.71 3.29 2.59 2.52 2.50
VGG-19_BN 6.17 6.67 4.86 3.56 2.88 2.74 2.76
Xception 6.44 5.35 4.90 4.47 4.41 4.41 4.36
DNN 1 2 4 8 16 32 64
AlexNet 28.88 13.00 8.58 6.56 5.39 4.77
BN-Inception 35.52 26.48 25.10 23.89 21.21 20.47
CaffeResNet-101 84.47 91.37 70.33 63.53 56.38 53.73
DenseNet-121 (k=32) 66.43 50.87 50.20 43.89 40.41 38.22
DenseNet-169 (k=32) 137.96 130.27 110.82 100.56 92.97 88.94
DenseNet-201 (k=32) 84.57 61.71 62.62 53.73 49.28 46.26
DenseNet-161 (k=48) 103.20 76.11 77.10 68.32 62.73 59.14
DPN-68 113.08 52.73 42.85 43.32 38.18 36.40 36.22
DPN-98 243.51 148.51 135.30 125.92 123.34 118.68 117.27
DPN-131 330.15 204.69 184.89 172.25 165.59 162.67 160.66
FBResNet-152 133.68 147.75 113.48 105.78 94.26 97.47
GoogLeNet 32.11 27.19 23.29 21.66 19.77 19.96
Inception-ResNet-v2 198.95 141.29 127.97 130.25 117.99 116.47
Inception-v3 79.39 59.04 56.46 51.79 47.60 46.85
Inception-v4 158.00 120.23 106.77 102.21 95.51 95.40
MobileNet-v1 15.06 11.94 11.34 11.03 10.82 10.58 10.55
MobileNet-v2 20.51 14.58 13.67 13.56 13.18 13.10 12.72
NASNet-A-Large 437.20 399.99 385.75 383.55 389.67
NASNet-A-Mobile 133.87 62.91 33.72 30.62 29.72 28.92 28.55
ResNet-101 84.52 77.90 71.23 67.14 58.11
ResNet-152 124.67 113.65 101.41 96.76 82.35
ResNet-18 21.16 15.30 14.35 13.82 11.99 10.73 12.45
ResNet-34 39.88 28.82 27.51 24.97 20.41 18.48 17.97
ResNet-50 53.09 44.84 41.20 38.79 35.72
ResNeXt-101 (32x4d) 115.37 90.93 84.64 79.66 77.63
ResNeXt-101 (64x4d) 177.40 155.77 144.82 137.43 134.07
SE-ResNet-101 118.13 105.11 96.71 91.68 80.99
SE-ResNet-152 169.73 155.08 139.72 133.59 116.97
SE-ResNet-50 69.65 61.37 55.33 51.87 47.80
SE-ResNeXt-101 (32x4d) 139.62 122.01 112.05 105.34 102.39
SE-ResNeXt-50 (32x4d) 80.08 69.86 67.20 62.66 61.19
SENet-154 309.48 240.80 221.84 211.00 207.06 201.49 201.66
ShuffleNet 36.58 22.61 13.80 13.36 12.91 12.66 12.50
SqueezeNet-v1.0 17.00 16.47 15.03 13.97 13.25 12.89 12.70
SqueezeNet-v1.1 11.05 9.88 8.80 7.90 7.38 7.20 7.04
VGG-11 106.44 125.84 85.84 60.10 32.56 30.51 32.27
VGG-11_BN 101.58 122.82 86.26 54.50 47.81 47.31 41.26
VGG-13 122.59 148.80 108.28 75.99 70.57 64.88 62.79
VGG-13_BN 129.69 153.68 113.90 81.19 76.39 70.59 67.38
VGG-16 151.52 169.92 129.89 96.81 91.72
VGG-16_BN 163.37 176.35 136.85 103.45 98.11
VGG-19 178.04 192.86 152.28 117.92 112.39
VGG-19_BN 185.18 198.66 159.20 124.88 119.15
Xception 98.96 93.40 90.49 87.65 86.89
(a) (b)
FPS >1000 >250 >125 >62.5 >30 >15 >5 <=5
ms <1 <4 <8 <16 <33 <66 <200 >=200
this research.
REFERENCES
[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,
no. 7553, p. 436, 2015.
[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in Neural Infor-
mation Processing Systems (NIPS), 2012, pp. 1097–1105.
[3] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large scale visual
recognition challenge,” International Journal of Computer Vision, vol. 115,
no. 3, pp. 211–252, 2015.
[4] A. Canziani, A. Paszke, and E. Culurciello, “An analysis of deep
neural network models for practical applications,” arXiv preprint
arXiv:1605.07678, 2016.
[5] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
Z. Wojna, Y. Song, S. Guadarrama et al., “Speed/accuracy trade-offs
for modern convolutional object detectors,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017, pp. 7310–
7311.
[6] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
pytorch,” 2017.
[7] S. Bianco, R. Cadène, L. Celona, and P. Napoletano, “Paper github repos-
itory,” https://github.com/CeLuigi/models-comparison.pytorch, 2018 (ac-
cessed September 25, 2018).
[8] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[9] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” in International Conference
on Machine Learning (ICML), 2015, pp. 448–456.
[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,
2015.
[11] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and
K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer param-
eters and< 0.5 mb model size,” arXiv preprint arXiv:1602.07360, 2016.
[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE, 2016, pp. 770–778.
[13] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Conference on Com-
puter Vision and Pattern Recognition (CVPR). IEEE, 2016, pp. 2818–
6 VOLUME 4, 2018
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
(a) (b)
FIGURE 3: Top-1 accuracy vs. number of images processed per second (with batch size 1) using the Titan Xp (a) and Jetson
TX1 (b).
TABLE 2: Memory consumption of the different DNN mod-
els considered on the Titan Xp for different batch sizes.
DNN 1 2 4 8 16 32 64
AlexNet 0.82 0.83 0.83 0.83 0.84 0.92 0.99
BN-Inception 0.67 0.71 0.81 0.97 1.29 1.97 3.24
CaffeResNet-101 0.78 0.80 0.80 0.80 0.81 0.95 1.06
DenseNet-121 (k=32) 0.67 0.67 0.67 0.69 0.71 0.71 0.99
DenseNet-169 (k=32) 0.87 0.87 0.88 0.91 0.93 0.97 1.04
DenseNet-201 (k=32) 0.72 0.72 0.73 0.75 0.77 0.80 0.87
DenseNet-161 (k=48) 0.76 0.77 0.77 0.80 0.82 0.88 0.96
DPN-68 0.65 0.65 0.66 0.67 0.67 0.68 0.71
DPN-98 0.87 0.88 0.90 0.92 0.98 1.10 1.29
DPN-131 0.95 0.95 0.96 0.97 1.05 1.04 1.28
FBResNet-152 0.89 0.90 0.92 0.94 0.97 1.12 1.31
GoogLeNet 0.66 0.70 0.76 0.87 1.09 1.51 2.35
Inception-ResNet-v2 0.87 0.88 0.88 0.89 0.91 0.95 1.02
Inception-v3 0.72 0.73 0.75 0.75 0.77 0.83 0.92
Inception-v4 0.80 0.81 0.82 0.84 0.90 0.90 1.18
MobileNet-v1 0.63 0.64 0.64 0.65 0.67 0.71 0.78
MobileNet-v2 0.63 0.63 0.63 0.64 0.66 0.70 0.78
NASNet-A-Large 1.09 1.19 1.38 1.78 2.56 4.12 7.26
NASNet-A-Mobile 0.63 0.65 0.67 0.71 0.79 0.93 1.23
ResNet-101 0.82 0.83 0.86 0.93 1.08 1.37 1.94
ResNet-152 0.89 0.90 0.92 1.00 1.15 1.43 2.01
ResNet-18 0.67 0.68 0.68 0.69 0.71 0.75 0.89
ResNet-34 0.74 0.74 0.75 0.80 0.90 1.09 1.47
ResNet-50 0.74 0.74 0.77 0.85 0.99 1.28 1.86
ResNeXt-101 (32x4d) 0.77 0.78 0.78 0.79 0.84 0.87 1.06
ResNeXt-101 (64x4d) 0.90 0.92 0.91 0.96 1.01 1.19 1.38
SE-ResNet-101 1.09 1.11 1.10 1.13 1.13 1.27 1.36
SE-ResNet-152 1.19 1.21 1.25 1.35 1.54 1.93 2.69
SE-ResNet-50 1.02 1.04 1.08 1.18 1.38 1.76 2.53
SE-ResNeXt-101 (32x4d) 1.07 1.07 1.08 1.09 1.12 1.16 1.25
SE-ResNeXt-50 (32x4d) 1.00 1.03 1.08 1.19 1.38 1.76 2.53
SENet-154 1.31 1.32 1.33 1.36 1.40 1.48 1.65
ShuffleNet 0.91 0.91 0.92 0.93 0.95 0.99 1.05
SqueezeNet-v1.0 0.92 0.92 0.92 0.93 0.94 0.97 1.02
SqueezeNet-v1.1 0.90 0.90 0.91 0.92 0.94 0.99 1.07
VGG-11 1.41 1.43 1.43 1.43 1.53 1.55 1.81
VGG-11_BN 1.44 1.49 1.59 1.78 2.39 3.59 5.99
VGG-13 1.44 1.43 1.51 1.60 2.02 2.41 3.99
VGG-13_BN 1.44 1.49 1.59 1.78 2.39 3.59 5.99
VGG-16 1.46 1.51 1.61 1.80 2.41 3.61 6.02
VGG-16_BN 1.46 1.51 1.61 1.80 2.41 3.61 6.02
VGG-19 1.49 1.54 1.63 1.83 2.43 3.64 6.04
VGG-19_BN 1.49 1.54 1.63 1.83 2.43 3.64 6.04
Xception 1.03 1.05 1.06 1.08 1.16 1.24 1.53
GB <1 <2 <3 <4 <5 <6 <7 >=7
FIGURE 4: Plot of the initial static allocation of the model
parameters (i.e. the model complexity) and the total memory
utilization with batch size 1 on the Titan Xp.
2826.
[14] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4,
inception-resnet and the impact of residual connections on learning,”
in International Conference Learning Representations (ICLR) Workshop,
2016.
[15] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely
connected convolutional networks,” in Conference on Computer Vision
and Pattern Recognition (CVPR), vol. 1, no. 2. IEEE, 2017, p. 3.
[16] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 5987–5995.
[17] F. Chollet, “Xception: Deep learning with depthwise separable convo-
lutions,” in Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE, 2017.
[18] Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng, “Dual path networks,”
in Advances in Neural Information Processing Systems (NIPS), 2017, pp.
4467–4475.
[19] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Con-
ference on Computer Vision and Pattern Recognition (CVPR). IEEE,
VOLUME 4, 2018 7
Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
TABLE 3: Top 5 models (sorted in decreasing Top-1 accuracy) satisfying memory consumption (≤0.7GB, ≤1.0GB, ≤1.4GB)
and inference speed (≥15FPS, ≥30FPS, ≥60FPS) constraints on the Titan Xp (a) and Jetson TX1 (b).
Titan Xp Jetson
≤ 0.7GB, @15FPS Acc. ≤ 0.7GB, @30FPS Acc. ≤ 0.7GB, @60FPS Acc. ≤ 0.7GB, @15FPS Acc. ≤ 0.7GB, @30FPS Acc. ≤ 0.7GB, @60FPS Acc.
DPN-68 75.95 DPN-68 75.95 DPN-68 75.95 DenseNet-121 (k=32) 74.47 MobileNet-v2 71.81 MobileNet-v1 69.52
DenseNet-121 (k=32) 74.47 DenseNet-121 (k=32) 74.47 DenseNet-121 (k=32) 74.47 BN-Inception 73.48 ResNet-18 69.64
NASNet-A-Mobile 74.10 NASNet-A-Mobile 74.10 BN-Inception 73.48 MobileNet-v2 71.81 MobileNet-v1 69.52
BN-Inception 73.48 BN-Inception 73.48 MobileNet-v2 71.81 ResNet-18 69.64 GoogLeNet 66.45
MobileNet-v2 71.81 MobileNet-v2 71.81 ResNet-18 69.64 MobileNet-v1 69.52
≤ 1.0GB, @15FPS Acc. ≤ 1.0GB, @30FPS Acc. ≤ 1.0GB, @60FPS Acc. ≤ 1.0GB, @15FPS Acc. ≤ 1.0GB, @30FPS Acc. ≤ 1.0GB, @60FPS Acc.
Inception-ResNet-v2 80.28 Inception-ResNet-v2 80.28 SE-ResNeXt-50(32x4d) 79.11 ResNet-50 76.01 MobileNet-v2 71.81 MobileNet-v1 69.52
Inception-v4 80.10 Inception-v4 80.10 ResNet-152 78.25 DenseNet-121 (k=32) 74.47 ResNet-18 69.64 SqueezeNet-v1.1 58.18
DPN-131 79.44 DPN-131 79.44 Inception-v3 77.50 BN-Inception 73.48 MobileNet-v1 69.52
DPN-98 79.23 DPN-98 79.23 FBResNet-152 77.44 ResNet-34 73.27 GoogLeNet 66.45
SE-ResNeXt-50(32x4d) 79.11 SE-ResNeXt-50(32x4d) 79.11 ResNet-101 77.31 MobileNet-v2 71.81 SqueezeNet-v1.1 58.18
≤ 1.4GB, @15FPS Acc. ≤ 1.4GB, @30FPS Acc. ≤ 1.4GB, @60FPS Acc. ≤ 1.4GB, @15FPS Acc. ≤ 1.4GB, @30FPS Acc. ≤ 1.4GB, @60FPS Acc.
NASNet-A-Large 82.50 NASNet-A-Large 82.50 SE-ResNeXt-50(32x4d) 79.11 ResNet-50 76.01 MobileNet-v2 71.81 MobileNet-v1 69.52
SENet-154 81.32 Inception-ResNet-v2 80.28 Xception 78.79 DenseNet-121 (k=32) 74.47 ResNet-18 69.64 SqueezeNet-v1.1 58.18
Inception-ResNet-v2 80.28 SE-ResNeXt-101(32x4d) 80.28 SE-ResNet-101 78.42 BN-Inception 73.48 MobileNet-v1 69.52
SE-ResNeXt-101(32x4d) 80.28 Inception-v4 80.10 ResNet-152 78.25 ResNet-34 73.27 GoogLeNet 66.45
Inception-v4 80.10 DPN-131 79.44 SE-ResNet-50 77.61 MobileNet-v2 71.81 SqueezeNet-v1.1 58.18
2018.
[20] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable
architectures for scalable image recognition,” in Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 2018.
[21] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.
[22] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-
bilenetv2: Inverted residuals and linear bottlenecks,” in Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE, 2018, pp.
4510–4520.
[23] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely
efficient convolutional neural network for mobile devices,” in Conference
on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.
[24] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
neural networks with pruning, trained quantization and huffman coding,”
arXiv preprint arXiv:1510.00149, 2015.
SIMONE BIANCO is assistant professor of Com-
puter Science at Department of Informatics, Sys-
tems and Communication of the University of
Milano-Bicocca, Italy. He obtained the PhD in
Computer Science from the University of Milano-
Bicocca, in 2010. He obtained the BSc and the
MSc degree in Mathematics from the University
of Milano-Bicocca, respectively in 2003 and 2006.
His current research interests include computer
vision, machine learning, optimization algorithms,
and color imaging.
REMI CADENE is a PhD student and teaching
assistant at LIP6 (Computer Science laboratory) of
Sorbonne University, France. In 2016, he received
a MSc degree in Computer Science at Sorbonne
University. His primary research interests are in
the fields of Machine Learning, Computer Vision
and Natural Language Processing. He is currently
focusing on Neural Networks, Multimodal Learn-
ing and Weakly Supervised Learning.
LUIGI CELONA is currently a postdoctoral fel-
low at DISCo (Department of Informatics, Sys-
tems and Communication) of the University of
Milano-Bicocca, Italy. In 2018 and 2014, he ob-
tained respectively the PhD and the MSc degree in
Computer Science at DISCo. In 2011, he obtained
the BSc degree in Computer Science from the
University of Messina. His current research in-
terests focus on image analysis and classification,
machine learning and face analysis.
PAOLO NAPOLETANO is assistant professor of
Computer Science at Department of Informatics,
Systems and Communication of the University of
Milano-Bicocca. In 2007, he received a PhD in
Information Engineering from the University of
Salerno. In 2003, he received a Master’s degree
in Telecommunications Engineering from the Uni-
versity of Naples Federico II. His current research
interests include machine learning for multi-modal
data classification and understanding.
8 VOLUME 4, 2018
"
5,"Food Recognition using Fusion of Classifiers
based on CNNs
Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
Universitat de Barcelona and Computer Vision Center, Spain
{eduardo.aguilar,marc.bolanos,petia.ivanova}@ub.edu
Abstract. With the arrival of convolutional neural networks, the com-
plex problem of food recognition has experienced an important improve-
ment in recent years. The best results have been obtained using methods
based on very deep convolutional ceural cetworks, which show that the
deeper the model,the better the classification accuracy will be obtain.
However, very deep neural networks may suffer from the overfitting prob-
lem. In this paper, we propose a combination of multiple classifiers based
on different convolutional models that complement each other and thus,
achieve an improvement in performance. The evaluation of our approach
is done on two public datasets: Food-101 as a dataset with a wide variety
of fine-grained dishes, and Food-11 as a dataset of high-level food cate-
gories, where our approach outperforms the independent CNN models.
Keywords: Food Recognition, Fusion Classifiers, CNN
1 Introduction
In the field of computer vision, food recognition has caused a lot of interest
for researchers considering its applicability in solutions that improve people’s
nutrition and hence, their lifestyle [1]. In relation to the healthy diet, traditional
strategies for analyzing food consumption are based on self-reporting and manual
quantification [2]. Hence, the information used to be inaccurate and incomplete
[3]. Having an automatic monitoring system and being able to control the food
consumption is of vital importance, especially for the treatment of individuals
who have eating disorders, want to improve their diet or reduce their weight.
Food recognition is a key element within a food consumption monitoring
system. Originally, it has been approached by using traditional approaches [4,5],
which extracted ad-hoc image features by means of algorithms based mainly on
color, texture and shape. More recently, other approaches focused on using Deep
Learning techniques [5,6,7,8]. In these works, feature extraction algorithms are
not hand-crafted and additionally, the models automatically learn the best way
to discriminate the different classes to be classified. As for the results obtained,
there is a great difference (more than 30%) between the best method based
on hand-crafted features compared to newer methods based on Deep Learning,
where the best results have been obtained with Convolutional Neural Networks
(CNN) architectures that used inception modules [8] or residual networks [7].
ar
X
iv
:1
70
9.
04
86
4v
1 
 [c
s.C
V]
  1
4 S
ep
 20
17
2 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
Fig. 1. Example images of Food-101 dataset. Each image represents a dish class.
Food recognition can be considered as a special case of object recognition,
being the most active topic in computer vision lately. The specific part is that
dish classes have a much higher inter-class similarity and intra-class variation
than usual ImageNet objects (cars, animals, rigid objects, etc.) (see Fig.1). If we
analyze the last accuracy increase in the ImageNet Large Scale Visual Recog-
nition Challenge (ILSVRC) [9], it has been improving thanks to the depth in-
crease of CNN models [10,11,12,13] and also thanks to the use of CNNs fusion
strategies[11,13]. The main problem of CNNs is the need of large datasets to
avoid overfitting the network as well as the need of high computational power
for training them.
Considering the use of different networks trained on the same data, one can
observe that patterns misclassified by the different models would not necessarily
overlap [14]. This suggests that different classifiers could potentially offer com-
plementary information that can be used to improve the final performance [14].
An option to combine the outputs of different classifiers was proposed in [15],
where the authors used what they call a decision templates scheme instead of
simple aggregation operators such as the product or average. As they showed,
this scheme maintains a good performance using different training set sizes and
is also less sensitive to particular datasets compared to the other schemes.
In this article, we introduce the fusion concept to the CNN framework, with
the purpose of demonstrating that the combination of the classifiers’ output, by
using a decision template scheme, allows to improve the performance on the food
recognition problem. Our contributions are the following: 1) we propose the first
food recognition algorithm that fuses the output of different CNN models, 2) we
show that our CNNs Fusion approach has better performance compared to the
use of CNN models separately, and 3) we demonstrate that our CNNs Fusion
approach keeps a high performance independently of the target (dishes, family
of dishes) and dataset (validating it on 2 public datasets).
The organization of the article is as follows. In section 2, we present the CNNs
Fusion methodology. In section 3, we present the datasets, the experimental setup
and discuss the results. Finally, in section 4, we describe the conclusions.
Food Recognition using Fusion of Classifiers based on CNNs 3
Fig. 2. General scheme of our CNNs Fusion approach.
2 Methodology
In this section, we describe the CNN Fusion methodology (see Fig. 2), which is
composed of two main steps: training K CNN models based on different archi-
tectures and fusing the CNN outputs using the decision templates scheme.
2.1 Training of CNN models
The first step in our methodology involves separately training two CNN models.
We chose two different kind of models winners of the ILSVRC in the object
recognition task. Both models won or are based on the winner of the challenges
made in 2014 and 2015 proposing novel architectures: the first based its design in
”inception models” and the second in ”residual networks”. First, each model was
pre-trained on the ILSVRC data. Later, all layers were fine-tuned by a certain
number of epochs, selecting for each one the model that provides the best results
in the validation set and that will be used in the fusion step.
2.2 Decision templates for classifiers fusion
Once we trained the models on the food dataset, we combined the softmax
classifier outputs of each model using the Decision Template (DT) scheme [15].
Let us annotate the output of the last layer of the k-th CNN model as
(ω1,k, . . . , ωC,k), where c = 1, ..., C is the number of classes and k = 1, ...K
is the index of the CNN model (in our case, K=2). Usually, the softmax func-
tion is applied, to obtain the probability value of model k to classify image x to
a class c: pk,c(x) =
eωk,c∑C
c=1 e
ωk,c
. Let us consider the k-th decision vector Dk:
Dk(x) = [pk,1(x), pk,2(x), ..., pk,C(x)]
Definition [15]: A Decision Profile, DP for a given image x is defined as:
DP (x) =
 p1,1(x) p1,2(x) ... p1,C(x). . .
pK,1(x) pK,2(x) ... pK,C(x)
 (1)
4 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
Definition [15]: Given N training images, a Decision Template is defined
as a set of matrices DT = (DT 1, . . . , DTC), where the c-th element is obtained
as the average of the decision profiles (1) on the training images of class c:
DT c =
∑N
j=1DP (xj)× Ind(xj , c)∑N
j=1 Ind(xj , c)
,
where Ind(xj , c) is an indicator function with value 1 if the training image
xj has a crisp label c, and 0, otherwise [16].
Finally, the resulting prediction for each image is determined considering the
similarity s(DP (x), DT c(x)) between the decision profile DP (x) of the test im-
age and the decision template of class c, c = 1, . . . , C. Regarding the arguments
of the similarity function s(., .) as fuzzy sets on some universal set with K × C
elements, various fuzzy measures of similarity can be used. We chose different
measures [15], namely 2 measures of similarity, 2 inclusion indices, a consistency
measure and the Euclidean Distance. These measures are formally defined as:
S1(DT
c, DP (x)) =
∑K
k=1
∑C
i=1 min(DT
c
k,i, DPk,i(x))∑K
k=1
∑C
i=1 max(DT
c
k,i, DPk,i(x))
,
S2(DT
c, DP (x)) = 1− sup
u
{∣∣DT ck,i −DPk,i(x)∣∣ : c = 1, . . . , C, k = 1, . . . ,K},
I1(DT
c, DP (x)) =
∑K
k=1
∑C
i=1 min(DT
c
k,i, DPk,i(x))∑K
k=1
∑C
i=1DT
c
k,i
,
I2(DT
c, DP (x)) = inf
u
{max(DT ck,i, DPk,i(x)) : c = 1, . . . , C, k = 1, . . . ,K},
C(DT c, DP (x)) = sup
u
{min(DT ck,i, DPk,i(x)) : c = 1, . . . , C, k = 1, . . . ,K},
N(DT c, DP (x)) = 1−
∑K
k=1
∑C
i=1(DT
c
k,i −DPk,i(x))2
K × C ,
where DT ck,i is the probability assigned to the class i by the classifier k in the
DT c, DT ck,i is the complement of DT
c
k,i calculated as 1 − DT ck,i, and DPk,i(x)
is the probability assigned by the classifier k to the class i in the DP calculated
for the image, x. The final label, L is obtained as the class that maximizes the
similarity, s between DP (x) and DT c: L(x) = argmaxc=1,...,C{s(DT c, DP (x))}.
3 Experiments
3.1 Datasets
The data used to evaluate our approach are two very different images: Food-11
[17] and Food-101 [4], which are chosen in order to verify that the classifiers
Food Recognition using Fusion of Classifiers based on CNNs 5
fusion provides good results regardless of the different properties of the tar-
get datasets, such as intra-class variability (the first one is composed of many
dishes of the same general category, while the second one is composed of specific
fine-grained dishes), inter-class similarity, number of images, number of classes,
images acquisition condition, among others.
Food-11 is a dataset for food recognition [17], which contains 16,643 im-
ages grouped into 11 general categories of food: pasta, meat, dessert, soup, etc.
(see Fig. 3). The images were collected from existing food datasets (Food-101,
UECFOOD100, UECFOOD256) and social networks (Flickr, Instagram). This
dataset has an unbalanced number of images for each class with an average of
1,513 images per class and a standard deviation of 702. For our experiments, we
used the same data split, images and proportions, provided by the authors [17].
These are divided as 60% for training, 20% for validation and 20% for test. In
total, it consists of 9,866, 3,430 and 3,347 images for each set, respectively.
Fig. 3. Images from the Food-11 dataset. Each image corresponds to a different class.
Food-101 is a standard to evaluate the performance of visual food recogni-
tion [4]. This dataset contains 101.000 real-world food images downloaded from
foodspotting.com, which were taken under unconstrained conditions. The au-
thors chose the top 101 most popular classes of food (see Fig. 1) and collected
1,000 images for each class: 75% for training and 25% for testing. With respect
to the classes, these consist of very diverse and fine-grained dishes of various
countries, but also with highly intra-class variation and inter-class similarity in
most occasions. In our experiments, we used the same data splits provided by the
authors. Unlike Food-11, and keeping the procedure followed by other authors
[5,7,8], in this dataset we directly validate and test on the same data split.
3.2 Experimental Setup
Every CNN model was previously trained on the ILSVRC dataset. Following,
we adapted them by changing the output of the models to the number of classes
for each target dataset and fine-tuned the models using the new images. For the
training of the CNN models, we used the Deep Learning framework Keras1.
The models chosen for Food-101 dataset due to their performance-efficiency
ratio were InceptionV3 [18] and ResNet50 [13]. Both models were trained during
48 epochs with a batch size of 32, and a learning rate of 5 × 10−3 and 1 ×
10−3, respectively. In addition, we applied a decay of 0.1 during the training
of InceptionV3 and of 0.8 for ResNet50 every 8 epochs. The parameters were
chosen empirically by analyzing the training loss.
1 www.keras.io
6 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
As to the Food-11 dataset, we kept the ResNet50 model, but changed In-
ceptionV3 by GoogLeNet [12], since InceptionV3 did not generalize well over
Food-11. We believe that the reason is the small number of images for each class
not sufficient to avoid over-fitting; the model quickly obtained a good result
in the training set, but a poor performance on the validation set. GoogLeNet
and Resnet50 were trained during 32 epochs with a batch size of 32 and 16,
respectively. The other parameters used for the ResNet50 were the same used
for Food-101. In the case of GoogLeNet, we used a learning rate of 1× 10−3 and
applied a decay of 0.1 during every 8 epochs, that turned out empirically the
optimal parameters for our problem.
3.3 Data Preprocessing and Metrics
The preprocessing made during the training, validation and testing phases was
the following. During the training of our CNN models, we applied different pre-
processing techniques on the images with the aim of increasing the samples and
to prevent the over-fitting of the networks. First, we resized the images keeping
the original aspect ratio as well as satisfying the following criteria: the smallest
side of the resulting images should be greater than or equal to the input size
of the model; and the biggest side should be less than or equal to the maximal
size defined in each model to make random crops on the image. In the case of
InceptionV3, we set to 320 pixels as maximal size, for GoogLeNet and ResNet50
the maximal size was defined as 256 pixels. After resizing the images, inspired
by [8], we enhanced them by means of a series of random distortions such as:
adjusting color balance, contrast, brightness and sharpness. Finally, we made
random crops of the images, with a dimension of 299x299 for InceptionV3 and
of 224x224 for the other models. Then, we applied random horizontal flips with
a probability of 50%, and subtracted the average image value of the ImageNet
dataset. During validation, we applied a similar preprocessing, with the differ-
ence that we made a center crop instead of random crops and that we did not
apply random horizontal flips. During test, we followed the same procedure than
in validation (1-Crop evaluation). Furthermore, we also evaluated the CNN using
10-Crops, which are: upper left, upper right, lower left, lower right and center
crop, both in their original setup and also applying an horizontal flip [10]. As for
10-Crops evaluation, the classifier gets a tentative label for each crop, and then
majority voting is used over all predictions. In the cases where two labels are
predicted the same number of times, the final label is assigned comparing their
highest average prediction probability.
We used four metrics to evaluate the performance of our approach, overall
Accuracy (ACC), Precision (P), Recall (R), and F1 score.
3.4 Experimental Results on Food-11
The results obtained during the experimentation on Food-11 dataset are shown
in Table 1 giving the error rate (1 - accuracy) for the best CNN models, compared
to the CNNs Fusion. We report the overall accuracy by processing the test data
Food Recognition using Fusion of Classifiers based on CNNs 7
using two procedures: 1) a center crop (1-Crop), and 2) using 10 different crops
of the image (10-Crops). The experimental results show an error rate of less
than 10 % for all classifiers, achieving a slightly better performance when using
10-Crops. The best accuracy is achieved with our CNNs Fusion approach, which
is about 0.75% better than the best result of the classifiers evaluated separately.
On the other hand, the baseline classification on Food-11 was given by their
authors, who obtained an overall accuracy of 83.5% using GoogLeNet models
fine-tuned in the last six layers without any pre-processing and post-processing
steps. Note that the best results obtained with our approach have been using
the pointwise measures (S2, I2). The particularity of these measures is that they
penalize big differences between corresponding values of DTs and DP being from
the specific class to be assigned as the rest of the class values. From now on, in
this section we only report the results based on the 10-Crops procedure.
Table 1. Overall test set error rate of Food-11 obtained for each model. The distance
measure is shown between parenthesis in the CNNs Fusion models.
Authors Model 1-Crop 10-Crops N/A
[17] GoogLeNet - - 16.5%
us GoogLeNet 9.89% 9.29% -
us ResNet50 6.57% 6.39% -
us CNNs Fusion (S1) 6.36% 5.86% -
us CNNs Fusion (S2) 6.12% 5.65% -
us CNNs Fusion (I1) 6.36% 5.89% -
us CNNs Fusion (I2) 6.30% 5.65% -
us CNNs Fusion (C) 6.45% 6.07% -
us CNNs Fusion (N) 6.36% 5.92% -
As shown in Table 2, the CNNs Fusion is able to properly classify not only
the images that were correctly classified by both baselines, but in some occasions
also when one or both fail. This suggests that in some cases both classifiers may
be close to predicting the correct class and combining their outputs can make a
better decision.
Table 2. Percentage of images well-classified and misclassified on Food-11 using our
CNNs Fusion approach, distributed by the results obtained with GoogLeNet (CNN1)
and ResNet50 (CNN2) models independently evaluated.
CNNs evaluated independently
CNNs Fusion Both wrong CNN1 wrong CNN2 wrong Both fine
Well-classified 3.08% 81.77% 54.76% 99.97%
Misclassified 96.92% 18.23% 45.24% 0.03%
Samples misclassified by our model are shown in Fig. 4, where most of them
are produced by mixed items, high inter-class similarity and wrongly labeled
images. We show the ground truth (top) and the predicted class (bottom) for
each sample image.
8 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
Fig. 4. Misclassified Food-11 examples: predicted labels (on the top), and the
groundtruth (on the bottom).
In Table 3, we show the precision, recall and F1 score obtained for each
class separately. By comparing the F1 score, the best performance is achieved
for the class Noodles Pasta and the worst for Dairy products. Specifically, the
class Noddles Pasta only has one image misclassified, which furthermore is a
hard sample, because it contains two classes together (see items mixed in Figure
4). Considering the precision, the worst results are obtained for the class Bread,
which is understandable considering that bread can sometimes be present in
other classes. In the case of recall, the worst results are obtained for Dairy
products, where an error greater than 8% is produced for misclassifying several
images as class Dessert. The cause of this is mainly, because the class Dessert has
a lot of items in their images that could also belong to the class Dairy products
(e.g. frozen yogurt or ice cream) or that are visually similar.
Table 3. Some results obtained on the Food-11 using our CNNs Fusion approach.
Class #Images Precision Recall F1
Bread 368 88.95% 91.85% 90.37%
Dairy products 148 89.86% 83.78% 86.71%
Meat 432 94.12% 92.59% 93.35%
Noodles Pasta 147 100.00% 99.32% 99.66%
Rice 96 94.95% 97.92% 96.41%
Vegetable Fruit 231 98.22% 95.67% 96.93%
3.5 Experimental Results on Food-101
The overall accuracy on Food-101 dataset is shown in Table 4 for two classi-
fiers based on CNN models, and also for our CNNs Fusion. The overall accu-
racy is obtained by means of the evaluation of the prediction using 1-Crop and
10-Crops. The experimental results show better performance (about 1% more)
using 10-Crops instead of 1-Crop. From now on, in this section we only report
the results based on the 10-Crops procedure. In the same way as observed in
Food-11, the best accuracy obtained with our approach was by means of point-
wise measures S2, I2, where the latter provides a slightly better performance.
Food Recognition using Fusion of Classifiers based on CNNs 9
Again, the best accuracy is also achieved by the CNNs Fusion, which is about
1.5% higher than the best result of the classifiers evaluated separately. Note that
the best performance on Food-101 (overall accuracy of 90.27%) was obtained us-
ing WISeR [7]. In addition, the authors show the performance by another deep
learning-based approaches, in which three CNN models achieved over a 88% (In-
ceptionV3, ResNet200 and WRN [19]). However, WISeR, WRN and ResNet200
models were not considered in our experiments since they need a multi-GPU
server to replicate their results. In addition, those models have 2.5 times more
parameters than the models chosen, which involve a high cost computational
especially during the learning stage. Following the article steps, our best results
replicating the methods were those using InceptionV3 and ResNet50 models used
as a base to evaluate the performance of our CNNs Fusion approach.
Table 4. Overall test set accuracy of Food-101 obtained for each model.
Author Model 1-Crop 10-Crops N/A
[8] InceptionV3 - - 88.28%
[7] ResNet200 - 88.38% -
[7] WRN - 88.72% -
[7] WISeR - 90.27% -
us ResNet50 82.31% 83.54% -
us InceptionV3 83.82% 84.98% -
us CNNs Fusion (S1) 85.52% 86.51% -
us CNNs Fusion (S2) 86.07% 86.70% -
us CNNs Fusion (I1) 85.52% 86.51% -
us CNNs Fusion (I2) 85.98% 86.71% -
us CNNs Fusion (C) 85.24% 86.09% -
us CNNs Fusion (N) 85.53% 86.50% -
As shown in Table 5, in this dataset the CNNs Fusion is also able to properly
classify not only the images that were correctly classified for both classifiers,
but also when one or both fail. Therefore, we demonstrate that our proposed
approach maintains its behavior independently of the target dataset.
Table 5. Percentage of images well-classified and misclassified on Food-101 using our
CNNs Fusion approach, distributed by the results obtained with InceptionV3 (CNN1)
and ResNet50 (CNN2) models independently evaluated.
CNNs evaluated independently
CNNs Fusion Both wrong CNN1 wrong CNN2 wrong Both fine
Well-classified 1.95% 73.07% 64.95% 99.97%
Misclassified 98.05% 26.93% 35.05% 0.03%
Some examples of FPs and FNs are shown in Figure 5. Analyzing the misclas-
sified images like Steak, we chose a representive images per class corresponding
to the top three classes with highest error, and in Edamame the unique im-
age misclassified. Regarding the statistical results, in Table 6 are shown the top
3 better and worst classification results on Food-101. We highlight the classes
10 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
Fig. 5. Misclassified examples for the Food-101 classes that obtained the worst (steak)
and best (edamame) classification results by F1 score (groundtruth label - bottom).
with the worst and best results. As for the worst class (Steak), the precision and
recall achieved are 60.32% and 59.60%, respectively. Interestingly, about 26%
error in the precision and 30% error in the recall is produced with only three
classes: Filet mignon, Pork chop and Prime rib. As shown in Fig. 5, these are
fine-grained classes with high inter-class similarities that imply high difficulty
for the classifier, because it is necessary to identify small details that allow to
determine the corresponding class of the images. On the other hand, the best
class (Edamame) was classified achieving 99.60% of precision and 100% of re-
call. Unlike Steak, Edamame is a simple class to classify, because it has a low
intra-class variation and low inter-class similarities. In other words, the images
in this class have a similar visual appearance and they are quite different from
the images of the other classes.
Table 6. Top 3 better and worst classification results on Food-101.
Class Precision Recall F1
Spaghetti Bolognese 94.47% 95.60% 95.03%
Macarons 97.15% 95.60% 96.37%
Edamame 99.60% 100.00% 99.80%
Steak 60.32% 59.60% 59.96%
Pork Chop 75.71% 63.60% 69.13%
Foie Gras 72.96% 68.00% 70.39%
4 Conclusions
In this paper, we addressed the problem of food recognition and proposed a CNNs
Fusion approach based on the concepts of decision templates and decision profiles
and their similarity that improves the classification performance with respect to
using CNN models separately. Evaluating different similarity measures, we show
that the optimal one is based on the infinimum of the maximum between the
complementary of the decision templates and the decision profile of the test
images. On Food-11, our approach outperforms the baseline accuracy by more
than 10% of accuracy. As for Food-101, we used two CNN architectures providing
the best state of the art results where our CNNs Fusion strategy outperformed
Food Recognition using Fusion of Classifiers based on CNNs 11
them again. As a future work, we plan to evaluate the performance of the CNN
Fusion strategy as a function of the number of CNN models.
Acknowledgement
This work was partially funded by TIN2015-66951-C2, SGR 1219, CERCA,
ICREA Academia’2014, CONICYT Becas Chile, FPU15/01347 and Grant 20141510
(Marato´ TV3). We acknowledge Nvidia Corp. for the donation of Titan X GPUs.
References
1. Waxman, A., Norum, K.R.: WHO Global Strategy on Diet, Physical Activity and
Health. Food and Nutrition Bulletin. 25, 292-302 (2004).
2. Shim, J.-S., Oh, K., Kim, H.C.: Dietary assessment methods in epidemiologic stud-
ies. Epidemiology and Health. e2014009 (2014).
3. Rumpler, W. V, Kramer, M., Rhodes, D.G., Moshfegh, a J., Paul, D.R.: Identifying
sources of reporting error using measured food intake. European Journal of Clinical
Nutrition. 62, 544-552 (2008).
4. Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 - Mining discriminative com-
ponents with random forests. In: Lecture Notes in Computer Science. pp. 446-461
(2014).
5. Liu, C., Cao, Y., Luo, Y., Chen, G., Vokkarane, V., Ma, Y.: Deepfood: Deep learning-
based food image recognition for computer-aided dietary assessment. In: Lecture
Notes in Computer Science. pp. 37-48 (2016).
6. Yanai, K., Kawano, Y.: Food image recognition using deep convolutional network
with pre-training and fine-tuning. In: ICMEW. pp. 1-6 (2015).
7. Martinel, N., Foresti, G.L., Micheloni, C.: Wide-Slice Residual Networks for Food
Recognition. arXiv Prepr. (2016).
8. Hassannejad, H., Matrella, G., Ciampolini, P., De Munari, I., Mordonini, M.,
Cagnoni, S.: Food Image Recognition Using Very Deep Convolutional Networks.
In: Proceedings of the 2nd International Workshop on MADiMa. pp. 41-49 (2016).
9. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision. 115,
211-252 (2015).
10. Krizhevsky, A., Sutskever, I., Geoffrey E., H.: ImageNet Classification with Deep
Convolutional Neural Networks. Adv. Neural Inf. Process. Syst. 25. 1-9 (2012).
11. Zeiler, M.D., Fergus, R.: Visualizing and Understanding Convolutional Networks.
In: Computer VisionECCV. pp. 818-833 (2014).
12. Szegedy, C., Wei Liu, Yangqing Jia, Sermanet, P., Reed, S., Anguelov, D., Erhan,
D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. pp.
1-9 (2015).
13. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.
In: CVPR. pp. 770-778 (2016).
14. Kittler, J., Hatef, M.: On combining classifiers. IEEE Transactions on Pattern
Analysis and Machine Intelligence. 20, 226-239 (1998).
15. Kuncheva, L.I., Bezdek, J.C., Duin, R.P.W.: Decision templates for multiple clas-
sifier fusion: an experimental comparison. Pattern Recognition. 34, 299-314 (2001).
12 Eduardo Aguilar, Marc Bolan˜os, and Petia Radeva
16. Kuncheva, L.I., Kounchev, R.K. , Zlatev, R.Z.: Aggregation of multiple classifica-
tion decisions by fuzzy templates. In: EUFIT. 1470-1474 (1995).
17. Singla, A., Yuan, L., Ebrahimi, T.: Food/Non-food Image Classification and Food
Categorization using Pre-Trained GoogLeNet Model. In: Proceedings of the 2nd
International Workshop on MADiMa. pp. 3-11 (2016).
18. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the Incep-
tion Architecture for Computer Vision. In: CVPR. pp. 2818-2826 (2016).
19. Zagoruyko, S., Komodakis, N.: Wide Residual Networks. arXiv Prepr. (2016).
"
6,"1Communication Theoretic Data Analytics
Kwang-Cheng Chen, Shao-Lun Huang, Lizhong Zheng, H. Vincent Poor
Abstract—Widespread use of the Internet and social
networks invokes the generation of big data, which is
proving to be useful in a number of applications. To deal
with explosively growing amounts of data, data analytics
has emerged as a critical technology related to computing,
signal processing, and information networking. In this pa-
per, a formalism is considered in which data is modeled as a
generalized social network and communication theory and
information theory are thereby extended to data analytics.
First, the creation of an equalizer to optimize information
transfer between two data variables is considered, and
financial data is used to demonstrate the advantages. Then,
an information coupling approach based on information
geometry is applied for dimensionality reduction, with a
pattern recognition example to illustrate the effectiveness.
These initial trials suggest the potential of communication
theoretic data analytics for a wide range of applications.
Index Terms—big data, social networks, data analysis,
communication theory, information theory, information
coupling, equalization, information fusion, data mining,
knowledge discovery, information centric processing
I. INTRODUCTION
With the booming of Internet and mobile commu-
nications, (big) data analytics has emerged as a crit-
ical technology, adopting techniques such as machine
learning, graphical models, etc. to mine desirable infor-
mation for a wide array of information communication
technology (ICT) applications [1][2][3][4][5][6]. Data
mining or knowledge discovery in databases (KDD)
has been used as a synonym for analytics on computer
generated data. To achieve the purpose of data analytics,
there are several major steps: (i) based on the selection
of data set(s), pre-processing of the data for effective
or easy computation, (ii) processing of data or data
mining, likely adopting techniques from statistical infer-
ence and artificial intelligence, and (iii) post-processing
to appropriately interpret results of data analytics as
knowledge. Knowledge discovery aims at either verifi-
cation of user hypotheses or prediction of future patterns
This research was supported in part by the US Air Force under the
grant AOARD-14-4053, the US NSF grant CCF-1216476, the U.S.
Army Research Office under MURI Grant W911NF-11-1-0036, and
the U. S. National Science Foundation under Grant CCF-1420575.
K.C. Chen and S.-L. Huang are with the Graduate Institute of Com-
munication Engineering, National Taiwan University, ckc@ntu.edu.tw
S.-L. Huang and Lizhong Zheng are with the Department of Elec-
trical Engineering and Computer Science, Massachusetts Institute of
Technology, shaolin@mit.edu and lizhong@mit.edu
H.V. Poor is with the Department of Electrical Engineering, Prince-
ton University, poor@princeton.edu
from data/observations. Statistical methodologies deal
with uncertain or nondeterministic reasoning and thus
models, and are the focus of this paper. Machine learning
and artificial intelligence are useful to analyze data,
e.g. [2][3][7], typically via regression and/or classifi-
cation. With advances in supervised and unsupervised
learning, inferring the structure of knowledge, such as
inferring Bayesian network structure from data, is one of
the most useful information technologies [8]. In recent
decades, considerable research effort has been devoted
to various aspects of data mining and data analysis,
but effective data analytics are still needed to address
the explosively growing amount of data resulting from
Internet, mobile, and social networks.
A core technological direction in data analytics lies
in processing high-dimensional data to obtain low-
dimensional information via computational reduction
algorithms, namely by nonlinear approaches [9][10],
compressive sensing [11], or tensor geometric analy-
sis [12]. In spite of remarkable achievements, with the
exponential growth in data volume, it is very desirable to
develop more effective approaches to deal with existing
challenges including effective algorithms of scalable
computation, complexity and data size, outlier detec-
tion and prediction, etc. Furthermore, modern wireless
communication systems and networks to support mobile
Internet and Internet of Things (IoT) applications require
effective transport of information, while appropriate data
analytics enable communication spectral efficiency via
proper context-aware computation [13]. The technologi-
cal challenge for data analytics due to very large numbers
of devices and data volume remains on the list of the
most necessary avenues of inquiry. At this time, state-of-
the-art data analytics primarily deal with data processing
through computational models and techniques, such as
deep learning [14]. There lack efforts to examine the
mechanism of data generation [15] and subsequent rela-
tionships among data, which motivates the investigation
of data analytics by leveraging communication theory
and information theory in this paper.
As indicated in [16] and other works, relationships
among data can be viewed as a type of generalized social
network. The data variables can be treated as nodes in
a network and their corresponding relationships can be
considered to be links governed by random variables (or
random processes by considering a time index). Such
scenarios are particularly relevant for today’s interactive
ar
X
iv
:1
50
1.
05
37
9v
1 
 [c
s.I
T]
  2
2 J
an
 20
15
2Internet data from/related to social networks, social
media, collective behavior from crowds, and sensed data
collected from sensors in cyber-physical systems (CPS)
or Internet of Things. Therefore, with the aid of this
generalized social network concept, we propose a new
communication theoretic methodology of information-
centric processing for (big) data analytics in this pa-
per. Furthermore, by generalizing the scenario from a
communication link into a communication network, we
may use ideas from network information theory and
information geometry to develop a novel technology
known as information coupling [37], which suggests a
new information-centric approach to extraction of low-
dimensional information from high-dimensional data
based on information transfer. These technological op-
portunities describe a complete communication theoretic
view of data analytics.
The rest of this paper is organized as follows. Section
II presents our modeling of data into a generalized social
network and its resemblance to typical communication
system models. Section III describes the setting of our
proposed communication theoretic data analytics to more
effectively process the data, using financial data to
illustrate the processing methodology with comparisons
to well-known techniques. Related literature is reviewed
to better explain our proposed methodology. Section IV
briefly introduces the rationale from information geom-
etry and the principle of information coupling to realize
dimensionality reduction, with an image pattern recogni-
tion example to show the effectiveness of this new idea
based on network information theory. Finally, we make
concluding remarks in Section V, with suggested open
problems to fully understand and to most effectively
revisit data mining and knowledge discovery in (big) data
analytics. In addition to its potential for creating new
methods for data analytics, this new application scenario
also creates a new dimension for communication and
information theory.
II. SOCIAL NETWORK MODELING OF DATA
As noted in [16], entities (e.g. data) with relationships
can be viewed as social networks, and thus social net-
work analysis and statistical communication theory share
commonalities in many cases. For data analytics, it is
common to face a situation in which we have two related
variables, say X and Y . When there exists uncertainty
in observing these two variables, it is common to model
these two variables as random variables. If a time index
is involved, say the variables are observed or sampled in
sequence, these two variables are actually two random
processes. Consequently, each sequence of data drawn
from a variable is actually a sample path (i.e. sampled
data) of the random process. An intuitive way of exam-
ining the relationship between the two processes is to
Fig. 1. Graphical model of network variables for a large data set.
compute the correlation coefficient between these two
sampled data sequences.
For big data problems in an Internet setting, we are
often facing a substantial number of variables up to
thousands or even millions in order, and therefore must
rely on machine learning to handle such scenarios to
predict or otherwise to infer from data. One of the
key problems is to identify low-dimensional information
from high-dimensional data, as a key issue of knowledge
discovery. Recently, another vision, known as small data,
has emerged to more precisely deal with variables of
data on a human scale [17]. Therefore, in data analytics,
whether addressing big data or small data, effective and
precise inference from data is always the fundamental
challenge. An approach different from machine learning
arises by extracting embedded information from data.
More precisely, for example, we may identify the infor-
mation flowing from variable X to variable Y just as in
a typical point-to-point digital communication system.
Unfortunately, real world problems are much more
complicated than a single communication link, and there
are many more variables involved. Figure 1 depicts the
social network structure of a large data set through
realization of graphical models and Bayesian networks,
while each node (i.e. variable) represents a data variable
(actually a vector of data) and each link represents the
relationship and causality between two data variables.
Such relationships between two data variables usually
exhibit uncertainty due to the nature of data or imperfect
observations, and thus require probabilistic modeling.
Even more challenging, such causal relationship among
large numbers of variables may not be known, and thus
a challenge is to determine or to learn the knowledge
discovery structure [6][19][20].
The social network analysis of data can be performed
in different ways, such as using graphical models with
machine learning techniques [2][16]. However, as noted,
such widely applied methodologies focus on data pro-
3Fig. 2. Communication theory in signal flow or graphical model to
show causal relationship in data variables.
cessing and inference, rather than considering informa-
tion transfer. Communications can be generally viewed
as transmission of information from one location to
another location as illustrated in Figure 2(a). We may
further use signal flow of random variables to abstractly
portray such a system as in Figure 2(b). The channel as a
link between random variables X and Y , can be charac-
terized by the condition probability distribution f(y|x).
When a channel involves multiple intermediate variables
relating X and Y , this results in receive diversity as
shown in Figure 2(c).
More advanced communication theory, namely multi-
ple access communication, has been developed in recent
decades and may be useful for Internet data analyt-
ics. Multiuser detection (MUD), though commonly con-
nected with code division multiple access (CDMA), gen-
erally represents situations in which multiple user signals
(no need to be orthogonal) are simultaneously transmit-
ted then detected over the same frequency band [18]. In
such situations, the signal model can be described as
Y = (AR)X+N
where X is the transmitted signal vector; Y is the
received signal vector embedded in noise N; R denotes
the correlation matrix among signals used by transmitter-
receiver pairs and A is a diagonal matrix containing
received amplitudes. The non-diagonal part of AR re-
sults in multiple-access interference (MAI). Similarly, a
multiple antenna (MIMO) signal model can be described
mathematically as
Y = HX+N
where H is the channel gain matrix [21]. From the
similarity in mathematical structure of MUD and MIMO
systems, a duality in receiver structures can be also
observed. This is fundamentally the same information
Fig. 3. Distributed detection for sensor networks.
flow structure as in data analytics, and so multiuser
communication theory provides a new tool to com-
prehend information flow in general social networking
models. In [16], recommender systems are illustrated as
an example for this potential.
In this paper, we will delineate the connection between
data analytics and social network analysis, considering a
link in a generalized social network to be equivalent to a
communication link. Consequently, we can leverage the
knowledge of communication theory to investigate data
analytics, a process we term communication theoretic
data analytics. Processing on graphs to extract motif
information aims at alleviating the complexity of data
analytics [22], and bears somewhat the same spirit as
the communication theoretic data analytics. Section III
will introduce the optimal receiver to tackle nonlinear
distortion by using an equalizer, and thereby to optimize
information transfer for more effective data analytics. A
further interesting communication model is the sensor
network illustrated in Figure 3, where an information
source is detected by multiple sensors that send their
measurements to a fusion center for decisions [23]. The
number of sensors might be large but the actual informa-
tion source might be simple. Directly processing sensor
data might be a big data problem but a single source
may induce simplification by considering information
transfer, which alternatively suggests an information
theoretic formulation of information coupling toward
critical dimension reduction in data analytics, which will
be introduced in Section IV.
III. COMMUNICATION THEORETIC IMPLEMENTATION
OF DATA ANALYTICS AND APPLICATIONS
Using the communication theoretic setup, we will
demonstrate how to deal with practical data analytics.
To infer useful results from big data, we will be able to
acquire some knowledge, say the conditional probability
structure f(yn|xm) in a general social network modeling
of big data as in Figure 1. Through communication the-
ory, we may treat Xm as an information transmitter and
Yn as the information receiver. In our setting, f(yn|xm)
represents the communication channel, which can be
corrupted by noise due to imperfect sampling, noisy
observation, and possible interference from unknown
variables or outliers. This point-to-point scenario is well
studied in communication theory.
4Fig. 4. Equalizer structure to extract the causal relationship between
two variables, from X to Y .
A common and most fundamental scenario in (big)
data analytics is to find the relationship between two
variables, X and Y , while each variable represents a
set or a series of data. Let us borrow the concept
of Bayesian networks or graphical models in machine
learning. We may subsequently infer the realization y of
Y based on the observation of the realization x of X .
Suppose there exists a causal relationship X → Y , with
uncertainty (or embedded in an unknown mechanism
allowing information transfer). As noted above, this
causal relationship can be represented by the conditional
or transition probability f(y|x), which can be considered
as a communication channel. Direct computation and
inference therefore proceed based on this structure. If
we intend to infer the knowledge structure such as this
simple causal relationship, machine learning on (big)
data becomes a principal problem [2][5][6][7].
A. Equalizer to Optimize Information Transfer
Again, our view of this simple causal relationship
considers information transfer from X to Y as a com-
munication channel. In this context, if we want to deter-
mine the causal relationship of data due to information
transfer, according to communication theory, we should
establish an equalizer for the channel to better receive
such information. The equalizer in data analytics is most
appropriately of the form of an adaptive equalizer, with
earlier observed data used for the purpose of training to
obtain the weighting coefficients. The most challenging
task in machine learning is knowledge discovery, i.e.,
identifying the causal relationship among variables. The
communication theoretic approach supplies a new angle
from which to examine this task, and its information
theoretic insight will be investigated in Section IV via
information coupling.
Well known in communication theory, an equalizer
serves as a part of an optimal receiver to detect a
transmitted signal from a noisy channel distorted by
nonlinearities and other impairments. In order to infer
from another data variable or time series, we may take
advantage of the same concept to process the data.
Proper implementation of an equalizer could enhance
the causal relationship between data variables and time
series, and thus allow better judgement or utilization of
the causal relationship. This is one core issue in knowl-
edge discovery of data analytics. In big data analytics,
this knowledge problem has a very large number of
variables. As in Figure 1, we focus on the problem
of identifying the causal relationships between the set
of variables X1, X2, · · · , XM and the set of variables
Y1, Y2, · · · , YN , specified by appropriate weights. This is
identical to the following multiple access communication
problem:
(X1, X2, . . . , XM )[hij ]M×N = (Y1, Y2, . . . , YN )T (1)
where [hij ] = H is analogous to the channel matrix. This
knowledge discovery problem in data analytics is thus
equivalent to a blind channel estimation/identification
problem in multiple access communication. Since a
feedback channel may not exist in general, this is a blind
problem. However, for online processing, equivalent
feedback might not be impossible, which we leave as an
open issue. We start from some simple cases to illustrate
this idea.
• Information diversity: A variable X may influence
a number of variables, say Y1, Y2, · · · , YN , which is
a form of information diversity. To identify a causal
relationship between X and Yn, this is precisely a
multi-channel estimation problem such as arises in
wireless communication. Since feedback in causal
data relationships is generally impossible, such a
class of problems falls under the category of blind
multi-channel estimation/identification [24][25].
• Information fusion: Another class to consider
is the causal relationship from many data vari-
ables to influence a single data variable, say
X1, X2, · · · , XM to Y . This corresponds to multi-
input-single-output channel estimation or identifica-
tion, which is a rather overlooked subject. However,
another similar problem, source separation has been
well studied.
B. Applications to Inference on Financial Time Series
A useful way to demonstrate our analytical method-
ology is to consider financial time series data, which
has been well studied in the literature. The purpose
is to demonstrate the prediction of stock prices from
other factors. In this example, we are trying to predict
the stock price of Taiwan Semiconductor Manufacturing
Corp. (TSMC), which is the world’s largest contract
chip fabrication company. To demonstrate information
transfer and thus communication theoretic data analytics,
we consider two factors, which appear to be somewhat
indirectly related to stock prices in Taiwan but are
potentially influential: the exchange rate between US
5Fig. 5. Prediction of TSMC stock price by (a) multivariate linear regression, and (b) multivariate Bayesian estimation, where blue dots denote
actual prices and red crosses denote prices inferred from the exchange rate and NASDAQ index.
dollars (USD) and New Taiwan Dollars (NTD, the local
currency in Taiwan), and the NASDAQ index which
primarily consists of high-tech stocks in the US.
Let the time series of the exchange rate between USD
and NTD be X1[n], n = 0, 1, . . . and the time series of
the NASDAQ index be X2[n], n = 0, 1, . . .. The time
series for the stock price of TSMC is Y [n], n = 0, 1, . . ..
Now, both X1[n] and X2[n] may influence Y [n]. This
classical problem has been typically handled by multi-
variate time series analysis, which serves as a benchmark
without introducing more advanced techniques.
Now we treat this information fusion problem as a
two-channel information transfer (and thus communi-
cation) problem: X1[n] → Y [n] and X2[n] → Y [n].
To proceed, we establish an equalizer to filter the data
in each channel. The equalizer is typically of the tap-
delay-line type, while the time unit is one day since our
data uses the closing rate/index/price. The length of this
adaptive equalizer is L and the corresponding weighting
coefficients are determined via training. We treat 2009-
2013 as the training period and then infer 2014 data in
an online way. The order of the adaptive equalizer and
the weighting coefficients are learned during the training
period, and they are kept and used during inference. Our
numerical experiment shows that
• Each individual factor (exchange rate or NASDAQ
index) is surprisingly useful but each alone is not
good enough to infer the TSMC stock price. Fur-
thermore, via classical methods such as linear least
squares or Bayesian estimation [26], the exchange
rate appears to be a much less predictive factor than
the NASDAQ index, which is to be expected since
the NASDAQ index to a certain extent can represent
high-tech stocks including TSMC.
• It is expected that multivariate statistical analysis
would help in this case. We adopt multivariate linear
regression [27] and multivariate Bayesian estima-
Fig. 6. Communication theoretic data analytics using an equalizer to
optimize information transfer from (a) the exchange rate, and (b) the
NASDAQ index to the TSMC stock price.
tion [26] as benchmark techniques. The inference
of TSMC stock prices from the exchange rate and
NASDAQ index is shown in Figure 5. The mean
square errors for both techniques perform similarly
to the results using only the NASDAQ index.
• We implement the tap-delay-line equalizer structure
of Figure 4 to optimize information transfer. Based
on the mean square error (MSE) criterion, we search
6Fig. 7. Inference via MRC information combining two equalized data
processing channels, where the length of the equalizer for the exchange
rate is 16 (days) and the length of the equalizer for the NASDAQ index
is 61 (around 3 months).
for the best length equalizer and corresponding
weighting coefficients, which are then used for
inference. During the inference period of 2014,
the length remains the value from training but
the weighting coefficients are updated online. In
Figure 6, we surprisingly observe excellent per-
formance of inference using the exchange rate,
as effective as the NASDAQ index. This result
illustrates different insights into the correlation of
data from the traditional approaches, since these
approaches suggest that the NASDAQ index can
describe the TSMC stock better. Therefore, our
result demonstrates the potential of this communica-
tion theoretic data processing methodology and the
potential of considering information transfer. Thus,
the potential of information-centric data processing
over conventional machine learning is worth further
study.
• Similar to diversity combining in digital communi-
cation systems, information combining in commu-
nication theoretic data analytics potentially further
improves the performance of inference. Maximal
ratio combining (MRC) is well known to be op-
timal for diversity combining based on signal-to-
interference-plus-noise ratio (SINR). Using a simi-
lar concept as equation (15), we develop MRC in-
formation combining to weight equalized channels
inversely proportional to the MSE in the training
period. Such weights in MRC information com-
bining can be updated online. Figure 7 depicts
the prediction results (red crosses) and true values
(blue dots). We calculate the mean square error and
find even better performance than using only the
exchange rate. The MSE can be lower than those
of multivariate linear regression and multivariate
Bayesian estimation.
In Appendix A, we develop the following rules of
thumb for communication theoretic data analytics, while
subject to further enhancements.
Problem: To infer Y based on X1, X2, . . . , XN .
Procedure:
(1) Use an equalizer (i.e. optimal receiver) implemen-
tation to identify causal relationships among data
variables, X1 → Y,X2 → Y, . . . ,XN → Y , with
the corresponding MSEs according to the training
dataset(s).
(2) Select Nc data variables to transfer sufficient infor-
mation (or sufficiently small MSE errors in training)
to identify the structure of knowledge by keeping
the length and coefficients of the equalizer, or by
online update of coefficients.
(3) Conduct MRC information fusion of these Nc data
variables as in Fig. 12 to infer.
Remark 1. The conjecture that this communication the-
oretic data analytics approach delivers more desirable
performance comes from optimizing information transfer
and avoiding cross-interference among data variables
(similar to multiple access interference in multiuser
communication), while existing multivariate statistical
analysis or statistical learning multiplexes all data vari-
ables together to result in multiple access interference
in data analytics. Furthermore, for each data variable,
a selected equalizer length with coefficients is used,
then information is combined with other data variables,
to allow better matching to extract information in this
communication theoretic data analytics approach. Each
equalizer is designated to match a specific data variable,
while multivariate analysis usually deals with a common
fixed depth of observed data in processing for all data
variables. More observations may not bring in more rele-
vant information but rather additional noise/interference.
Although recent research suggests a duality between
time series and network graphs [28][29], information-
centric processing of data suggested by communication
theory supplies a unique and generally applicable view of
inference, even though its extension to more complicated
network graphical relationship of data is still open.
Note that during the training period, the computational
complexity is high, however, the computation load is
rather minor in the inference stage.
Remark 2. Applying information theoretic data analytics
such as mutual information and information divergence
beyond correlation have been proposed in the literature,
e.g. [30][31]. However, such efforts have not systemat-
ically applied information-centric processing techniques
based on communication systems as suggested in this
paper. In the mean time, though we have illustrated only
one-hop network graphical inference, these methods may
be applied further for data cleaning, data filtering, iden-
tification of important data variables for inference, and
7identification of causal relationships among data vari-
ables to support knowledge discovery in data analytics.
To fuse heterogeneous information, fuzzy logic [30] or
Markov logic [31] is usually employed. In the proposed
approach, information combining of different-depth in-
formation transfer alternatively serves the purpose in an
effective way, while time series data mining typically
considers similarity measured in terms of distance [32].
At this point, we cannot conclude that communi-
cation theoretic data analytics are better than multi-
variate analysis and other machine learning techniques,
as many advanced techniques such as Kalman filter-
ing [33], graphical models [34], or role discovery [35]
that are somewhat similar to our proposed approach have
not been considered in our comparison. However, via
the above example, the communication theoretic data
analytical approach indeed demonstrates its potential,
particularly as a way of fusing information, while it
seems more difficult to achieve a similar purpose by pure
multivariate analysis. Some remaining open problems for
communication theoretic data analytics are
• How to measure the information transfer from each
variable Xi → Y , i = 1, 2, . . ..
• How to determine a sufficient amount of informa-
tion transfer? And thereby, how to determine which
variables should be considered in data analytics.
A more realistic but complicated scenario involves
information fusion and information diversity at the same
time, which is a multiuser detection or MIMO problem.
Due to space limitations, it is not possible to explore
this idea here, in spite of its potential applicability to
different problems, such as recommender systems etc.
Some open issues include:
• Joint prediction of Y1, Y2, . . . , YN from
X1, X2, . . . , XM , and associated techniques
to effectively solve this problem, such as sub-space
approaches etc.
• Optimal MUD has NP-hard complexity, leading
to suboptimal receivers such as the de-correlating
receiver. Comparisons of such structures to mul-
tivariate time series analysis, mathematically and
numerically, are of interest.
IV. INFORMATION COUPLING
Thus far, we have intuitively viewed communication
theoretic data analytics as being centered on information
transfer among different data variables, and then applied
receiver techniques to enhance data analytics. This sug-
gests that the amount of information in the data is in fact
far less then the amount of (big) data. The methodology
in Section III deals with a number of data variables to ef-
fectively execute information processing analoguesly to
communication systems and multiuser communications.
The remaining challenge is to identify low-dimensional
information structure from high-dimensional (raw) data,
and to better construct intuition about communication
theoretic data analytics from information theory. In this
section, we aim to apply the recently developed infor-
mation coupling [37] to achieve this purpose, and also
provide information theoretic insights to information-
centric data processing.
A. Introduction to Information Coupling
From the analog between communication networks
and data analytics, intuition suggests the need for
information-centric processing in addition to data pro-
cessing for not only optimizing communication sys-
tems/networks but also mining important information
from (big) data. The conventional studies of information
processing are limited to data processing, thereby focus-
ing on representing information as bits, and transmitting,
storing, and reconstructing these bits reliably. To see
this, let us consider a random variable with M possible
values {1, 2, . . . ,M}. If we know its value reliably, then
we can describe this knowledge with a single integer,
and then further process the data with this known value.
On the other hand, if the value is not deterministically
acquirable, then we need to describe our knowledge with
an M -dimensional distribution PM (m), which requires
M − 1 real numbers to describe. Therefore, the data
processing task has to be performed in the space of
probability distributions.
When we move towards information-centric process-
ing, the general way to describe information processing
relies on the conditional distribution of the message,
conditioned on all the observations, at each node of
the network, e.g., PYn|Xm in Figure 1. Conventional
information theoretic approaches working on the dis-
tribution spaces in communication and data processing
are mostly based on coded transmission, in which the
desired messages are often quite large, which results in
the extremely high dimensionality of the belief vectors.
This is in fact one of the main difficulties of shifting the
data processing from data centric to information centric.
It turns out that this difficulty comes from the fact that
the distribution space itself is not a flat vector space, but
is a rather complicated manifold. Amari’s work [36] on
information geometry provides a tool to study this space,
but the analysis can be quite involved in many cases.
In this section, we propose a framework that allows us
to greatly simplify this challenge. In particular, we turn
our focus to low rate information contained in the data,
which is significant for describing the data. We call such
problems information coupling problems [37][38].
To formulate this problem mathematically, let us con-
sider a point-to-point communication scenario, where
a signal X is transmitted through a channel with the
8transition probability WY |X , which can be viewed as
a |Y| × |X | matrix, to generate an output Y . In the
conventional communication systems, we consider en-
coding a message U into the signal vector X , to form a
Markov relation U → X → Y . From which, an efficient
coding scheme aims to design both the distribution PU
and the conditional distributions PX|U=u to maximize
the mutual information I(U ;Y ), which corresponds to
the communication rate. Such optimization problems in
general do not have analytical solutions, and require nu-
merical methods such as the Blahut-Arimoto algorithm
to find the optimal value. More importantly, when we
allow coded transmissions, i.e., to replace X and Y by
n independent and identically distributed (i.i.d.) copies
of the pair, it is not clear a priori that the optimizing
solution would have any structure. Although Shannon
provided a separate proof for the point-to-point case
that the optimization of the multi-letter problem over
PXn|U should also have an i.i.d. structure, failure to
generalize this proof to multi-terminal problems remains
the biggest obstacle to solving network capacity and
subsequently design algorithms. In contrast, the informa-
tion coupling deals with the maximization of the same
objective function I(U ;Y ), but with an extra constraint
that the information encoded in X , measured by I(U ;X)
is small. With a slight strengthening this constraint can
be reduced to the condition that all the conditional
distributions PX|U (·|u), for a u, are close to the marginal
distribution PX . We refer the reader to [37] for the
details of this strengthening. With this extra constraint,
the linear information coupling problem for the point-
to-point channel can be formulated as
max
U→X→Y
1
n
I(U ;Y ), (2)
subject to:
1
n
I(U ;X) ≤ δ,
1
n
‖PX|U=u − PX‖2 = O(δ), ∀u, (3)
where δ is assumed to be small.
It turns out that the local constraint (3) in (2) that
assumes all conditional distributions are close to the
marginal distribution, plays the critical role of reducing
the manifold structure into a linear vector space. In
addition, the optimization problem, regardless of the
dimensionality, can always be solved analytically with
essentially the same routine. In order to show how the
local constraint helps to simplify the problem, we first
note that given the conditional distributions PX|U=u
are closed to PX for all u in terms of δ, the mutual
information I(U ;X) can be approximated up to the first
order as
I(U ;X) = δ ·
∑
u
PU (u) · ‖ψu‖2 + o(δ), (4)
where ψu is the perturbation vector with the entries
ψu(x) =
(
PX|U=u(x)− PX(x)
)
/
√
δ · PX(x), for all
x. This local approximation results from the first order
Taylor expansion of the Kullback-Leibler (K-L) diver-
gence D(PX|U=u‖PX) between PX|U=u and PX with
respect to (w.r.t.) δ. In addition, with this approximation
technique, we can similarly express the mutual informa-
tion at the receiver end as
I(U ;Y ) = δ ·
∑
u
PU (u) · ‖ψˆu‖2 + o(δ), (5)
where ψˆu(y) =
(
PY |U=u(y)− PY (y)
)
/
√
δ · PY (y).
Now, note that U → X → Y forms a Markov relation,
therefore both PY |U=u and PY , viewed as vectors, are
the output vectors of the channel transition matrix WY |X
with the input vectors PX|U=u and PX . This implies that
the vector ψˆu is the output vector of a linear map B with
ψu as the input vector, where
B ,
[√
PY
−1]
WY |X
[√
PX
]
, (6)
and
[√
PX
]
and
[√
PY
]
denote diagonal matrices with
diagonal entries PX(x) and PY (y). This linear map
B is called the divergence transition matrix (DTM) as
it carried the K-L divergence metric from the input
distribution space to the output distribution space.
We shall point out here that with this local approxi-
mation technique, both the input and output probability
distribution spaces are linearized as Euclidean spaces
by the tangent planes around the input and the output
distributions PX and PY . Hence, we can define the
coordinate system in both distribution spaces, such as the
inner product and orthonormal basis, as in the conven-
tional Euclidean spaces. Under such a coordinate system,
the mutual information I(U ;X) becomes the Euclidean
metric of the perturbation vector ψu averaged over
different values of u. Similarly, the mutual information
I(U ;Y ) can also be viewed as the Euclidean metric
of the perturbation vector B · ψu at the output space.
Hence, the optimization problem of maximizing the
mutual information I(U ;Y ) is turned into the following
linear algebra problem:
max .
∑
u
PU (u) · ‖B · ψu‖2 (7)
subject to:
∑
u
PU (u) · ‖ψu‖2 = 1.
In particular, U can without loss of the optimality be
designed as a uniform binary random variable, and the
goal of (7) is to find the input perturbation vector ψu
that provides the largest output image B ·ψu through the
linear map B. The solution of this problem then relies on
the singular value decomposition of B, and the optimal
ψu corresponds to the singular vector of B w.r.t. the
9PX PY
Bψ1ψ2 Bψ1Bψ2
Fig. 8. The divergence transition matrix B serves as a linear map
between two spaces, with right and left singular vectors as orthonormal
bases. Different input singular vectors have different output lengths at
the output space.
largest singular value. Figure 8 illustrates the geometric
intuition of linearized distributions spaces.
More importantly, this information coupling frame-
work with the locality constraint allows us to deal with
the multi-letter problems in information theory in a
systematic manner. To see this, consider the multi-letter
version of the problem (2)
max
U→Xn→Y n
1
n
I(U ;Y n), (8)
subject to:
1
n
I(U ;Xn) ≤ δ,
1
n
‖PXn|U=u − PXn‖2 = O(δ), ∀u,
in which the message is encoded in an n-dimensional
signal vector Xn and the optimization is over the distri-
bution space of PXn|U=u. By applying the same local
approximation technique, we can again linearize both
input and output spaces into Euclidean spaces, and the
linear map between these two spaces turns out to be
the tensor product Bn. Due to the fact that the singular
vectors of Bn are tensor products of the singular vectors
of B, we know that the optimal perturbation vector in
the multi-letter case has the tensor product form, and the
optimal conditional distribution PXn|U=u has an i.i.d.
structure [37]. More interestingly, this approach of deal-
ing with multi-letter information theory problems can be
easily carried to multi-terminal problems, in which all
the information theory problems are simply reduced to
the corresponding linear algebra problems. In particular,
the i.i.d. structure of PXn|U=u for the point-to-point
case was also observed by Shannon with an auxiliary
random variable approach; however, the generalization
of the auxiliary random variable approach to multi-
terminal problems, e.g., the general broadcast channel,
turns out to be difficult open problems. In a nutshell,
the information coupling and the local constraint help
us to reduce the manifold structure into a linear vector
space, where the optimization problem, regardless of the
dimensionality can always be solved analytically with
essentially the same routine.
Furthermore, the information coupling formulation not
only simplifies the analysis, but also suggests a new way
of communication over data networks or information
transfer over networks of data variables. Instead of trying
to aggregate all the informations available at a node,
pack them into data packets, and send them through
the outgoing links, the information coupling method-
ology seeks to transmit a small piece of information
at a time, riding on the existing data traffic [39]. The
network design of data variables thus focuses on the
propagation of a single piece of message, from the
source data variable to all destination data variables.
Each node in the network only alters a small fraction
of the transmitted symbols, according to the decoded
part of this message. The analytical simplicity of the
information coupling allows such transmissions to be
efficient, even in the presence of general broadcasting
and interference. Furthermore, information coupling can
be employed to obtain useful information from network
operation, as a complementary function for (wireless)
network tomography. Consequently, we can analyze the
covariance matrix of received signals at the fusion center
in a sensor network to form communities like social
networks such that energy efficient transmission and
device management can be achieved.
B. Implementation of Information Coupling
Using the information theoretic setup via informa-
tion coupling, we shall demonstrate how to deal with
practical data analytics. To infer useful results from big
data, we shall be able to acquire important knowledge
in general social network modeling of big data such as
Figure 1. In particular, we consider X1, . . . , XM as in-
formation transmitters and Y1, . . . , YN as the information
receivers. The probabilistic relationship between X’s
and Y ’s represents the communication channel, which
copes with the effects of imperfect sampling, noisy
observation, or interference from unknown variables or
outliers. In the following, we are going to demonstrate
the potential of extracting critical low dimensional infor-
mation from (big) data through the innovative informa-
tion coupling approach.
To demonstrate the idea, suppose that there is a hidden
source sequence xn = {x1, x2, . . . , xn}, i.i.d. generated
according to some distribution PX . Instead of observing
the hidden source directly, we are only allowed to
observe a sequence yn = {y1, y2, . . . , yn}, which can
be statistically viewed as the noisy outputs of the source
sequence through a discrete memoryless channel WY |X .
Traditionally, if we want to infer the hidden source from
the noisy observation yn, we would resort to a low-
dimensional sufficient statistic of yn that has all the
information one can tell about xn. However, in many
10
cases, such a sufficient statistic might be computationally
difficult to obtain due to the high dimensional structures
of xn and yn, which turns out to be the common obstacle
in dealing with big data. In contrast to seeking a useful
sufficient statistic, we would like to rather turn our
focus to consider the statistic from yn that is efficient
to describe a certain feature of xn.
In particular, there are many different ways to define
the efficiency of information extraction from data. From
the information theoretic point of view, we would like
to employ the mutual information as the measurement
of the information efficiency about the data. Rigorously,
we want to acquire a binary feature U in xn from the
observed data yn, such that the efficiency, measured by
I(U ;Y n), can be maximized. In order to find such a
feature, we shall formulate an optimization problem that
has the same form as the linear information coupling
problem (8), and the optimal solution of (8) character-
izes which feature of xn can be the most efficiently
extracted from the noisy observation yn in terms of
the mutual information metric. Therefore, from [37], we
can explicitly express the optimal solution PXn|U of (8)
as the tensor product of the distribution PX|U (x) =
PX(x) ±
√
δPX(x) · ψX(x), where ψX is the singular
vector of the DTM with the largest singular value.
Then, we want to estimate this piece of information
U from the noisy observation yn. For this purpose, we
apply the maximum likelihood principle, and the log-
likelihood function can be written as
li(y
n) = log
(
PY n|U=i(yn)
PY n(yn)
)
, i = 0, 1,
where PY n and PY n|U are the output distributions of
the channel WY |X with input distributions PXn and
PXn|U . Then, the decision rule depends on the sign of
l0(y
n)− l1(yn): when it is positive, we estimate Uˆ = 0,
otherwise, Uˆ = 1. Now, noting that both PY n|U and PY n
are product distributions, we can further simplify l0(yn)
as
l0(y
n) =
n∑
i=1
log
(
PY |U=i(yi)
PY (yi)
)
=
n∑
i=1
log
(
1 +
√
δ · ψY (yi)√
PY (yi)
)
'
√
δ ·
n∑
i=1
ψY (yi)√
PY (yi)
,
where in the last equation, we ignore all the higher
order terms of δ. We call ψY√
PY
the score function
ψY√
PY
: Y 7→ R, in which the empirical sum of this
function over the data y1, . . . , yn is the sufficient statistic
of a specific piece of information in xn that can be the
most efficiently estimated from yn. Figure 9 illustrates
x1 WY |X
ψY√
PY
y1
x2 WY |X
ψY√
PY
y2
xn WY |X
ψY√
PY
yn
Score n∑
i=1
ψY (yi)√
PY (yi)...
...
Fig. 9. The score function for the noisy observations.
the score function in this point-to-point setup. The score
function derived from the information coupling approach
provides the maximal likelihood statistics of the most
efficiently inferable information from the data, and we
call the score function the efficient statistic of the data.
The efficient statistic of the data can be deemed as a
low dimensional label corresponding to the most signif-
icant information of the data that can be employed in
further data processing tasks. In the next subsection, we
shall demonstrate how to apply the efficient statistic to
practical machine learning problems and its performance
through an image recognition example.
Finally, we would like to emphasize that the efficient
statistic can be useful in many machine learning scenar-
ios, such as image processing, network data mining and
clustering. Consider the social network modeling of big
data as Figure 1 with very large number of nodes in the
network. In this case, acquiring a meaningful sufficient
statistic for the data is usually an intractable task due
to the complicated network structure. Moreover, even if
it is possible to specify the sufficient statistic, the com-
putational complexity can still be extremely high due to
the high dimensional structure of the data. On the other
hand, the efficient statistic obtained from the information
coupling provides the information that, while low di-
mensional, keeps the most significant information about
the original data. This is precisely the main objective of
the dimension reduction or feature extraction studied in
machine learning subjects. Equalization in Section III
may be considered as an intuitive implementation of
information coupling in big data. In addition, in order
to acquire the efficient statistic from the data, we simply
need to solve the score function, i.e., the optimal singular
vector, which can be computationally efficient. There-
fore, we could see that information coupling potentially
provides a new framework for efficiently processing and
analyzing big networked data.
C. Application to Dimension Reduction in Pattern
Recognition
Let us illustrate how the efficient statistic can be ap-
plied to practical data processing. For demonstration pur-
poses, we aim to address the image recognition task of
11
(a)
X1
X2
X3
X4
Y1
Y2
Y3
Y4
WYi|Xi
(b)
Fig. 10. (a) Handwriting Recognition between the number “1” and “2”
via the noisy images. (b) Ising Model of Noisy Images. The pixels of
the clean image can be viewed as random variables Xi. After passing
through the channel, the pixels are corrupted by the noise to different
levels, and the collection of noisy pixels are the random variables Yi.
handwriting numbers “1” and “2” through noisy images,
as illustrated in Figure 10(a). We consider these 2-D
images as from an Ising model as shown in Figure 10(b).
Each clean pixel in Figure 10(b) is passed through one
of the parallel independent noisy observation channels,
to get a noisy image. In abstract, we can think of the
pixels of the clean image as a collection of random
variables X1, X2, . . . , XN . Then, passing through noisy
observation channels with a transition kernel PY N |XN ,
the pixels of the noisy image is a collection of random
variables Y1, Y2, . . . , YN . Now, to apply the efficient
statistic to acquire the most significant feature, we shall
in principle go through the following procedures:
(1) To determine the divergence transition matrix B,
we shall determine the distributions PXN and PY N
as well as the transition kernel PY N |XN . The dis-
tributions PXN and PY N can be learned from the
empirical distribution of the images viewed as N -
dimensional vectors. In addition, we design the
transition kernel PY N |XN in this image recognition
Error
probability
Fig. 11. Experimental results for the separation efficiency with respect
to different values of e.
example.
(2) Solve the singular value decomposition of B and
determine the optimal left singular vector ψ of B.
Note that ψ has the dimensionality |Y|N .
(3) The efficient statistic is then specified by the
score function ψ(yN )/PY N (yN ) of the data yN =
{y1, . . . , yN}, which can be obtained by the yN -th
entry of the vector ψY , divided by PY N (yN ).
Now, let us demonstrate the application of this pro-
cedure to a practical image recognition problem. Here,
we employ the MNIST Database [40] of handwritten
digits as the test images, where each image is of size
19 × 19 pixels and each pixel value is scaled to have
one of four values as in Figure 10(a). In particular, as
shown in Figure 10(a), we have a mixture of images of
handwritten digits 1 and 2, and we assume that we can
observe the noisy version of these images. Our goal is
to separate these noisy images with respect to different
digits by computing the score of these images with our
algorithm and ordering them.
To this end, we view the (clean) images as generated
from the Ising model, in which each pixel corresponds
to the nodes Xi in the Ising model. Then, we pass each
pixel in the images independently through a discrete
memoryless channel with transition matrix
1− 2e 2e e e2
e 1− 3e 2e e4
e 0 1− 4e e4
0 e e 1− e
 . (9)
Here, the transition matrix is chosen merely to facilitate
our simulation, and e is the parameter measuring the
noise level of the channel. After passing clean pixels
through the channel, we observe the noisy version of
these images, where each noisy pixel corresponds to Yi
in this setup. Clearly, the empirical joint input and output
distributions can be obtained by the statistics of the
images. Then, we can apply our algorithm to compute
the score for each noisy image, and then order these
scores to separate images with respect to different digits.
To measure the performance of our algorithm, we
classify a batch of 2N images with N of 1’s and N of
2’s. After ordering the scores, an ideal classifier should
12
have the N lowest scored images belonging to one
digit and the N largest scores belonging to the other
digit. To compare with the ideal classifier, we define
the separation error probability as the proportion of the
pictures that is wrongly classified, i.e.
Error probability =
# of wrongly classified pictures
2N
(10)
The classifier is more efficient when the separation factor
is closer to 0. For different values of e, our algorithm has
the performance as in Figure 11. From this simulation
result, we can see that our algorithm is quite efficient in
separating images with respect to different digits. This
result tells that the efficient statistic is in fact a very
informative way to describe stochastic observations.
Remark 3. It might be curious at first glance that in
Figure 11, the error probability does not decay as the
noise level e grows. In fact, this phenomenon can be
explained as follows. Note that the score function defined
in this section not only depends on the data vectors XN ,
but also on the designed channel transition matrix (9).
Therefore, different channel transition matrices may pro-
vide different score function on the noisy data vectors
Y N . We shall notice that the score function is designed
to extract the feature that can be communicated the best
through the channel, but not necessary the best feature to
separate the two sets of images. Thus, the performance
of the image recognition may not be improved with
a less noisy channel. On the other hand, we should
understand our result as that, with a rather arbitrarily
designed channel transition matrix (9), we have obtained
a rather nice performance of error probabilities, which
does not require any extra learning other than the em-
pirical distributions of the data, i.e., completely unsu-
pervised. Thus, our result demonstrates a new potential
of applying communication and information theory to
machine learning problems.
Remark 4. Dimension reduction is one of the central
topics in statistics, machine learning, pattern recogni-
tion, and data mining, and has been studied inten-
sively. Celebrated techniques addressing this subject
including principal component analysis (PCA) [41], K-
means clustering [42], independent component analysis
(ICA) [43], and regression analysis [44], where many
efficient algorithms have been developed to implement
these approaches [45]-[6]. In particular, these approaches
mainly focus on dealing with the space of the data, rather
than addressing the information flow embedded in the
data. On the other hand, recent studies have suggested
the trend of information-centric data processing [16],
thus advocating the research direction of analyzing the
underlying information flow of networked data. The
information coupling approach can be considered as a
technique that aims to provide a framework to reach this
goal from the information theoretic perspective. From the
discussions in this section, we can see that information
coupling studies the data analysis problems from the
angle of distribution space but not simply the data space,
Thus, information coupling potentially provides a fresh
view of how information can be exchanged between
different terminals in implementing the data processing
tasks, which not only helps to more deeply understand
the existing approaches, but also opens a new door to
develop new technologies.
Remark 5. While this simple image recognition exam-
ple illustrates the feasibility of introducing information
coupling to data analysis problems, there are critical
challenges for future research:
• How to develop efficient iterative algorithms that
exploit the structure of the graphical models to com-
pute the singular vectors and evaluate the scores.
• In the case where some training data are available,
how the information coupling approach can be
adjusted to cooperate with the side information.
• Except for the most informative bit, how can we
extract the second and third bits from the data, and
how these bits can be applied to deal with practical
data analysis tasks.
V. CONCLUSIONS
Statistical analysis on big data has usually been treated
as an exercise in statistical data processing. With the
help of statistical communication theory, we have intro-
duced a new methodology to enable information-centric
processing (or statistical information processing) for big
data. Hopefully, this opens new insights into both big
data analytics and statistical communication theory.
Although we have demonstrated initial feasibility of
this methodology, there are further critically associated
challenges ahead, namely
• How to identify appropriate or enough variables to
influence one variable (or a set of variables).
• How to detect outliers [48].
• How to generalize big data analytics using large
communication network analysis beyond multiuser
communications.
• How to interpret and adopt traditional machine
learning approaches and data processing technolo-
gies, such as (un)supervised learning, feature se-
lection, blind source separation, via the techniques
developed in network communication theories.
APPENDIX A
EQUALIZER IMPLEMENTATION FOR COMMUNICATION
THEORETIC DATA ANALYTICS
As in Fig. 4, by proper selection of adaptive algorithm
and step size, the output of the equalizer after training
13
Fig. 12. Maximal-Ratio Combining of Two Equalized Data Variables
(Time Series).
period gives the inference
yˆ[n] =
L∑
l=0
wlx[n− l]. (11)
Based on the minimum MSE criterion, the purpose of
training data is to obtain
argmin
w

(
y[n]−
L∑
l=0
wlx[n− l]
)2 , (12)
and
argmin
L

(
y[n]−
L∑
l=0
wlx[n− l]
)2 .
The first equation is to obtain the vector of weighting
coefficients, and the second equation is to identify the
most appropriate observation depth, L. Once we identify
L, we keep it and therefore the equalizer structure to
infer data. We may keep the same set of coefficients or
update online. Please note we may also obtain a predictor
as follows:
yˆ[n+ 1] =
L∑
l=0
wlx[n− l], (13)
where we cannot go into further detail due to the length
constraint on this paper.
When we have two (or more) data variables to infer
another data variable, say using X1 and X2 to infer Y ,
we have to use information fusion as in Figure 12. Again,
we adopt the minimum MSE criterion, to yield
min
αm[n]
E

(
Y −
M∑
m=1
αm[n]yˆm[n]
)2 , (14)
where
yˆm[n] =
Lm∑
l=0
wl,mxm[n− l].
Necessary conditions for this minimization gives the
solution for αm[n]. The consequent estimator is therefore
yˆ[n] =
M∑
m=1
αm[n]yˆm[n] =
M∑
m=1
αm[n]
Lm∑
l=0
wl,mxm[n− l]
(15)
which is defined as the maximal ratio combining of
equalized multivariate regression of different optimal
observation lengths Lm, m = 1, . . . ,M . This design re-
alizes the idea of maximizing information flow between
data variables or time series. For ease of implementation,
we may set αm[n] = αm, or we may adopt selective
combining and equal-gain combining.
Remark 6. A conjecture to explain why we intend to
equalize data of a certain length Lm, instead of the
entire data set, is that earlier components in the time
series may introduce very noisy information, like inter-
ference or noise in multiuser communication systems
or simply weakly correlated information after a large
time separation. Such lengths Lm, m = 1, . . . . ,M ,
for data variables X1, . . . , XM , represent the span/range
of useful data for inference. Of course, based on the
MSE, we may further select useful data variables among
X1, . . . , XM . Similar concepts are not rare in machine
learning, for example, to identify support vectors in
support vector machines (SVMs). What we are doing
here is more effective implementation by properly se-
lecting data variables, range of observations, and finally
weighting coefficients in each equalizer, for multivariate-
regression leveraging the optimization of information
transfer between relational data variables.
ACKNOWLEDGMENT
The authors thank Jia-Pei Lu of National Taiwan Uni-
versity, and Fabia´n Kozynski of Massachusetts Institute
of Technology for their programming of the numerical
examples.
REFERENCES
[1] X.Wu, X. Zhu, G.Q. Wu, W. Ding, “Data Mining with Big Data,”
IEEE Trans. On Knowledge and Data Engineering, vol. 26, no. 1,
pp. 97-107, Jan. 2014.
[2] M.J. Wainwright, M.I. Jordan, “Graphical Models, Exponential
Families, and Variational Inference,” Foundations and Trends in
Machine Learning, vol. 1, no. 1-2, pp. 1-305, January 2008.
[3] J. Han, M. Kamber, Data Mining: Concepts and Techniques, 2nd
edition, 2006.
[4] U. Fayyad, G. Piatetsky-Shapiro, R. Smyth, “From Data Mining
to Knowledge Discovery in Databases,” AI Magazine, vol. 17, no.
3, pp. 37-54, 1996.
[5] F. V. Jensen and T. D. Nielsen, Bayesian Networks and Decision
Graphs, 2nd ed., Springer, 2007.
[6] D. Koller and N. Friedman, Probabilistic Graphical Models:
Principles and Techniques., The MIT Press, 2009.
[7] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical
Learning, 2nd edition, Springer, 2009.
14
[8] R. Silva, R. Scheines, C. Glymour, P. Spirtes, “Learning the Struc-
ture of Linear Latent Variable Models,” J. of Machine Learning
Research, vol. 7, pp. 191-246, 2006.
[9] J.B. Tenenbaum, V. de Silva, J.C. Langford, “A Global Geometric
Framework for Nonlinear Dimensionality Reduction,” Science,
vol. 290, pp. 2319-2323, December 22, 2000.
[10] S.T. Roweis, L.K. Saul, “Nonlinear Dimensionality Reduction
by Locally Linear Embedding,” Science, vol. 290, pp. 2323-2326,
December 22, 2000.
[11] S. Ji, Y. Xue, L. Carin, Bayesian Compressive Sensing,” IEEE
Trans. On Signal Processing, vol. 56, no. 6, pp. 2346-2356, June
2008.
[12] T.G. Kolda, B.W. Bader, “Tensor Decomposition and Applica-
tions,” SIAM Review, vol. 51, no. 3, pp. 455-500, 2009.
[13] S.C. Lin, K.C. Chen, “Improving Spectrum Efficiency via In-
Network Computations in Cognitive Radio Sensor Networks,”
IEEE Trans. on Wireless Communications, vol. 13, no. 3, pp. 1222-
1234, March 2014.
[14] X.-W. Chen, A. Lin, “Big Data Deep Learning: Challenges and
Perspectives,” IEEE Access, vol. 2, pp. 514-525, 2014.
[15] W. Tan, M.B. Blake, I. Saleh, S. Dustdar, “Social-Network-
Sourced Big Data Analytics,” IEEE Internet Computing, pp. 62-69,
Sep/Oct 2013.
[16] K.C. Chen, M. Chiang, H.V. Poor, “From Technological Net-
works to Social Networks,” IEEE Journal on Selected Areas in
Communications, vol. 31, no. 9, pp. 548-572, September Supple-
ment 2013.
[17] D. Estrin, “Small Data Where n = me,” Communication of the
ACM, vol. 57, no. 4, pp. 32-34, April 2014.
[18] S. Verdu´, Multiuser Detection, Cambridge University Press, 1998.
[19] M. Koivisto, K. Sood, “Exact Bayesian Structure Discovery in
Bayesian Networks,” J. Machine Learning Research, vol. 5, pp.
549-573, May 2004.
[20] Z. Wang, L. Chan, “Learning Causal Relations in Multivariate
Time Series Data,” ACM Trans. On Intelligent Systems and Tech-
nologies, vol. 3, no. 4, Article 76, September 2012.
[21] G. Stuber, J. Barry, S McLaughlin, G. Li, M. Ingram, T. Pratt,
“Broadband MIMO-OFDM Wireless Communications,” IEEE
Journal on Selected Areas in Communications, vol. 92, no. 2, pp.
271-294, Feb. 2004.
[22] A. Sandryhalia, J.M.F. Moura, “Big Data Analysis with Signal
Processing on Graphs,” IEEE Signal Processing Magazine, vol.
31, no. 5, pp. 80-90, September 2014.
[23] R.S. Blum, S.A. Kassam, H.V. Poor, “Distributed Detection with
Multiple Sensors: Part II,” Proceeding of the IEEE, vol. 85, no. 1,
pp. 64-79, Jan. 1997.
[24] L. Tong, S. Perreau, “Multichannel Blind Identification: From
Subspace to Maximum Likelihood Methods,” Proceeding of the
IEEE, vol. 86, no. 10, pp. 1951-1968, October 1998.
[25] C. Yu, L. Xie, Y.C. Soh, “Blind Channel and Source Estimation
in Networked Systems,” IEEE Trans. on Signal Processing, vol.
62, no. 17, pp. 4611-4626, Sep. 2014
[26] S.M. Kay, Fundamentals of Statistical Signal Processing: Esti-
mation Theory, Prentice-Hall, 1993.
[27] A.D. Rencher, W.F. Christensen, Methods of Multivariate Anal-
ysis, John Wiley & Sons, 2012.
[28] L. Lacasa, B. Luque, F. Baillesteros, J. Luque, J.A. Nuno,
“From Time Series to Complex Networks: The Visible Graph,”
Proceeding National Academy of Science, vol. 105, no. 13, pp.
4932-4975, April 1 2008.
[29] A.S.L.O. Campanharo, et al., “Duality Between Time Series and
Networks,” PLoS ONE, vol. 6, no. 8, e23378, August 2011.
[30] M. Last, Y. Klein, A. Kandel, “Knowledge Discovery in Time
Series Databases,” IEEE Trans. On Systems, Man, Cybernetics –
Part B: Cybernetics, vol. 31, no. 1, pp. 160-169, Feb. 2001.
[31] I.S. Dhillon, S. Mallela, D.S. Modha, “Information-Theoretical
Co-clustering,” Proceedings of ACM SIGKDD, 2003.
[32] P. Esling, C. Agon, “Time-series Data Mining,” ACM Computing
Survey, vol. 45, no. 1, Article 12, November 2012.
[33] A.C. Harvey, R.G. Pierse, “Estimating Missing Observations in
Economic Time Series,” J. American Statistical Association, vol.
79, no. 385, pp. 125-131, Mar. 1984
[34] D. Barber, A.T. Cemgil, “Graphical Models in Time Series,”
IEEE Signal Processing Mag., Nov. 2010.
[35] R.A. Rossi, N.K. Ahmed, “Role Discovery in Networks,” to
appear in the IEEE Tr. on Knowledge and Data Engnieering.
[36] S.-I. Amari, H. Nagaoka, Methods of Information Geometry,
Oxford University Press, 2000.
[37] S.-L. Huang, L. Zheng, “Linear Information Coupling Problems,”
Proceedings of the IEEE International Symposium on Information
Theory, July 2012. (Full paper submitted to the IEEE Trans. On
Information Theory 2014.)
[38] S. Borade, L. Zheng, “Euclidean Information Theory,” Proceed-
ings of the IEEE International Zurich Seminars on Communica-
tions, March, 2008.
[39] S.-L. Huang, C. Suh, L. Zheng, “Euclidean Information Theory
of Networks,” Proceedings of the IEEE International Symposium
on Information Theory, July 2013.
[40] MNIST Handwritten Digit Database, from http://yann.lecun.com/
exdb/mnist/
[41] I. T. Jolliffe, Principal Component Analysis, Springer Verlag,
1986.
[42] C. Ding, X. He, “K-means Clustering via Principal Component
Analysis,” In Proceedings of the Twenty-First International Con-
ference on Machine Learning (ICML ’04). ACM, New York, NY,
USA, pp. 29-36.
[43] A. Hyvarinen, J. Karhunen, E. Oja Independent Component
Analysis, New York: J. Wiley 2001.
[44] D. A. Freedman, Statistical Models: Theory and Practice,
Cambridge University Press, 2005.
[45] R. E. Kalman, “A New Approach to Linear Filtering and Pre-
diction Problems,” Transaction of the ASME–Journal of Basic
Engineering, vol. 82, no. Series D, pp. 35-45, March 1960.
[46] A. P. Dempster, N. M. Laird, D. B. Rubin, “Maximum Likelihood
from Incomplete Data Via the EM Algorithm,” J. Royal Stat. Soc.
B. 39, 1, 138.
[47] M. Singh, M. Valtorta, “Construction of Bayesian Network
Structures from Data: A Brief Survey and an Efficient Algorithm,”
International Journal of Approximate Reasoning, vol. 12, no. 2,
pp. 111131, 1995.
[48] A. Tajer, V.V. Veeravalli, H.V. Poor, “Outlying Sequence Detec-
tion in Large Data Sets,” IEEE Signal Processing Magazine, vol.
31, no. 5, pp. 44-56, September 2014.
Kwang-Cheng Chen (M’89-SM’94-F’07)
Kwang-Cheng Chen (M89-SM94-F07) re-
ceived the B.S. from the National Taiwan
University in 1983, and the M.S. and Ph.D
from the University of Maryland, College
Park, United States, in 1987 and 1989, all
in electrical engineering. From 1987 to 1998,
Dr. Chen worked with SSE, COMSAT, IBM
Thomas J. Watson Research Center, and Na-
tional Tsing Hua University, in mobile com-
munications and networks. Since 1998, Dr.
Chen has been with National Taiwan University, Taipei, Taiwan, ROC,
and is the Distinguished Professor and Associate Dean for academic
affairs in the College of Electrical Engineering and Computer Science,
National Taiwan University. He has been actively involving in the
organization of various IEEE conferences as General/TPC chair/co-
chair, and has served in editorships with a few IEEE journals. Dr. Chen
also actively participates in and has contributed essential technology to
various IEEE 802, Bluetooth, and LTE and LTE-A wireless standards.
Dr. Chen is an IEEE Fellow and has received a number of awards
such as the 2011 IEEE COMSOC WTC Recognition Award, 2014
IEEE Jack Neubauer Memorial Award and 2014 IEEE COMSOC AP
Outstanding Paper Award. Dr. His recent research interests include
wireless communications, network science, and data analytics.
15
Shao-Lun Huang received the B.S. degree
with honor in 2008 from the Department
of Electronic Engineering, National Taiwan
University, Taiwan, and the M.S. and Ph.D.
degree in 2010 and 2013 from the Depart-
ment of Electronic Engineering and Com-
puter Sciences, Massachusetts Institute of
Technology. Since 2013, he has been working
jointly in the Department of Electrical En-
gineering at the National Taiwan University
and the Department of Electrical Engineering
and Computer Science at the Massachusetts Institute of Technology,
where he is currently a postdoctoral researcher. His research interests
include communication theory, information theory, machine learning
and social networks.
Lizhong Zheng received the B.S and M.S.
degrees, in 1994 and 1997 respectively, from
the Department of Electronic Engineering,
Tsinghua University, China, , and the Ph.D.
degree, in 2002, from the Department of
Electrical Engineering and Computer Sci-
ences, University of California, Berkeley.
Since 2002, he has been working in the
Department of Electrical Engineering and
Computer Sciences, where he is currently
a professor of Electrical Engineering and
Computer Sciences. His research interests include information theory,
wireless communications, and statistical inference. He received the
IEEE Information Theory Society Paper Award in 2003, and NSF
CAREER award in 2004, and the AFOSR Young Investigator Award
in 2007.
H. Vincent Poor (S’72, M’77, SM’82, F’87)
received the Ph.D. degree in EECS from
Princeton University in 1977. From 1977 un-
til 1990, he was on the faculty of the Univer-
sity of Illinois at Urbana-Champaign. Since
1990 he has been on the faculty at Princeton,
where he is the Michael Henry Strater Uni-
versity Professor of Electrical Engineering
and Dean of the School of Engineering and
Applied Science. Dr. Poor’s research interests
are in the areas of information theory, statis-
tical signal processing and stochastic analysis, and their applications
in wireless networks and related fields including social networks and
smart grid. Among his publications in these areas are the recent books
Principles of Cognitive Radio (Cambridge University Press, 2013) and
Mechanisms and Games for Dynamic Spectrum Allocation (Cambridge
University Press, 2014).
Dr. Poor is a member of the National Academy of Engineering, the
National Academy of Sciences, and is a foreign member of Academia
Europaea and the Royal Society. He is also a fellow of the American
Academy of Arts and Sciences, the Royal Academy of Engineering
(U. K), and the Royal Society of Edinburgh. He received the Marconi
and Armstrong Awards of the IEEE Communications Society in 2007
and 2009, respectively. Recent recognition of his work includes the
2014 URSI Booker Gold Medal, and honorary doctorates from Aalborg
University, Aalto University, the Hong Kong University of Science and
Technology, and the University of Edinburgh.
"
7,"Advances in photonic reservoir computing  
on an integrated platform 
Kristof Vandoorne1, Student Member, IEEE, Martin Fiers1, Student Member, IEEE, Thomas Van 
Vaerenbergh1, Student Member, IEEE, David Verstraeten2, Member, IEEE, Benjamin Schrauwen2, 
Member, IEEE, Joni Dambre2, Peter Bienstman1, Member, IEEE 
1 Photonics Research Group, Department of Information Technology, Ghent University – imec,  
Sint-Pietersnieuwstraat 41, 9000 Gent, Belgium 
2
 PARIS, Department of Electronics and Information Systems, Ghent University,  
Sint-Pietersnieuwstraat 41, 9000 Gent, Belgium 
* Tel: +32 9 264 3447, Fax: +32 9 264 3593, e-mail: Kristof.Vandoorne@intec.UGent.be 
ABSTRACT 
Reservoir computing is a recent approach from the fields of machine learning and artificial neural networks to 
solve a broad class of complex classification and recognition problems such as speech and image recognition. As 
is typical for methods from these fields, it involves systems that were trained based on examples, instead of 
using an algorithmic approach. It originated as a new training technique for recurrent neural networks where the 
network is split in a reservoir that does the ‘computation’ and a simple readout function. This technique has been 
among the state-of-the-art. So far implementations have been mainly software based, but a hardware 
implementation offers the promise of being low-power and fast. We previously demonstrated with simulations 
that a network of coupled semiconductor optical amplifiers could also be used for this purpose on a simple 
classification task. This paper discusses two new developments. First of all, we identified the delay in between 
the nodes as the most important design parameter using an amplifier reservoir on an isolated digit recognition 
task and show that when optimized and combined with coherence it even yields better results than classical 
hyperbolic tangent reservoirs. Second we will discuss the recent advances in photonic reservoir computing with 
the use of resonator structures such as photonic crystal cavities and ring resonators. Using a network of 
resonators, feedback of the output to the network, and an appropriate learning rule, periodic signals can be 
generated in the optical domain. With the right parameters, these resonant structures can also exhibit spiking 
behaviour. 
Keywords: photonic reservoir computing, integrated optics, semiconductor optical amplifiers, nonlinear optics, 
optical neural networks, speech recognition. 
1. INTRODUCTION 
Reservoir Computing (RC) is a training concept for Recurrent Neural Networks (RNNs), introduced a few years 
ago [1,2]. It comes from the field of machine learning where systems are trained based on examples. In RC a 
randomly initialized RNN, called the reservoir, is used but left untrained. The states of all the nodes of this 
reservoir are fed into a linear readout, which can be trained with simple and well established methods, usually 
linear regression. Hence, difficulties, such as slow convergence, associated with training a full recurrent network 
are avoided. For several complex machine learning tasks it has been demonstrated that RC equals or outperforms 
other state-of-the-art techniques. An example is the prediction of the Mackey-Glass chaotic time series several of 
orders of magnitude better than classic methods [1]. 
Most reported results on reservoir computing use a (randomized) network of hyperbolic tangent or spiking 
neurons. However, recent work has indicated that a wide range of sufficiently high-dimensional nonlinear 
dynamic systems can be used as a reservoir [3]. Most implementations thus far have been software based, hence 
the pursuit of finding a suitable hardware platform for performing the reservoir calculation. This transition offers 
the potential for huge power consumption savings and speed enhancement. Photonics is an interesting candidate 
technology for building reservoirs, because it offers a range of different nonlinear interactions working on 
different timescales.   
In a previous paper we have shown that a network of Semiconductor Optical Amplifiers (SOAs) can be used as 
a reservoir on a benchmark speech recognition task [4]. In this paper we investigate some of the important design 
parameters of a photonic reservoir on the same task but with babble noise added to make it more difficult. 
Section 2 describes the design of classical reservoirs and section 3 the details of our photonic SOA reservoir. The 
isolated digit recognition task we use to compare classical and SOA reservoirs is discussed in section 4 and the 
obtained results in section 5. It turns out that the interconnection delay is a very important design parameter and 
that there is an optimal delay for this task. At this optimal delay coherent SOA reservoirs perform better then 
classical reservoirs. 
2. RESERVOIR COMPUTING 
Echo State Networks (ESNs) are an implementation of RC where all the nodes are hyperbolic tangent functions 
and the nodes are usually randomly connected [1]. The reservoir state x[t] is updated through the following 
formula: 
 ]),[][tanh(][ tttt resin xWuWx +=∆+  (1) 
where u[t] represents the inputs, the matrix Win the weights for all connections from input to reservoir and Wres 
is the interconnection weight matrix of the reservoir. Although the reservoir itself remains untrained (Wres is kept 
fixed during training), its performance depends critically on its dynamical regime, determined by the gain and 
loss in the network. Optimal performance is usually obtained near the edge of stability, i.e. the region in between 
stable and unstable or chaotic behaviour, because the system's memory is optimized there. Hence, to obtain good 
performance, we need to be able to tune a reservoir's dynamic regime to this edge-of-stability. A common 
measure for the dynamic regime is the spectral radius, the largest eigenvalue of the system's Jacobian, calculated 
at its maximal gain state (for classical hyperbolic tangent reservoirs, this corresponds to the largest eigenvalue of 
the network's interconnection weight matrix Wres). The spectral radius is an indication of the stability of the 
network. If its value is larger than one, the network might become unstable. Tuning the spectral radius close to 
one often yields reservoirs with close to optimal performance.  
3. PHOTONIC RESERVOIR COMPUTING 
Dedicated photonic hardware offers a lot of promise, but there are some fundamental differences between 
photonic and classical reservoirs. For example, optical information is usually encoded in changing power levels 
which are non-negative, in contrast to traditional RC which uses real values, and this makes it difficult to have 
negative weights and to subtract signals. In this paper we use coherent light which can be described by complex 
amplitudes )exp( Φ= iPA with P  the power of the light and Φ  the phase. 
3.1 Semiconductor optical amplifiers 
SOAs show gain saturation in their behaviour and that makes them the optical device closest to the hyperbolic 
tangent functions used in classical ESN reservoirs. That is the reason, besides being broadband and able to 
compensate losses in the network; we chose them as a first medium to verify the usefulness of photonic 
reservoirs. The resemblance can be seen in Fig. 1(a) showing the simulated steady state curve of an SOA and a 
scaled hyperbolic tangent. The similarity shows especially in the low-power regime (inputs < 1 mW) where the 
behaviour is near linear. This is the regime used for the simulations in this paper. The SOA model we use was 
proposed by Agrawal [5]. It captures the most important features such as gain saturation, carrier lifetime and 
gain dependent phase shift. 
               
 
Figure 1. (a) the upper branch of a scaled tanh and the simulated steady state behaviour of an SOA  
(b) 4x4 swirl topology 
 
3.2 Topology 
The topology used throughout the simulations is called a swirl topology as the information rotates through the 
network (Fig. 1(b)). It can be easily enlarged, while keeping the length of all connections equal. It is a nearest-
neighbour topology which is easier to make on a planar chip, than random networks. We lose some freedom in 
connectivity that exists in software implementations, but avoid the need for a lot of crossings. Since RC was 
initially conceived for RNNs, it is important to have some feedback connections in the network, hence the 
swirling of the network. Having the same length for every connection makes it easier to investigate and model 
the influence of certain connection parameters. Every connection is defined by three parameters: the time delay 
t∆ it introduces, the phase change ∆Φ and the loss P∆ . The loss determines the total amount of gain and loss 
in the network and hence the value of the spectral radius. As was mentioned in the beginning of section 3 the 
light is represented by complex amplitudes and since the connections influence both the phase and loss of the 
light they act as complex weights. Therefore to incorporate the influence of coherence and interference the 
spectral radius has to be calculated from the now complex interconnection matrix Wres, while also including the 
gain of the SOAs. Therefore the maximum gain of every SOA, i.e. at zero input (Fig. 1(a)), has to be 
incorporated in Wres. 
4. SPEECH RECOGNITION 
Speech recognition is very difficult to solve but reservoir computing with classical neural networks has been 
employed with success [6]. The task used in this paper is the discrimination between spoken digits, the words 
'zero' to 'nine', uttered by 5 female speakers. The dataset and the simulation framework for classical reservoirs is 
publicly available [7]. As is standard for speech recognition, some pre-processing of the raw speech signal is 
performed before it is fed into the reservoir. Often these methods involve a transformation to the frequency 
domain and highlighting certain frequencies specific for the ear by using some kind of ear model. The model 
used for the results in this paper was the Lyon ear model [8]. We added babble noise from the NOISEX database, 
with a signal-to-noise ratio of 3 dB to increase the complexity of the task [9]. The performance is measured with 
the Word Error Rate (WER), which gives the ratio of incorrectly classified samples to the total number of 
samples. 
Reservoir memory is related to the typical time scales of the reservoir itself. Therefore, to achieve optimal 
memory in a reservoir, the relevant time scales of the input signals must be adapted to those of the physical 
reservoir implementation. Audio signals are very slow, so we accelerated the speech signal to accord with 
timescales typical for the delays in our SOA network. The new durations of the digits are in the order of a few 
hundred ps. Hence, although we use this task to demonstrate the potential of photonic reservoir computing, we 
do not propose to use photonic reservoirs as a platform for standard real-time, slow audio signals. 
5. RESULTS 
In our experiments the input consists of 77 channels, coming from the pre-processing of the speech data. With 
such high-dimensional input, the number of nodes needs to be sufficiently large. All the experiments were 
therefore done with a rectangular 9x9 swirl network of 81 nodes. Before feeding input to the network, the 
different channels were mixed together in a random manner for every node and made non-negative by shifting 
them upwards with the minimum over all the inputs. The output of all the nodes is given to ten linear classifiers 
(one for every digit). These classifiers are trained on a part of the dataset and tested on a different part. The 
classifier with the strongest response for a certain input 'wins'.  
For every simulation a sweep was done over the loss and phase change ),( ∆Φ∆P  in every connection. The 
loss affects the spectral radius. A result of such a sweep is shown in Fig. 2. The darker a region, the better the 
performance. In this case a very short delay time was used (6.25 ps) and the optimal performance is only 
obtained for a very narrow phase region. Phase is important, since it determines the interference of the light 
when it combines in front of every SOA. In fact, below the instability boundary (spectral radius < 1), the 
reservoir is much more sensitive to phase than it is to gain. 
 
Figure 2.Performance of a coherent SOA network with swirl topology and a delay of 6.25 ps. 
 
 5.1 Delay 
The third parameter that defines the connections is the time delay. If we change this parameter and take each 
time the optimal result of the kind of sweep described in the previous section, we get a plot as in Fig. 3. First of 
all this figure shows that there exists an optimal delay for a given duration of the audio signal and the optimal 
delay corresponds to roughly half of the length of the speech signal. Second, it shows that classical networks 
with real-valued information perform worse than coherent networks with a complex-valued representation of 
information, no matter if the node is an SOA or a tanh. This makes that coherence has an added value for 
reservoirs when the phase can be controlled and that the interconnection length is a very important design 
parameter for SOA reservoirs. 
 
Figure 3. Performance of a classical tanh reservoir with real-valued information compared with two coherent 
networks. All networks have a swirl topology and an optimal performance at the same interconnection delay. 
6. CONCLUSIONS 
We have shown that a network of SOAs can be used as a reservoir for reservoir computing on an isolated digit 
recognition task and identified delay as an important design parameter. A network with optimal delay and 
coherence achieves even better results on this task than classical real-valued tanh reservoirs. Future research will 
focus on making a small-scale hardware implementation to validate the simulation results of this paper. 
ACKNOWLEDGEMENTS 
This work has been carried out in the framework of the IAP project Photonics@be of the Belgian Science Policy 
and the ERC project NaResCo. K. Vandoorne acknowledges the Special Research Fund (BOF) of Ghent 
University for a specialization grant. 
 REFERENCES 
[1] H. Jaeger, H. Haas: Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless 
communication, Science, vol. 304, pp. 78-80, Apr 2004. 
[2] W. Maass, T. Natschlager, H. Markram: Real-time computing without stable states: A new framework for 
neural computation based on perturbations, Neural Computation, vol. 14, pp. 2531-2560, Nov 2002. 
[3] D. Verstraeten, et al.: Pattern classification with CNNs as reservoirs, in Proc. International Symposium on 
Nonlinear Theory and its Applications (NOLTA), Budapest, Hungary, Sep 2008. 
[4] K. Vandoorne et al.: Photonic Reservoir Computing: a New Approach to Optical Information Processing, 
in Proc. ICTON 2010, Munich, Germany, June 2010, paper Th.A4.3. 
[5] G. P. Agrawal, N. A. Olsson,: Self-Phase Modulation and Spectral Broadening of Optical Pulses in 
Semiconductor-Laser Amplifiers, IEEE Journal of Quantum Electronics, vol. 25, pp. 2297-2306, Nov 
1989. 
[6] D. Verstraeten, et al.: Isolated word recognition with the liquid state machine: a case study, Information 
Processing Letters, vol.95, pp. 521-528, Sep 2005. 
[7] http://snn.elis.ugent.be/rctoolbox 
[8] R. Lyon: A computational model of filtering, detection and compression in the cochlea, in Proc. IEEE 
ICASSP, May 1982, pp. 1282-1285.  
[9] http://spib.rice.edu/spib/select_noise.html 
"
8,"ar
X
iv
:1
70
9.
09
42
9v
1 
 [c
s.C
V]
  2
7 S
ep
 20
17
1
FoodNet: Recognizing Foods Using Ensemble of
Deep Networks
Paritosh Pandey∗, Akella Deepthi∗, Bappaditya Mandal and N. B. Puhan
Abstract—In this work we propose a methodology for an
automatic food classification system which recognizes the contents
of the meal from the images of the food. We developed a multi-
layered deep convolutional neural network (CNN) architecture
that takes advantages of the features from other deep networks
and improves the efficiency. Numerous classical handcrafted
features and approaches are explored, among which CNNs are
chosen as the best performing features. Networks are trained and
fine-tuned using preprocessed images and the filter outputs are
fused to achieve higher accuracy. Experimental results on the
largest real-world food recognition database ETH Food-101 and
newly contributed Indian food image database demonstrate the
effectiveness of the proposed methodology as compared to many
other benchmark deep learned CNN frameworks.
Index Terms—Deep CNN, Food Recognition, Ensemble of
Networks, Indian Food Database.
I. INTRODUCTION AND CURRENT APPROACHES
THERE has been a clear cut increase in the health con-sciousness of the global urban community in the previous
few decades. Given the rising number of cases of health
problems attributed to obesity and diabetes reported every
year, people (including elderly, blind or semi-blind or dementia
patients) are forced to record, recognize and estimate calories
in their meals. Also, in the emerging social networking photo
sharing, food constitutes a major portion of these images.
Consequently, there is a rise in the market potential for such
fitness apps products which cater to the demand of logging
and tracking the amount of calories consumed, such as [1],
[2]. Food items generally tend to show intra-class variation
depending upon the method of preparation, which in turn is
highly dependent on the local flavors as well as the ingre-
dients used. This causes large variations in terms of shape,
size, texture, and color. Food items also do not exhibit any
distinctive spatial layout. Variable lighting conditions and the
point of view also lead to intra-class variations, thus making
the classification problem even more difficult [3], [4], [5].
Hence food recognition is a challenging task, one that needs
addressing.
In the existing literature, numerous methodologies assume
that the texture, color and shape of food items are well
defined [6], [7], [8], [9]. This may not be true because of
the local variations in the method of food preparation, as
well as the ingredients used. Feature descriptors like histogram
P. Pandey, A. Deepthi and N. B. Puhan are with the School of Electrical
Science, Indian Institute of Technology (IIT), Bhubaneswar, Odisha 751013,
India. E-mail: {pp20, da10, nbpuhan}@iitbbs.ac.in
B. Mandal is with the Kingston University, London, Surrey KT1 2EE,
United Kingdom. Email: b.mandal@kingston.ac.uk
∗ Represents equal contribution from the authors.
of gradient, color correlogram, bag of scale-invariant feature
transform, local binary pattern, spatial pyramidal pooling,
speeded up robust features (SURF), etc, have been applied
with some success on small datasets [9]. Hoashi et al. in
[10], and Joutou et al. in [11] propose multiple kernel learning
methods to combine various feature descriptors. The features
extracted have generally been used to train an SVM [12],
with a combination of these features being used to boost the
accuracy.
A rough estimation of the region in which targeted food
item is present would help to raise the accuracy for cases
with non-uniform background, presence of other objects and
multiple food items [13]. Two such approaches use standard
segmentation and object detection methods [14] or asking the
user to input a bounding box providing this information [15].
Kawano et al. [15], [16] proposed a semi-automated approach
for bounding box formation around the image and developed
a real-time recognition system. It is tedious, unmanageable
and does not cater to the need of full automation. Automatic
recognition of dishes would not only help users effortlessly
organize their extensive photo collections but would also help
online photo repositories make their content more accessible.
Lukas et al. in [17] have used a random forest to find
discriminative region in an image and have shown to under
perform convolutional neural network (CNN) feature based
method [18].
In order to improve the accuracy, Bettadapura et al. in
[19] used geotagging to identify the restaurant and search
for matching food item in its menu. Matsuda et al. in [20]
employed co-occurrence statistics to classify multiple food
items in an image by eliminating improbable combinations.
There has been certain progress in using ingredient level
features [21], [22], [23] to identify the food item. A variant of
this method is the usage of pairwise statistics of local features
[24]. In the recent years CNN based classification has shown
promise producing excellent results even on large and diverse
databases with non-uniform background. Notably, deep CNN
based transferred learning using fine-tuned networks is used
in [25], [26], [27], [28], [29] and cascaded CNN networks
are used in [30]. In this work, we extend the CNN based
approaches towards combining multiple networks and extract
robust food discriminative features that would be resilient
against large variations in food shape, size, color and texture.
We have prepared a new Indian food image database for this
purpose, the largest to our knowledge and experimented on two
large databases, which demonstrates the effectiveness of the
proposed framework. We will make all the developed models
and Indian food database available online to public [31].
2Section II describes our proposed methodology and Section III
provides the experimental results before drawing conclusions
in Section IV.
II. PROPOSED METHOD
Our proposed framework is based on recent emerging very
large deep CNNs. We have selected CNNs because their ability
to learn operations on visual data is extremely good and they
have been employed to obtain higher and higher accuracies
on challenges involving large scale image data [32]. We have
performed extensive experiments using different handcrafted
features (such as bag of words, SURF, etc) and CNN feature
descriptors. Experimental results show that CNNs outperform
all the other methods by a huge margin, similar to those
reported in [9] as shown in Table I. It can be seen that CNN
based methods (SELC & CNN) features performs much better
as compared to others.
TABLE I
ACCURACY (%) OF HANDCRAFTED & CNN FEATURES ON ETH FOOD-101 DATABASE. THE METHODS ARE SURF +
BAG OF WORDS 1024 (BW1024), SURF + INDEPENDENT FISCHER VECTOR 64 (SURF+IFV64), BAG OF WORDS
(BW), INDEPENDENT FISCHER VECTORS (IFV), MID-LEVEL DISCRIMINATIVE SUPERPIXEL (MDS), RANDOM
FOREST DISCRIMINATIVE COMPONENT (RFDC), SUPERVISED EXTREME LEARNING COMMITTEE (SELC) AND
ALEXNET TRAINED FROM SCRATCH (CNN).
Methods BW1024 SURF+IFV64 BW IFV MDS RFDC SELC CNN
Top-1 33.47 44.79 28.51 38.88 42.63 50.76 55.89 56.40
A. Proposed Ensemble Network Architecture
We choose AlexNet architecture by Krizhevsky et al. [18]
as our baseline because it offers the best solution in terms
of significantly lesser computational time as compared to any
other state-of-the-art CNN classifier. GoogLeNet architecture
by Szegedy et al. [33] uses the sparsity of the data to create
dense representations that give information about the image
with finer details. It develops a network that would be deep
enough, as it increases accuracy and yet have significantly
less parameters to train. This network is an approximation
of the sparse structure of a convolution network by dense
components. The building blocks called Inception modules,
is basically a concatenation of filter banks with a mask size
of 1 × 1, 3 × 3 and 5 × 5. If the network is too deep, the
inception modules lead to an unprecedented rise in the cost of
computation. Therefore, 1×1 convolutions are used to embed
the data output from the previous layers.
ResNet architecture by He et al. [34] addresses the problem
of degradation of learning in networks that are very deep. In
essence a ResNet is learning on residual functions of the input
rather than unreferenced functions. The idea is to reformulate
the learning problem into one that is easier for the network to
learn. Here the original problem of learning a function H(x)
gets transformed into learning non-linearly by various layers
fitting the functional form H(x) = Γ(x) + x, which is easier
to learn, where the layers have already learned Γ(x) and the
original input is x. These CNN networks are revolutionary
in the sense that they were at the top of the leader board
of ImageNet classification at one or other time [32], with
ResNet being the network with maximum accuracy at the time
Fig. 1. Our proposed CNN based ensemble network architecture.
of writing this paper. The main idea behind employing these
networks is to compare the increment in accuracies with the
depth of the network and the number of parameters involved in
training. Our idea is to create an ensemble of these classifiers
using another CNN on the lines of a Siamese network [35]
and other deep network combinations [36].
In a Siamese network [35], two or more identical subnet-
works are contained within a larger network. These subnet-
works have the same configuration and weights. It has been
used to find comparisons or relationships between the two
input objects or patches. In our architecture, we use this idea
to develop a three layered structure to combine the feature
outputs of three different subsections (or subnetworks) as
shown in Fig. 1. We hypothesize that these subnetworks with
proper fine-tuning would individually contribute to extract
better discriminative features from the food images. However,
the parameters along with the subnetwork architectures are
different and the task is not that of comparison (as in case
of Siamese network [35]) but pursue classification of food
images. Our proposition is that the features once added with
appropriate weights would give better classification accuracies.
Let I(w, h, c) represents a pre-processed input image of size
w × h pixels to each of the three fine-tuned networks and c
is the number of channels of the image. Color images are
used in our case. We denote C(m,n, q) as the convolutional
layer, where m and n are the sides length of the receptive
field and q is the number of filter banks. Pooling layer is
denoted by P (s, r), where r is the side length of the pooling
receptive field and s is the number of strides used in our
CNN model. In our ensemble net we did not use pooling. But
in our fine-tuned networks pooling is employed with variable
parameters. GoogLeNet for example uses overlapping pooling
in the inception module. All convolution layers are followed
by ReLU layers (see the text in Sec II-B) considered as an in-
built activation. L represents the local response normalization
layer. Fully connected layer is denoted by F (e), where e is
the number of neurons. Hence, the AlexNet CNN model after
fine-tuning is represented as:
ΦA ≡ I(227, 227, 3) −→ C(11, 4, 96) −→ L −→ P (2, 3) −→ C(5, 1, 256)
−→ L −→ P (2, 3) −→ C(3, 1, 384) −→ C(3, 1, 384) −→ C(3, 1, 256)
−→ P (2, 3) −→ F (4096) −→ F (4096) −→ F (e).
(1)
AlexNet is trained in a parallel fashion, referred as a depth
of 2. Details of the architecture can be found in [18]. For
GoogLeNet we need to define the inception module as:
D(c1, cr3, c3, cr5, c5, crM), where c1, c3 and c5 represent
number of filter of size 1×1, 3×3 and 5×5, respectively. cr3
and cr5 represent number of 1×1 filters used in the reduction
layer prior to 3× 3 and 5× 5 filters, and crM represents the
3number of 1×1 filters used as reduction after the built in max
pool layer. Hence GoogLeNet is fine-tuned as:
ΦG ≡ I(224, 224, 3) −→ C(7, 2, 64) −→ P (2, 3) −→ L −→ C(1, 1, 64)
−→ C(3, 1, 192) −→ L −→ P(2, 3) −→ D(64, 96, 128, 16, 32, 32)
−→ D(128, 128, 192, 32, 96, 64) −→ P (2, 3) −→
D(192, 96, 208, 16, 48, 64) −→ D(160, 112, 224, 24, 64, 64) −→
D(128, 128, 256, 24, 64, 64) −→ D(112, 144, 288, 32, 64, 64) −→
D(256, 160, 320, 32, 128, 128) −→ P (2, 3) −→ D(256, 160, 320, 32,
128, 128) −→ D(384, 192, 384, 48, 128, 128) −→ P
∗
(1, 7) −→ F (e),
(2)
P ∗ refers to average pooling rather than max pooling used ev-
erywhere else. For fine-tuned ResNet, each repetitive residual
unit is presented inside as R and it is defined as:
ΦR ≡ I(224, 224, 3) −→ C(7, 2, 64) −→ P(2, 3) −→ 3 × R(C(1, 1, 64)
−→ C(3, 1, 64) −→ C(1, 1, 256)) −→ R(C(1, 2, 128) −→ C(3, 2, 128)
−→ C(1, 2, 512)) −→ 3 × R(C(1, 1, 128) −→ C(3, 1, 128)
−→ C(1, 1, 512)) −→ R(C(1, 2, 256) −→ C(3, 2, 256) −→
C(1, 2, 1024)) −→ 5 × R(C(1, 1, 256) −→ C(3, 1, 256) −→
C(1, 1, 1024)) −→ R(C(1, 2, 512) −→ C(3, 2, 512) −→ C(1, 2, 2048))
−→ 2 × R(C(1, 1, 512) −→ C(3, 1, 512) −→ C(1, 1, 2048))
−→ P
∗
(1, 7) −→ F (e).
(3)
Batch norm is used after every convolution layer in ResNet.
The summations at the end of each residual unit are followed
by a ReLU unit. For all cases, the length of F (e) depends
on the number of categories to classify. In our case, e is the
number of classes. Let Fi denote the features from each of the
fine-tuned deep CNNs given by (1)-(3), where i ∈ {A,G,R}.
Let the concatenated features are represented by Ω(O, c),
where O is the output features from all networks, given by:
O = concatenate(wiFi) | ∀ i, (4)
where wi is the weight given to features from each of the
networks with the constraint, such that Σiwi = 1. We define
the developed ensemble net as the following:
ΦE ≡ Ω(e ∗ η, c) −→ ReLU −→ F (e) −→ SoftMax,
(5)
where η is the number of fine-tuned networks. The SoftMax
function or the normalized exponential function is defined as:
S(F )j =
expFj
∑e
k=1 exp
Fk
, for j = 1, 2, . . . , e, (6)
where exp is the exponential. The final class prediction
D ∈ {1, 2, . . . , e} is obtained by finding the maximum of
the values of S(F )j , given by:
D = argmax
j
(S(F )j), for j = 1, 2, . . . , e. (7)
B. Network Details
The ensemble net we designed consists of three layers as
shown in Fig. 1. Preprocessed food images are used to fine-
tune all the three CNN networks: AlexNet, GoogLeNet and
ResNet. Then the first new layer one concatenates the features
obtained from the previously networks, passing it out with a
rectified linear unit (ReLU) non-linear activation. The outputs
are then passed to a fully connected (fc) layer that convolves
the outputs to the desired length of the number of classes
present. This is followed by a softmax layer which computes
the scores obtained by each class for the input image.
The pre-trained models are used to extract features and train
a linear kernel support vector machine (SVM). The feature
outputs of the fully connected layers and max-pool layers of
AlexNet and GoogLeNet are chosen as features for training
and testing the classifiers. For feature extraction, the images
are resized and normalized as per the requirement of the
networks. For AlexNet we used the last fully connected layer
to extract features (fc7) and for GoogLeNet we used last max
pool layer (cls3 pool). On the ETH Food 101 database, the
top-1 accuracy obtained remained in the range of 39.6% for
AlexNet to 44.06% for GoogLeNet, with a feature size varying
from a minimum of 1000 features per image to 4096 features
per image. Feature length of the features extracted out of the
last layer is 1000. The feature length out of the penultimate
layer of AlexNet gave a feature length of 4096 features, while
the ones out of GoogLeNet had a feature length of 1024. All
the three networks are fine-tuned using the ETH Food-101
database. The last layer of filters is removed from the network
and replaced with an equivalent filter giving an output of the
size 1 × 1× 101, i.e., a single value for 101 channels. These
numbers are interpreted as scores for each of the food class
in the dataset. Consequently, we see a decrease in the feature
size from 1× 1000 for each image to 1× 101 for each image.
AlexNet is trained for a total of 16 epochs.
We choose the MatConvNet [37] implementation of
GoogLeNet with maximum depth and maximum number of
blocks. The implementation consists of 100 layers and 152
blocks, with 9 Inception modules (very deep!). To train
GoogLeNet, the deepest softmax layer is chosen to calculate
objective while the other two are removed. The training ran
for a total of 20 epochs. ResNet’s smallest MatConvNet model
with 50 layers and 175 blocks is used. The capacity to use any
deeper model is limited by the capacity of our hardware. The
batch size is reduced to 32 images for the same reason. ResNet
is trained with the data for 20 epochs. The accuracy obtained
increased with the depth of the network. The ensemble net
is trained with normalized features/outputs of the above three
networks. Parametrically weights are decided for each network
feature by running the experiments multiple times. A total of
30 epochs are performed. A similar approach is followed while
fine-tuning the network for Indian dataset. As the number of
images is not very high, jitters are introduced in the network
to make sure the network remains robust to changes. Same
depth and parameters are used for the networks. The output
feature has a length of 1 × 1 × 50 implying a score for each
of the 50 classes.
III. EXPERIMENTAL SETUP AND RESULTS
The experiments are performed on a high end server with
128GB of RAM equipped with a NVDIA Quadro K4200 with
4GB of memory and 1344 CUDA cores. We performed the
experiments on MATLAB 14a using the MatConvNet library
offered by vlFeat [38]. Caffe’s pre-trained network models
imported in MatConvNet are used. We perform experiments
on two databases: ETH Food-101 Database and and our own
newly contributed Indian Food Database.
A. Results on ETH Food-101 Database
ETH Food-101 [17] is the largest real-world food recog-
nition database consisting of 1000 images per food class
4Fig. 2. Top row: 10 sample Indian food images. Bottom two rows: one of
the food samples (1 class) variations (20 images).
picked randomly from foodspotting.com, comprising of 101
different classes of food. So there are 101,000 food images
in total, sample images can be seen in [17]. The top 101
most popular and consistently named dishes are chosen and
randomly sampled 750 training images per class are extracted.
Additionally, 250 test images are collected for each class, and
are manually cleaned. Purposefully, the training images are not
cleaned, and thus contain some amount of noise. This comes
mostly in the form of intense colors and sometimes wrong
labels to increase the robustness of the data. All images are
rescaled to have a maximum side length of 512 pixels. In
all our experiments we follow the same training and testing
protocols as that in [17], [9].
All the real-world RGB food images are converted to HSV
format and histogram equalization are applied on only the
intensity channel. The result is then converted back to RGB
format. This is done to ensure that the color characteristics
of the image does not change because of the operation and
alleviate any bias that could have been present in the data due
to intensity/illumination variations.
TABLE II
ACCURACY (%) FOR ETH FOOD-101 AND COMPARISON
WITH OTHER METHODS AFTER FINE-TUNING.
Network/Features Top-1 Top-5 Top-10
AlexNet 42.42 69.46 80.26
GoogLeNet 53.96 80.11 88.04
Lukas et al. [17] 50.76 - -
Kawano et al. [15] 53.50 81.60 89.70
Martinel et al. [9] 55.89 80.25 89.10
ResNet 67.59 88.76 93.79
Ensemble Net 72.12 91.61 95.95
TABLE III
ACCURACY (%) FOR INDIAN FOOD DATABASE AND
COMPARISON WITH OTHER METHODS AFTER
FINE-TUNING.
Network/Features Top-1 Top-5 Top-10
AlexNet 60.40 90.50 96.20
GoogLeNet 70.70 93.40 97.60
ResNet 43.90 80.60 91.50
Ensemble Net 73.50 94.40 97.60
Table II shows the Top-1, Top-5 and Top-10 accuracies
using numerous current state-of-the-art methodologies on this
database. We tried to feed outputs from the three networks into
the SVM classifier but the performance was not good. We
have noted only the highest performers, many more results
can be found in [9]. It is evident that with fine-tuning the
network performance has increased to a large extent. Fig. 3
(a) shows accuracies with the ranks plot up to top 10, where
the rank r : r ∈ {1, 2, . . . , 10} shows corresponding accuracy
of retrieving at least 1 correct image among the top r retrieved
images. This kind of graphs show the overall performance of
the system at different number of retrieved images. From Table
II and Fig. 3 (a), it is evident that our proposed ensemble net
has outperformed consistently all the current state-of-the-art
methodologies on this largest real-world food database.
1 2 3 4 5 6 7 8 9 1040
50
60
70
80
90
100
Rank
Ac
cu
ra
cy
 (%
)
 
 
AlexNet
GoogLeNet
ResNet
Ensemble Net
(a)
1 2 3 4 5 6 7 8 9 1040
50
60
70
80
90
100
Rank
Ac
cu
ra
cy
 (%
)
 
 
AlexNet
GoogLeNet
ResNet
Ensemble Net
(b)
Fig. 3. Rank vs Accuracy plots using various CNN frameworks, (a) for ETH
Food 101 Database and (b) for Indian Food Database.
B. Results on Indian Food Database
One of the contributions of this paper is the setting up of
an Indian food database, the first of its kind. It consists of 50
food classes having 100 images each. Some sample images
are shown in Fig. 2. The classes are selected keeping in mind
the varied nature of Indian cuisine. They differ in terms of
color, texture, shape and size as the Indian food lacks any
kind of generalized layout. We have ensured a healthy mix
of dishes from all parts of the country giving this database a
true representative nature. Because of the varied nature of the
classes present in the database, it offers the best option to test
a protocol and classifier for its robustness and accuracy. We
collected images from online sources like foodspotting.com,
Google search, as well as our own captured images using hand-
held mobile devices. Extreme care was taken to remove any
kind of watermarking from the images. Images with textual
patterns are cropped, most of the noisy images discarded and
a clean dataset is prepared. We also ensured that all the images
are of a minimum size. No upper bound on image size has
been set. Similar to the ETH Food-101 database protocol, we
have randomly selected 80 food images per class for 50 food
classes in the training and remaining in the test dataset.
Fig. 3 (b) shows accuracies with the ranks plot up to top 10
and Table III shows the Top-1, Top-5 and Top-10 accuracies
using some of the current state-of-the-art methodologies on
this database. Both these depict that our proposed ensemble
of the networks (Ensemble Net) is better at recognizing food
images as compared to that of the individual networks. ResNet
under performs as compared to GoogLeNet and AlexNet
probably because of the lack of sufficient training images
to train the network parameters. For overall summary: as is
evident from these figures (Fig. 3 (a) and (b)) and tables
(Tables II and III) that there is no single second best method
that outperforms all others methods in both the databases,
however, our proposed approach (Ensemble Net) outperforms
all other methods consistently for all different ranks in both
the databases.
IV. CONCLUSIONS
Food recognition is a very crucial step for calorie estimation
in food images. We have proposed a multi-layered ensemble
of networks that take advantages of three deep CNN fine-tined
subnetworks. We have shown that these subnetworks with
proper fine-tuning would individually contribute to extract
5better discriminative features from the food images. However,
in these subnetworks the parameters are different, the sub-
network architectures and tasks are different. Our proposed
ensemble architecture outputs robust discriminative features
as compared to the individual networks. We have contributed
a new Indian Food Database, that would be made available
to public for further evaluation and enrichment. We have
conducted experiments on the largest real-world food images
ETH Food-101 Database and Indian Food Database. The
experimental results show that our proposed ensemble net
approach outperforms consistently all other current state-of-
the-art methodologies for all the ranks in both the databases.
REFERENCES
[1] MealSnap:, “Magical meal logging for iphone,” 2017. [Online].
Available: http://mealsnap.com
[2] Eatly:, “Eat smart (snap a photo of your meal
and get health ratings),” 2017. [Online]. Available:
https://itunes.apple.com/us/app/eatly-eat-smart-snap-photo/id661113749
[3] A. Myers, N. Johnston, V. Rathod, A. Korattikara, A. N. Gorban,
N. Silberman, S. Guadarrama, G. Papandreou, J. Huang, and K. Murphy,
“Im2calories: Towards an automated mobile vision food diary,” in IEEE
International Conference on Computer Vision (ICCV), 2015, pp. 1233–
1241.
[4] B. Mandal and H.-L. Eng., “3-parameter based eigenfeature regular-
ization for human activity recognition,” in 35th IEEE International
Conference on Acoustics Speech and Signal Processing (ICASSP), Mar
2010, pp. 954–957.
[5] B. Mandal, L. Li, V. Chandrasekhar, and J. H. Lim, “Whole space sub-
class discriminant analysis for face recognition,” in IEEE International
Conference on Image Processing (ICIP), Quebec city, Canada, Sep 2015,
pp. 329–333.
[6] S. Sasano, X. H. Han, and Y. W. Chen, “Food recognition by com-
bined bags of color features and texture features,” in 9th International
Congress on Image and Signal Processing, BioMedical Engineering and
Informatics (CISP-BMEI), Oct 2016, pp. 815–819.
[7] C. Pham and T. N. T. Thanh, “Fresh food recognition using feature
fusion,” in International Conference on Advanced Technologies for
Communications, Oct 2014, pp. 298–302.
[8] B. Mandal, W. Zhikai, L. Li, and A. Kassim, “Evaluation of descriptors
and distance measures on benchmarks and first-person-view videos
for face identification,” in International Workshop on Robust Local
Descriptors for Computer Vision, ACCV, Singapore, Nov 2014, pp. 585–
599.
[9] N. Martinel, C. Piciarelli, and C. Micheloni, “A supervised extreme
learning committee for food recognition,” Computer Vision and Image
Understanding, vol. 148, pp. 67–86, 2016.
[10] H. Hajime, T. Joutou, and K. Yanai, “Image recognition of 85 food
categories by feature fusion,” in IEEE International Symposium on
Multimedia (ISM), Dec 2010.
[11] T. Joutou and K. Yanai, “A food recognition system with multiple kernel
learning,” in IEEE International Conference on Image Processing, Nov
2009.
[12] C. Cortes and V. Vapnik, “Support-vector networks,” Machine Learning,
vol. 20, no. 3, pp. 273–297, 1995.
[13] S. Liu, D. He, and X. Liang, “An improved hybrid model for automatic
salient region detection,” IEEE Signal Processing Letters, vol. 19, no. 4,
pp. 207–210, Apr 2012.
[14] M. Bolan˜os and P. Radeva, “Simultaneous food localization and
recognition,” CoRR, vol. abs/1604.07953, 2016. [Online]. Available:
http://arxiv.org/abs/1604.07953
[15] Y. Kawano and K. Yanai, “Real-time mobile food recognition system,”
in Computer Vision and Pattern Recognition Workshops (CVPRW), Jun
2013.
[16] K. Yanai and Y. Kawano, “Food image recognition using deep con-
volutional network with pre-training and fine-tuning,” in 2015 IEEE
International Conference on Multimedia Expo Workshops (ICMEW),
June 2015, pp. 1–6.
[17] L. Bossard, M. Guillaumin, and L. Van Gool, “Food-101 – mining dis-
criminative components with random forests,” in European Conference
on Computer Vision, 2014, pp. 446–461.
[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in Neural Infor-
mation Processing Systems, 2012.
[19] V. Bettadapura, E. Thomaz, A. Parnami, G. D. Abowd, and I. Essa,
“Leveraging context to support automated food recognition in restau-
rants,” in Proceedings of the IEEE Winter Conference on Applications
of Computer Vision, 2015, pp. 580–587.
[20] M. Yuji and K. Yanai, “Multiple-food recognition considering co-
occurrence employing manifold ranking,” in Proceedings of the 21st
International Conference on Pattern Recognition, Nov 2012.
[21] X. Wang, D. Kumar, N. Thorne, M. Cord, and F. Precioso, “Recipe
recognition with large multimodal food dataset,” in IEEE International
Conference on Multimedia & Expo Workshops (ICMEW), Jul 2015.
[22] J. Chen and C.-w. Ngo, “Deep-based ingredient recognition for cooking
recipe retrieval,” in Proceedings of the ACM on Multimedia Conference,
2016, pp. 32–41.
[23] J. Baxter, “Food recognition using ingredient-level features.” [Online].
Available: http://jaybaxter.net/6869 food project.pdf
[24] S. Yang, M. Chen, D. Pomerleau, and R. Sukthankar, “Food recognition
using statistics of pairwise local features,” in Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition, Jun 2010.
[25] S. Song, V. Chandrasekhar, B. Mandal, L. Li, J. Lim, G. S. Babu,
P. P. San, and N. Cheung, “Multimodal multi-stream deep learning for
egocentric activity recognition,” in 2016 IEEE Conference on Computer
Vision and Pattern Recognition Workshops, CVPR Workshops 2016, Las
Vegas, NV, USA, 2016, pp. 378–385.
[26] S. Zhang, H. Yang, and Z.-P. Yin, “Transferred deep convolutional neural
network features for extensive facial landmark localization,” IEEE Signal
Processing Letters, vol. 23, no. 4, pp. 478–482, Apr 2016.
[27] H. Park and K. M. Lee, “Look wider to match image patches with
convolutional neural networks,” IEEE Signal Processing Letters, vol. PP,
no. 99, pp. 1–1, Dec 2016.
[28] A. G. del Molino, B. Mandal, J. Lin, J. Lim, V. Subbaraju, and
V. Chandrasekhar, “Vc-i2r@imageclef2017: Ensemble of deep learned
features for lifelog video summarization,” in Working Notes of CLEF
2017 - Conference and Labs of the Evaluation Forum, Dublin, Ireland,
Sep 2017.
[29] T. Gan, Y. Wong, B. Mandal, V. Chandrasekhar, and M. Kankanhalli,
“Multi-sensor self-quantification of presentations,” in ACM Multimedia,
Brisbane, Australia, Oct 2015, pp. 601–610.
[30] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and
alignment using multitask cascaded convolutional networks,” IEEE
Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, Oct 2016.
[31] B. Mandal, “Publication and database,” 2017. [Online]. Available:
https://sites.google.com/site/bappadityamandal/publications-1
[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-
Fei, “Imagenet large scale visual recognition challenge,” Int. J. Comput.
Vision, vol. 115, no. 3, pp. 211–252, Dec 2015.
[33] C. Szegedy and et al., “Going deeper with convolutions,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[34] H. Kaiming and et al., “Deep residual learning for image recognition,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.
[35] J. Bromley, I. Guyon, Y. LeCun, E. Sa¨ckinger, and R. Shah, “Sig-
nature verification using a “siamese” time delay neural network,” in
Proceedings of the 6th International Conference on Neural Information
Processing Systems (NIPS), 1993, pp. 737–744.
[36] H. Zuo, H. Fan, E. Blasch, and H. Ling, “Combining convolutional
and recurrent neural networks for human skin detection,” IEEE Signal
Processing Letters, vol. 24, no. 3, pp. 289–293, Mar 2017.
[37] VLFEAT, “Vlfeat open source,” 2017. [Online]. Available:
http://www.vlfeat.org/matconvnet/
[38] ——, “Vlfeat open source,” 2017. [Online]. Available: www.vlfeat.org
"
9,"A Taxonomy of Deep Convolutional Neural Nets for Computer Vision ∗
Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri,
Nikita Prabhu, Srinivas S S Kruthiventi and R. Venkatesh Babu
Video Analytics Lab, Indian Institute of Science, Bangalore
http://val.serc.iisc.ernet.in/
Abstract
Traditional architectures for solving computer vision problems and the degree of success they enjoyed have
been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling
alternative – that of automatically learning problem-specific features. With this new paradigm, every problem in
computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important
to understand what kind of deep networks are suitable for a given problem. Although general surveys of this
fast-moving paradigm (i.e. deep-networks) exist, a survey specific to computer vision is missing. We specifically
consider one form of deep networks widely used in computer vision - convolutional neural networks (CNNs). We
start with “AlexNet” as our base CNN and then examine the broad variations proposed over time to suit different
applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners
intending to use deep-learning techniques for computer vision.
1 Introduction
Computer vision problems like image classification and object detection have traditionally been approached using
hand-engineered features like SIFT by [Lowe, 2004] and HoG by [Dalal and Triggs, 2005]. Representations based on
the Bag-of-visual-words descriptor (see [Yang et al., 2007]), in particular, enjoyed success in image classification. These
were usually followed by learning algorithms like Support Vector Machines (SVMs). As a result, the performance
of these algorithms relied crucially on the features used. This meant that progress in computer vision was based
on hand-engineering better sets of features. With time, these features started becoming more and more complex -
resulting in a difficulty with coming up better, more complex features. From the perspective of the computer vision
practitioner, there were two steps to be followed: feature design and learning algorithm design, both of which were
largely independent.
Meanwhile, some researchers in the machine learning community had been working on learning models which
incorporated learning of features from raw images. These models typically consisted of multiple layers of non-linearity.
This property was considered to be very important - and this lead to the development of the first deep learning models.
Early examples like Restricted Boltzmann Machines ([Hinton, 2002]) , Deep Belief Networks ([Hinton et al., 2006]) and
Stacked Autoencoders ([Vincent et al., 2010]) showed promise on small datasets. The primary idea behind these works
was to leverage the vast amount of unlabelled data to train models. This was called the ‘unsupervised pre-training’
stage. It was believed that these ‘pre-trained’ models would serve as a good initialization for further supervised tasks
such as image classification. Efforts to scale these algorithms on larger datasets culminated in 2012 during the ILSVRC
competition (see [Russakovsky et al., 2015]), which involved, among other things - the task of classifying an image
into one of thousand categories. For the first time, a Convolutional Neural Network (CNN) based deep learnt model
by [Krizhevsky et al., 2012] brought down the error rate on that task by half, beating traditional hand-engineered
approaches. Surprisingly, this could be achieved by performing end-to-end supervised training, without the need
for unsupervised pre-training. Over the next couple of years, ‘Imagenet classification using deep neural networks’
by [Krizhevsky et al., 2012] became one of the most influential papers in computer vision. Convolutional Neural
Networks, a particular form of deep learning models, have since been widely adopted by the vision community. In
particular, the network trained by Alex Krizhevsky, popularly called “AlexNet” has been used and modified for
various vision problems. Hence, in this article, we primarily discuss CNNs, as they are more relevant to the vision
community. With the plethora of deep convolutional networks that exist for solving different tasks, we feel the time
is right to summarize CNNs for a survey. This article can also serve as a guide for beginning practitioners in deep
learning/computer vision.
∗This article has been published in Frontiers in Robotics and AI. Please refer to http://goo.gl/6691Bm for the original article.
1
ar
X
iv
:1
60
1.
06
61
5v
1 
 [c
s.C
V]
  2
5 J
an
 20
16
Figure 1: An illustration of the weights in the AlexNet model. Note that after every layer, there is an implicit ReLU
non-linearity. The number inside curly braces represents the number of filters with dimensions mentioned above it.
The paper is organized as follows. We first develop the general principles behind CNNs (Section 2), and then
discuss various modifications to suit different problems (Section 3). Finally, we discuss some open problems (Section
4) and directions for further research.
2 Introduction to Convolutional Neural Networks
The idea of a Convolutional Neural Network (CNN) is not new. This model had been shown to work well for
hand-written digit recognition by [LeCun et al., 1998]. However, due to the inability of these networks to scale to
much larger images, they slowly fell out of favour. This was largely due to memory and hardware constraints, and the
unavailability of large amounts of training data. With increase in computational power thanks to wide availability of
GPUs, and the introduction of large scale datasets like the ImageNet (see [Russakovsky et al., 2015]) and the MIT
Places dataset (see [Zhou et al., 2014]), it was possible to train larger, more complex models. This was first shown
by the popular AlexNet model which was discussed earlier. This largely kick-started the usage of deep networks in
computer vision.
2.1 Building Blocks of CNNs
In this section, we shall look at the basic building blocks of CNNs in general. This assumes that the reader is
familiar with traditional neural networks, which we shall call “fully connected layers” in this article. Figure 1 shows a
representation of the weights in the AlexNet model. While the first five layers are convolutional, the last three are
fully connected layers.
2.1.1 Why convolutions?
Using traditional neural networks for real-world image classification is impractical for the following reason: Consider
a 2D image of size 200 × 200 for which we would have have 40, 000 input nodes. If the hidden layer has 20, 000
nodes, the size of the matrix of input weights would be 40, 000 × 20, 000 = 800 Million. This is just for the first
layer – as we increase the number of layers, this number increases even more rapidly. Besides, vectorizing an image
completely ignores the complex 2D spatial structure of the image. How do we build a system that overcomes both
these disadvantages?
One way is to use 2D convolutions instead of matrix multiplications. Learning a set of convolutional filters (each of
11× 11, say) is much more tractable than learning a large matrix (40000× 20000). 2D convolutions also naturally take
the 2D structure of images into account. Alternately, convolutions can also be thought of as regular neural networks
with two constraints (See [Bishop, 2006]):
• Local connectivity: This comes from the fact that we use a convolutional filter with dimensions much smaller
than the image it operates on. This contrasts with the global connectivity paradigm typically relevant to
vectorized images.
2
• Weight sharing: This comes from the fact that we perform convolutions, i.e. we apply the same filter across
the image. This means that we use the same local filters on many locations in the image. In other words, the
weights between all these filters are shared.
There is also evidence from visual neuroscience for similar computations within the human brain. [Hubel and Wiesel, 1962]
found two types of cells in the primary visual cortex - the simple cells and the complex cells. The simple cell responded
primarily to oriented edges and gratings - which are reminiscent of Gabor filters, a special class of convolutional filters.
The complex cells were also sensitive to these egdes and grating. However, they exhibited spatial invariance as well.
This motivated the Neocognitron model by [Fukushima, 1980], which proposed the learning of convolutional filters in
an artificial neural network. This model is said to have inspired convolutional networks, which are analogous to the
simple cells mentioned above.
In practical CNNs however, the convolution operations are not applied in the traditional sense wherein the filter
shifts one position to the right after each multiplication. Instead, it is common to use larger shifts (commonly referred
to as stride). This is equivalent to performing image down-sampling after regular convolution.
If we wish to train these networks on RGB images, one would need to learn multiple multi-channel filters. In the
representation in Figure 1, the numbers 11× 11× 3, along with {96} below C1 indicates that there are 96 filters in
the first layers, each of spatial dimension of 11× 11, with one for each of the 3 RGB channels.
We note that this paradigm of convolution like operations (location independent feature-detectors) is not entirely
suitable for registered images. As an example, images of faces require different feature-detectors at different spatial
locations. To account for this, [Taigman et al., 2014] consider only locally-connected networks with no weight-sharing.
Thus, the choice of layer connectivity depends on the underlying type of problem.
2.1.2 Max-Pooling
The Neocognitron model inspired the modelling of simple cells as convolutions. Continuing in the same vein, the
complex cells can be modelled as a max-pooling operation. This operation can be thought of as a max filter, where
each n× n region is replaced with it’s max value. This operation serves two purposes:
1. It picks out the highest activation in a local region, thereby providing a small degree of spatial invariance. This
is analogous to the operation of complex cells.
2. It reduces the size of the activation for the next layer by a factor of n2. With a smaller activation size, we need
a smaller number of parameters to be learnt in the later layers.
2.1.3 Non-linearity
Deep networks usually consist of convolutions followed by a non-linear operation after each layer. This is necessary
because cascading linear systems (like convolutions) is another linear system. Non-linearities between layers ensure
that the model is more expressive than a linear model.
In theory, no non-linearity has more expressive power than any other, as long as they are continuous, bounded
and monotonically increasing (see [Hornik, 1991]). Traditional feedforward neural networks used the sigmoid (σ(x) =
1
1+e−x ) or the tanh (tanh(x) =
ex−e−x
ex+e−x ) non-linearities. However, modern convolutional networks use the ReLU
(ReLU(x) = max(0, x)) non-linearity. CNNs with this non-linearity have been found to train faster, as shown by
[Nair and Hinton, 2010].
Recently, [Maas et al., 2013] introduced a new kind of non-linearity, called the leaky-ReLU. It was defined as
Leaky-ReLU(x) = max(0, x) + αmin(0, x), where α is a pre-determined parameter. [He et al., 2015] improved on this
by suggesting that the α parameter also be learnt, leading to a much richer model.
2.2 Depth
The Universal Approximation theorem by [Hornik, 1991] states that a neural network with a single hidden layer is
sufficient to model any continuous function. However, [Bengio, 2009] showed that such networks need an exponentially
large number of neurons when compared to a neural network with many hidden layers. Recently, [Romero et al., 2014],
and [Ba and Caruana, 2014] explicitly showed that a deeper neural network can be trained to perform much better
than a comparatively shallow network.
Although the motivation for creating deeper networks was clear, for a long time researchers did not have an
algorithm that could efficiently train neural networks with more than 3 layers. With the introduction of greedy
layerwise pre-training by [Hinton et al., 2006], researchers were able to train much deeper networks. This played a
major role in bringing the so-called Deep Learning systems into mainstream machine learning. Modern deep networks
such as AlexNet have 7 layers. More recent networks like VGGnet by [Simonyan and Zisserman, 2014b] and GoogleNet
by [Szegedy et al., 2014] have 19 and 22 layers respectively were shown to perform much better than AlexNet.
3
2.3 Learning algorithm
A powerful, expressive model is of no use without an algorithm to learn the model’s parameters efficiently. The greedy
layerwise pre-training approaches in the pre-AlexNet era attempted to create such an efficient algorithm. However, for
computer vision tasks, it turned out that a simpler supervised training procedure was enough to learn a powerful
model.
Learning is generally performed by minimization of certain loss functions. Tasks based on classification use the
softmax loss function or the sigmoid cross entropy function, while those involving regression use the euclidean error
function. In the example of Figure 1, the output of the FC8 layer is trained to represent one of thousand classes of the
dataset.
2.3.1 Gradient-based optimization
Neural networks are generally trained using the backpropogation algorithm (see [Rumelhart et al., 1988]), which uses
the chain rule to speed up the computation of the gradient for the gradient descent (GD) algorithm. However, for
datasets with thousands (or more) of data points, using GD is impractical. In such cases, an approximation called
the Stochastic Gradient Descent (SGD) is used. It has been found that training using SGD generalizes much better
than training using GD. However, one disadvantage is that SGD is very slow to converge. To counteract this, SGD is
typically used with a mini-batch, where the mini-batch typically contains a small number of data-points (∼ 100).
Momentum (see [Polyak, 1964]) belongs to a family of methods that aim to speed the convergence of SGD. This is
largely used in practice to train deep networks, and is often considered as an essential component. Other extensions
like Adagrad by [Duchi et al., 2011], Nesterov’s accelerated GD by [Nesterov, 1983] , Adadelta by [Zeiler, 2012] and
Adam by [Kingma and Ba, 2014] are known to work equally well, if not better than vanilla momentum in certain
cases. For detailed discussion on how these methods work, the reader is encouraged to read [Sutskever et al., 2013].
2.3.2 Dropout
When training a network with a large number of parameters, an effective regularization mechanism is essential to
combat overfitting. Usual approaches such as `1 or `2 regularization on the weights of the neural net have been found
to be insufficient in this aspect. Dropout is a powerful regularization method introduced by [Hinton et al., 2012] which
has been shown to work well for large neural nets. To use dropout, we randomly drop neurons with a probability
p during training. As a result, only a random subset of neurons are trained in a single iteration of SGD. At test
time, we use all neurons, however we simply multiply the activation of each neuron with p to account for the scaling.
[Hinton et al., 2012] showed that this procedure was equivalent to training a large ensemble of neural nets with shared
parameters, and then using their geometric mean to obtain a single prediction.
Many extensions to dropout like DropConnect by [Wan et al., 2013] and Fast Dropout by [Wang and Manning, 2013]
have been shown to work better in certain cases. Maxout by [Goodfellow et al., 2013] is a non-linearity that improves
performance of a network which uses dropout.
2.4 Tricks to increase performance
While the techniques and components described above are theoretically well-grounded, certain tricks are crucial to
obtaining state-of-the-art performance.
It is well known that machine learning models perform better in the presence of more data. Data augmentation is
a process by which some geometric transforms are applied to training data to increase their number. Some examples
of commonly used geometric transforms include random cropping, RGB jittering, image flipping and small rotations.
It has been found that using augmented data typically boosts performance by about 3% (see [Chatfield et al., 2014]).
Also well-known is the fact that an ensemble of models perform better than one. Hence, it is the commonplace to
train several CNNs and average their predictions at test time. Using ensembles has been found to typically boost
accuracy by 1-2% (see [Simonyan and Zisserman, 2014b] and [Szegedy et al., 2014]) .
2.5 Putting it all together: AlexNet
The building blocks discussed above largely describe AlexNet as a whole. As shown in Figure 1, only layers 1,2 and 5
contain max-pooling, while dropout is only applied to the last two fully connected layers as they contain the most
number of parameters. Layers 1 and 2 also contain Local Response Normalization, which has not been discussed as
[Chatfield et al., 2014] showed that its absence does not impact performance.
This network was trained on the ILSVRC 2012 training data, which contained 1.2 million training images belonging
to 1000 classes. This was trained on 2 GPUs over the course of one month. The same network can be trained today in
little under a week using more powerful GPUs (see [Chatfield et al., 2014]). The hyper-parameters of the learning
algorithms like learning rate, momentum, dropout and weight decay were hand tuned. It is also interesting to
4
note the trends in the nature of features learnt at different layers. The earlier layers tend to learn gabor-like oriented
edges and blob-like features, followed by layers that seem to learn more higher order features like shapes. The very
last layers seem to learn semantic attributes such as eyes or wheels, which are crucial parts in several categories. A
method to visualize these was provided by [Zeiler and Fergus, 2014].
2.6 Using Pre-trained CNNs
One of the main reasons for the success of the AlexNet model was that it was possible to directly use the pre-trained
model to do various other tasks which it was not originally intended for. It became remarkably easy to download a
learnt model, and then tweak it slightly to suit the application at hand. We describe two such ways to use models in
this manner.
2.6.1 Fine-tuning
Given a model trained for image classification, how does one modify it to perform a different (but related) task? The
answer is to just use the trained weights as an initialization and run SGD again for this new task. Typically, one uses
a learning rate much lower than what was used for learning the original net. If the new task is very similar to the task
of image classification (with similar categories), then one need not re-learn a lot of layers. The earlier layers can be
fixed and only the later, more semantic layers need to be re-learnt. However, if the new task is very different, one
ought to either re-learn all layers, or learn everything from scratch. The number of layers to re-learn also depends on
the number of data points available for training the new task. The more the data, the higher is the number of layers
that can be re-learnt. The reader is urged to refer to [Yosinski et al., 2014] for more thorough guidelines.
2.6.2 CNN activations as features
As remarked earlier, the later layers in AlexNet seem to learn visually semantic attributes. These intermediate
representations are crucial in performing 1000-way classification. Since these represent a wide variety of classes, one
can use the FC7 activation of an image as a generic feature descriptor. These features have been found to be better
than hand-crafted features like SIFT or HoG for various computer vision tasks.
[Donahue et al., 2013] first introduced the idea of using CNN activations as features and performed tests to
determine their suitability for various tasks. [Babenko et al., 2014] proposed to use the activations of fully-connected
layers for image retrieval, which they dubbed “Neural Codes”. [Razavian et al., 2014] used these activations for
various tasks and concluded that off-the-shelf CNN features can serve as a hard-to-beat baseline for many tasks.
[Hariharan et al., 2014] used activations across layers as a feature. Specifically, they look at the activations produced
by a single image pixels across the network and pool them together. They were found to be useful for fine-grained
tasks such as keypoint localization.
2.7 Improving AlexNet
The performance of AlexNet motivated a number of CNN-based approaches, all aimed at a performance improvement
over and above that of AlexNet’s. Just as AlexNet was the winner for ILSVRC challenge in 2012, a CNN-based net
Overfeat by [Sermanet et al., 2013a] was the top-performer at ILSVRC-2013. Their key insight was that training
a convolutional network to simultaneously classify, locate and detect objects in images can boost the classification
accuracy and the detection and localization accuracy of all tasks. Given its multi-task learning paradigm, we discuss
Overfeat when we discuss hybrid CNNs and multi-task learning in Section 3.5.
GoogleNet by [Szegedy et al., 2014], the top-performer at ILSVRC-2014, established that very deep networks can
translate to significant gains in classification performance. Since naively increasing the number of layers results in a
large number of parameters, the authors employ a number of “design tricks”. One such trick is to have a trivial 1× 1
convolutional layer after a regular convolutional layer. This has the net effect of not only reducing the number of
parameters, but also results in CNNs with more expressive power. This design trick is laid out in better detail in the
work of [Szegedy et al., 2014] where the authors show that having one or more 1× 1 convolutional layers is akin to
having a multi-layer perceptron network processing the outputs of the convolutional layer that precedes it. Another
trick that the authors utilize is to involve inner layers of the network in the computation of the objective function
instead of the typical final softmax layer (as in AlexNet). The authors attribute scale invariance as the reason behind
this design decision.
VGG-19 and its variants by [Simonyan and Zisserman, 2014b] is another example of a high-performing CNN where
the deeper-is-better philosophy is applied in the net design. An interesting feature of VGG design is that it forgoes
larger sized convolutional filters for stacks of smaller sized filters. These smaller sized filters tend to be chosen so that
they contain approximately the same number of parameters as the larger filters they supposedly replace. The net
effect of this design decision is efficiency and regularization-like effect on parameters due to the smaller size of the
filters involved.
5
Figure 2: Object detection system of [Girshick et al., 2014] using deep features extracted from image regions
3 CNN Flavours
3.1 Region-based CNNs
Most CNNs trained for image recognition are trained using a dataset of images containing a single object. At test
time, even in case of multiple objects, the CNN may still predict a single class. This inherent problem with the
design of the CNNs is not restricted to image classification alone. For example, the problem of object detection and
localization requires not only classifying the image but also estimating the class and precise location of the object(s)
present in the image. Object detection is challenging since we potentially want to detect multiple objects with varying
sizes within a single image. It generally requires processing the image patch-wise, looking for the presence of objects.
Neural nets have been employed in this way for detecting specific objects like faces in [Vaillant et al., 1994] and
[Rowley et al., 1998] and for pedestrians by [Sermanet et al., 2013c].
Meanwhile, detecting a set of object-like regions in a given image - also called region proposals or object proposals
- has gained a lot of attention (see [Uijlings et al., 2013]). These region proposals are class agnostic and reduce the
overhead incurred by the traditional exhaustive sliding window approach. These region proposal algorithms operate at
low level and output hundreds of object like image patches at multiple scales. In order to employ a classification net
towards the task of object localization, image patches of different scales have to be searched one at a time.
Recent work by [Girshick et al., 2014] attempt to solve the object localization problem using a set of region
proposals. During test time, the method generates around 2000 category independent region proposals using selective
search by [Uijlings et al., 2013] from the test image. They employ a simple affine image warping to feed each of these
proposals to a CNN trained for classification. The CNN then describes each of these regions with a fixed size high
level semantic feature. Finally, a set of category specific linear SVMs classify each region, as shown in Figure 2. This
method achieved the best detection results on the PASCAL VOC 2012 dataset. As this method uses image regions
followed by a CNN, it is dubbed R-CNN (Region-based CNN).
A series of works adapted the R-CNN approach to extract richer set of features at patch or region level to solve a
wide range of target applications in vision. However, CNN representations lack robustness to geometric transformations
restricting their usage. [Gong et al., 2014a] show empirical evidence that the global CNN features are sensitive to
general transformations such as translation, rotation and scaling. In their experiments, they report that this inability
of global CNN features translates directly into a loss in the classification accuracy. They proposed a simple technique to
pool the CNN activations extracted from the local image patches. The method extracts image patches in an exhaustive
sliding-window manner at different scales and describes each of them using a CNN. The resulting dense CNN features
are pooled using VLAD (see [Jegou et al., 2012]) in order to result in a representation which incorporates spatial as
well as semantic information.
Instead of considering the image patches at exhaustive scales and image locations, [Mopuri and Babu, 2015] utilize
the objectness prior to automatically extract the image patches at different scales. They build a more robust image
representation by aggregating the individual CNN features from the patches for an image search application.
[Wei et al., 2014] extended the capability of a CNN that is trained to output a single label into predicting multiple
labels. They consider an arbitrary number of region proposals in an image and share a common CNN across all of
them in order to obtain individual predictions. Finally, they employ a simple pooling technique to produce the final
multi-label prediction.
3.2 Fully Convolutional Networks
The success of Convolutional Neural Networks in the tasks of image classification (see [Krizhevsky et al., 2012,
Szegedy et al., 2014]) and object detection (see [Girshick et al., 2014]) has inspired researchers to use deep networks
for more challenging recognition problems like semantic object segmentation and scene parsing. Unlike image
classification, semantic segmentation and scene parsing are problems of structured prediction where every pixel in the
image grid needs to be assigned a label of the class to which it belongs (e.g., road, sofa, table etc.). This problem of
per-pixel classification has been traditionally approached by generating region-level (e.g. superpixel) hand crafted
6
Figure 3: Fully Convolutional Net: AlexNet modified to be fully convolutional for performing semantic object
segmentation on PASCAL VOC 2012 dataset with 21 classes
features and classifying them using a Support Vector Machine (SVM) into one of the possible classes.
Doing away with these engineered features, [Farabet et al., 2013a] used hierarchical learned features from a
convolutional neural net for scene parsing. Their approach comprised of densely computing multi-scale CNN features
for each pixel and aggregating them over image regions upon which they are classified. However, their method
still required the post-processing step of generating over-segmented regions, like superpixels, for obtaining the final
segmentation result. Additionally, the CNNs used for multi-scale feature learning were not very deep with only three
convolution layers.
Later, [Long et al., 2015] proposed a fully convolutional network architecture for learning per-pixel tasks, like
semantic segmentation, in an end-to-end manner. This is shown in Figure 3. Each layer in the fully convolutional
net (FullConvNet) performs a location invariant operation i.e., a spatial shift of values in the input to the layer will
only result in an equivalent scaled spatial shift in its output while keeping the values nearly intact. This property of
translational invariance holds true for the convolutional and maxpool layers which form the major building blocks
of a FullConvNet. Further, these layers have an output-centred, fixed-size receptive field on its input blob. These
properties of the layers of FullConv Net allow it to retain the spatial structure present in the input image in all of its
intermediate and final outputs.
Unlike CNNs used for image classification, a FullConvNet does not contain any densely connected/inner product
layers as they are not translation invariant. The restriction on the size of input image to a classification CNN (e.g.,
227x227 for AlexNet [Krizhevsky et al., 2012], 224x224 for VGG [Simonyan and Zisserman, 2014b]) is imposed due
to the constraint on the input size to its inner product layers. Since a FullConvNet does not have any of these inner
product layers, it can essentially operate on input images of any arbitrary size.
During the design of CNN architectures, one has to make a trade-off between the number of channels and the
spatial dimensions for the data as it passes through each layer. Generally, the number of channels in the data are
made to increase progressively while bringing down its spatial resolution, by introducing stride in the convolution and
max-pool layers of the net. This is found to be an effective strategy for generating richer semantic representations in a
hierarchical manner. While this method enables the net to recognize complex patterns in the data, it also diminishes
the spatial resolution of the data blob progressively after each layer. While this is not a major concern for classification
nets which require only a single label for the entire image, this results in per-pixel prediction only at a sub-sampled
resolution in case of FullConvNets. For tackling this problem, [Long et al., 2015] have proposed a deconvolution layer
which brings back the spatial resolution from the sub-sampled output through a learned upsampling operation. This
upsampling operation is performed at intermediate layers of various spatial dimensions and are concatenated to obtain
pixel-level features at the original resolution.
On the other hand, [Chen et al., 2014] adopted a more simplistic approach for maintaining resolution by removing
the stride in the layers of FullConvNet, wherever possible. Following this, the FullConvNet predicted output is modeled
as a unary term for Conditional Random Field (CRF) constructed over the image grid at its original resolution. With
labelling smoothness constraint enforced through pair-wise terms, the per-pixel classification task is modeled as a
CRF inference problem. While this post-processing of FullConvNet’s coarse labelling using CRF has been shown to
be effective for pixel-accurate segmentation, [Zheng et al., 2015] have proposed a better approach where the CRF
constructed on image is modeled as a Recurrent Neural Network (RNN). By modeling the CRF as an RNN, it can be
integrated as a part of any Deep Convolutinal Net making the system efficient at both semantic feature extraction
and fine-grained structure prediction. This enables the end-to-end training of the entire FullConvNet + RNN system
using the stochastic gradient descent (SGD) algorithm to obtain fine pixel-level segmentation.
Visual Saliency Prediction is another important problem considered by researchers. This task involves predicting
the salient regions of an image given by human eye fixations. Works by [Vig et al., 2014] and [Liu et al., 2015] proposed
CNN-based approaches for estimating the saliency score for constituent image patches using deep features. As a result,
they did not use a FullConvNet architecture. In contrast, [Kruthiventi et al., 2015] proposed a fully convolutional
architecture - DeepFix which learnt to predict saliency for the entire image in an end-to-end fashion and attained a
7
Figure 4: Two stream architecture for video classification from [Simonyan and Zisserman, 2014a]
superior performance. Their network characterized the multi-scale aspects of the image using inception blocks and
captured the global context using convolutional layers with large receptive fields. Another work, by [Li et al., 2015b],
proposed a multi-task FullConvNet architecture - DeepSaliency for joint saliency detection and semantic object
segmentation. Their work showed that learning features collaboratively for two related prediction tasks can boost
overall performance.
3.3 Multi-modal networks
The success of CNNs on standard RGB vision tasks is naturally extended to works on other perception modalities
like RGB-D and motion information in the videos. Recently, there has been an increasing evidence for the successful
adaptation of the CNNs to learn efficient representations from the depth images. [Socher et al., 2012] exploited the
information from color and depth modalities for addressing the problem of classification. In their approach, a single
layer of CNN extracts low level features from both the RGB and depth images separately. These low level features
from each modality are given to a set of RNNs for embedding into a lower dimension. Concatenation of the resulting
features forms the input to the final soft-max layer. The work by [Couprie et al., 2013] extended the CNN method
of [Farabet et al., 2013b] to label the indoor scenes by treating depth information as an additional channel to the
existing RGB data. Similarly, [Wang et al., 2014a] adapt an unsupervised feature learning approach to scene labeling
using RGB-D input with four channels. [Gupta et al., 2014] proposed an encoding for the depth images that allows
CNNs to learn stronger features than from the depth image alone. They encode depth image into three channels at
each pixel: horizontal disparity, height above ground, and the angle the pixel’s local surface normal makes with the
inferred gravity direction. Their approach for object detection and segmentation processes RGB and the encoded
depth channels separately. The learned features are fused by concatenating and further fed into a SVM.
Similarly, one can think of extending these works for video representation and understanding. When compared
to still images, videos provide important additional information in the form of motion. However, majority of the
early works that attempted to extend CNNs for video, fed the networks with raw frames. This makes for a much
difficult learning problem. [Jhuang et al., 2007], proposed a biologically inspired model for action recognition in
videos with a predefined set of spatio-temporal filters in the initial layer. Combined with a similar but spatial
HMAX (Hierarchical model and X) model, [Kuehne et al., 2011] proposed spatial and temporal recognition streams.
[Ji et al., 2013] addressed an end-to-end learning of the CNNs for videos for the first time using 3-D convolutions over
a bunch of consecutive video frames. A more recent work by [Karpathy et al., 2014] propose a set of techniques to
fuse the appearance information present from a stack of consecutive frames in a video. However, they report that the
net that processes individual frames performs on par with the net that operates on a stack of frames. This might
suggest that, the learnt spatio-temporal filters are not suitable to capture the motion patterns efficiently.
A more suitable CNN model to represent videos is proposed in a contemporaneous work by [Simonyan and Zisserman, 2014a],
which is called two-stream network approach. Though the model in [Kuehne et al., 2011] is also a two stream model,
the main difference is that the streams are shallow and implemented with hand-crafted models. The reason for the
success of this approach is the natural ability of the videos to be separated into spatial and temporal components. The
spatial component in the form of frames captures the appearance information like the objects present in the video.
The temporal component in the form of motion (optical flow) across the frames captures the movement of the objects.
These optical flow estimates can be obtained either from classical approaches (see [Baker and Matthews, 2004]) or
8
Figure 5: Toy RNN Example: Problem of sequence addition. The inputs and outputs are shown in blue. The red cells
correspond to the hidden units. An unrolled version of the RNN is shown.
deep-learnt approaches (see [Weinzaepfel et al., 2013]).
This approach models the recognition system dividing into two parallel streams as depicted in Fig.4. Each is
implemented by a dedicated deep CNN, whose predictions are later fused. The net for the spatial stream is similar
to the image recognition CNN and processes one frame at a time. However, the temporal stream takes the stacked
optical flow of a bunch of consecutive frames as input and predicts the action. Both the nets are trained separately
with the corresponding input. An alternative motion representation using the trajectory information similar to
[Wang and Schmid, 2013] is also observed to perform similar to optical flow.
The most recent methods that followed [Simonyan and Zisserman, 2014a] have similar two-stream architecture.
However, their contribution is to find the most active spatio-temporal volume for the efficient video representation.
Inspired from the recent progress in the object detection in images, [Gkioxari and Malik, 2015] built action models
from shape and motion cues. They start from the image proposals and select the motion salient subset of them and
extract saptio-temporal features to represent the video using the CNNs.
[Wang et al., 2015a] employ deep CNNs to learn discriminative feature maps and conduct trajectory constrained
pooling to summarize into an effective video descriptor. The two streams operate in parallel extracting local deep
features for the volumes centered around the trajectories.
In general, these multi-modal CNNs can be modified and extended to suit any other kind of modality like audio,
text to complement the image data leading to a better representation of image content.
3.4 CNNs with RNNs
While CNNs have made remarkable progress in various tasks, they are not very suitable for learning sequences.
Learning such patterns requires memory of previous states and feedback mechanisms which are not present in CNNs.
RNNs are neural nets with at least one feedback connection. This looping structure enables the RNN to have an
internal memory and to learn temporal patterns in data.
Figure 5 shows the unrolled version of a simple RNN applied to a toy example of sequence addition. The problem
is defined as follows: Let at be a positive number, corresponding to the input at time t. The output at time t is given
by
St =
t∑
i=1
ai
We consider a very simple RNN with just one hidden layer. The RNN can be described by equations below.
ht+1 = fh(Wih × at +Whh × ht)
St+1 = fo(Who × ht+1)
where Wih,Whh,Who are learned weights and fh and fo are non-linearities. For the toy problem considered above,
the weights learned would result in Wih = Whh = Who = 1. Let us consider the non-linearity to be ReLu. The
equations would then become,
ht+1 = ReLu(at + ht)
St+1 = ReLu(ht+1)
Thus, as shown in Figure 5, the RNN stores previous inputs in memory and learns to predict the sum of the sequence
up to the current timestep t.
9
Figure 6: LRCN: A snapshot of the model at time t. It refers to the frame and OF t the optical flow at t. The video is
classified by averaging the output At over all t.
As with CNNs, recurrent neural networks have been trained with various back propagation techniques. These
conventional methods however, resulted in the vanishing gradient problem, i.e. the errors sent backward over the network,
either grew very large or vanished leading to problems in convergence. In 1997, [Hochreiter and Schmidhuber, 1997]
introduced LSTM (Long Short Term Memory), which succeeded in overcoming the vanishing gradient problem, by
introducing a novel architecture consisting of units called Constant Error Carousels. LSTMs were thus able to learn
very deep RNNs and successfully remembered important events over long (thousands of steps) durations of time.
Over the next decade, LSTM’s became the network of choice for several sequence learning problems, especially in
the fields of speech and handwriting recognition (see [Graves et al., 2009, Graves et al., 2013]). In the sections that
follow, we shall discuss applications of RNNs in various computer vision problems.
3.4.1 Action recognition
Recognizing human actions from videos has long been a pivotal problem in the tasks of video understanding and
surveillance. Actions, being events which take place over a finite length of time, are excellent candidates for a joint
CNN-RNN model.
In particular, we discuss the model proposed by [Donahue et al., 2014]. They use RGB as well as optical flow
features to jointly train a variant of Alexnet combined with a single layer of LSTM (256 hidden units). Frames of the
video are sampled, passed through the trained network and classified individually. The final prediction is obtained by
averaging across all the frames. A snapshot of this model at time t is shown in Figure 6.
3.4.2 Image and video captioning
Another important component of scene understanding is the textual description of images and videos. Relevant textual
description also helps complement image information, as well as form useful queries for retrieval.
RNNs (LSTMs) have long been used for machine translation (see [Bahdanau et al., 2014, Cho et al., 2014]). This
has motivated its use for the purpose of image description. [Vinyals et al., 2014] have developed an end-to-end system,
by first encoding an image using a CNN and then using the encoded image as an input to a language generating
RNN. [Karpathy and Fei-Fei, 2014] propose a multimodal deep network that aligns various interesting regions of the
image, represented using a CNN feature, with associated words. The learned correspondences are then used to train a
bi-directional RNN. This model is able, not only to generate descriptions for images, but also to localize different
segments of the sentence to their corresponding image regions. The multimodal RNN (m-RNN) by [Mao et al., 2014]
combines the functionalities of the CNN and RNN by introducing a new multimodal layer, after the embedding and
recurrent layers of the RNN. [Mao et al., 2015] further extend the m-RNN by incorporating a transposed weight
sharing strategy, enabling the network to learn novel visual concepts from images.
[Venugopalan et al., 2014] move beyond images and obtain a mean-pooled CNN representation for a video. They
train an LSTM to use this input to generate a description for the video. They further improve upon this task by
developing S2VT [Venugopalan et al., 2015] a stacked LSTM model which accounts for both the RGB as well as flow
information available in videos. [Pan et al., 2015] use both 2-D and 3-D CNNs to obtain a video embedding. They
introduced two types of losses which are used to train both the LSTM and the visual semantic embedding.
10
Figure 7: The facial expression recognition system of [Devries et al., 2014] which utilizes facial landmark (shown
overlaid on the face towards the right of the image) recognition as an auxiliary task which helps improve performance
on the main task of expression recognition.
3.4.3 Visual Question answering
Real understanding of an image should enable a system not only to make a statement about it, but also to answer
questions related to it. Therefore answering questions based on visual concepts in an image is the next natural step
for machine understanding algorithms. Doing this, however, requires the system to model both the textual question
and the image representation, before generating an answer conditioned on both the question and the image.
A combination of CNN and LSTM has proven to be effective in this task too, as evidenced by the work of
[Malinowski et al., 2015] who train an LSTM layer to accept both the question as well a CNN representation of the
image and generate the answer. [Gao et al., 2015] use two LSTM’s with shared weights along with a CNN for the
task. Their experiments are performed on a multilingual dataset containing Chinese questions and answers along with
its English translation. [Antol et al., 2015] provide a dataset for the task of visual question answering containing both
real world images and abstract scenes.
3.5 Hybrid learning methods
3.5.1 Multi-task learning
Multi-task learning is essentially a machine learning paradigm wherein the objective is to train the learning system
to perform well on multiple tasks. Multi-task learning frameworks tend to exploit shared representations that exist
among the tasks to obtain a better generalization performance than counterparts developed for a single task alone.
In CNNs, multi-task learning is realized using different approaches. One class of approaches utilize a multi-task
loss function with hyper-parameters typically regulating the task losses. For example, [Girshick, 2015] employ a
multi-task loss to train their network jointly for classification and bounding-box regression tasks thereby improving
performance for object detection. [Zhang et al., 2014] propose a facial landmark detection network which adaptively
weights auxiliary tasks (e.g. head pose estimation, gender classification, age estimation) to ensure that a task that
is deemed not beneficial to accurate landmark detection is prevented from contributing to the network learning.
[Devries et al., 2014] demonstrate improved performance for facial expression recognition task by designing a CNN for
simultaneous landmark localization and facial expression recognition. A hallmark of these approaches is the division of
tasks into primary task and auxiliary task(s) wherein the purpose of the latter is typically to improve the performance
of the former (see Figure 7).
Some approaches tend to have significant portions of the original network modified for multiple tasks. For
instance, [Sermanet et al., 2013b] replace pre-trained layers of a net originally designed to provide spatial (per-pixel)
classification maps with a regression network and fine-tune the resulting net to achieve simultaneous classification,
localization and detection of scene objects.
Another class of multi-task approaches tend to have task-specific sub-networks as a characteristic feature of CNN
design. [Li et al., 2015a] utilize separate sub-networks for the joint point regression and body part detection tasks.
[Wang et al., 2015b] adopt a serially stacked design wherein a localization sub-CNN and the original object image are
fed into a segmentation sub-CNN to generate its object bounding box and extract its segmentation mask. To solve an
unconstrained word image recognition task, [Jaderberg et al., 2014a] propose an architecture consisting of a character
sequence CNN and an N-gram encoding CNN which act on an input image in parallel and whose outputs are utilized
along with a CRF model to recognize the text content present within the image.
11
3.5.2 Similarity learning
Apart from classification, CNNs can also be used for tasks like metric learning and rank learning. Rather than asking
the CNN to identify objects, we can instead ask it to verify whether two images contain the same object or not. In
other words, we ask the CNN to learn which images are similar, and which are not. Image retrieval is one application
where such questions are routinely asked.
Structurally, Siamese networks resemble two-stream networks discussed previously. However, the difference here is
that both ‘streams’ have identical weights. Siamese networks consist of two seperate (but identical) networks, where two
images are fed in as input. Their activations are combined at a later layer, and the output of the network consists of a
single number, or a metric, which is a notion of distance between the images. Training is done so that images which are
considered to be similar have a lower output score than images which are considered different. [Bromley et al., 1993]
introduced the idea of Siamese networks and used it for signature verification. Later on, [Chopra et al., 2005] extended
it for face verification. [Zagoruyko and Komodakis, 2015] further extended and generalized this to learning similarity
between image patches.
Triplet networks are extensions of siamese networks used for rank learning. [Wang et al., 2014b] first used this
idea for learning fine-grained image similarity learning.
4 Open problems
In this section, we briefly mention some open research problems in deep learning, particularly of interest to computer
vision. Several of these problems are already being tackled in several works.
• Training CNNs requires tuning of a large number of hyper-parameters, including those involving the model
architecture. An automated way of tuning such as that by [Snoek et al., 2012] is crucial for practitioners.
However, that requires multiple models to be trained, which can be both time consuming and impractical for
large networks.
• [Nguyen et al., 2014] showed that one can generate artificial images that result in CNNs producing a high
confidence false prediction. In a related line of work, [Szegedy et al., 2013] showed that natural images can
be modified in an imperceptible manner to produce a completely different classification label. Although
[Goodfellow et al., 2014b] attempted to reduce the effects of such adversarial examples, it remains to be seen
whether that can be completely eliminated.
• It is well known (see [Gong et al., 2014b]) that CNNs are robust to small geometric transforms. However, we
would like them to be invariant. The study of invariance for extreme deformation is largely missing.
• Along with using a large number of data points, CNN models are also large and (relatively) slow to evaluate.
While there has been a lot of work in reducing number of parameters (see [Hinton et al., 2015, Denil et al., 2013,
Collins and Kohli, 2014, Jaderberg et al., 2014b, Srinivas and Babu, 2015]), it is not clear how to train non-
redundant models in the first place.
• CNNs are presently trained in a one-shot way. The formulation of an online method of training would be
desirable for robotics applications.
• Unsupervised learning is one more area where we expect to deploy deep learning models. This would enable us
to leverage the massive amounts of unlabelled image data on the web. Classical deep networks like autoencoders
and restricted boltzmann machines were formulated as unsupervised models. While there has been a lot of
interesting recent work in the area (see [Goodfellow et al., 2014a, Bengio et al., 2013, Kingma and Welling, 2013,
Kulkarni et al., 2015]), a detailed discussion of these is beyond the scope of this paper.
5 Concluding remarks
In this article, we have surveyed the use of deep learning networks - convolutional neural networks in particular - for
computer vision. This enabled complicated hand-tuned algorithms being replaced by single monolithic algorithms
trained in an end-to-end manner. However, despite our best efforts it may not be possible to capture the entire gamut
of deep learning research - even for computer vision - in this paper. We point the reader to other reviews, specifically
those by [Bengio, 2009], [LeCun et al., 2015] and [Schmidhuber, 2015]. These reviews are more geared towards deep
learning in general, while ours is more focussed on computer vision. We hope that our article will be useful to vision
researchers beginning to work in deep learning.
12
References
[Antol et al., 2015] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. (2015). Vqa:
Visual question answering. arXiv preprint arXiv:1505.00468.
[Ba and Caruana, 2014] Ba, J. and Caruana, R. (2014). Do deep nets really need to be deep? In Ghahramani, Z.,
Welling, M., Cortes, C., Lawrence, N., and Weinberger, K., editors, Advances in Neural Information Processing
Systems 27, pages 2654–2662. Curran Associates, Inc.
[Babenko et al., 2014] Babenko, A., Slesarev, A., Chigorin, A., and Lempitsky, V. (2014). Neural codes for image
retrieval. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T., editors, Computer Vision ECCV 2014, volume
8689 of Lecture Notes in Computer Science, pages 584–599. Springer International Publishing.
[Bahdanau et al., 2014] Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning
to align and translate. arXiv preprint arXiv:1409.0473.
[Baker and Matthews, 2004] Baker, S. and Matthews, I. (2004). Lucas-kanade 20 years on: A unifying framework.
International journal of computer vision, 56(3):221–255.
[Bengio, 2009] Bengio, Y. (2009). Learning deep architectures for ai. Found. Trends Mach. Learn., 2(1):1–127.
[Bengio et al., 2013] Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2013). Deep generative stochastic
networks trainable by backprop. arXiv preprint arXiv:1306.1091.
[Bishop, 2006] Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics).
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
[Bromley et al., 1993] Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., Sa¨ckinger, E., and
Shah, R. (1993). Signature verification using a siamese time delay neural network. International Journal of Pattern
Recognition and Artificial Intelligence, 7(04):669–688.
[Chatfield et al., 2014] Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. (2014). Return of the devil in the
details: Delving deep into convolutional nets. In Proceedings of the British Machine Vision Conference, Nottingham,
UK. BMVA Press.
[Chen et al., 2014] Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. (2014). Semantic image
segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062.
[Cho et al., 2014] Cho, K., Van Merrie¨nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,
Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078.
[Chopra et al., 2005] Chopra, S., Hadsell, R., and LeCun, Y. (2005). Learning a similarity metric discriminatively,
with application to face verification. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference on, volume 1, pages 539–546. IEEE.
[Collins and Kohli, 2014] Collins, M. D. and Kohli, P. (2014). Memory bounded deep convolutional networks. arXiv
preprint arXiv:1412.1442.
[Couprie et al., 2013] Couprie, C., Farabet, C., Najman, L., and LeCun, Y. (2013). Indoor semantic segmentation
using depth information. CoRR, abs/1301.3572.
[Dalal and Triggs, 2005] Dalal, N. and Triggs, B. (2005). Histograms of oriented gradients for human detection. In
Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1,
pages 886–893. IEEE.
[Denil et al., 2013] Denil, M., Shakibi, B., Dinh, L., Ranzato, M. A., and de Freitas, N. (2013). Predicting parameters
in deep learning. In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K., editors, Advances in
Neural Information Processing Systems 26, pages 2148–2156. Curran Associates, Inc.
[Devries et al., 2014] Devries, T., Biswaranjan, K., and Taylor, G. W. (2014). Multi-task learning of facial landmarks
and expression. In Computer and Robot Vision (CRV), 2014 Canadian Conference on, pages 98–103.
[Donahue et al., 2014] Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K.,
and Darrell, T. (2014). Long-term recurrent convolutional networks for visual recognition and description. arXiv
preprint arXiv:1411.4389.
13
[Donahue et al., 2013] Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2013).
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
[Duchi et al., 2011] Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.
[Farabet et al., 2013a] Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013a). Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1915–1929.
[Farabet et al., 2013b] Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013b). Learning hierarchical features
for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915–1929.
[Fukushima, 1980] Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193–202.
[Gao et al., 2015] Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. (2015). Are you talking to a machine?
dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612.
[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich feature hierarchies for accurate
object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE
Conference on, pages 580–587. IEEE.
[Girshick, 2015] Girshick, R. B. (2015). Fast R-CNN. CoRR, abs/1504.08083.
[Gkioxari and Malik, 2015] Gkioxari, G. and Malik, J. (2015). Finding action tubes. In CVPR.
[Gong et al., 2014a] Gong, Y., Wang, L., Guo, R., and Lazebnik, S. (2014a). Multi-scale orderless pooling of deep
convolutional activation features. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T., editors, Computer Vision
ECCV 2014, volume 8695 of Lecture Notes in Computer Science, pages 392–407. Springer International Publishing.
[Gong et al., 2014b] Gong, Y., Wang, L., Guo, R., and Lazebnik, S. (2014b). Multi-scale orderless pooling of
deep convolutional activation features. In Computer Vision ECCV 2014, pages 392–407. Springer International
Publishing.
[Goodfellow et al., 2014a] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,
A., and Bengio, Y. (2014a). Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence,
N., and Weinberger, K., editors, Advances in Neural Information Processing Systems 27, pages 2672–2680. Curran
Associates, Inc.
[Goodfellow et al., 2014b] Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014b). Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572.
[Goodfellow et al., 2013] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout
networks. arXiv preprint arXiv:1302.4389.
[Graves et al., 2009] Graves, A., Liwicki, M., Ferna´ndez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A
novel connectionist system for unconstrained handwriting recognition. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 31(5):855–868.
[Graves et al., 2013] Graves, A., Mohamed, A.-r., and Hinton, G. (2013). Speech recognition with deep recurrent
neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,
pages 6645–6649. IEEE.
[Gupta et al., 2014] Gupta, S., Ross, Arbelaez, P., and Malik, J. (2014). Learning rich features from RGB-D images
for object detection and segmentation.
[Hariharan et al., 2014] Hariharan, B., Arbela´ez, P., Girshick, R., and Malik, J. (2014). Hypercolumns for object
segmentation and fine-grained localization. arXiv preprint arXiv:1411.5752.
[He et al., 2015] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. arXiv preprint arXiv:1502.01852.
[Hinton et al., 2015] Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.
[Hinton, 2002] Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771–1800.
14
[Hinton et al., 2006] Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527–1554.
[Hinton et al., 2012] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012).
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.
[Hochreiter and Schmidhuber, 1997] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural
computation, 9(8):1735–1780.
[Hornik, 1991] Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural networks,
4(2):251–257.
[Hubel and Wiesel, 1962] Hubel, D. H. and Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional
architecture in the cat’s visual cortex. The Journal of physiology, 160(1):106.
[Jaderberg et al., 2014a] Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. (2014a). Deep structured
output learning for unconstrained text recognition. CoRR, abs/1412.5903.
[Jaderberg et al., 2014b] Jaderberg, M., Vedaldi, A., and Zisserman, A. (2014b). Speeding up convolutional neural
networks with low rank expansions. arXiv preprint arXiv:1405.3866.
[Jegou et al., 2012] Jegou, H., Perronnin, F., Douze, M., Sanchez, J., Perez, P., and Schmid, C. (2012). Aggregating
local image descriptors into compact codes. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
34(9):1704–1716.
[Jhuang et al., 2007] Jhuang, H., Serre, T., Wolf, L., and Poggio, T. (2007). A biologically inspired system for action
recognition. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1–8.
[Ji et al., 2013] Ji, S., Xu, W., Yang, M., and Yu, K. (2013). 3d convolutional neural networks for human action
recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):221–231.
[Karpathy and Fei-Fei, 2014] Karpathy, A. and Fei-Fei, L. (2014). Deep visual-semantic alignments for generating
image descriptions. arXiv preprint arXiv:1412.2306.
[Karpathy et al., 2014] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014).
Large-scale video classification with convolutional neural networks. In Computer Vision and Pattern Recognition
(CVPR), 2014 IEEE Conference on, pages 1725–1732. IEEE.
[Kingma and Ba, 2014] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
[Kingma and Welling, 2013] Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114.
[Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep
convolutional neural networks. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances in
Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc.
[Kruthiventi et al., 2015] Kruthiventi, S. S., Ayush, K., and Babu, R. V. (2015). Deepfix: A fully convolutional neural
network for predicting human eye fixations. arXiv preprint arXiv:1510.02927.
[Kuehne et al., 2011] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., and Serre, T. (2011). Hmdb: a large video
database for human motion recognition. In Computer Vision (ICCV), 2011 IEEE International Conference on,
pages 2556–2563. IEEE.
[Kulkarni et al., 2015] Kulkarni, T. D., Whitney, W., Kohli, P., and Tenenbaum, J. B. (2015). Deep convolutional
inverse graphics network. arXiv preprint arXiv:1503.03167.
[LeCun et al., 2015] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436–444.
[LeCun et al., 1998] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324.
[Li et al., 2015a] Li, S., Liu, Z.-Q., and Chan, A. (2015a). Heterogeneous multi-task learning for human pose estimation
with deep convolutional neural network. International Journal of Computer Vision, 113(1):19–36.
[Li et al., 2015b] Li, X., Zhao, L., Wei, L., Yang, M., Wu, F., Zhuang, Y., Ling, H., and Wang, J. (2015b). Deepsaliency:
Multi-task deep neural network model for salient object detection. arXiv preprint arXiv:1510.05484.
15
[Liu et al., 2015] Liu, N., Han, J., Zhang, D., Wen, S., and Liu, T. (2015). Predicting eye fixations using convolutional
neural networks.
[Long et al., 2015] Long, J., Shelhamer, E., and Darrell, T. (2015). Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
3431–3440.
[Lowe, 2004] Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of
computer vision, 60(2):91–110.
[Maas et al., 2013] Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier nonlinearities improve neural network
acoustic models. In Proc. ICML, volume 30.
[Malinowski et al., 2015] Malinowski, M., Rohrbach, M., and Fritz, M. (2015). Ask your neurons: A neural-based
approach to answering questions about images. arXiv preprint arXiv:1505.01121.
[Mao et al., 2015] Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. (2015). Learning like a child: Fast
novel visual concept learning from sentence descriptions of images. arXiv preprint arXiv:1504.06692.
[Mao et al., 2014] Mao, J., Xu, W., Yang, Y., Wang, J., and Yuille, A. (2014). Deep captioning with multimodal
recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632.
[Mopuri and Babu, 2015] Mopuri, K. and Babu, R. (2015). Object level deep feature pooling for compact image
representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pages 62–70.
[Nair and Hinton, 2010] Nair, V. and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann
machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814.
[Nesterov, 1983] Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate o
(1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376.
[Nguyen et al., 2014] Nguyen, A., Yosinski, J., and Clune, J. (2014). Deep neural networks are easily fooled: High
confidence predictions for unrecognizable images. arXiv preprint arXiv:1412.1897.
[Pan et al., 2015] Pan, Y., Mei, T., Yao, T., Li, H., and Rui, Y. (2015). Jointly modeling embedding and translation
to bridge video and language. arXiv preprint arXiv:1505.01861.
[Polyak, 1964] Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1–17.
[Razavian et al., 2014] Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). Cnn features off-the-shelf:
an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014
IEEE Conference on, pages 512–519. IEEE.
[Romero et al., 2014] Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y. (2014). Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550.
[Rowley et al., 1998] Rowley, H. A., Baluja, S., and Kanade, T. (1998). Neural network-based face detection. IEEE
TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 20(1):23–38.
[Rumelhart et al., 1988] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1988). Learning representations by
back-propagating errors. Cognitive modeling, 5:3.
[Russakovsky et al., 2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,
A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L. (2015). Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252.
[Schmidhuber, 2015] Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks,
61:85–117.
[Sermanet et al., 2013a] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013a). Overfeat:
Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229.
[Sermanet et al., 2013b] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013b).
Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229.
16
[Sermanet et al., 2013c] Sermanet, P., Kavukcuoglu, K., Chintala, S., and LeCun, Y. (2013c). Pedestrian detection
with unsupervised multi-stage feature learning. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE
Conference on, pages 3626–3633. IEEE.
[Simonyan and Zisserman, 2014a] Simonyan, K. and Zisserman, A. (2014a). Two-stream convolutional networks for
action recognition in videos. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K., editors,
Advances in Neural Information Processing Systems 27, pages 568–576.
[Simonyan and Zisserman, 2014b] Simonyan, K. and Zisserman, A. (2014b). Very deep convolutional networks for
large-scale image recognition. arXiv preprint arXiv:1409.1556.
[Snoek et al., 2012] Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian optimization of machine
learning algorithms. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances in Neural
Information Processing Systems 25, pages 2951–2959. Curran Associates, Inc.
[Socher et al., 2012] Socher, R., Huval, B., Bath, B., Manning, C. D., and Ng, A. Y. (2012). Convolutional-recursive
deep learning for 3d object classification. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors,
Advances in Neural Information Processing Systems 25, pages 656–664.
[Srinivas and Babu, 2015] Srinivas, S. and Babu, R. V. (2015). Data-free parameter pruning for deep neural networks.
In Xianghua Xie, M. W. J. and Tam, G. K. L., editors, Proceedings of the British Machine Vision Conference
(BMVC), pages 31.1–31.12. BMVA Press.
[Sutskever et al., 2013] Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization
and momentum in deep learning. In Proceedings of the 30th international conference on machine learning (ICML-13),
pages 1139–1147.
[Szegedy et al., 2014] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,
and Rabinovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842.
[Szegedy et al., 2013] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R.
(2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.
[Taigman et al., 2014] Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). Deepface: Closing the gap to
human-level performance in face verification. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE
Conference on, pages 1701–1708. IEEE.
[Uijlings et al., 2013] Uijlings, J., van de Sande, K., Gevers, T., and Smeulders, A. (2013). Selective search for object
recognition. International Journal of Computer Vision.
[Vaillant et al., 1994] Vaillant, R., Monrocq, C., and Le Cun, Y. (1994). Original approach for the localisation of
objects in images. IEE Proceedings-Vision, Image and Signal Processing, 141(4):245–250.
[Venugopalan et al., 2015] Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., and Saenko, K.
(2015). Sequence to sequence–video to text. arXiv preprint arXiv:1505.00487.
[Venugopalan et al., 2014] Venugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R., and Saenko, K. (2014).
Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729.
[Vig et al., 2014] Vig, E., Dorr, M., and Cox, D. (2014). Large-scale optimization of hierarchical features for saliency
prediction in natural images. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2798–2805. IEEE.
[Vincent et al., 2010] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising
autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of
Machine Learning Research, 11:3371–3408.
[Vinyals et al., 2014] Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014). Show and tell: A neural image
caption generator. arXiv preprint arXiv:1411.4555.
[Wan et al., 2013] Wan, L., Zeiler, M., Zhang, S., Cun, Y. L., and Fergus, R. (2013). Regularization of neural networks
using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages
1058–1066.
[Wang et al., 2014a] Wang, A., Lu, J., Wang, G., Cai, J., and Cham, T.-J. (2014a). Multi-modal unsupervised feature
learning for rgb-d scene labeling. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T., editors, Computer Vision
– ECCV 2014, volume 8693 of Lecture Notes in Computer Science, pages 453–467. Springer.
17
[Wang and Schmid, 2013] Wang, H. and Schmid, C. (2013). Action recognition with improved trajectories. In IEEE
International Conference on Computer Vision, Sydney, Australia.
[Wang et al., 2014b] Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., and Wu, Y.
(2014b). Learning fine-grained image similarity with deep ranking. In Computer Vision and Pattern Recognition
(CVPR), 2014 IEEE Conference on, pages 1386–1393. IEEE.
[Wang et al., 2015a] Wang, L., Qiao, Y., and Tang, X. (2015a). Action recognition with trajectory-pooled deep-
convolutional descriptors. In CVPR, pages 4305–4314.
[Wang and Manning, 2013] Wang, S. and Manning, C. (2013). Fast dropout training. In Proceedings of the 30th
International Conference on Machine Learning (ICML-13), pages 118–126.
[Wang et al., 2015b] Wang, X., Zhang, L., Lin, L., Liang, Z., and Zuo, W. (2015b). Deep joint task learning for
generic object extraction. CoRR, abs/1502.00743.
[Wei et al., 2014] Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. (2014). CNN: single-label to
multi-label. CoRR, abs/1406.5726.
[Weinzaepfel et al., 2013] Weinzaepfel, P., Revaud, J., Harchaoui, Z., and Schmid, C. (2013). Deepflow: Large
displacement optical flow with deep matching. In Computer Vision (ICCV), 2013 IEEE International Conference
on, pages 1385–1392. IEEE.
[Yang et al., 2007] Yang, J., Jiang, Y.-G., Hauptmann, A. G., and Ngo, C.-W. (2007). Evaluating bag-of-visual-words
representations in scene classification. In Proceedings of the international workshop on Workshop on multimedia
information retrieval, pages 197–206. ACM.
[Yosinski et al., 2014] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep
neural networks? In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K., editors, Advances
in Neural Information Processing Systems 27, pages 3320–3328. Curran Associates, Inc.
[Zagoruyko and Komodakis, 2015] Zagoruyko, S. and Komodakis, N. (2015). Learning to compare image patches via
convolutional neural networks. arXiv preprint arXiv:1504.03641.
[Zeiler and Fergus, 2014] Zeiler, M. and Fergus, R. (2014). Visualizing and understanding convolutional networks.
In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T., editors, Computer Vision ECCV 2014, volume 8689 of
Lecture Notes in Computer Science, pages 818–833. Springer International Publishing.
[Zeiler, 2012] Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.
[Zhang et al., 2014] Zhang, Z., Luo, P., Loy, C. C., and Tang, X. (2014). Learning and transferring multi-task deep
representation for face alignment. CoRR, abs/1408.3967.
[Zheng et al., 2015] Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., and Torr,
P. (2015). Conditional random fields as recurrent neural networks. arXiv preprint arXiv:1502.03240.
[Zhou et al., 2014] Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva, A. (2014). Learning deep features for
scene recognition using places database. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger,
K., editors, Advances in Neural Information Processing Systems 27, pages 487–495. Curran Associates, Inc.
18
"
10," Procedia Computer Science  22 ( 2013 )  737 – 744 
1877-0509 © 2013 The Authors. Published by Elsevier B.V.
Selection and peer-review under responsibility of KES International
doi: 10.1016/j.procs.2013.09.155 
ScienceDirect
17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - 
KES2013 
Development of a Typing Skill Learning Environment with 
Diagnosis and Advice on Fingering Errors 
Masato Sogaa,*, Takuya Tamurab, Hirokazu Takia 
aFaculty of Systems Engineering, Wakayama University, 930 Sakaedani, Wakayama-shi, Wakayama, 640-8510 Japan 
bGodaiOA, Taya Building, 16-1 Nakayashikicho, Tanabe-shi, Wakayama, 646-0035 Japan 
Abstract 
Existing application software for touch typing training cannot diagnose fingering errors. Given this fact, we developed a 
skill learning environment for touch typing training that can diagnose fingering errors by recognizing fingers with color 
markers using image recognition technique. This study developed two systems: a learning support environment for an 
experimental group and a learning environment for a control group. We evaluated the effect of the learning environment 
that can diagnose fingering errors for the experimental group, by comparison with the other learning environment for the 
control group. 
 
© 2013 The Authors. Published by Elsevier B.V. 
Selection and peer-review under responsibility of KES International. 
 
Keywords: Touch typing; Blind touch; Skill; Learning environment; Fingering; Open CV; Mistype; Typing; 
1. Introduction 
Some application software for learning touch typing skills has been developed. In the literature, one report 
[1] describes a touch-typing training tool that can detect whether a trainee looks at a display or a keyboard 
during training. Another system [2] provides computer-network-based touch typing training that uses the 
principle of competition among students. Another report [3] describes a learning environment for touch typing 
skill using augmented reality technology. 
Some online touch-typing training systems have also been developed. Two, which are shown on their 
respective web sites [4] [5], are online training systems displaying the use of a keyboard and fingers. The 
system presents texts to be typed, and indicates the key and the finger used for the purpose by highlighting 
 
 
 
* Corresponding author. Tel.: +81-73-457-8457 
E-mail address: soga@sys.wakayama-u.ac.jp 
Available online at www.sciencedirect.com
electi  a  eer-re ie  er res si ilit  f  I ter ati al
Open access under CC BY-NC-ND license.
Open access under CC BY-NC-ND license.
738   Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
them on the picture for typing each character. However, the systems cannot diagnose fingering errors by which 
a learner hits the correct key but with the incorrect finger.  
We developed a typing skill learning environment that can diagnose fingering errors. Then we used this 
system to assess the typing of group of experiment participants. Moreover, we developed a learning support 
system for a control group, which is similar to conventional typing training application software. We compared 
the typing skill improvement rates of the respective groups. 
We did not use an existing touch typing learning support system, but instead developed an original 
environment for the control group because we wanted to give the same test texts to both groups. 
2. Developed Systems 
We developed two systems: a learning environment for the experimental group and a learning environment 
for the control group. 
2.1. Learning environment for the experimental group 
The experimental group system consists of a PC, a display monitor, a keyboard, and a web camera supported 
by metallic bars over the keyboard, as shown in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Learning environment for the experimental group 
The system selects a text from a database at random. Then it shows a learner the text in a console window on 
the display monitor. The learner types the text using the keyboard. The typed text is also shown in the console 
window on the display monitor. When the learner mistypes a character, the system indicates the mistyping error 
by not showing the typed character on the console window. 
Furthermore, when the learner has typed a correct character but using a wrong finger, the system signals the 
fingering error and indicates, using a message, the correct finger to be used. Figure 2(a) presents a scene in 
which the system explains the fingering error by a message on the console window. When a learner has had a 
mistyping error or a fingering error in the experimental group, the learner cannot proceed without typing 
correctly. 
 The system uses image recognition technique to sense the finger positions by Open CV. A learner places 
colored markers on the fingernails, as shown in Figure 2(b). The web camera monitors positions of learner’s 
fingers by tracing the colored markers. Colored markers of the finger tops are recognized by the image 
recognition process. The colored markers consist of two pink markers, two green markers, two yellow markers, 
and two blue markers. Correspondence between each finger and each color is fixed. Therefore, the system can 
739 Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
identify each finger top position by each color. The system can sense the coordinate value of each finger 
position, and can ascertain whether a key is hit by the correct finger or not. 
The system shows a hint window on the display monitor in which a picture of learner’s typing taken by the 
monitoring web camera is shown. Moreover, in the picture, the system shows a line between the finger and the 
key which should be used for typing the next character. Figure 2(b) shows the hint window. 
 
  
Fig. 2. (a) Console window showing the text and message. (b) Hint window for a learner in the experimental group 
2.2. System for the control group 
The system for the control group consists of a PC, a display monitor and a keyboard. The system for the 
control group shows text for typing, text typed by a learner, and messages for mistyping errors as well as the 
system for the experimental group. However, the system for the control group does not indicate any message 
for fingering errors. Figure 3 shows that the hint window of the system indicates a key to be hit next and a 
finger to be used next by a picture. The system for the control group represents typical existing application 
software for learning touch typing skills. 
No existing touch typing learning support system was used. For this study, we developed an original 
environment because we had planned to use the same test texts with both groups. 
 
 
Fig. 3. Hint window for the control group 
740   Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
3. Evaluation Experiment 
We evaluated the system for the experimental group by comparison with the system for the control group. 
3.1. Overview of the experiment 
The 12 participants, who are university students, were divided into an experimental group and a control 
group. Each group included six participants. Subjects of each group trained themselves for typing using each 
dedicated system. We performed a pre-test and a post-test before and after the training. We calculated each 
participant’s improvement rate by subtracting each score of the pre-test from each score of the post-test. 
 
3.2. Goal of the experiment 
A goal of the experiment was to verify a learning effect by the learning environment for the experimental 
group. The learning environment for the experimental group detects not only a learner’s mistyping errors but 
also fingering errors. We assessed the effect by comparison to the effect of the system for the control group. 
Another goal was to verify the learner’s awareness and improvement of attitude for fingering errors. We 
performed a questionnaire survey of both groups for the learner’s awareness and improvement of attitude for 
fingering errors. Then we compared the results of the surveys. 
 
3.3. Workflow of the experiment 
At the beginning, we performed a questionnaire survey about touch typing and fingering. Subsequently, we 
conducted a pre-test of both groups. Subjects in both groups typed some texts to assess their typing skills using 
no learning environment. After the pre-test, participants in the experimental group trained themselves for 15 
min using the learning environment for the experimental group, which can detect not only mistyping errors but 
also fingering errors and which can display advice on how to correct the errors. However, participants in the 
control group trained themselves for 15 min using the learning environment for the control group, which can 
detect only mistyping errors. After the training, we administered a post-test to both groups in the same way as 
the pre-test. Finally, we performed a questionnaire survey. 
 
3.4. Questionnaire survey before Pre-test 
We administered a questionnaire survey to participants on their typing skill before the pre-test. The 
questions in the survey were the following. Choices or units for answers are indicated in the parentheses below. 
(1) How many years and months have passed since you began to use a PC? (years, months) 
(2) What percentage of keys of numbers, symbols, and alphabet characters can you hit by touch typing? (Less 
than 30%, 30–60%, 60–90%, More than 90%) 
(3) Can you perform correct fingering during touch typing? (I do not know correct fingering. / It depends on 
keys. / I can do it almost.) 
(4) Have you used any learning environment for typing skills? If you have used it, how long have you used it? 
(months) 
 
741 Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
3.5. Pre-test 
For the pre-test, we showed participants only the console window, as shown in Figure 2(a), and asked them 
to type as they usually do. When a participant had a typing error, the system rejected the mistyped character 
and displayed an error message. However, when a participant had a fingering error, the system did not indicate 
it to the participant and the system accepted the character that was typed using a wrong finger. Texts to be 
typed in the pre-test were selected at random from a database. The pre-test ended when a participant finished 
typing all the text selections. We measured the number of mistyping errors, the number of fingering errors, and 
the duration necessary for typing all of the texts. 
Texts to be typed consisted of alphabet characters, symbols, and numbers. In addition, the set of the texts 
includes every character at least twice. 
3.6. Skill training for touch typing 
After finishing the pre-test, participants in the experimental group trained themselves for 15 min using the 
learning environment for the experimental group, which can detect not only mistyping errors but also fingering 
errors and which can display advice on how to correct the errors. However, participants in the control group 
trained themselves for 15 min using the learning environment for the control group which can detect only 
mistyping errors. 
We told the participants of both groups that they should train themselves for correct fingering and typing as 
fast as possible and that they should avoid mistyping. We also told them that they should be conscious of the 
home position for touch typing. Moreover, we told them specifications of the learning environments. For 
example, either the shift key is usable even if both of shift keys are highlighted in the hint window, or no hint 
will be indicated if the next key to be hit is a space key. 
Subjects in both groups trained themselves using the same text selections as those in the pre-test. 
  
3.7. Post-test 
After the training, we conducted post-tests with both groups in the same way as the pre-test. The post-test 
included the following conditions: that participants should be conscious of correct fingering and that they 
should type as quickly as possible. 
Texts to be typed in the post-test differed from the texts in the pre-test because we wanted to avoid a 
situation in which the participants kept memorizing each text. 
Finally, we administered a questionnaire survey. 
 
4. Results of experiment 
This chapter presents a description of results of the experiment. 
4.1. Results of the experiment 
Table 1 presents results for the experimental group. Table 2 shows the results for the control group. In the 
column of participant ID in the tables, ‘Pre’ represents pre-test, and ‘Post’ represents post-test. 
The total texts to be typed included 447 characters in the pre-test and 354 characters in the post-test. 
Fingering errors were counted only if a participant typed a correct character using a wrong finger. 
742   Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
Table 1. Results of the experimental group 
Subject ID Number of 
fingering errors 
Number of 
mistyping 
Duration time 
(S) 
E1 (Pre) 206 29 322 
E1 (Post) 50 42 378 
E2 (Pre) 6 23 291 
E2 (Post˅ 7 7 203 
E3 (Pre) 252 7 441 
E3 (Post) 77 23 329 
E4 (Pre) 181 42 238 
E4 (Post) 26 8 331 
E5 (Pre) 15 25 350 
E5 (Post) 4 23 266 
E6 (Pre) 22 11 261 
E6 (Post) 12 15 211 
 
Table 2. Results of the control group 
Subject ID Number of 
fingering errors 
Number of 
mistyping 
Duration time 
(S) 
C1 (Pre) 3 31 282 
C1 (Post) 6 18 232 
C2 (Pre) 39 146 337 
C2 (Post) 19 51 298 
C3 (Pre) 38 22 272 
C3 (Post) 18 28 211 
C4 (Pre) 70 61 347 
C4 (Post) 41 26 237 
C5 (Pre) 81 33 350 
C5 (Post) 23 94 486 
C6 (Pre) 43 24 210 
C6 (Post) 40 9 207 
 
 
4.2. Improvement rate 
We calculated improvement rates related to the number of fingering errors, the number of mistyping errors, 
and the duration. The equation of the calculation is the following. 
R = (Pre – Post)/Pre 
There, R is the improvement rate. Pre is each participant’s value in the pre-test. Post denotes each participant’s 
value in the post-test. The improvement rates indicate their reduction ratios. We calculated it because simple 
subtraction between pre-test and post-test does not reflect improvement well in case that each value in the pre-
test is comparatively small. 
 Figure 4(a) shows the improvement rate of the experimental group. Figure 4(b) presents the improvement 
rate of the control group. 
 
743 Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
 
Fig. 4. (a) Improvement rate of experimental group; (b) Improvement rate of control group 
 
4.3. Results of questionnaire survey 
We also administered a questionnaire survey. Table 3 and Table 4 show the results of the survey for each 
group. Each number in each table represents one of the quantities of 5 Likert scale by each participant as 
follows. 
1. No 
2. Probably no 
3. Middle 
4. Probably yes 
5. Yes 
Two questions were asked: “Is it easy to use the system?”, and “Were you able to notice and to correct 
fingering errors during the experiment? 
 
Table 3. Results of the questionnaire survey of the experimental group 
                    Question 
 
Subject ID 
Is it easy to use the system? Were you able to notice and to 
correct fingering errors during 
the experiment? 
E1 4 3 
E2 4 3 
E3 4 5 
E4 5 5 
E5 5 5 
E6 4 5 
Average 4.33 4.33 
 
 
 
 
744   Masato Soga et al. /  Procedia Computer Science  22 ( 2013 )  737 – 744 
Table 4. Results of the questionnaire survey of control group 
                    Question 
 
Subject ID 
Is it easy to use the system? Were you able to notice and to 
correct fingering errors during 
the experiment? 
C1 5 4 
C2 3 4 
C3 5 3 
C4 3 3 
C5 4 1 
C6 4 2 
Average 4.00 2.83 
 
  
4.4. Consideration 
The results of the questionnaire survey show that the learning environment for the experimental group was 
able to notify learners of fingering errors and how to correct the errors. 
However, we were unable to find any significant difference of improvement rates between the groups. One 
reason might be that the training time was only 15 min, although a long time is usually needed to improve skills. 
The other reason is that participants who already had quite a high level of touch typing skill before training 
could not achieve much improvement. 
 
5. Conclusion 
For use in this study, we developed two systems: a learning support environment for the experimental group 
and a learning environment for the control group. We evaluated the effect of the learning environment that can 
diagnose fingering errors for the experimental group by comparison with the other learning environment for the 
control group. The questionnaire survey result shows that the learning environment for the experimental group 
was able to notify learners of fingering errors and how to correct the errors. However, results showed no 
significant difference of improvement rates between the groups. 
If we increase the number of participants and gather only novices as participants, then it might be possible to 
find a significant difference of improvement rates between the groups. We will perform another experiment 
with such refined conditions to verify the learning effect quantitatively. 
References 
[1] Imamura, T., Nagai, T., Nakano, H., “Development of Touch-Typing Training Tool with Eye-Gaze Detecting Function”, IPSJ SIG 
Technical Reports, 2012-CE-117(5), pp.1-8, 2012. (in Japanese) 
[2] Tanaka, K., “Development of Network-based Competitive Japanese Touch-Typing System”, Ikoma Keizai Ronsou 2(1), pp.199-215, 
2004-04-25, Kinki University, 2004. (in Japanese) 
[3] Tsujimoto, S., Soga, M., Taki, H., “Development of a Typing Skill Learning Support Environment by using Augmented Reality”, 
Technical Report of IEICE, 111(473), pp.149-153, 2012. (in Japanese) 
[4] NaruhodoTyping, http://www.naruhodo.net/kb/typing.html 
[5] e-typing, http://www.e-typing.ne.jp/ 
 
"
11,"	 1	
Computational Protein Design with Deep Learning Neural 
Networks 
 
Jingxue Wang1, Huali Cao1, John Z.H. Zhang1-4, and Yifei Qi1,2* 
 
1Shanghai Engineering Research Center of Molecular Therapeutics and New Drug Development, 
School of Chemistry and Molecular Engineering, East China Normal University, Shanghai, 
200062, China 
2NYU-ECNU Center for Computational Chemistry at NYU Shanghai, Shanghai 200062, China 
3Department of Chemistry, New York University, NY, NY 10003, USA 
4Collaborative Innovation Center of Extreme Optics, Shanxi University, Taiyuan, Shanxi 030006 
 
*Correspondence to: yfqi@chem.ecnu.edu.cn 
 
Abstract 
Computational protein design has a wide variety of applications. Despite its remarkable 
success, designing a protein for a given structure and function is still a challenging task. 
On the other hand, the number of solved protein structures is rapidly increasing while the 
number of unique protein folds has reached a steady number, suggesting more structural 
information is being accumulated on each fold. Deep learning neural network is a 
powerful method to learn such big data set and has shown superior performance in many 
machine learning fields. In this study, we applied the deep learning neural network 
approach to computational protein design for predicting the probability of 20 natural 
amino acids on each residue in a protein. A large set of protein structures was collected 
and a multi-layer neural network was constructed. A number of structural properties were 
extracted as input features and the best network achieved an accuracy of 38.3%. Using 
the network output as residue type restraints was able to improve the average sequence 
identity in designing three natural proteins using Rosetta. Moreover, the predictions from 
our network show ~3% higher sequence identity than a previous method. Results from 
this study may benefit further development of computational protein design methods. 
 
 
 
  
	 2	
Introduction 
Proteins perform a vast number of functions in cells including signal transduction, DNA 
replication, catalyzing reactions, etc. Engineering and designing proteins for specific 
structure and function not only deepen our understanding of the protein sequence-
structure relationship, but also have wide applications in chemistry, biology and 
medicine.1 Over the past three decades, remarkable successes have been achieved in 
protein design, in which some of the designs were guided by computational methods. 
Examples of some recent successful computational protein designs include novel folds,2 
novel enzymes,3,4 vaccines,5,6 antibodies,5,7,8 novel protein assemblies,9-13 ligand-binding 
proteins,14,15 and membrane proteins.16-18 Comprehensive coverage of the protein designs 
until 2014 is provided by Samish,19 and more recent ones are reviewed elsewhere.20-23 In 
general, the input of computational protein design is the backbone structure of a target 
protein (or part of a target protein). Through computational sampling and optimization, 
sequences that are likely fold to the desired structure are generated for experimental 
verification. The scoring function usually contains physics-based terms such as van der 
Waals and electrostatic energy as well as knowledge-based terms such as sidechain 
rotamer24 and backbone dihedral preference obtained from statistics of protein 
structures.25,26 In many cases, the sequences from computational design are subject to 
further filtering by considering various factors such as shape complementarity9 and in 
silico folding free energy landscape,27,28 where human experience and familiarity with the 
designed protein play an important role, indicating that there is a gap between current 
design methods and fully automatic designs. 
 
On the other hand, while the number of known protein structures is increasing rapidly, 
the number of unique protein folds is saturating. As of July 2017, there are ~132,000 
structures in the protein data bank (PDB)29 with a yearly increase of ~10,000, but the 
number of unique folds has not changed in the past few years, suggesting more data are 
accumulated on each fold, and therefore statistical learning and utilizing the existing 
structures are likely able to improve the design methods.30,31 Recently, two statistical 
potentials for protein design have been developed,32,33 and the ABACUS potential34 has 
been successfully used in designing proteins.33,35 While these statistical potentials have a 
physical basis, machine learning especially deep-learning neural network has recently 
become a popular method to analyze big data sets, extract complex features, and make 
accurate predictions.36 
 
Deep-learning neural network, as a machine learning technique, is becoming increasingly 
powerful with the development of new algorithms and computer hardware, and has been 
applied to learning massive data sets in a variety of fields such as image recognition,37 
language processing,38 and game playing.39 Particularly in computational 
biology/chemistry, it has been used in protein-ligand scoring,40-42 protein-protein 
interaction prediction,43 protein secondary structure prediction,44-49 protein contact map 
prediction,50-52 and compound toxicity53,54 and liver injury prediction,55 among others.56 
In many cases it shows better performance than other machine learning methods. The 
advantage of using deep neural network is that it can learn high-order features from 
simple input data such as atom coordinates and types. The technical details such as 
network architecture, data representations vary from application to application, but the 
	 3	
fundamental requirement of applying deep neural network is the availability of a large 
amount of data. With the aforementioned rich protein structure data available, it is 
promising to apply deep neural network in computational protein design. Zhou and 
coworkers have used the neural network approach to tackle this problem and developed 
the SPIN method to predict the sequence profile of a protein given the backbone 
structure.57 The input features of SPIN include φ, ψ dihedrals of the target residue, 
sequence profile of 5-residue fragment derived from similar structures (the target residue 
and four subsequent residues), and a rotamer-based energy profile of the target residue 
using the DFIRE potential.58 SPIN was trained on 1532 non-redundant proteins and 
reaches a sequence identity of 30.3% on a test set containing 500 proteins. This sequence 
identity is at the lower boundary of homologous protein59 and is not sufficient to improve 
protein design significantly. 
 
In this study, we applied deep-learning neural networks in computational protein design 
using new structural features, new network architecture, and a larger protein structure 
data set, with the aim of improving the accuracy in protein design. Instead of taking the 
whole input structure into account, we use a sliding widow method that has been used in 
protein secondary structure prediction, and predict the residue identity of each position 
one by one. We consider the target residue and its neighboring residues in three-
dimensional spaces, with the assumption that the identity of the target residue should be 
compatible with its surrounding residues. We collected a large set of high-resolution 
protein structures and extracted the coordinates of each residue and its environment. The 
performance of the neural network on different input setups was compared, and 
application of the network outputs in protein design was investigated.  
 
Results 
 
Network architecture, input, and training 
The input of the computational protein design problem is the backbone structure of a 
protein (or part of a protein). Instead of predicting the residue types of all positions in the 
input protein simultaneously, we consider each target residue and its neighbor residues 
(for simplicity, non-protein residues are not considered). In the simplest case, we 
consider a target position and its closest neighboring residue determined by Cα-Cα 
distance, and feed their input features to a neural network that consists of an input layer, 
several hidden layers and a softmax layer as output. The output dimension of the softmax 
layer is set to 20 so that the 20 output numbers that sum to one can be interpreted as the 
probabilities of 20 residue types of the target residue. This network is named residue 
probability network hereinafter (Figure 1A). Such a simple network that considers only 
one neighbor residue obviously cannot make satisfactory predictions. In this study, we 
take into account the target residue and its 10-30 neighbor residues by repeatedly using 
the residue probability network that shares the same parameters. This setup is similar to 
the application of convolution layer in image recognition where the same convolution 
network is applied to different regions of the input image. One drawback of this setup is 
that the output of each target-neighbor residue pair is equally weighted. Apparently, some 
neighbor residues have larger impacts on the identity of the target residue than others. To 
overcome this, we construct another network that takes the same input as the residue 
	 4	
probability network but outputs a single number as the weight (Figure 1B). The output of 
the residue probability network is multiplied by this weight and then concatenated. 
Several fully-connected layers are then constructed on top of the weighted residue 
probabilities and a 20-dimentional softmax layer is used as the final output (Figure 1C), 
which can be interpreted as the probabilities of 20 residue types of the target residue.  
 
The input for the residue probability and weight network consists of features from the 
target residue and one of its neighbor residues (Figure 1C). The features include basic 
geometric and structural properties of the residues such as Cα-Cα distance, cos and sin 
values of backbone dihedrals φ, ψ and ω, relative location of the neighbor residue to the 
target residue determined by a unit vector from the Cα atom of the central residue to the 
Cα atom of the neighbor residue, three-type secondary structures, number of backbone-
backbone hydrogen-bonds, and solvent accessible surface area of backbone atoms (see 
Methods). To train the neural network, we collected high-resolution protein structures 
from PDB using filtering conditions including structure determination method, resolution, 
chain length, and sequence identity (see Methods). Briefly, three data sets are prepared 
based on three sequence identity cutoffs (30%, 50%, and 90%, referred to as SI30, SI50, 
and SI90) to remove homologous proteins. For each of these data sets, each residue and 
its N (N=10, 15, 20, 25, 30) closest neighbor residues based on Cα-Cα distance are 
extracted as a cluster. These clusters are randomly split into five sets for five-fold cross-
validation. Hereinafter, we will use SI30N10 to refer to the dataset from 30% sequence 
identity cutoff with 10 neighbor residues. Similar naming rules apply to other datasets as 
well. The number of layers and nodes in each fully-connected layer were determined by 
training and test on the smallest data set SI30N10. The neural network training was 
performed for 1000 epochs to ensure convergence. 
	 5	
 
Figure 1. Architecture of the neural networks. (A) The residue probability network, (B) 
Weight network, and (C) The full network. The residue probability and weight networks 
are used as subnetworks that share the same set of network parameters for different inputs. 
Each input consists of the features from the target residue and one of its neighbor 
residues. 
 
Overall and amino acid specific accuracy 
Table 1 shows the overall accuracy (percent of residues that are correctly predicted) and 
standard deviations on different datasets from five-fold cross-validation. As expected, 
datasets with higher protein identity cutoffs show better accuracy due to more data 
samples and higher similarities between samples. However, considering that the number 
of data samples almost doubled from SI30 to SI90 dataset, the improvement in accuracy 
is not significant. Furthermore, in each protein identity cutoff, including 15 neighbor 
residues show best accuracy. Including fewer neighboring residues likely under-
represents the environment of the target residue, whereas including too many neighbor 
residues will generate noises in the inputs and thus require more data samples for training. 
An alternative way of extracting the neighbor residues is to use a certain distance cutoff. 
However, this strategy requires that the input size of the neutral network to be flexible, 
which will be investigated in future studies. 
 
	 6	
Table 1. Accuracy from five-fold cross-validation of the neural network on different 
datasets with different number of neighbor residues. 
Identity cutoff N=10 N=15 N=20 N=25 N=30 
30% 0.329 
(0.001)* 
0.340 
(0.005) 
0.333 
(0.009) 
0.331 
(0.006) 
0.321 
(0.015) 
50% 0.353 
(0.003) 
0.364 
(0.005) 
0.358 
(0.005) 
0.359 
(0.006) 
0.342 
(0.007) 
90% 0.367 
(0.001) 
0.383 
(0.004) 
0.382 
(0.006) 
0.379 
(0.007) 
0.352 
(0.013) 
*Numbers in parentheses are standard deviations. 
We next exam the amino-acid specific accuracy using the results from the SI90N15 
dataset that has the best overall accuracy. To this end, we define the recall and precision 
for each amino acid. Recall is the percent of native residues that are correctly predicted 
(recovered), and precision is the percent of predictions that are correct. Pro and Gly have 
higher recall and precision than other residues with Pro achieves 92.1% recall and 62.7% 
precision (Figure 2). This is because Pro has an exceptional conformational rigidity and 
Gly is highly flexible in terms of backbone dihedrals. A neural network can easily learn 
these distinct structural properties. The amino acids that have lower recall/precision 
generally have lower abundance in the training set, for example Met, Gln and His, 
although we already applied bias to these low-abundance amino acids in training. To 
further characterize the amino-acid specific accuracy, we calculated the probability of 
each native amino acid being predicted as 20 amino acids, and plot it in a 2D native vs 
predicted heat map (Figure 3). The amino acids in x- and y-axis are ordered by their 
properties and similarities with each other. The diagonal grids show higher probabilities, 
as expected. Interestingly, there are several groups along the diagonal including RK, DN, 
VI, and FYW, indicating that the neural network frequently predicts one amino acid as 
another within each group. Considering the similarities of amino acids within each group, 
replacing one amino acid with another from the same group probably does not disrupt the 
protein structure, which suggests that the neural network may mispredict the native 
amino acid, but still provide a reasonable answer.  
 
 
Figure 2. Recall and precision of different amino acids of the network trained on the 
SI90N15 dataset. Recall is the percent of native residues that are correctly predicted 
(recovered), and precision is the percent of predictions that are correct. 
	 7	
 
 
Figure 3. Probability of each amino acid being predicted as 20 amino acids. 
 
Top-K accuracy and its application in protein design 
Because the output of the neural network is the probabilities of 20 amino acids at a target 
position, in addition to the accuracy mentioned above, it is also possible to calculate the 
top-K accuracy: if the native amino acid is within the top-K predictions (K amino acids 
that have the highest probabilities), the prediction is considered correct. The top-2, 3, 5, 
and 10 accuracy of the network trained on the SI90N15 dataset reaches 54.3%, 64.0%, 
76.3%, and 91.7% respectively, suggesting the native amino acids are enriched in the first 
half of the predictions (Figure 4). A simple application of such information is to restrain 
the available amino acid types at a target position during protein design. As an illustrative 
example, we applied the top-3, 5, and 10 predictions as residue-type restraints in 
designing three proteins including an all-α protein (PDB ID 2B8I60), an all-β protein 
(PDB ID 1HOE61), and a mixed αβ protein (PDB ID 2IGD, Figure 5). None of these 
proteins are included in our training set. The crystal structures of these proteins were used 
as inputs for the neural network trained on SI90N15 dataset. The top-3, 5, and 10 amino 
acids for each position were used as restraints in the fixed-backbone design program 
fixbb in Rosetta.62 As a control, we listed the top one accuracy of the neural network on 
these proteins, and also performed fixed-backbone design without any residue-type 
restraints (all 20 natural amino acids are allowed at each position). As fixbb uses a 
stochastic design algorithm, we generated 500 sequences for each protein and calculated 
the average sequence identity to the native proteins (Table 2). In the three proteins, using 
information from the neural network predictions improves the average sequence identity, 
but the best K value is system dependent, and in some cases the results are worse than 
those in restraints-free designs (e.g., top-1 in 1HOE).  
	 8	
 
Figure 4. Top-K accuracy of the neural network trained on the SI90N15 dataset. 
 
Figure 5. Structures of the proteins used in protein design with residue-type restraints. 
 
Table 2. Average sequence identity of Rosetta fixed-backbone design on three proteins 
with/without residue-type restraints. 
Protein No-restrain* Top 1 Top 3* Top 5* Top 10* 
2B8I 0.276±0.033 0.337 0.306±0.017 (0.558) 
0.354±0.021 
(0.688) 
0.293±0.037 
(0.883) 
1HOE 0.408±0.026 0.338 0.473±0.018 (0.635) 
0.441±0.018 
(0.689) 
0.416±0.028  
(0.851) 
2IGD 0.409±0.034 0.475 0.473±0.023 (0.705) 
0.401±0.028 
(0.754) 
0.408±0.032  
(0.967) 
*Sequence identities are presented as average ± standard deviation from 500 designs. 
Numbers in parentheses are maximal possible identities given the residue-type restraints. 
 
 
Comparison with SPIN 
Finally, we compare the performance of our network with SPIN developed by Zhou and 
coworkers.57 SPIN was trained on 1532 non-redundant proteins and reaches a sequence 
identity of 30.3% on a test set containing 500 proteins. The training and test set were 
collected using a sequence identity cutoff of 30%. SPIN was also evaluated on 50 
proteins from the 500 test proteins for comparison with Rosetta.62 As the structure ID of 
these 50 proteins are known, we set out to compare out network with SPIN on these 50 
	 9	
proteins. For a fair comparison, we re-trained our network on the SI30N15 dataset 
without the 50 proteins. Table 3 lists the average sequence identity from both methods 
when top 1 to 10 predictions are considered. Our network shows ~3% higher identity 
than SPIN. The number of data samples almost tripled in our study (~1.5 million training 
residues for our network and ~0.45 million residues for SPIN assuming each protein has 
300 residues) but the improvement of accuracy is not significant, indicating certain 
limitation in learning sequence information from protein structures.  
Nonetheless, the networks trained on the larger data set in this study could still be 
beneficial to computational protein design. As in real applications, amino acid probability 
learned on larger data set could be more useful, as long as it is not biased. As an example, 
we tested both methods on the de novo designed protein Top7 (PDB ID 1QYS,2 not 
included in our training set). The top 1 prediction from SPIN shows an identity of 0.250, 
while the top 1 predictions from the SI30N15, SI50N15, and SI90N15 network have 
identities of 0.283, 0.304, and 0.402. 
In addition to comparing sequence identities, we also compared out predictions with the 
position-specific scoring matrix (PSSM) from PSI-BLAST.63 The PSSMs of the 50 test 
proteins were obtained by running PSI-BLAST against the non-redundant protein 
sequences database available at ftp://ftp.ncbi.nlm.nih.gov/blast/db/, and converted to 
pseudo probability matrixes of 20 amino acids at each residue. The root mean square 
error (RMSE) of the matrixes to those predicted by our network and SPIN were 
calculated. Our network and SPIN show very similar RMSE values (0.139 for our 
network and 0.141 for SPIN). It should be noted that SPIN was trained on PSSMs from 
PSIBLAST for predicting sequence profiles whereas our network was trained on protein 
sequences only. 
 
Table 3. Average sequence identity of SPIN and our network on 50 test proteins. 
 Top 1 Top 2 Top 3 Top 5 Top 10 
SPIN 0.302 0.453 0.552 0.677 0.868 
This study* 0.330 
(0.002) 
0.487 
(0.005) 
0.585 
(0.002) 
0.717 
(0.001) 
0.896 
(0.002) 
*Numbers in parentheses are standard deviations from 5 networks trained on the same 
dataset with different random number seeds. 
 
Discussions 
In this study, we have developed deep-learning neural networks for computational protein 
design. The networks achieve an accuracy of 38.3% on the dataset with 90% sequence 
identity cutoff when 15 neighboring residues are included. This accuracy is limited not 
only by our neural network approach but also by the nature of protein structures. It is 
known that two proteins with low sequence identity (~30%) can fold into similar 
structures.59 In a DNA repair enzyme 3-methyladenine DNA glycosylase, the probability 
that a random mutation can inactivate the protein was found to be 34%±6%, indicating a 
large proportion of mutations can be tolerated in this protein.64 Moreover, residues at 
active sites are subject to functional restraints and are not necessary the most stable 
ones.65 Our neural network approach is similar to a structural comparison method that 
extracts and integrates similar local structures from known structures. Therefore, its 
accuracy is limited by the average number of tolerable amino acids at each residue in the 
	 10	
training set. Fortunately, the native amino acid is concentrated in the top predictions (top-
5 and 10 accuracies are 76.3% and 91.7%). By integration the network output with 
molecular-mechanics scoring functions, it should be possible to identify the correct 
amino acid from the top predictions and further improve the accuracy. Particularly, the 
network preforms well on Gly and Pro, due to its ability to learn distinct structural 
features, but less satisfying on hydrophobic residues that are likely more important for the 
correct folding of a protein. Including solvation energy in the molecular mechanics 
scoring functions is probably a promising way for future development. 
 
In our approach, the environment of a target residue is simply considered using the N 
closest residues based on Cα-Cα distances. This method may exclude some residues that 
have important interactions with the target residue. To quantitatively characterize this, we 
calculated the distance rank (M, the rank of Cα-Cα distance of a neighbor residue among 
all residues surrounding the target residue) of neighbor residues that have contacts (heavy 
atom distance < 4.5 Å) with the target residue in our dataset, and found that 96.2% 
contacting residues have M≤20, and 98.9% contacting residues have M≤30, which means 
3.8% and 1.1% of the contacting residues are not included in the environment if N=20 
and 30, respectively. Moreover, for terminal residues that are highly exposed, 20 
neighbors may contain residues that do not have contacts with the target residue, which 
will generate noises in the inputs. Using distance cutoff instead of residue number cutoff 
may solve this problem. However, the distance cutoff method requires the input size to be 
highly flexible from several residues to tens of residues, which should be carefully 
considered during network construction. 
 
Knowing the possible amino acids with good confidence at the designing positions may 
reduce the search space significantly and increase the chance to make a successful design. 
Our test of Rosetta design on three proteins shows that it is possible to improve the 
sequence identity by using the output from our neural network as residue-type restraints. 
However, the optimal number of amino acids to be used as restraints is system dependent. 
More importantly, in our neural network, the prediction on each residue is independent 
from each other. For real designs, it is important to simultaneously consider the identities 
of the neighbor residues by using molecular-mechanics-based or statistical scoring 
functions like the ones in Rosetta. In this regard, the predicted probability of each amino 
acid should be explicitly taken into account. As the prediction of a trained neural network 
on a protein structure only takes several seconds, we expect our approach to pave the way 
for further development of computational protein design methods. 
 
Methods 
Datasets and input features 
The training set was collected from PDB29 using the following criteria: (1) the structure is 
determined with x-ray crystallography, (2) the resolution is better than 2 Å, (3) the chain 
length is longer than 50, and (4) the structure does not have any DNA/RNA molecules. 
To investigate the effects of sequence homology on prediction accuracy, the structures 
that satisfy these conditions were retrieved with 30%, 50%, and 90% sequence identities. 
The resulting entries were cross-referenced with the OPM database66 to remove 
membrane proteins. Structures that have D-amino acids were also discarded. The 
	 11	
resulting structure dataset consists of 10173 (30% sequence identity), 14064 (50% 
sequence identity), and 17607 structures (90% sequence identity). To remove the bias 
from non-biological interface in the crystal asymmetric unit, the biological assembly 
provided by PDB was used. If multiple biological assemblies exist for one structure, the 
first assembly from PDB was used. For each of these structures, non-protein residues 
such as water, ion, and ligand were removed, and each protein residue and its N closest 
(N=10, 15, 20, 25, 30, ranked based on Cα-Cα distance) neighboring residues were 
extracted as a structural cluster. Clusters that have any atoms with an occupancy < 1 or 
missing backbone atoms were discarded. Protein oligomeric state was also considered 
during cluster extraction so that if a structure contains several identical subunits, only one 
of the subunits was used. Each cluster was then translated and orientated so that the Cα, 
N, and C atoms of the target residue are located at the origin, the –x axis, and the z=0 
plane, respectively.  
 
The input features for the neural networks are (1) for the central residues: cos and sin 
values of backbone dihedrals φ, ψ and ω, total solvent accessible surface area (SASA) of 
backbone atoms (Cα, N, C, and O), and three-type (helix, sheet, loop) secondary structure; 
(2) for the neighbor residues: cos and sin values of backbone dihedrals φ, ψ, and ω, total 
SASA of backbone atoms, Cα-Cα distance to the central residue, unit vector from the Cα 
atom of the central residue to the Cα atom of the neighbor residue, Cα-N unit vector of the 
neighbor residue, Cα-C unit vector of the neighbor residue, three-type secondary 
structure, and number of backbone-backbone hydrogen bonds between the central residue 
and the neighbor residue. The Cα-Cα distance, Cα-Cα, Cα-N, and Cα-C unit vectors were 
used to define the exact position and orientation of the neighbor residue with respect to 
the central residue. cos and sin values of the dihedrals were used because the dihedrals 
that range from -180 to 180 are not continuous at -180 and 180. The SASA value was 
calculated using the Naccess program67 on the whole protein structure (not on a structural 
cluster) with the sidechain atom removed, because during protein design, the identity of a 
residue and thus its sidechain atoms are unknown. Secondary structure was assigned with 
Stride.68 All other features were calculated with an in-house program.  
 
Deep neural-network learning 
The neural network was constructed using the Keras library (http://keras.io) with rectified 
linear unit (ReLU) as the activation function for all layers. Training was performed using 
the categorical cross entropy as the loss function and the stochastic gradient descent 
method for optimization with a learning rate of 0.01, a Nesterov momentum of 0.9, and a 
batch size of 40,000. To account for the different abundance of each residue type in the 
training set, the training samples were weighted as: Wi = Nmax/Ni, where Nmax is the 
maximal number of samples of all 20 residue types, and Ni is the number of samples of 
residue type i. This bias would force the neural network to learn more from the residue 
types that are underrepresented in the training set. The output of the neural-network is the 
probability of 20 amino acids for the central residue of a cluster. 
 
Rosetta design 
Rosetta design was carried out with the fixbb program and talaris2014 score in Rosetta 
3.7.62 The crystal structures of the design targets were used as inputs without any prior 
	 12	
minimization. 500 designs were performed for each protein with and without residue-type 
restraints, which were incorporated using the “-resfile” option. 
  
Acknowledgements 
This work was supported by the National Natural Science Foundation of China (Grant no. 
31700646) to Y.Q. and (Grant no. 21433004) J.Z., Ministry of Science and Technology 
of China (Grant no. 2016YFA0501700), NYU Global Seed Grant, and Shanghai Putuo 
District (Grant 2014-A-02) to J.Z.. We thank the Supercomputer Center of East China 
Normal University for providing us computer time.  
 
  
	 13	
References 
1 Sandhya, S., Mudgal, R., Kumar, G., Sowdhamini, R. & Srinivasan, N. Protein 
sequence design and its applications. Curr Opin Struct Biol 37, 71-80 (2016). 
2 Kuhlman, B. et al. Design of a novel globular protein fold with atomic-level 
accuracy. Science 302, 1364-1368 (2003). 
3 Jiang, L. et al. De novo computational design of retro-aldol enzymes. Science 
319, 1387-1391 (2008). 
4 Rothlisberger, D. et al. Kemp elimination catalysts by computational enzyme 
design. Nature 453, 190-195 (2008). 
5 Correia, B. E. et al. Computational design of epitope-scaffolds allows induction 
of antibodies specific for a poorly immunogenic HIV vaccine epitope. Structure 
18, 1116-1126 (2010). 
6 Correia, B. E. et al. Proof of principle for epitope-focused vaccine design. Nature 
507, 201-206 (2014). 
7 Leaver-Fay, A. et al. Computationally Designed Bispecific Antibodies using 
Negative State Repertoires. Structure 24, 641-651 (2016). 
8 Lewis, S. M. et al. Generation of bispecific IgG antibodies by structure-based 
design of an orthogonal Fab interface. Nat Biotechnol 32, 191-198 (2014). 
9 Bale, J. B. et al. Accurate design of megadalton-scale two-component icosahedral 
protein complexes. Science 353, 389-394 (2016). 
10 Gonen, S., DiMaio, F., Gonen, T. & Baker, D. Design of ordered two-dimensional 
arrays mediated by noncovalent protein-protein interfaces. Science 348, 1365-
1368 (2015). 
11 Hsia, Y. et al. Design of a hyperstable 60-subunit protein dodecahedron. Nature 
535, 136-139 (2016). 
12 King, N. P. et al. Accurate design of co-assembling multi-component protein 
nanomaterials. Nature 510, 103-108 (2014). 
13 King, N. P. et al. Computational design of self-assembling protein nanomaterials 
with atomic level accuracy. Science 336, 1171-1174 (2012). 
14 Tinberg, C. E. et al. Computational design of ligand-binding proteins with high 
affinity and selectivity. Nature 501, 212-216 (2013). 
15 Zhou, L. et al. A protein engineered to bind uranyl selectively and with 
femtomolar affinity. Nat Chem 6, 236-241 (2014). 
16 Zhang, Y. et al. Computational design and experimental characterization of 
peptides intended for pH-dependent membrane insertion and pore formation. ACS 
Chem Biol 10, 1082-1093 (2015). 
17 Korendovych, I. V. et al. De novo design and molecular assembly of a 
transmembrane diporphyrin-binding protein complex. J Am Chem Soc 132, 
15516-15518 (2010). 
18 Joh, N. H. et al. De novo design of a transmembrane Zn(2)(+)-transporting four-
helix bundle. Science 346, 1520-1524 (2014). 
19 Samish, I. in Computational protein design   (ed Ilan Samish) Ch. 2, 21-94 
(Humana Press, 2016). 
20 Huang, P. S., Boyken, S. E. & Baker, D. The coming of age of de novo protein 
design. Nature 537, 320-327 (2016). 
	 14	
21 Yang, W. & Lai, L. Computational design of ligand-binding proteins. Curr Opin 
Struct Biol 45, 67-73 (2016). 
22 Norn, C. H. & Andre, I. Computational design of protein self-assembly. Curr 
Opin Struct Biol 39, 39-45 (2016). 
23 Liu, H. & Chen, Q. Computational protein design for given backbone: recent 
progresses in general method-related aspects. Curr Opin Struct Biol 39, 89-95 
(2016). 
24 Shapovalov, M. V. & Dunbrack, R. L., Jr. A smoothed backbone-dependent 
rotamer library for proteins derived from adaptive kernel density estimates and 
regressions. Structure 19, 844-858 (2011). 
25 Li, Z., Yang, Y., Zhan, J., Dai, L. & Zhou, Y. Energy functions in de novo protein 
design: current challenges and future prospects. Annu Rev Biophys 42, 315-335 
(2013). 
26 Boas, F. E. & Harbury, P. B. Potential energy functions for protein design. Curr 
Opin Struct Biol 17, 199-204 (2007). 
27 Doyle, L. et al. Rational design of alpha-helical tandem repeat proteins with 
closed architectures. Nature 528, 585-588 (2015). 
28 Bhardwaj, G. et al. Accurate de novo design of hyperstable constrained peptides. 
Nature 538, 329-335 (2016). 
29 Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res 28, 235-242 
(2000). 
30 Broom, A., Trainor, K., MacKenzie, D. W. & Meiering, E. M. Using natural 
sequences and modularity to design common and novel protein topologies. Curr 
Opin Struct Biol 38, 26-36 (2016). 
31 Khersonsky, O. & Fleishman, S. J. Why reinvent the wheel? Building new 
proteins based on ready-made parts. Protein Sci 25, 1179-1187 (2016). 
32 Topham, C. M., Barbe, S. & Andre, I. An Atomistic Statistically Effective Energy 
Function for Computational Protein Design. J Chem Theory Comput 12, 4146-
4168 (2016). 
33 Xiong, P. et al. Protein design with a comprehensive statistical energy function 
and boosted by experimental selection for foldability. Nat Commun 5, 5330 
(2014). 
34 Xiong, P., Chen, Q. & Liu, H. Computational Protein Design Under a Given 
Backbone Structure with the ABACUS Statistical Energy Function. Methods Mol 
Biol 1529, 217-226 (2017). 
35 Zhou, X. et al. Proteins of well-defined structures can be designed without 
backbone readjustment by a statistical model. J Struct Biol 196, 350-357 (2016). 
36 LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015). 
37 Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-
Scale Image Recognition. ArXiv e-prints 1409 (2014). 
<http://adsabs.harvard.edu/abs/2014arXiv1409.1556S>. 
38 Collobert, R. & Weston, J. A unified architecture for natural language processing: 
deep neural networks with multitask learning. Proceedings of the 25th 
international conference on Machine learning, 160-167 (2008). 
39 Silver, D. et al. Mastering the game of Go with deep neural networks and tree 
search. Nature 529, 484-489 (2016). 
	 15	
40 Gomes, J., Ramsundar, B., Feinberg, E. N. & Pande, V. S. Atomic Convolutional 
Networks for Predicting Protein-Ligand Binding Affinity. ArXiv e-prints 1703 
(2017). <http://adsabs.harvard.edu/abs/2017arXiv170310603G>. 
41 Wallach, I., Dzamba, M. & Heifets, A. AtomNet: A Deep Convolutional Neural 
Network for Bioactivity Prediction in Structure-based Drug Discovery. ArXiv e-
prints 1510 (2015). 
<http://adsabs.harvard.edu/abs/2015arXiv151002855W>. 
42 Ragoza, M., Hochuli, J., Idrobo, E., Sunseri, J. & Koes, D. R. Protein-Ligand 
Scoring with Convolutional Neural Networks. J Chem Inf Model 57, 942-957 
(2017). 
43 Sun, T. L., Zhou, B., Lai, L. H. & Pei, J. F. Sequence-based prediction of protein 
protein interaction using a deep-learning algorithm. Bmc Bioinformatics 18 
(2017). 
44 Heffernan, R. et al. Improving prediction of secondary structure, local backbone 
angles, and solvent accessible surface area of proteins by iterative deep learning. 
Sci Rep 5, 11476 (2015). 
45 Li, Z. & Yu, Y. Protein Secondary Structure Prediction Using Cascaded 
Convolutional and Recurrent Neural Networks. ArXiv e-prints 1604 (2016). 
<http://adsabs.harvard.edu/abs/2016arXiv160407176L>. 
46 Wang, S., Peng, J., Ma, J. & Xu, J. Protein Secondary Structure Prediction Using 
Deep Convolutional Neural Fields. Sci Rep 6, 18962 (2016). 
47 Busia, A., Collins, J. & Jaitly, N. Protein Secondary Structure Prediction Using 
Deep Multi-scale Convolutional Neural Networks and Next-Step Conditioning. 
ArXiv e-prints 1611 (2016). 
<http://adsabs.harvard.edu/abs/2016arXiv161101503B>. 
48 Kaae Sønderby, S. & Winther, O. Protein Secondary Structure Prediction with 
Long Short Term Memory Networks. ArXiv e-prints 1412 (2014). 
<http://adsabs.harvard.edu/abs/2014arXiv1412.7828K>. 
49 Faraggi, E., Zhang, T., Yang, Y., Kurgan, L. & Zhou, Y. SPINE X: improving 
protein secondary structure prediction by multistep learning coupled with 
prediction of solvent accessible surface area and backbone torsion angles. J 
Comput Chem 33, 259-267 (2012). 
50 Wang, S., Sun, S., Li, Z., Zhang, R. & Xu, J. Accurate De Novo Prediction of 
Protein Contact Map by Ultra-Deep Learning Model. PLoS Comput Biol 13, 
e1005324 (2017). 
51 Di Lena, P., Nagata, K. & Baldi, P. Deep architectures for protein contact map 
prediction. Bioinformatics 28, 2449-2457 (2012). 
52 Eickholt, J. & Cheng, J. Predicting protein residue-residue contacts using deep 
networks and boosting. Bioinformatics 28, 3066-3072 (2012). 
53 Mayr, A., Klambauer, G., Unterthiner, T. & Hochreiter, S. DeepTox: Toxicity 
Prediction using Deep Learning. Frontiers in Environmental Science 3 (2016). 
54 Unterthiner, T., Mayr, A., Klambauer, G. & Hochreiter, S. Toxicity Prediction 
using Deep Learning. ArXiv e-prints 1503 (2015). 
<http://adsabs.harvard.edu/abs/2015arXiv150301445U>. 
55 Xu, Y. et al. Deep Learning for Drug-Induced Liver Injury. J Chem Inf Model 55, 
2085-2093 (2015). 
	 16	
56 Goh, G. B., Hodas, N. O. & Vishnu, A. Deep learning for computational 
chemistry. J Comput Chem 38, 1291-1307 (2017). 
57 Li, Z., Yang, Y., Faraggi, E., Zhan, J. & Zhou, Y. Direct prediction of profiles of 
sequences compatible with a protein structure by neural networks with fragment-
based local and energy-based nonlocal profiles. Proteins 82, 2565-2573 (2014). 
58 Zhou, H. & Zhou, Y. Distance-scaled, finite ideal-gas reference state improves 
structure-derived potentials of mean force for structure selection and stability 
prediction. Protein Sci 11, 2714-2726 (2002). 
59 Rost, B. Twilight zone of protein sequence alignments. Protein Eng 12, 85-94 
(1999). 
60 Lee, J. H. et al. Crystal structure and functional studies reveal that PAS factor 
from Vibrio vulnificus is a novel member of the saposin-fold family. J Mol Biol 
355, 491-500 (2006). 
61 Pflugrath, J. W., Wiegand, G., Huber, R. & Vertesy, L. Crystal structure 
determination, refinement and the molecular model of the alpha-amylase inhibitor 
Hoe-467A. J Mol Biol 189, 383-386 (1986). 
62 Leaver-Fay, A. et al. ROSETTA3: an object-oriented software suite for the 
simulation and design of macromolecules. Methods Enzymol 487, 545-574 
(2011). 
63 Altschul, S. F. et al. Gapped BLAST and PSI-BLAST: a new generation of 
protein database search programs. Nucleic Acids Res 25, 3389-3402 (1997). 
64 Guo, H. H., Choe, J. & Loeb, L. A. Protein tolerance to random amino acid 
change. Proc Natl Acad Sci U S A 101, 9205-9210 (2004). 
65 Tokuriki, N., Stricher, F., Serrano, L. & Tawfik, D. S. How protein stability and 
new functions trade off. PLoS Comput Biol 4, e1000002 (2008). 
66 Lomize, M. A., Lomize, A. L., Pogozheva, I. D. & Mosberg, H. I. OPM: 
orientations of proteins in membranes database. Bioinformatics 22, 623-625 
(2006). 
67 'NACCESS', Computer Program (Department of Biochemistry and Molecular 
Biology, University College London., 1993). 
68 Frishman, D. & Argos, P. Knowledge-based protein secondary structure 
assignment. Proteins 23, 566-579 (1995). 
 
 
"
12,"Handwriting Recognition of Historical Documents with few labeled data
Edgard Chammas and Chafic Mokbel
University of Balamand
El-Koura, Lebanon
{edgard,chafic.mokbel}@balamand.edu.lb
Laurence Likforman-Sulem
Institut Mines Telecom, Telecom ParisTech and Universite´ Paris-Saclay
Paris, France
laurence.likforman@telecom-paristech.fr
Abstract—Historical documents present many challenges
for offline handwriting recognition systems, among them, the
segmentation and labeling steps. Carefully annotated text-
lines are needed to train an HTR system. In some scenarios,
transcripts are only available at the paragraph level with no
text-line information. In this work, we demonstrate how to
train an HTR system with few labeled data. Specifically, we
train a deep convolutional recurrent neural network (CRNN)
system on only 10% of manually labeled text-line data from a
dataset and propose an incremental training procedure that
covers the rest of the data. Performance is further increased
by augmenting the training set with specially crafted multi-
scale data. We also propose a model-based normalization
scheme which considers the variability in the writing scale at
the recognition phase. We apply this approach to the publicly
available READ dataset1. Our system achieved the second
best result during the ICDAR2017 competition [1].
Keywords-CRNN, handwriting recognition, historical docu-
ments, variability, multi-scale training, model-based normal-
ization scheme, limited labeled data
I. INTRODUCTION
Most state-of-the-art offline handwriting text recognition
(HTR) systems work at the line level by transforming
the text-line image into a sequence of feature vectors.
These features are fed into an optical model (e.g, recurrent
neural network) in order to recognize the handwritten
text. Recent work on text detection and localization [2]
at the document level, and joint line segmentation and
recognition at the paragraph level [3] showed promising
results. However, the best recognition results are still
achieved by the systems working at the line level [4]. The
automatic segmentation of paragraphs into lines is even
more challenging on historical documents. Old manuscripts
are often acquired as low resolution images with degraded
quality, with overlapping characters across adjacent text-
lines (see figure 1). Supervised (or at least semi-supervised)
paragraph segmentation is needed to label each text-line in
order to train an HTR system. However, this is a tedious
and time consuming task that is not always feasible for
different reasons (budget, time, priority, and availability of
text data). When transcriptions are primarily provided at
the paragraph level, the first challenge consists in aligning
the training transcription data with the corresponding lines
in the image. In this paper, we propose to perform such
an alignment after training a first recognition system on a
limited amount of annotated data. The first system serves
to bootstrap the whole process. We also suggest to augment
1https://read.transkribus.eu/
the amount of data by generating multiscale synthetic data
in order to better consider the scale factor in the test
images. We apply this approach to the READ dataset, a
multilingual Latin offline handwriting dataset. The training
data provided during the ICDAR2017 competition2 were
part of the Alfred Escher Letter Collection (AEC), with
a large vocabulary of more than 130k words. The test
data were letter documents from the same period of
AEC. In section II, we present our state of the art deep
convolutional recurrent neural network (CRNN) that we
used in ICDAR2017 competition on handwritten text
recognition. During the competition, 10000 pages were
available for training with transcriptions provided at the
paragraph level only. In section III, we demonstrate how
to train an HTR system by using a small amount of
manually segmented and labeled text-lines to create a
bootstrap model. We further improve the performance of
our system by augmenting the training set with specially
crafted synthetic data, explicitly taking into consideration
the variability in the writing scale (section IV). In section
V, we propose a model-based normalization scheme that
considers the writing scale variability in the test data.
Our system achieved the second best result during the
ICDAR2017 competition.
Figure 1: Old manuscripts from the READ 2017 dataset.
2https://scriptnet.iit.demokritos.gr/competitions/8/
ar
X
iv
:1
81
1.
07
76
8v
1 
 [c
s.C
V]
  1
0 N
ov
 20
18
II. CRNN SYSTEM DESCRIPTION
Our system is a deep Convolutional Recurrent Neural
Network (CRNN) inspired from the VGG16 architecture
[5] used for image recognition. We use a stack 13
convolutional (3× 3 filters, 1× 1 stride) layers followed
by three Bidirectional LSTM layers with 256 units per
layer. Each LSTM unit has one cell with enabled peephole
connections. Spacial pooling (max) is employed after
some convolutional layers. To introduce non-linearity, the
Rectified Linear Unit (ReLU) activation function was
used after each convolution. It has the advantage of being
resistant to the vanishing gradient problem while being
simple in terms of computation, and was shown to work
better than sigmoid and tanh activation functions [6]. A
square shaped sliding window is used to scan the text-line
image in the direction of the writing. The height of the
window is equal to the height of the text-line image, which
has been normalized to 64 pixels. The window overlap
is equal to 2 pixels to allow continuous transition of the
convolution filters. For each analysis window of 64× 64
pixels in size, 16 feature vectors are extracted from the
feature maps produced by the last convolutional layer and
fed into the observation sequence. It is worth noting that
the amount of feature vectors extracted from each sliding
windows is important. The number must be reasonable
as to provide a good sampling for the image. Based on
previous experiments, we found out that oversampling
(32 feature vectors per window) and under-sampling (8
feature vectors per window) will decrease the performance.
Sixteen feature vectors were found to work best for our
architecture. Since for each of the 16 columns of the
last 512 feature maps, the columns of height 2 pixels
are concatenated into a feature vector of size 1024 (512×2).
Thanks to the CTC objective function [7], the system is
end-to-end trainable. The convolutional filters and the
LSTM units weights are thus jointly learned within the
back-propagation procedure. We chose to keep the network
simple with a relatively small number of parameters. We
thus combine the forward and backward outputs at the
end of the BLSTM stack [8] rather than at each BLSTM
layer. We also chose not to add additional fully-connected
layers. The LSTM unit weights were initialized as per [9]
method, which proved to work well and helps the network
convergence faster. This allows the network to maintain a
constant variance across the network layers which keeps
the signal from exploding to a high value or vanishing to
zero.
The weight matrix Wij were initialized with a uniform
distribution given as Wij ∼ U(−
√
6
n ,
√
6
n ), where n is the
total number of input and output neurons at the layer
(assuming all layers are of the same size).
Adam optimizer [10] was used to train the network with
initial learning rate of 0.001. This algorithm could be
thought of as an upgrade for RMSProp [11], offering
bias correction and momentum [12]. It provides adaptive
learning rates for the stochastic gradient descent update
computed from the first and second moments of the
gradients. It also stores an exponentially decaying average
of the past squared gradients (similar to Adadelta [13] and
RMSprop) and the past gradients (similar to momentum).
Batch normalization as described in [14], was added after
each convolutional layer in order to accelerate the training
process. It basically works by normalizing each batch
by both mean and variance. The network was trained in
an end-to-end fashion with the CTC loss function [7].
A token passing algorithm was used for decoding [15].
It integrates a bigram language model with modified
Kneser-Ney discounting [16], built from the available
training data. It is worth noting that no preprocessing is
needed. The system works directly on raw images. The
full architecture is provided at the end of this paper (figure
5) and the code can be found on GitHub3.
III. INCREMENTAL TRAINING WITH FEW LABELED DATA
With no line information provided, few labeled text-
lines are needed to bootstrap the training process. We
used an automatic segmentation algorithm to extract line
images from the document images. The algorithm selects
candidate baselines by analyzing contours distribution. It
then assigns each contour to one of the baselines based on a
number of criteria, related to the average distance between
two lines and the distance between the contour center
and the line (see figure 2). Only 10% of the pages were
manually verified, making sure the line segmentation is
correct, and used to bootstrap the training process. Besides
the 10,000 training pages, 50 annotated pages at the line
level were provided during the competition and were used
for validation in the training process. The initial recognition
system, trained on 10% of the data, achieved 9.2% raw label
error rate (LER). This performance can be considered good
enough to allow an incremental training of the network
from the rest of the data.
Figure 2: Candidate baselines with contours bounding
boxes.
As a next step, the system was set to recognize the
remaining 90% of the segmented line images in the training
set. The recognized lines were mapped to lines in the
ground-truth data for each page, based on the Levenshtein
distance [17] between the text lines. A mapping is consid-
ered valid when the edit distance is less than or equal to
half the length of the reference line. Following this process,
and according to this threshold on the Levenshtein distance,
80% of the available text-lines were selected to retrain the
system, while the rest (20%) were discarded. The retrained
system achieved a relative decrease of 20% in raw LER on
the validation set (see table I). The process could have been
3https://github.com/0x454447415244/HandwritingRecognitionSystem
restarted after having trained the system with the new data,
or even iterated. An improved recognition performance
could have recovered more training lines. However, we
have noticed that most of the discarded line images in the
first iteration resulted from wrong segmentation (e.g., two
text-lines in a single image, cropped text-line, etc), due
to the fact that the algorithm is sensitive to the writing
skew. Therefore, more advanced segmentation algorithms
are needed to improve the selection/training process, like
the ones based on Seam Carving technique [18] and
dynamic programming, which would have resulted in fewer
segmentation errors and therefore more labeled training
data. The whole process can be summarized at the end of
this paper (algorithm 1).
Table I: System performance on the validation set with
different amount of training data.
System Number of text-lines Label Error Rate (LER)
10% training data ~20k 9.2%
80% training data ~160k 7.4%
IV. INTEGRATION OF MULTI-SCALE TRAINING DATA
To further enhance the performance of the system, we
exploited the variability in the writing scale to augment
the training set with text-line images at multiple scales.
Based on a vertical scale score [19], the training lines were
first classified into 3 classes (Large, Medium and Small)
via Jenks natural breaks optimization algorithm [20]. By
dividing the training set over the three classes, the data
volume per class become smaller. To address this problem,
we expanded the training set for each class by adding
synthetic data resulting from scaling the other classes data.
For example, we reduce the large images and stretch the
small ones (by a predetermined factor for each class) to
expand the number of medium sized images. Or we reduce
the medium and large sized images to extend the set of
small images, etc. To calculate the scaling factors by which
a certain image of a given scale class is enlarged or reduced,
the average scale measurement score is calculated on the
data. For instance, to transform an image I of class X
to an image J of class Y , we scale I by E(Y )/E(X),
where E(X) and E(Y) are the average scale score values
for class X and Y respectively. We retrained the baseline
system on multi-scale data for one epoch and achieved a
6.5% raw LER; a relative improvement by 12% from the
previous system.
Figure 3: An example from the READ dataset where a
text-line classified as Medium scale is transformed into a
Large and Small scale versions.
V. MODEL-BASED NORMALIZATION SCHEME
To further improve the performance, we proposed to
consider the variability in the writing scale in a model-based
normalization scheme, where the test data are equalized
in order to best fit the core model. In general, consider
the recognition phase where a test image characterized by
a specific variability is provided at the input of a system
trained on a general training set. According to the statistical
decision theory, the recognition task identifies the most
likely word sequence given the observations as:
sˆ = argmax
s
Pr(s|X) (1)
where s represents a word sequence, and X the obser-
vation sequence. To cope with a variability factor θ in a
test image, it is supposed that a transformation Tθ(.) exists
with contextual parameter vector θ permitting to reduce this
variability to a minimum. It is assumed that this parameter
is hidden and cannot be measured. A normalized version
of the input image X can be defined as:
X̂ = Tθ(X) (2)
Assuming the contextual parameter vector θ belongs
to a finite set, equation 1 can integrate the normalization
defined in equation 2 to become:
sˆ = argmax
s
∑
θ
Pr(s, θ|X)
= argmax
s
∑
θ
Pr(s|θ,X)Pr(θ|X)
= argmax
s
∑
θ
Pr(s|Tθ(X))Pr(θ|X)
(3)
For all possible normalizations of the input X , the system
produces solutions with the corresponding scores, consid-
ered as posterior probabilities. A combination of the scores
permits to re-select the optimal solution (see figure 4). This
is considered as an approximation of the right-hand term
of equation 3.
Figure 4: Model-based normalization scheme.
We generated multiple versions of the test data by
vertically scaling each text-line image to multiple scales
(0.7, 0.8,..., 1.3). By considering equation 3, we could
write:
sˆ = argmax
s
1.3∑
θ=0.7
Pr(s|Tθ(X))Pr(θ|X) (4)
We approximate equation 4 by the means of ROVER
method [21]. The combination of the recognition scores
of the different normalized versions of the test image
has yielded to a relative improvement of 14% in WER
from the baseline system. In Table II, we provide the
word error rate (WER) and character error rate (CER)
obtained with the different systems along with the result
of the BYU (Computer Science Department) team who
won the first place during the competition. The results
show the significant increase in performance using the
incremental training of our CRNN system. They also
show a significant improvement when better considering
the variability of writing scale. Finally, our best system
achieves comparable results with the system ranked first in
the contest. With 5.5% running OOV words [1], we believe
the main difference in performance can be explained by
our use of a bigram word language model. It is worth
noting that our results can further be improved by using a
more performant segmentation, which would also leads to
more training data.
Table II: Effect of multi-scale data on the performance.
System CER WER
CRNN (1) 9.18% 25.07%
CRNN retrained with multi-scale data (2) 7.95% 23.09%
(2) + model-based normalization scheme 7.74% 21.58%
BYU System 7.01% 19.06%
VI. CONCLUSIONS AND PERSPECTIVES
In this work, we presented a state-of-the-art CRNN
system for text-line recognition of historical documents.
We showed how to train such system with few labeled
text-line data. Specifically, we proposed to bootstrap an
incremental training procedure with only 10% of manually
labeled text-line data from the READ 2017 dataset. We also
improved the performance of the system by augmenting the
training set with specially crafted synthetic data at multi-
scale. At the end, we proposed a model-based normalization
scheme by introducing the notion of the variability in the
writing scale to the test data. The combination of the multi-
scale trained system results on multi-scale test data has
yielded the best result. Our system achieved the second
position in ICDAR2017 competition, with comparable
performance to the winning system, while noting that
the overall performance depends on both segmentation
and recognition tasks. Our results can be improved by
improving the segmentation algorithm which will permit
to use more training data. Despite the complex network
architecture, we noticed the large impact of the variability
in the writing scale on the performance. As a future work,
we will be looking into the possibilities for integrating
this variability in the modeling. Possibly via an attention
mechanism.
VII. ACKNOWLEDGMENT
We gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Titan Xp GPU used
for this research.
REFERENCES
[1] J. A. Sa´nchez, V. Romero, A. H. Toselli, M. Villegas,
and E. Vidal, “Icdar2017 competition on handwritten text
recognition on the read dataset,” in Document Analysis and
Recognition (ICDAR), 2017 14th International Conference
on. IEEE, 2017.
[2] B. Moysset, C. Kermorvant, and C. Wolf, “Full-page text
recognition: Learning where to start and when to stop,”
arXiv preprint arXiv:1704.08628, 2017.
[3] T. Bluche, “Joint line segmentation and transcription for end-
to-end handwritten paragraph recognition,” in Advances in
Neural Information Processing Systems, 2016, pp. 838–846.
[4] P. Voigtlaender, P. Doetsch, and H. Ney, “Handwriting recog-
nition with large multidimensional long short-term memory
recurrent neural networks,” in Frontiers in Handwriting
Recognition (ICFHR), 2016 15th International Conference
on. IEEE, 2016, pp. 228–233.
[5] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014.
[6] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai,
T. Liu, X. Wang, and G. Wang, “Recent advances in convo-
lutional neural networks,” arXiv preprint arXiv:1512.07108,
2015.
[7] A. Graves, S. Ferna´ndez, F. Gomez, and J. Schmidhu-
ber, “Connectionist temporal classification: labelling un-
segmented sequence data with recurrent neural networks,”
in Proceedings of the 23rd international conference on
Machine learning. ACM, 2006, pp. 369–376.
[8] A. Zeyer, R. Schlu¨ter, and H. Ney, “Towards online-
recognition with deep bidirectional lstm acoustic models.”
in INTERSPEECH, 2016, pp. 3424–3428.
[9] X. Glorot and Y. Bengio, “Understanding the difficulty of
training deep feedforward neural networks,” in Proceedings
of the Thirteenth International Conference on Artificial
Intelligence and Statistics, 2010, pp. 249–256.
[10] D. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” arXiv preprint arXiv:1412.6980, 2014.
[11] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide
the gradient by a running average of its recent magnitude,”
COURSERA: Neural networks for machine learning, vol. 4,
no. 2, pp. 26–31, 2012.
[12] N. Qian, “On the momentum term in gradient descent
learning algorithms,” Neural networks, vol. 12, no. 1, pp.
145–151, 1999.
[13] M. D. Zeiler, “Adadelta: an adaptive learning rate method,”
arXiv preprint arXiv:1212.5701, 2012.
[14] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating
deep network training by reducing internal covariate shift,”
in International Conference on Machine Learning, 2015,
pp. 448–456.
[15] A. Fischer, “Handwriting recognition in historical docu-
ments,” PhD diss, 2012.
[16] S. F. Chen and J. Goodman, “An empirical study of smooth-
ing techniques for language modeling,” in Proceedings of
the 34th annual meeting on Association for Computational
Linguistics. Association for Computational Linguistics,
1996, pp. 310–318.
[17] V. I. Levenshtein, “Binary codes capable of correcting
deletions, insertions, and reversals,” in Soviet physics
doklady, vol. 10, no. 8, 1966, pp. 707–710.
[18] N. Arvanitopoulos and S. Su¨sstrunk, “Seam carving for text
line extraction on color and grayscale historical manuscripts,”
in Frontiers in Handwriting Recognition (ICFHR), 2014
14th International Conference on. IEEE, 2014, pp. 726–
731.
[19] E. Chammas, C. Mokbel, and L. Likforman-Sulem, “Ex-
ploitation de le´chelle de´criture pour ame´liorer la reconnais-
sance automatique des textes manuscrits arabes,” Document
nume´rique, vol. 19, no. 2, pp. 95–115, 2016.
[20] G. F. Jenks, “The data model concept in statistical mapping,”
International yearbook of cartography, vol. 7, pp. 186–190,
1967.
[21] J. G. Fiscus, “A post-processing system to yield reduced
word error rates: Recognizer output voting error reduction
(rover),” in Automatic Speech Recognition and Understand-
ing, 1997. Proceedings., 1997 IEEE Workshop on. IEEE,
1997, pp. 347–354.
Algorithm 1 Incremental alignment process
Require: TrainSet: Set of all training pages
Require: RefText: Ground-truth text paragraph for each page
for each page P in TrainSet do
Lines[]← Segment(P )
RefLineIndex← 0
for each line L in Lines do
RecSeq ← Recognize(L)
while RefLineIndex < length(RefText[P ]) do
RefSeq ← RefText[P ][RefLineIndex]
EditDistance← Levenshtein(RecSeq,RefSeq)
if EditDistance < 0.5× length(RefSeq) then
Map(L,RefSeq)
RefLineIndex← RefLineIndex+ 1
end if
end while
end for
end for
Figure 5: Recognition system.
"
13,"Deep Neural Network with l2-norm Unit for
Brain Lesions Detection
Mina Rezaei, Haojin Yang, Christoph Meinel
Hasso Plattner Institute,
Prof.-Dr.-Helmert-Strae 2-3, 14482 Potsdam, Germany
{mina.rezaei,haojin.yang,christoph.meinel}@hpi.de
Abstract. Automated brain lesions detection is an important and very
challenging clinical diagnostic task, because the lesions have different
sizes, shapes, contrasts and locations. Deep Learning recently shown
promising progresses in many application fields, which motivates us to
apply this technology for such important problem. In this paper we pro-
pose a novel and end-to-end trainable approach for brain lesions clas-
sification and detection by using deep Convolutional Neural Network
(CNN). In order to investigate the applicability, we applied our approach
on several brain diseases including high and low grade glioma tumor,
ischemic stroke, Alzheimer diseases, by which the brain Magnetic Res-
onance Images (MRI) have been applied as input for the analysis. We
proposed a new operation unit which receives features from several pro-
jections of a subset units of the bottom layer and computes a normalized
l2-norm for next layer. We evaluated the proposed approach on two dif-
ferent CNN architectures and number of popular benchmark datasets.
The experimental results demonstrate the superior ability of the pro-
posed approach.
Keywords: Multimodal CNN, l2-norm unit, Brain lesion detection and
localization
1 Introduction
Annually in the United State alone 24,000 adult and 4,830 children will be diag-
nosed as new cases of brain cancer. A lot of people have died due to brain tumor,
multiple sclerosis, ischemic stroke and Alzheimer diseases 1. Medical imaging is
an important tool for brain diseases diagnosis in case of surgical or chemical
planning. Magnetic Resonance Imaging (MRI) can provide rich information for
premedication and surgery medication, which is extremely helpful for evaluating
the treatment and lesion progress. However the raw data extracted from MR
images is hard to be directly applied for diagnosis due to the large amount of
the data. An accurate brain lesion detection and classification algorithm based
on MR images might be able to improve the prediction accuracy and efficiency,
that enables a better treatment planning and optimize the diagnostic progress.
1 http://www.cancer.net/cancer-types/brain-tumor/statistics
ar
X
iv
:1
70
8.
05
22
1v
1 
 [c
s.C
V]
  1
7 A
ug
 20
17
2 Convolutional Neural Networs for Lesions Detection in Brain MRI
As mentioned by Menze et al. [5], the number of clinical study for automatic
brain lesion detection has grown significantly in the last several decades. Some
brain lesions such as ischemic strokes, or even tumors can appear with differ-
ent shapes, inappropriate sizes and unpredictable locations within the brain.
Furthermore, different types of MRI machines with specific acquisition proto-
cols may provide MR images with a wide variety of gray scale representations
on the same lesion cells. Recent research has shown strong ability of Convolu-
tional Neural Network (CNN) for learning hierarchical representation of image
data without requiring any effort to design handcrafted features [15,21,10]. This
technology became very popular in computer vision society for image classifi-
cation [14,12], object detection [22,8,18], medical image classification [17,7] and
segmentation[19,6]. As mentioned by LeCun et al. in [15]: different layers of
a network are capable of different levels of abstraction, and capture different
amount of structures from the patterns present in the image.
In this work we investigate the applicability of CNN for brain lesions de-
tection. Our goal is to perform localization and classification of single as well
as multiple anatomic regions in volumetric clinical images from various image
modalities. To this end we propose a novel framework based on CNN with l2-
norm unit. A detailed evaluation on parameter variations and network architec-
tures has been provided. We show that l2-norm operation unit is robust to the
error variations in the classification task and is able to improve the prediction
result. We conducted experiments on a number of brain MRI datasets, which
demonstrate the excellent generalization ability of our approach. The contribu-
tion of this work can be summarized as following:
– We propose a robust solution for brain lesions classification. We achieved
promising results on four different brain diseases (The overall accuracy is
over 95%).
– We applied multiple MRI modalities as network input, and this improved
the dice coefficient up to 30% on ISLES benchmark.
– We implemented l2-norm unit in Caffe [13] framework for both CPU and
GPU computation. The experimental results demonstrate the superior abil-
ity of l2-norm in various tasks.
The rest of the paper is organized as follows: Chapter 2 describes the pro-
posed approach, Chapter 3 presents the detailed experimental results. Chapter 4
concludes the paper and gives an outlook on future work.
2 Methodology
In this chapter we will describe our deep network for classification and detection
task in detail. The core techniques applied in our approach are depicted as well.
In the recent deep learning context, a deep neural network can be built driven
by two principles: Modularity and Residual learning. Modularity is a set of re-
peatable smaller neural network unit which enables the learning of high-level
visual representations. The bottleneck module of the Inception architecture [23]
Convolutional Neural Networs for Lesions Detection in Brain MRI 3
Fig. 1. Exemplary residual building block. The block on the left side shows a vanilla
residual block, where the one on the right side is a dense block applied in our classifi-
cation network.
and the corresponding units in VGG-Net [20] can be considered as typical ex-
amples. In such networks the wide and depth have been significantly increased.
On the other hand residual learning [12] considers new way to each layer.
Every consequent layer is responsible for, in effect, fine tuning the output from a
previous layer by just adding a learned “residual” connection to the input. This
essentially drives the new layer to learn something different from what the input
has already encoded. Another important advantage is that such residual connec-
tions can help in handling gradient vanishing problem in very deep networks [12].
Figure 1 shows an exemplary residual building block, where F (x) + x denotes
the element-wise addition of the original input and the residual connection. The
block on the left depicts vanilla residual unit proposed by He et al. [12], where
the one on the right side is a dense block that we utilize in our classification
network.
2.1 l2-norm Unit
| Xi,j |=

x1,1 x1,2 ... x1,j
x2,1 x2,2 ... x2,j
... ... ... ...
xi,1 xi,2 ... xi,j
 (1)
Forward :| xi,j |= 2
√∑
xi,j2 (2)
Backward : ∂ | xi,j | = n∂(
∑
xi,j)
2 2
√∑
xi,j2
(3)
In linear algebra, the size of a vector v is called the norm of v. The two-norm
(also known as the l2-norm, mean-square norm, or least-squares norm) of a vec-
tor v is defined by Equation 2. Assume we have a 2D matrix Xi,j (cf. Equation 1)
which is the output of the specific patch of ai,j from the first convolution layer.
Then for each item in feed forward or backward pass we calculate the l2-norm
4 Convolutional Neural Networs for Lesions Detection in Brain MRI
Fig. 2. Brain diseases classification architecture
as described by Equation 2 and 3. We consider l2-norm operation as a pooling
function and apply it to reduce the dimension of the learned representations,
which is able to obtain better generalization ability. For example in the classifi-
cation task an input volume of size 224×224×64 is pooled by l2-norm operator
with filter size 2 and stride 2 into an output volume of size 112× 112× 64.
2.2 Brain Abnormality Classification
Recently, ResNet (Deep Residual Network) [12] achieves the state-of-the-art per-
formance in object detection and other vision related tasks. As mentioned above
we explored the ResNet architecture with l2-norm unit for brain abnormality
classification. Figure 2 depicts the network architecture. Our classification net-
work takes 2D images with three channels, while each channel contains a gray
scale copy with the same size and same plane from various MRI modalities with
respective class label l={0,1,..,4}. Each gray scale copy extracted from T1, T1c
and FLAIR of the same MRI categories has been mapped to the Red, Green
and Blue channels of a standard image container, respectively. The proposed
network strongly inspired by vanilla ResNet block depicted by Figure 1.
As shown in Figure 2, we apply l2-norm operation after the first convolution
layer and before the first inner product layer. In the experiments we observed
that the l2-norm layer performs a similar effect as a pooling operator, which
reduces the spatial size of the feature representations and extracts features that
are not covered by standard pooling operators. This allows the network to learn
more distinguished feature information such as variance from the data stream,
which could improve the overall generalization ability of the model.
Convolutional Neural Networs for Lesions Detection in Brain MRI 5
Fig. 3. proposed architecture with 16 convolutions and l2-norm unit for recognition
and localization of brain lesion
2.3 Brain Lesions Detection
Unlike image classification, object detection extracts location and region infor-
mation of a target object within an image. Figure 3 represents our network for
brain abnormality detection. In our workflow, we extract and apply multiple
modalities from MRI images, where the images are sampled in 2D slices from
the axial, coronal and sagittal view with various sizes. Inspired by Fast R-CNN
network [9], we build our CNN network based on VGG-16 [20] style architecture
as the feature extractor. Instead of using max-pooling and spatial max-pooling
we place the l2-norm unit after the second convolution(conv1-2) layer and before
the first fully connected (inner product) layer respectively. We utilize selective
search [24] to generate object proposals, which is a set of object bounding boxes.
The proposal sampling process is performed on top of dense feature layer after
layer conv5-3. We confirm the suggested solution by Girshick et al. [9] to come
over on heterogeneous collection of computed proposals and divide them into a
pyramid grid of sub-windows. Here three pyramid levels 4 × 4, 2 × 2, 1 × 1 and
l2-norm “pooling” have been applied in each sub-window to generate the cor-
responding output grid cell. Subsequently each output feature vector is further
fed into a sequence of fully connected layers, which is followed by two sibling
output layers: the SVM (Support Vector Machine) classifier for object class es-
timation [16], and the bounding box regression layer to calculate the loss of
proposed object bounding boxes. The overall training is performed in the super-
vised manner, and the loss of the whole network sums losses from both object
classification and bounding box regression.
3 Exprimental Results
In the experiment we applied real patient data from five popular benchmarks to
evaluate the proposed methods. For classification task we totally compiled 1500
MRI images with label of healthy, tumor-HGG, tumor-LGG, Alzheimer and
multiple sclerosis. We consider 20% of the data for testing and 80% for training.
IXI dataset [1] contains 600 MRI images from normal, healthy subjects. The MRI
image acquisition protocol for each subject includes six modalities, from which
we have used T1, T2, PD, MRA images. The first column of Figure 4 shows the
healthy brain images from IXI dataset in the sagittal, coronal and axial sections.
The BraTS2016 benchmark [2,5] prepared the data in two part of High and Low
6 Convolutional Neural Networs for Lesions Detection in Brain MRI
Fig. 4. We trained the proposed network on five different categories of brain MRI. The
1st column shows healthy brain in sagittal, coronal and axial section. The 2nd and 3rd
columns show high and low grad glioma, while 4th and 5th columns present some brain
images on Alzheimer and multiple sclerosis.
Grade Glioma (HGG/LGG) Tumor. All images have been aligned to the same
anatomical template and interpolated to 1 mm, 3 voxel resolution. The training
dataset consists of 220 HGG and 108 LGG MRI images which for each patient
T1, T1contrast, T2, FLAIR and ground truth labeled by medical experts have
been provided. Alzheimer disease dataset2 comes from Open Access Series of
Imaging Studies (OASIS). The dataset consists of a cross-sectional collection of
416 subjects aged from 18 to 96. For each subject, 3 or 4 individual T1-weighted
MRI scans were obtained in single scan sessions. 18 MRI images with multiple
sclerosis from ISBI challenges 2008[3] have also been applied in the classification
task. ISLES benchmark 2016 [4] (Ischemic Stroke Lesion Segmentation) comes
from MICCAI challenge in two part, by which we used only SPES dataset with
30 brain images with 7 modalities in our task. An visual overview of the applied
datasets can be found in Figure 4.
Table 1. Brain lesions classification performance of the re-designed ResNet architec-
ture using l2-norm unit. The involved classes include healthy, tumor-HGG, tumor-LGG,
Alzheimer and multiple sclerosis. The last two rows show the comparison results to the
most recent methods.
Total MRI Accuracy Sensitivity Specificity Recall Kappa
Our method 1500 95.308% 0.91 0.87 87.65 0.92
Justin S et al. [17] 191 91.43% - - - -
Abbadi et al. [7] 50 94% 0.85 0.87 - -
2 http://www.oasis-brains.org/
Convolutional Neural Networs for Lesions Detection in Brain MRI 7
Fig. 5. The confusion matrix of the classification results. X-axis shows predicted re-
sults, where Y-axis gives the actual labels.
Fig. 6. Learning curves of brain lesions classification
Because the MRI volumes in the BraTS and ISLES datasets do not possess
an isotropic resolution, we prepared 2D slices in sagittal, axial and coronal view.
As mentioned by Havaei et al. [11], unfortunately brain imaging data are rarely
balanced due to the small size of the lesion compared to the rest of the brain.
For example the volume of a stroke is rarely more than 1% of the entire brain
and a tumor (even large glioblastomas) never occupies more than 4% of the
brain. Training a deep network with imbalanced data often leads to very low
true positive rate since the system gets to be biased towards the one class that
is over represented. To overcome this problem we have chosen volume of MRI
with lesions, and augmented training data by using horizontal and ventricle
flipping, multiple scaling. By using a re-designed ResNet architecture described
in section 2, we achieved over 95% classification accuracy as shown in Table 1,
while Figure 5 demonstrates the confusion matrix of the classification result. We
also compared our result with the most recent deep learning based approaches
as shown in Table 1, where the reference method also used IXI, OASIS datasets.
8 Convolutional Neural Networs for Lesions Detection in Brain MRI
Table 2. Dice Similarity Coefficient (DSC) results (for brain lesions detection perfor-
mance measurement) on the BraTS2016 and ISLES2016 dataset by using incremental
modalities. F/D column means the FLAIR modality in BraTS dataset and DWI modal-
ity in ISLES dataset.
T1 T1c T2 F/D Dice-BraTS16 Dice-ISLES
x - - - 61.8 % 42%
- x - - 33.76 % 27%
- - x - 36.7 % 39.98%
- - - x 73.38 % 50.71%
- x x x 81.53 % 54.23%
x x - x 82.6 % 54.67%
x - x x 83.19 % 53.09%
x x x - 82.73 % 54.7%
x x x x 83.53 % 56.87%
Figure 6 shows learning curves of testing accuracy, training and testing losses
during the training process.
For brain lesions detection experiment we applied both BraTS and ISLES
datasets. We used 70% of the data for training, 10% for validation and 20% for
testing. It is expected that more generalized features could be able to learned
from multiple modalities, and the testing accuracy based on more generalized
features should be gained. The brain lesions detection results from Table 2 proved
our assumption, where better detection results were achieved by increasing the
data modalities in the model training. The detection result can be improved by
20% in BraTS and 30% in ISLES dataset.
Table 3. Evaluation result of the detection network with and without l2-norm unit,
which demonstrates the performance gains by using l2-norm unit.
Dice(without l2-norm unit) Dice(with l2-norm unit)
BraTS16 72% 83.53%
ISLES16 53.65% 56.87
From Table 2, we can also infer that the FLAIR modality is the most rele-
vant one for identifying the complete tumor(Dice: 73.38%), However in ISLES
benchmark we don’t have this modality and it is justified less accuracy on this
category. It motivated us to work on generating the missing modalities in the
future. The subjects in Figure 7 are from our testing set, for which the model
is not trained on, the detection results from these subjects could give a good
estimation of the model performance.
Table 3 demonstrates the evaluation results of the detection architectures
with and without l2-norm unit. From which we can easily realize the superior
ability of the proposed l2-norm operator. We are able to improve the detection
performance significantly on both datasets by using this novel operator.
Convolutional Neural Networs for Lesions Detection in Brain MRI 9
Fig. 7. Visual results of our brain lesions detection approach on Axial, Coronal and
Sagittal views. The subjects are selected from the validation set.
4 Conclusion
In this paper, we explored two important clinical tasks: brain lesions classification
and detection. We proposed end-to-end trainable approaches based on state-of-
the-art deep convolutional neural networks. We implemented a novel pooling
operator: l2-norm unit which can effectively generalize the network, and make the
learned model more robust. The applicability, model accuracy and generalization
ability have been evaluated by using a set of publicly available datasets. As the
future work we will further investigate the automatic segmentation of tumor
regions based on the detection results.
References
1. http://brain-development.org/ixi-dataset/
2. https://www.virtualskeleton.ch/BRATS/Start2016/
3. http://www.medinfo.cs.ucy.ac.cy/index.php/downloads/datasets/
4. http://www.isles-challenge.org/ISLES2016/
5. B. Menze, M.R., Leemput, K.V.: The multimodal brain tumor image segmentation
benchmark (brats). In: IEEE Trans. on Medical Imaging (2014)
10 Convolutional Neural Networs for Lesions Detection in Brain MRI
6. Dai, J., He, K., Sun, J.: Instance-aware semantic segmentation via multi-task net-
work cascades. In: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE
Conference on (2016)
7. El Abbadi, N.K., Kadhim, N.E.: Brain cancer classification based on features and
artificial neural network. Brain 6(1) (2017)
8. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 580–587 (2014)
9. Girshick, R.B.: Fast R-CNN. CoRR abs/1504.08083 (2015)
10. Gulcehre, C., Cho, K., Pascanu, R., Bengio, Y.: Learned-norm pooling for deep
feedforward and recurrent neural networks. In: Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. pp. 530–546. Springer
Berlin Heidelberg (2014)
11. Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal,
C., Jodoin, P.M., Larochelle, H.: Brain tumor segmentation with deep neural net-
works. Medical image analysis 35, 18–31 (2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on
(2016)
13. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.B., Guadar-
rama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding.
CoRR
14. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-
volutional neural networks. In: Advances in neural information processing systems.
pp. 1097–1105 (2012)
15. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444
(2015)
16. Liu, G., Zhang, X., Zhou, S.: Multi-class classification of support vector machines
based on double binary tree. In: Natural Computation, 2008. ICNC’08. Fourth
International Conference on. vol. 2, pp. 102–105. IEEE (2008)
17. Paul, J.S., Plassard, A.J., Landman, B.A., Fabbri, D.: Deep learning for brain
tumor classification. vol. 10137, pp. 1013710–1013710–16 (2017)
18. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. CoRR abs/1506.01497 (2015)
19. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-
cal image segmentation. In: International Conference on Medical Image Computing
and Computer-Assisted Intervention. pp. 234–241. Springer International Publish-
ing (2015)
20. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
21. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1–9
(2015)
22. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:
Advances in Neural Information Processing Systems. pp. 2553–2561 (2013)
23. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. CoRR (2015)
24. Uijlings, J., van de Sande, K., Gevers, T., Smeulders, A.: Selective search for object
recognition. International Journal of Computer Vision (2013)
"
14,"Feature-Fused SSD: Fast Detection for Small Objects 
 
Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao,Guangming Shi, Jinjian Wu 
School of Electronic Engineering, Xidian University, China 
xmxie@mail.xidian.edu.cn 
ABSTRACT   
Small objects detection is a challenging task in computer vision due to its limited resolution and information. In order 
to solve this problem, the majority of existing methods sacrifice speed for improvement in accuracy. In this paper, we 
aim to detect small objects at a fast speed, using the best object detector Single Shot Multibox Detector (SSD) with 
respect to accuracy-vs-speed trade-off as base architecture. We propose a multi-level feature fusion method for 
introducing contextual information in SSD, in order to improve the accuracy for small objects. In detailed fusion 
operation, we design two feature fusion modules, concatenation module and element-sum module, different in the way of 
adding contextual information. Experimental results show that these two fusion modules obtain higher mAP on PASCAL 
VOC2007 than baseline SSD by 1.6 and 1.7 points respectively, especially with 2-3 points improvement on some small 
objects categories. The testing speed of them is 43 and 40 FPS respectively, superior to the state of the art 
Deconvolutional single shot detector (DSSD) by 29.4 and 26.4 FPS. Code is available at https://github.com/wnzhyee/ 
Feature-Fused-SSD. 
Keywords: Small object detection, feature fusion, real-time, single shot multi-box detector. 
 
1. INTRODUCTION  
Reliably detecting small objects is a quite challenging task due to their limited resolution and information in images. 
Considering this problem, many existing methods [1-4] have demonstrated considerable improvement brought by 
exploiting the contextual information. For example, in Fig. 1, it is quite difficult to recognize the sailing boats without 
taking the sea into account. Besides, both the people and bike provide an evidence for the existing of the bottle and the 
person, which have a relationship in some extent. Another common method used for small object detection is enlarging 
the small regions [5] for better fitting the features of pre-trained network. Since the enlarging method would increase the 
computation greatly, we do not consider it. 
       
Figure 1. The scene of sea, person, and bike provide the evidence of the existence of the sailing boats, bottle, and person. 
When considering real-time detection, most of these studies are based on region-based object detection architecture, 
including RCNN [6], SPPnet [7], Fast RCNN [8], Faster RCNN [9], which cannot detect small objects in a fast way. The 
fast detector Single Shot Multibox Detector (SSD [10]) presents large improvement in speed, for eliminating region 
proposals and the subsequent pixel resampling stage. To improve accuracy for small objects, Deconvolutional Single 
Shot Detector (DSSD [4]) uses the SSD architecture as baseline. Since it is concentrated on improving accuracy by using 
the base network residual-101 [11], it sacrifices lot of speed inevitably. Thus, for small object detection, accuracy-vs-
speed trade-off is still hard to balance currently, remaining a challenging problem to be solved. 
In this paper, we aim to obtain fast detection for small objects. To achieve this, we use the best object detector SSD 
with respect to accuracy-vs-speed trade-off as our base architecture, which can do real-time detection. We propose a 
multi-level feature fusion method for adding contextual information in SSD baseline, in order to improve the accuracy 
for small objects. In detailed fusion operation, we instantiate this feature fusion method carefully with two modules, 
concatenation module and element-sum module. Since context may introduce useless background noises, it is not always 
useful for small object detection. In detail, concatenation module uses a 1×1 convolution layer for learning the weights 
of the fusion of the target information and contextual information, which can reduce the interference of useless 
background noises. Element-sum module uses equivalent weights set manually and fuses the multi-level features in a 
compulsory way, which can enhance the effectiveness of useful context. Experimental results show that our two feature 
fusion modules obtain higher mAP on PASCAL VOC2007 than baseline SSD by 1.6 and 1.7 points respectively, 
especially with 2-3 points improvement on some small objects categories, such as aero, bird, boat, pot plant, TV monitor 
and so on. The testing speed of them is 43 and 40 FPS respectively, which is in a real-time fashion. 
 
2. RELATED WORK 
Context: Many previous studies have demonstrated that contextual information plays an important role in object 
detection task, especially for small objects. The common method for introducing contextual information is exploiting the 
combined feature maps within a ConvNet for prediction. For example, ION [2] extracts VGG16 [12] features from 
multiple layers of each region proposal using ROI pooling [8], and concatenate them as a fixed-size descriptor for final 
prediction. HyperNet [3], GBD-Net [13] and AC-CNN [14] also adopt a similar method that use the combined feature 
descriptor of each region proposal for object detection. Because the combined features come from different layers, they 
have different levels of abstraction of input image. So that the feature descriptors of each region proposal contain fine-
grained local features and contextual features. However, these methods are all based on region proposal method and pool 
the feature descriptors from combined feature maps, which increases the memory footprint as well as decreases the speed 
of detection. 
Multi-scale representation: Multi-scale representation has been proven useful for many detection tasks. Many 
previous detection architectures use single-scale representation, such as RCNN [6], Fast RCNN [8], Faster RCNN [9], 
and YOLO [15]. They predict confidence and localization from the features extracted by the top-most layer within a 
ConvNet, which increases the heavy burden of the last layer. Differently, SSD [10] uses multi-scale representation that 
detect objects with different scales and aspect ratios from multiple layers. With smaller object detection, SSD uses the 
features from the shallower layers, while exploits the features from the deeper layers for bigger objects detection. In 
order to further improve the accuracy of SSD, especially for small objects, DSSD adds extra deconvolution layers in the 
end of SSD. By integrating every prediction layer and its deconvolution layer, the contextual information is injected, 
which makes the predictions for small objects more accurate. 
Since the methods for introducing contextual information into region-based object detectors are also multi-scale 
representation, they are quite different from SSD in the way of predicting objects localization and confidence.  
 
3. MULTI-LEVEL FEATURE FUSION MODULE IN SSD 
We extract the multi-level features from SSD model, which is the state of the art object detector with respect to 
accuracy-vs-speed trade-off. In this section, we introduce the SSD briefly and then illustrate the two multi-level feature 
fusion modules in detail, which exploit the useful local context for final classification and regression. 
Conv1_x
Conv2_x
Conv3_x Conv4_x Conv5_x Fc_x
SSD layers
Pr
ed
ic
tio
n 
la
ye
r+
Feature fusion module
VGG16 layers
 
Figure 2. Feature-fused SSD architecture. 
3.1 Single Shot Multibox Detector 
The Single shot multibox detector (SSD) is based on the VGG16 base network and ends with several newly added 
layers. Our model is illustrated in Fig. 2. Instead of using the last feature map of the ConvNet, SSD uses the pyramidal 
feature hierarchy in multiple layers within a ConvNet to predict objects with different scales. That is using shallower 
layers to predict smaller objects, while using deeper layers to predict bigger objects, thus it can reduce the prediction 
burden of the whole model. However, shallower layers often lack of semantic information which is important 
supplement for small object detection. Therefore, passing the semantic information captured in convolutional forward 
computation back to the shallower layers will improve the detection performance of small objects.  
Which layers to combine? We exploit the proper Conv layers to provide useful contextual information since that too 
large receptive field would often introduce large useless background noise. Because of SSD predicting small objects with 
its shallower layers, such as conv4_3, we do not use the feature fusion module for large objects in the deeper layers for 
less decrease of speed. For choosing the proper feature fusion layers, effective receptive fields in different layers are 
explored with deconvolution method. As shown in Fig. 3, take boats for example, the effective receptive field of the 
nearer boat within the SSD architecture is quite proper in conv4_3. Conv5_3 and fc6 have larger effective receptive 
fields. However, fc6, where uses dilated convolution, would introduce more background noise than conv5_3.    
     
     
     
(a) Image                        (b) Conv3_3                          (c) Conv4_3                        (d) Conv5_3                            (e) Fc6 
Figure 3. Effective receptive fields in SSD architecture. (a) is the input image, and (b), (c), (d), (e) show the effective fields of 
conv3_3, conv4_3, conv5_3 and fc6, respectively, shown with enhancement. 
In order to inject the contextual information into the shallower layers (conv4_3) which lacks of sematic information, 
we design two different modules for feature fusion, named as concatenation module and element-sum module. 
3.2 Concatenation Module 
The concatenation fusion module is shown in Fig. 4. In order to make the feature maps of conv5_3 layer the same size 
as conv4_3 layer, the conv5_3 layer is followed by a deconvolution layer, which is initialized by bilinear upsample. Two 
3×3 convolutional layers are used after conv4_3 layer and conv5_3 layer for learning the better features to fuse. Then 
normalization layers are following with different scales respectively, i.e. 10, 20, before concatenating them along their 
channel axis. The final fusion feature maps are generated by a 1×1 convolutional layer for dimension reduction as well 
as feature recombination.  
3.3 Element-Sum Module 
Another solution to fuse feature maps of two layers is using element-sum module. This module is illustrated in Fig. 5. 
It is the same as concatenation module except the fusion type. In this module, two feature maps contained different level 
features are summarized point to point with equivalent weights. In practice, this operation works well due to the two 
convolution layers used before, which learns features from conv4_3 and conv5_3 adaptively for better fusion effects. 
This module is inspired by DSSD [4] based on residual-101, which uses the learned deconvolution layer and element-
wise operation.  
 
Figure 4. Illustration of the concatenation module.  
C
onv 3x3x512
N
orm
alize(20)
R
eLU
Eltw
 SU
M
D
econv 
2x2x512
Conv 3x3x512
N
orm
alize(10)
 
Figure 5. Illustration of the element-sum module. 
Concatenation module fuses multi-level features with learning weights implemented by 1×1 convolutional layer, 
while element-sum module uses the equivalent weights set manually. With this difference, concatenation module can 
reduce the interference caused by useless background noises, and element-sum module can enhance the importance of 
the contextual information. 
 
4. EXPERIMENTAL RESULTS 
We evaluate our feature-fused SSD model on PASCAL VOC 2007 and 2012. In this section, the detection results of 
two feature fusion modules are compared with SSD and DSSD. In addition, the detection performance for small objects 
is shown and the comparisons are analyzed carefully. And we illustrate the testing speed of our two models in the end. 
4.1 Experimental Setup 
The feature-fused SSD is implemented based on SSD built on the Caffe [16] framework, and the VGG16 architecture, 
all of which are available online. The VGG16 model pre-trained on ImageNet dataset for the task of image classification, 
is reduced of fully connected layers when used as the base network of SSD. The baseline SSD is trained with the batch 
size of 16, and with the input size of 300×300. The training process starts with the learning rate at 10-3 for the first 80K 
iterations, which decreases to 10-4 and 10-5 at 10K and 12K. 
4.2 PASCAL VOC2007 
We train our models on the union of PASCAL VOC2007 and PASCAL VOC2012, which contain 20 categories in 
9,963 and 22,531 images, respectively. Both of the two feature fusion models are fine-tuned upon the well-trained SSD 
baseline for another 10K iterations. The learning rate is 10-3 for the first 60K iterations and then decreases to 10-4 and 10-
5 at 60K and 70K iterations respectively. All newly added layers initialized with “xavier”. 
We take several necessary tries when designing the most effective feature fusion modules. Firstly, we explore the 
proper layers to fuse with experimental results, which has been discussed in Figure 3 theoretically. As shown in Table 1, 
the mAP on general objects of PASCAL VOC 2007 is considerable. But we select the conv4_3 and conv5_3 layers to 
fuse, since fc6 has a larger receptive field than conv5_3 for small objects which would introduce much more background 
noises. In addition, we try different numbers of kernels when designing the modules. In Table 2, we show that our 
module can work well even if the number of kernels reducing to 128, which means smaller and faster. 
                                                                                           Table 2. Detection results of different number of kernels  
Table 1. Detection results of different fusion layers 
 
 
 
 
 
 
General objects detection: In Table 3, we report the detection performance of the proposed modules. The two 
feature-fused SSD methods are both improved compared with their baseline SSD with respect to general objects 
detection. The feature-fused SSD with concatenation module obtains 78.8 mAP, while 78.9 mAP with element-sum 
module, which are 1.6 and 1.7 points higher than original SSD respectively. Moreover, our results are comparable with 
the state of the art performance DSSD 321.   
Table 3. Results on PASCAL VOC2007 test set (with IOU=0.5) 
 
 
 
 
 
 
 
 
 
 
Small objects detection: Since VOC2007 dataset contains 20 categories and every category may have small objects, 
we manually select 181 images mainly including small objects for evaluating our models better. These two methods 
achieve 3.6 and 2.0 mAP improvement compared with original SSD model, with respect to concatenation module and 
element-sum module. Detection results are shown in Fig. 6. We find that the detection performance of small objects with 
Kernel number concat Eltsum 
512 78.76 78.53 
384 78.63 78.92 
256 78.70 78.83 
128 78.68 78.67 
64 78.44 78.37 
32 78.05 78.14 
layers concat Eltsum 
Conv4_3 77.27 
Conv4_3+conv5_3 78.76 78.53 
Conv4_3+fc6 78.56 78.51 
Conv3_3+Conv4_3+conv5_3 78.48 78.39 
Method mAP Aero bike bird boat bottle bus car cat chair cow 
SSD300 77.2 78.8 85.3 75.7 71.5 49.1 85.7 86.4 87.8 60.6 82.7 
DSSD321 78.6 81.9 84.9 80.5 68.4 53.9 85.6 86.2 88.9 61.1 83.5 
Elt_sum 78.9 82.0 86.5 78.0 71.7 52.9 86.6 86.9 88.3 63.2 83.0 
Concat 78.8 82.4 85.7 77.8 73.8 52.3 87.5 86.8 87.6 62.6 82.1 
Method mAP table dog horse mbike person plant sheep sofa train tv 
SSD300 77.2 76.5 84.9 86.7 84.0 79.2 51.3 77.5 78.7 86.7 76.3 
DSSD321 78.6 78.7 86.7 88.7 86.7 79.7 51.7 78.0 80.9 87.2 79.4 
Elt_sum 78.9 76.8 86.1 88.5 87.5 80.4 53.9 80.6 79.5 88.2 77.9 
Concat 78.8 76.6 86.1 88.2 86.6 80.3 53.7 78.0 80.1 87.3 78.0 
specific background improved obviously, such as small car, aero, bird and so on. And objects which often appear 
together with relative objects are detected more accurate. For example, the people in a car as well as the bike beside a 
person provide supplementary evidence for the existence of each other.  
Performance comparison of two fusion modules: Consider small object detection. When looking into the difference 
of these two feature-fused SSD methods, we analyze fusion methods and their detection results carefully. In Fig. 7, a pot 
plant is occluded by a desk bump, which is not the useful contextual information of it. Since the concatenation module 
uses learned weights to combine the target feature and the contextual feature, it can select useful contextual information 
and weaken the interference of the background noises. Unfortunately, element-sum model combines both target features 
and context in an equivalent way, thus it cannot select the useful contextual information adaptively. On the contrary, in 
Fig. 8, cars are blurry in the scene, so that the context is necessary for detection. In this case, element-sum module works 
better than the concatenation module, because the latter has much more choice that may not learn the relationship 
between the target and context well. 
    
(a) SSD detection results 
    
(b) Concatenation model detection results 
    
(c) Element-sum model detection results 
             Figure 6. Detection results of feature-fused SSD. (a), (b), and (c) are detection results of original SSD, feature-fused 
SSD with concatenation module, and with element-sum module, respectively. All the images in four columns show that useful 
contextual information provide evidence for the existence of the small objects.  
            
Figure 7. Left: Detection results of concatenation model. Right: Detection results of element-sum model. The pot plant is occluded by 
a desk bump, and concatenation model can weaken the interference of background noises, while element-sum model cannot.  
            
Figure 8. Left: Detection results of concatenation model. Right: Detection results of element-sum model. The cars in this image is 
small and blurry, so that the contextual information is necessary for their detection. Element-sum model exploits this context quite 
well, while concatenation model cannot. 
4.3 Running Time 
We evaluate running time for these two feature fusion methods on PASCAL VOC 2007 test dataset, as shown in 
Table 4.  The detection speed of the two methods, concatenation module and element-sum module, is 40 FPS and 43 FPS 
respectively, slower than SSD original model because of the extra feature fusion layers. However, our methods still 
achieve real-time detection. Compared with the DSSD321 with 13.6 FPS, faster detection speed is obtained by our 
methods, with comparable accuracy 78.8 and 78.9. Because DSSD321 uses Residual-101 network as base network, 
while our methods exploit VGG16.The concatenation model uses three convolution layers with 512 kernels in each layer, 
and element-sum module uses two convolution layers with 384 kernels in each layer. Thus, the element-sum model is 
faster than the concatenation model by 3 FPS. 
Table 4. The running time illustration of different models 
Method Base Network mAP FPS 
SSD300[10] VGG16[12] 77.2 50 
DSSD321[4] Residual-101[11] 78.6 13.6 
Proposed concatenation model VGG16 78.8 40 
Proposed element-sum model VGG16 78.9 43 
 
5. CONCLUSION 
We have presented a feature fusion method in SSD obtains a fast and accurate detection for small objects. Compared 
with the state of the art object detector for small objects, our method achieves faster detection speed, with comparable 
accuracy. With the two fusion operations, we have shown the advantages of them in different cases. Since context is not 
always useful information for small object detection, which may introduce useless background noises sometimes, 
controlling the information transmission will be the further work to study. 
ACKNOWLEDGMENT 
This work is supported by Natural Science Foundation (NSF) of China (61472301, 61632019). 
REFERENCES 
[1] C. Chen, M. Liu, O. Tuzel, J. Xiao, “R-cnn for small object detection,” Asian Conference on Computer Vision, 214-
230, (2016) 
[2] S. Bell, C. Lawrence Zitnick, K. Bala, R. Girshick, “Inside-outside net: Detecting objects in context with skip 
pooling and recurrent neural networks,” Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, 2874-2883, (2016) 
[3] T. Kong, A. Yao, Y. Chen, F. Sun, “HyperNet: Towards accurate region proposal generation and joint object 
detection,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016) 
[4] C. Fu, W. Liu, A. Ranga, A. Tyagi, A. Berg, “DSSD: Deconvolutional single shot detector,” arXiv preprint 
arXiv:1701.06659, (2017) 
[5] P. Hu, D. Ramanan, “Finding tiny faces,” arXiv preprint arXiv:1612.04402, (2016) 
[6] R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic 
segmentation,” Proceedings of the IEEE conference on computer vision and pattern recognition, 580-587, (2014) 
[7] K. He, X. Zhang, S. Ren, J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” 
European Conference on Computer Vision, 346-361, (2014) 
[8] R. Girshick, “Fast r-cnn,” Proceedings of the IEEE international conference on computer vision, 1440-1448, (2015) 
[9] S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal 
networks,” Advances in neural information processing systems, 91-99, (2015) 
[10] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, A. Berg, “SSD: Single shot multibox detector,” 
European conference on computer vision, 21-37, (2016) 
[11] K. He, X. Zhang, S. Ren, J. Sun, “Deep residual learning for image recognition,” Proceedings of the IEEE 
conference on computer vision and pattern recognition, 770-778, (2016) 
[12] K. Simonyan, A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint 
arXiv:1409.1556, (2014). 
[13] X. Zeng, W. Ouyang, J. Yan, H. Li, T. Xiao, K. Wang, Y. Liu, Y. Zhou, B. Yang, Z. Wang, H. Zhou, X. Wang, 
“Crafting GBD-Net for Object Detection,” arXiv preprint arXiv:1610.02579, (2016) 
[14] J. Li, Y. Wei, X. Liang, J. Dong, T. Xu, J. Feng, S. Yan, “Attentive contexts for object detection,” IEEE 
Transactions on Multimedia, 19(5): 944-954, (2017) 
[15] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, “You only look once: Unified, real-time object detection,” 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 779-788, (2016) 
[16] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, “Caffe: 
Convolutional architecture for fast feature embedding,” Proceedings of the 22nd ACM international conference on 
Multimedia, 675-678, (2014) 
"
15,"  
B. Iyer, S. Nalbalwar and R. Pawade (Eds.) 
ICCASP/ICMMD-2016. Advances in Intelligent Systems Research.  
Vol. 137, Pp. 324-332.  
© 2017- The authors. Published by Atlantis Press 
This is an open access article under the CC BY-NC license (http://creativecommons.org/licens)es/by-nc/4.0/). 
 
  
 
Human Skin Detection Using RGB, HSV and YCbCr Color 
Models 
S. Kolkur1, D. Kalbande2, P. Shimpi2, C. Bapat2, and  J. Jatakia2 
1 Department of Computer Engineering, Thadomal Shahani Engineering College, Bandra,Mumbai, India 
2 Department of Computer Engineering, Sardar Patel Institute of Technology, Andheri,Mumbai, India  
{ kolkur.seema@gmail.com; drkalbande@spit.ac.in; prajwalshimpi@gmail.com; chai.bapat@gmail.com; 
jatakiajanvi12@gmail.com} 
Abstract. Human Skin detection deals with the recognition of skin-colored pixels and regions in a given image. 
Skin color is often used in human skin detection because it is invariant to orientation and size and is fast to pro-
cess. A new human skin detection algorithm is proposed in this paper. The three main parameters for recogniz-
ing a skin pixel are RGB (Red, Green, Blue), HSV (Hue, Saturation, Value) and YCbCr (Luminance, Chromi-
nance) color models. The objective of proposed algorithm is to improve the recognition of skin pixels in given 
images. The algorithm not only considers individual ranges of the three color parameters but also takes into ac-
count combinational ranges which provide greater accuracy in recognizing the skin area in a given image. 
Keywords: Skin Detection, Color Models, Image Processing, Classifier 
1   Introduction 
Skin detection is the process of finding skin-colored pixels and regions in an image or a video. This process is 
typically used as a preprocessing step to find regions that potentially have human faces and limbs in images [2]. 
Skin image recognition is used in a wide range of image processing applications like face recognition, skin dis-
ease detection, gesture tracking and human-computer interaction [1]. The primary key for skin recognition from 
an image is the skin color. But color cannot be the only deciding factor due to the variation in skin tone accord-
ing to different races. Other factors such as the light conditions also affect the results. Therefore, the skin tone is 
often combined with other cues like texture and edge features. This is achieved by breaking down the image into 
individual pixels and classifying them into skin colored and non-skin colored [1]. One simple method is to 
check if each skin pixel falls into a defined color range or values in some coordinates of a color space. There are 
many skin color spaces like RGB, HSV, YCbCr, YIQ, YUV, etc. that are used for skin color segmentation [1]. 
We have proposed a new threshold based on the combination of RGB, HSV and YCbCr values. The following 
factors should be considered for determining the threshold range:  
 
1) Effect of illumination depending on the surroundings. 
2) Individual characteristics such as age, sex and body parts. 
3) Varying skin tone with respect to different races. 
4) Other factors such as background colors, shadows and motion blur. 
 
The skin detection is influenced by the parameters like Brightness, Contrast, Transparency, Illumination, and 
Saturation. The detection is normally optimized by taking into consideration combinations of the mentioned 
parameters in their ideal ranges. 
Human Skin Detection Using RGB, HSV And Ycbcr Color Models                                                       325 
 
  
 
2   Literature Review 
In today's fast paced life, where personal health care has taken a back seat and lowest priority due to ever-
growing hustle for earning more and staying ahead of competition, the significance of health can hardly be over-
stated. At such crucial junctures, if technology can join hands with health sector, humanity will be blessed. In 
rural India, just like in any other developing country, ignorance towards personal health care is so rampant that 
skin diseases often go unnoticed and overlooked. The uneducated Indian rural masses wouldn't even know they 
have a skin disease until it reaches the last stage (or the most critical and dangerous phase which is often                     
incurable).  
Thus by logic, an intervention done by technology in the primitive or early stages of disease contraction would 
greatly help the diagnosis and avert the fatalities. Such a prognosis needs accurate detection of human skin. 
Skin detection techniques can be broadly classified as pixel-based techniques or region-based techniques. In the 
pixel-based skin detection, each pixel is classified as either skin or non-skin pixel individually depending on 
certain conditions. The skin detection based on color values is pixel-based. In region-based skin detection tech-
nique, spatial relationship of pixels is considered to define some area from given image as skin region. Initial 
skin region is grown bigger by adding more pixels based on its neighbors properties [6]. 
Using machine learning based on available data sets, a classifier can be trained to differentiate the image pixel 
by pixel (a skin pixel from a non-skin pixel). This can greatly help in setting a range of values which are valid 
for concluding that the pixel is a skin pixel. Then a set of area from the image is recognized as a skin image, 
using RGB (Red Green Blue), HSV (Hue Saturation Value) and YCbCr.  
This paper uses a threshold based methodology to detect whether an image is a skin image or not. It attempts to 
give a constructive and feasible solution to skin disease detection problem by implementing the different color 
models on the skin images. It formulates a range for RGB, HSV and YCbCr models which other papers have 
not ascertained. These ranges attempt to distinguish the skin pixels from the non-skin pixels. Most of the re-
search work in this area highlights the different methodologies that can be used for image recognition; different 
color models. However after a comparative study of strengths and weaknesses of these models; combination of 
RGB, HSV and YCbCr seem to fit for the purpose of recognizing skin images. 
3   Color Spaces 
Color space is a mathematical model to represent color information as three or four different color components. 
Different color spaces (models) are used for different applications such as computer graphics, image processing, 
TV broadcasting, and computer vision. Different color space is available for the skin detection. They are: RGB 
based color space (RGB, normalized RGB), Hue Based color space (HSI, HSV, and HSL), Luminance based 
color space (YCBCr, YIQ, and YUV)[5]. These models are explained subsequently in next sections. Color space 
selection is the primary process in skin color modeling and further for classification. One or more color spaces 
can give an optimal threshold value for detection of pixels of skin in a given image. The choice of appropriate 
color space is often determined by the skin detection methodology and the application. We use the following 
color spaces for recognizing skin pixels.  
3.1   Red, Green, and Blue (RGB) Color Model 
RGB color space is widely used and is normally the default color space for storing and representing digital im-
ages. We can get any other color space from a linear or non-linear transformation of RGB [1]. The RGB color 
space is the color space used by computers, graphics cards and monitors or LCDs. As shown in fig.1 it consists 
of three components, red, green and blue, the primary colors. Any color can be obtained by mixing the three 
base colors. Depending on how much is taken from each base color, any color can be created. Reversing this 
technique, a specific color can be broken down into its red, blue and green components as shown in equation 1 
to equation 3 [1]. These values can be used to find out similar colored pixels from the image. [7] explains skin 
color detection based on RGB color space. Normalized RGB is a representation that is easily obtained from the 
RGB values by a simple normalization procedure [1].  
326 Kolkur et.al. 
 
 
A remarkable property of this representation is that for matte surfaces, while ignoring ambient light, normalized 
RGB is invariant (under certain assumptions) to changes of surface orientation relatively to the light source [4]. 
 
BGR
R
r

                 (1) 
 
BGR
G
g

                 (2) 
       
BGR
B
b

                (3) 
 
 
 
 
 
 
 
 
 
 
 
 
                                                                  Fig. 1.RGB Color Model 
3.2   YCbCr (Luminance, Chrominance) Color Model 
YCbCr is an encoded non-linear RGB signal, commonly used by European television studios and for image 
compression work. As shown in fig. 2 color is represented by luma (which is luminance computed from non-
linear RGB) constructed as a weighted sum of RGB values [4]. YCbCr is a commonly used color space in digi-
tal video domain. Because the representation makes it easy to get rid of some redundant color information, it is 
used in image and video compression standards like JPEG, MPEG1, MPEG2 and MPEG4.The transformation 
simplicity and explicit separation of luminance and chrominance components makes YCbCr color space [3]. In 
this format, luminance information is stored as a single component (Y), and chrominance information is stored 
as two color-difference components (Cb and Cr). Cb represents the difference between the blue component and 
reference value. Cr represents the difference between the red component and a reference value. YCbCr values 
can be obtained from RGB color space according to eq. 4 to eq. 6. [7][8][9] uses YCbCr space for skin 
detection. 
 
 
 
 
 
 
 
 
 
                            
Fig. 2. YCbCr Color Model 
 
Human Skin Detection Using RGB, HSV And Ycbcr Color Models                                                       327 
 
  
 
 
 
 
 
𝑌 = 0.299𝑅 + 0.287𝐺 + 0.11𝐵   -------------------eq. 4 
𝐶𝑟 = 𝑅 − 𝑌     -------------------eq. 5 
𝐶𝑏 = 𝐵 − 𝑌     -------------------eq. 6 
3.3   Hue Saturation Value (HSV) Color Model 
The HSV color space is more intuitive to how people experience color than the RGB color space. As hue (H) 
varies from 0 to 1.0, the corresponding colors vary from red, through yellow, green, cyan, blue, and magenta, 
back to red. As saturation(S) varies from 0 to 1.0, the corresponding colors (hues) vary from unsaturated 
(shades of gray) to fully saturated (no white component). As value (V), or brightness, varies from 0 to 1.0, the 
corresponding colors become increasingly brighter. The hue component in HSV is in the range 0° to 360° angle 
all lying around a hexagon as shown figure 3 [3].  
With RGB the color will have values like (0.5, 0.5, 0.25), whereas for HSV it will be (30°, √3/4, 0.5). HSV is 
best used when a user is selecting a color interactively It is usually much easier for a user to get to a desired col-
or as compared to using RGB [3]. [9][11] explain use of HSV color space for skin detection. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 3.HSV Color Model 
4   Proposed Skin Detection Algorithm 
 
The proposed algorithm converts the entire image in a two dimensional matrix in which the column and row 
size is defined by the width and height of the image respectively. Once the image is divided, each entry consists 
of a pixel of the image. The ARGB color of that particular pixel is determined. The ARGB value retrieved from 
the image for each pixel is a 32-bit value. Hence to extract each sub-value i.e. red, green, blue and alpha we 
right shift this value by 24 bit in order to get the value of alpha. The alpha channel is normally used as an opaci-
ty channel. If apixel has a value of 0% in its alpha channel, it is fully transparent (and, thus, invisible), whereas 
a value of 100% in the alpha channel gives a fully opaque pixel (traditional digital images). Similarly, for red 
right we shift by 16 bits, for green right shift by 8 bits. The remaining value is of blue color.  
Bitwise AND operation with 0xff was applied on these calculated values in order to extract only the bits corre-
sponding to that particular color. The above entire procedure is applied to each and every pixel of the image. In 
order to make the recognition more precise the ARGB value is converted to HSV as well as YCbCr value using 
conversion factors and built-in functions. The HSV, YCbCr and ARGB value of each pixel is compared to the 
328 Kolkur et.al. 
 
 
standard values of a skin pixel and decision is made whether the pixel is a skin pixel or not depending on wheth-
er the values lie in a range of predefined threshold values for each parameter.  
The ranges for a skin pixel in different color spaces used by our algorithm are as follows: 
0.0 <= H <= 50.0 and 0.23 <= S <= 0.68 and 
R > 95 and G > 40 and B > 20 and R > G and R > B 
and | R - G | > 15 and A > 15 
 
OR 
 
R > 95 and G > 40 and B > 20 and R > G and R > B 
and | R - G | > 15 and A > 15 and Cr > 135 and 
Cb > 85 and Y > 80 and Cr <= (1.5862*Cb)+20 and 
Cr>=(0.3448*Cb)+76.2069 and 
Cr >= (-4.5652*Cb)+234.5652 and 
Cr <= (-1.15*Cb)+301.75 and 
Cr <= (-2.2857*Cb)+432.85nothing 
(H : Hue ; S: Saturation ; R : Red ; B: Blue ; G : Green ; Cr, Cb : Chrominance components ; Y : luminance 
component ) Figure 4 shows flowchart that illustrates  steps of the algorithm. 
 
5   Experimental Results 
Pratheepan dataset for human skin detection is used as baseline for comparing results [11]. The images in this 
dataset are downloaded randomly from Google for human skin detection research. These images are captured 
with a range of different cameras using different colour enhancement and under different illuminations. The 
dataset also contains Ground Truth images for sample images in dataset. Fig 5 shows results obtained on some 
of the images in the dataset. Each diagram shows original image, ground truth image and resultant image from 
our algorithm. Table 1 shows accuracy calculations on the images shown in fig 5 using following definitions 
[14]. True positive (TP) represents number of Skin pixels correctly identified as skin, True negative (TN) is 
number of Non-skin pixel correctly identified as non-skin, False positive (FP) is Non-skin pixel incorrectly 
identified as skin and False negative (FN) –Skin pixel incorrectly identified as non-skin. Precision and Accuracy 
is calculated using equations 7 and 8 respectively. Precision of 89.33% and accuracy of 94.43% was obtained 
on a subset of images from this set. 
  
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =
𝑇𝑃
𝑇𝑃+𝐹𝑃
     -------------------eq. 7 
 
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =
𝑇𝑃+𝑇𝑁
(𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁)
   -------------------eq. 8 
 
Table 1. Accuracy Calculations 
 
Sr. 
No. 
Total 
no of 
Pixels 
Skin  
pixels 
detected 
(Our 
algo.) 
Skin 
pixels 
in GT  
image 
Nonskin 
 Pixels 
detected 
(Our 
algo.) 
Nonskin 
 pixels 
in   
 GT  
image 
True 
Positive 
False 
Positive 
True 
Negative 
False 
Negative 
Precision Accuracy 
1 196608 54885 55836 140772 141723 54885 0 140772 951 100 99.5 
2 176418 29828 23089 153329 146590 23089 6739 146590 0 77.4 96.1 
3 114400 20497 21128 93272 93903 20497 0 93272 631 100 99.4 
4 108600 51191 49420 59180 57409 49420 1771 57409 0 96.5 98.3 
5 50000 19328 18926 31074 30672 18926 402 30672 0 97.9 99.1 
6 128000 72237 47359 80641 55763 47359 24878 55763 0 65.5 80.55 
 
 
Human Skin Detection Using RGB, HSV And Ycbcr Color Models                                                       329 
 
  
 
Additionally some sample images were collected from internet [12]. Fig. 6 shows the results obtained from the 
algorithm on sample images.  The bar chart in fig. 7 represents the number of skin pixels detected in the three 
different color spaces RGB , HSV & YCbCr for three images respectively 6a)-6c). All three color spaces are 
almost equally contributing in the process of skin pixel identification. The algorithm is implemented in JAVA. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
No 
Load Image 
 
Convert image 
into 2D array 
pixel array 
 
Access First/ 
Next Pixel 
 
Access First/ 
Last Pixel 
 
Extract ARGB 
value of pixel 
 
Convert RGB to 
HSV 
 
Convert RGB to 
YCbCr 
 
Check RGB, HSV, 
YCbCr values > 
threshold values 
Mask the pixel 
with black color 
Do not mask 
the pixel  
Display Modi-
fied image with 
NonSkin pixels 
masked  
End 
of image 
No 
Yes 
Yes 
Fig. 4 Flowchart of the proposed system 
 
330 Kolkur et.al. 
 
 
 
 
 
 
 
 
Fig. 5. Experimental results on Pratheepan Dataset 
 
(a) 
 
(b) 
 
(c) 
Fig. 6. Experimental results on sample images.  
Human Skin Detection Using RGB, HSV And Ycbcr Color Models                                                       331 
 
  
 
 
Fig. 7. Bar chart showing no. of Skin Pixels 
6   Conclusion 
This paper demonstrates a threshold based algorithm which recognizes skin image using the RGB-HSV-YCbCr 
model. The algorithm is capable of processing images of different light conditions such as brightness etc. Our 
algorithm gives promising results in terms of precision and accuracy when compared with baseline dataset as 
seen in fig. 6. The future scope of this algorithm is to detect face, hand as well as hand gestures which can be 
used for security purpose, aid for physically challenged (deaf) individuals or for skin disease detection. 
 
References 
[1] Patil, Prajakta M., and Y. M. Patil, ""Robust Skin Colour Detection and Tracking Algorithm"", Interna-
tional Journal of Engineering Research and Technology Vol. 1. No.8 (October-2012), ISSN: 2278-
0181 (2012). 
[2] Ahmed, E., Crystal, M., Dunxu H: “Skin Detection-a short Tutorial”, Encyclopedia of Biometrics, pp. 
1218–1224 ,Springer-Verlag Berlin, Heidelberg,(2009). 
[3] Poorani. M, Prathiba. T, Ravindran. G: “Integrated Feature Extraction for Image Retrieval”, Interna-
tional Journal of Computer Science and Mobile Computing vol.2 no. 2, 28-35(February2013) 
[4] B.D., Zarit, B.J., Super, and F.K.H. Quek: ""Comparison of five color models in skin pixel classifica-
tion”, Int. Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Sys-
tems, 58-63(Sept1999). 
[5] V. Vezhnevets, V. Sazonov, and A. Andreeva: “A Survey on Pixel- Based Skin Color Detection Tech-
niques”, Proc. Graphicon-2003, 85–92 (Sept 2003). 
[6] KB Shaik, P Ganesan, V Kalist, BS Sathish, JMM Jenitha:""Comparative study of skin color detection 
and segmentation in HSV and YCbCr color space"", Procedia Computer Science57 ,41-48(2015). 
[7] Brand, J., Mason, J.: “A comparative assessment of three approaches to pixel level human skin-
detection.” In Proc. of the International Conference on Pattern Recognition, vol. 1,2000, 1056-1059. 
[8] Jones, M.J., Rehg, J.M: “Statistical color models with application to skin detection”, International Jour-
nal of Computer Vision (IJCV),46(1), 81-96(2002). 
[9] Phung, S. L., Bouzerdoum, A., And Chai, D: “A novel skin color model in ycbcr color space and its 
application to human face detection” , IEEE International Conference on Image Processing (ICIP’ 
2002), vol. 1, 289-292(2002). 
[10] Chai, D., and Bouzerdoum, A.: “A bayesian approach to skin color classification in ycbcr color space”, 
Proceedings IEEE Region Ten Conference (TENCON’2000) Vol2, 421-424(2000). 
332 Kolkur et.al. 
 
 
[11] Albiol, A., Torres, L., Delp, E.: “Optimum color spaces for skin detection”, Proceedings of the Interna-
tional Conference on Image Processing (ICIP), 122- 124(2001). 
[12] Tan, Wei Ren, Chee Seng Chan, PratheepanYogarajah, and Joan Condell.: ""A fusion approach for effi-
cient human skin detection"", Industrial Informatics, IEEE Transactions on 8, no. 1,138-147(2012) 
[13] “Free Images – Pixabay”, https://www.pixabay.com, 2016, Web. 16 June 2016. 
[14] “Sensitivity_and_specificity-Wikipedia”,https://en.wikipedia.org, 2016, Web.16 June 2016 
 
 
 
 
 
 
 
 
 
"
16,"Large-Scale Plant Classification with Deep Neural Networks
Ignacio Heredia∗
Instituto de Fisica de Cantabria (CSIC-UC)
E-39005, Santander, Spain
ABSTRACT
This paper discusses the potential of applying deep learning tech-
niques for plant classication and its usage for citizen science in
large-scale biodiversity monitoring. We show that plant classica-
tion using near state-of-the-art convolutional network architectures
like ResNet50 achieves signicant improvements in accuracy com-
pared to the most widespread plant classication application in test
sets composed of thousands of dierent species labels. We nd that
the predictions can be condently used as a baseline classication
in citizen science communities like iNaturalist (or its Spanish fork,
Natusfera) which in turn can share their data with biodiversity
portals like GBIF.
KEYWORDS
deep learning, plant classication, citizen science, biodiversity mon-
itoring
ACM Reference format:
Ignacio Heredia. 2017. Large-Scale Plant Classication with Deep Neural
Networks. In Proceedings of ACM CF’17, Siena, Italy, May 15-17, 2017, 5 pages.
DOI: 10.1145/3075564.3075590
1 INTRODUCTION
The deep learning revolution has brought signicant advances
in a number of elds [1], primarily linked to image and speech
recognition. The standardization of image classication tasks like
the ImageNet Large Scale Visual Recognition Challenge [2] has
resulted in a reliable way to compare top performing architectures.
Since the AlexNet architecture [3], the rst ecient implementation
of convolutional neural networks using GPUs, the error in these
competitions has reached superhuman performance [4].
Despite this recent success in general image recognition, the
work in the biodiversity community relies heavily on hand labeled
image data assigned by a (relatively) small community of experts
and does not exploit these recent advances. This might be an im-
pediment to open the community to non expert users who, armed
with modern technologies handily embedded in a smartphone, can
push biodiversity monitoring to the next level. The use of deep
learning for plant classication is not novel [5, 6] but has mainly
focused in leaves and has been restricted to a limited amount of
species, therefore making it of limited use for large-scale biodi-
versity monitoring purposes. This same specicity issue applies
to some standardized plant datasets [7] which are very helpful to
∗Electronic address: iheredia@ifca.unican.es
ACM CF’17, Siena, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The denitive Version of Record was published in Proceedings of
ACM CF’17, May 15-17, 2017 , https://doi.org/10.1145/3075564.3075590.
evaluate the network performances but who are limited in variety
of species or in the diversity of the images (focusing mainly in ow-
ers or leaves). The PlantNet tool [8, 9], based on distant versions
of the IKONA algorithms, pioneered in creating an open access
tool to automate the task of recognizing a wide variety of species.
However it does not reach the performance of expert botanists. Ap-
plying the recent advances in convolutional neural networks could
have a positive impact in closing this performance gap. This could
be a large step towards building a reliable and general large-scale
plant recognition app that spreads the use of citizen science for
biodiversity monitoring.
2 THE DATASET
As training dataset we use the great collection of images which
are available in PlantNet under a Creative-Common Attribution-
ShareAlike 2.0 license. It consists of around 250K images belonging
to more than 6K plant species of Western Europe. These species are
distributed in 1500 genera and 200 families. Each image has been
labeled by experts and comes with a tag which species the focus
of the image, like ’habit’, ’ower’, ’leaf’, ’bark’, etc. Most images
have resolutions ranging from 200K to 600K pixels and aspect ratios
ranging from 0.5 to 2. The dataset is highly unbalanced because
most labels contain very few images.
We train on the whole dataset (without making validation or
test splits) as we intend to build a classier trained on the same
dataset as the PlantNet tool so that their performances can be fairly
compared. Also we believe that testing the classication perfor-
mance on a subset of PlantNet is not an accurate measure of the
performance of the net on real-world data as all the images in the
dataset are highly correlated (many photos inside a specie share au-
thor and are often taken from the same plant with slightly dierent
angles). Therefore at test time we will use three external datasets
to condently measure the performance of our net.
3 THE MODEL
As plant classication is not very dierent from general object clas-
sication, we expect that top performing architectures in the Ima-
geNet Large Scale Visual Recognition Challenge (ILSVRC) would
perform well in this task. Therefore we use as convolutional neural
network architecture the ResNet model [10] who won the ILSVRC’15.
This architecture consists of a stack of similar (so-called residual)
blocks, each block being in turn a stack of convolutional layers. The
innovation is that the output of a block is also connected with its
own input through an identity mapping path. This alleviates the
vanishing gradient problem, improving the gradient backward ow
in the network and allowing to train much deeper networks. We
choose our model to have 50 convolutional layers (aka. ResNet50).
As deep learning framework we use the Lasagne [11] module
built on top of Theano [12, 13]. We initialize the weights of the
ar
X
iv
:1
70
6.
03
73
6v
1 
 [c
s.L
G]
  1
2 J
un
 20
17
ACM CF’17, May 15-17, 2017, Siena, Italy Ignacio Heredia
model with the pretrained weights on the ImageNet dataset pro-
vided in the Lasagne Model Zoo. We train the model for 100 epochs
on a GTX 1080 for 6 days using Adam [14] as learning rule. During
training we apply standard data augmentation (as sheer, transla-
tion, mirror, etc) so that the network never sees the same image.
We do not apply rotation or upside down mirroring to the images
tagged as ’habit’, as it does not make much sense to have a tree or
a landscape upside down. After applying the transformations we
downscale the image to the ResNet standard 224×224 input size. 1
4 EXPERIMENTS AND DISCUSSION
Our goal is to achieve a performance that we consider to be useful
as baseline classication (ie. around 50% accuracy). However it
is dicult to assess how our net performance compares to other
existing algorithms for plant classication as the main competition
for plant classication, PlantCLEF [15], uses datasets composed
of images uploaded to PlantNet by users who might already be
present in our training set. Therefore we have composed three test
datasets with external photos.
To put the Resnet accuracy values into perspective, we will
compare them with the performance of the PlantNet tool on these
same three datasets. In the PlantNet tool you can upload an url, or
an image from your local disk, along with a tag suggestion and it
returns a list of suggested species. When assessing its performance
we report the best predictions across all tags (ie. we suppose the
user selects optimally the tag).
Finally for the ResNet50 evaluation we use random ten crop
testing with smaller data augmentation parameters than those used
during training.
4.1 The datasets
4.1.1 Google Search Image. For this dataset we select the 3680
labels (around 60% of all labels) with more than 12 images in our
training dataset. For each one of these labels we automatically re-
trieve the 10 rst images returned by the Google Image Search
engine. As this is done in an automated fashion some minor misla-
beled or corrupt examples might appear in the dataset. By choosing
only the most popular labels and retrieving the top results, we
expect to minimize the presence of mislabeled images.
4.1.2 Portuguese Flora. The Portuguese ora dataset [16] con-
sists in 23K images belonging to 2K species. To compose our test
dataset we select the 15K images belonging to one of the 1300
species which are also present in our training dataset.
4.1.3 iNaturalist. iNaturalist is a website were the user can
upload their observations, that can have one or several images,
and get help from the community to have them correctly labeled.
For composing our dataset we select only the observations with
research quality grade (ie. a consensus has been reached in the
community on the species or genus label). There are around 600K
such plant observations belonging to several ranks like species (97%
of the total), genus, variety, subspecies, hybrid, etc. Selecting the
observations tagged as (pure) species we end up with 900K images
belonging to 20K plant species. From this set of images we only
select the ones belonging to any of our 6K training species and we
1Code is available at github.com/IgnacioHeredia/plant_classification
FIG. 1: Example of non-trivial image classication with the
ResNet50. Here the true label is Verbascum Thapsus which is also
the rst predicted label.
Datasets Accuracy %
ResNet50 (ours) PlantNet (usual)
Top1 Top5 Top1 Top5
Google Search 40 63 18 37
Portuguese Flora 29 47 15 29
iNaturalist 33 49 18 30
Table 1: Accuracy results of the two algorithms for all three test
datasets.
end up with as test set composed of 300K images belonging to 3K
dierent species.
In a later stage we will see how the prediction accuracy improves
with observations containing 2 images or more. For that we end
up with a test set of 60K observations containing between 2 and 33
images belonging to 2600 species present in our training dataset.
Large-Scale Plant Classification with Deep Neural Networks ACM CF’17, May 15-17, 2017, Siena, Italy
FIG. 2: Detailed results for the iNaturalist dataset for observations
containing from 1 to 4 images. a) Top1 (solid line) and Top5 (dashed
line) accuracy as a function of the probability of the rst predicted
label. b) Proportion of observations that have to be discarded be-
cause they do not meet the desired condence.
4.2 Results and Discussion
The ResNet50 returns a list of probabilities that each label is the
correct label as shown in Fig 1. The top1 accuracy measures how
often the correct label is the highest probability label, while the
top5 accuracy measures how often the correct label is among the
ve labels with highest probability. Table 1 shows the top1 and
top5 accuracy results for all three datasets. We can notice that the
Resnet50 achieves ×2 and ×1.7 improvements for top1 and top5
accuracies consistently across datasets compared with the Plant-
Net tool. The overall accuracy is approximately constant although
slightly higher in the Google dataset probably due to higher image
quality.
Although the accuracy results are better than those obtained
with the PlantNet tool, they are far from being reliable enough
to be systematically used to predict tags for all observations. One
way to improve this is to only return an identication if the net
is condent enough about its prediction. The Fig 2a shows how
this accuracy improves when we only trust predictions who have a
top1 probability above a certain cuto. For example if we set the
cuto at 30% the top1 and top5 accuracies increase to 59% and 74%
respectively. The value of the cuto should be a trade-o between
how condent we want to be and how many observations we are
willing to discard. In Fig 2b we show the proportion of observations
that had to be discarded because they did not meet the desired
cuto probability. In the case of setting the cuto to 30%, we are
discarding 55% of the observations.
But increasing the condence cuto is not the only way to im-
prove the accuracy. Although observations with a single image are
the majority (91% of the total) in the iNaturalist dataset, there are
also observations with 2 (6%), 3 (2%), 4 (1%) and more images. If
we use jointly those images to identify average the predictions of
the observed specie we achieve much higher accuracies than with
only one image due to the lower inuence of random noise. For
example if we examine again with the cuto to 30%, we now have
top1 accuracies of 75%, 80% and 84% for observations with 2, 3 and
4 images respectively. However the proportion of discarded images
also increases compared to the 1 image case, reaching now 60%,
67% and 71%.
Although one might argue that those multi image observations
are a very small portion of the dataset (and therefore the improve-
ment in overall accuracy marginal), it is important to notice the
increasing trend of uploads of multi images observations in the
recent times. For example in the last three months of 2016, the 1
image observations were merely 60% of the total whereas the 2, 3
and 4 image observations went up to 23%, 11% and 4% respectively.
In Fig 3a we show the confusion matrix of the iNaturalist test
dataset predictions for observations with one image. We have or-
dered the species label in blocks of families and blocks of genera
to unravel the inherent block structure of the matrix. As we can
see, along the diagonal, which is densely populated as expected,
there are blocks of dierent sizes which denote groups inside which
confusion is frequent. Fig 3b zooms into one such a group where we
can see that inside the family of Plantaginaceae, the species belong-
ing to the genera Linaria, Plantago and Veronica are often confused
with other species within the same genera. Another example of
a typical block could be Fig 3c where we can see that the species
belonging to the family are often confused with other species of the
family irrespectively of their genus. Those three gures show that
even when the net’s condence is high but the prediction is wrong,
we will likely be able to extract useful information, either about
the correct genus or either about the correct family. This can be
valuable information for the users or experts to narrow the search
of the correct specie.
Finally, with regard to a future deployment in the iNaturalist
ecosystem, we have to note that those accuracy results are restricted
to the species present in our training dataset who merely represent
30% of all plant species present in iNaturalist. This is due to the fact
that we trained with just Western Europe species from PlantNet
while iNaturalist receives observations from all around the world.
This could be solved in future work by retraining the net with
both the iNaturalist and all the PlantNet images (including South
America, Indian Ocean and North Africa).
5 CONCLUSION
In this work we have built a large-scale plant classication algo-
rithm based on the ResNet convolutional neural network architec-
ture. We have evaluated the classication performance of our net
on the observations of iNaturalist and obtained that we were able
to classify almost half of these observations, who lied above a 30%
predictive cuto, with a top1 and top5 accuracies of 59% and 74%
respectively. We have then demonstrated that the user ability to
upload several images per observation (preferably of dierent plant
parts or from dierent angles) critically improved the nal accuracy.
Finally we obtained that even when the prediction was wrong it
was very likely that we could obtain some information about the
true genus or family, so that it could be used by experts or users to
narrow their search of the correct label.
In addition we have seen that trained with the same image
dataset, the ResNet architecture outperforms the most widespread
online public plant classication algorithm by around a factor of 2
in top1 and top5 accuracies. Besides our model does not require to
enter a suggested image tag along with the observation.
With all this information in hand we think that large-scale bio-
diversity projects like the Global Biodiversity Information Facility
ACM CF’17, May 15-17, 2017, Siena, Italy Ignacio Heredia
FIG. 3: a) Confusionmatrix for the iNaturalist dataset of observations with 1 image.We zoom b) in a region where dierent genera as confused
separately inside the same family and c) in a region where all the genera are confused inside the same family. We weight the counts in the
matrix with the probability of the prediction. All columns have been normalized to 1. We plot only the 1.5K labels with more observations so
that the matrix appears more dense.
Large-Scale Plant Classification with Deep Neural Networks ACM CF’17, May 15-17, 2017, Siena, Italy
(GBIF) [17] or LifeWatch [18], the European research infrastructure
on biodiversity, could very well benet from this new techniques to
build a fast and reliable method to automatically monitor biodiver-
sity. This tool can denitely open the eld to active contributions
of non expert users including citizen scientists.
For future work there are several ways to explore how to achieve
an increase in accuracy. The most obvious way is to increase the
training dataset size. It should be noted that iNaturalist contains
even more images than PlantNet so when training a net for deploy-
ment one should combine both datasets to increase the predictions
accuracy. Here we trained with just the PlantNet dataset so that
the comparison of performance with the PlantNet tool would be
fair. The other way is to implement architectural modications to
the net that lead to a better generalization error. Along this line
two promising variants of the Resnet architecture have recently ap-
peared. The rst one is the Stochastic Depth Network [19] in which
we remove randomly some residual blocks during training, allow-
ing the net to be more robust for generalization and to train deeper
networks. The second more promising variant is the DenseNet [20]
in which the skip identity connections are now connecting the
residual blocks at all scales. Lastly it is worth mentioning that the
Resnet50 is a quite space-consuming architecture so if we were
to implement plant recognition app in embedded devices, so that
the user could identify without connecting to the Net, one could
use some recent modications of shallower architectures that oer
almost as good performance with much less memory consumption
[21].
ACKNOWLEDGEMENTS
I want to thank Jesus Marco de Lucas and Fernando Aguilar for
their helpful comments and supervision. I also wanted to thank
PlantNet and Flora-on for their image’s open access policy, and the
EGI-Engage Lifewatch Competence Centre for their support. The
author is funded with the EU Youth Guarantee Initiative (Ministerio
de Economia, Industria y Competitividad, Secretaria de Estado de
Investigacion, Desarollo e Innovacion, through the Universidad de
Cantabria).
REFERENCES
[1] Yann LeCun, Yoshua Bengio, and Georey Hinton. Deep learning. Nature,
521(7553):436–444, may 2015.
[2] Olga Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[3] Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. Imagenet classication
with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
25, pages 1097–1105. Curran Associates, Inc., 2012.
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into
rectiers: Surpassing human-level performance on imagenet classication, 2015.
[5] Sue Han Lee, Chee Seng Chan, Paul Wilkin, and Paolo Remagnino. Deep-plant:
Plant identication with convolutional neural networks, 2015.
[6] Mads Dyrmann, Henrik Karstoft, and Henrik Skov Midtiby. Plant species clas-
sication using deep convolutional neural network. Biosystems Engineering,
151:72–80, 2016.
[7] M-E. Nilsback and A. Zisserman. Automated ower classication over a large
number of classes. In Proceedings of the Indian Conference on Computer Vision,
Graphics and Image Processing, Dec 2008.
[8] Pierre Bonnet et al. Plant identication: man vs. machine. Multimedia Tools and
Applications, 75(3):1647–1665, jun 2015.
[9] Alexis Joly et al. Interactive plant identication based on social image data.
Ecological Informatics, 23:22–34, sep 2014.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
for image recognition, 2015.
[11] Sander Dieleman et al. Lasagne: First release., August 2015.
[12] James Bergstra et al. Theano: a CPU and GPU math expression compiler. In
Proceedings of the Python for Scientic Computing Conference (SciPy), June 2010.
Oral Presentation.
[13] Frédéric Bastien et al. Theano: new features and speed improvements. Deep
Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization,
2014.
[15] Alexis Joly et al. LifeCLEF 2016: Multimedia life species identication challenges.
In Lecture Notes in Computer Science, pages 286–310. Springer Nature, 2016.
[16] Sociedade Portuguesa de Botânica. Flora-on: Flora de portugal interactiva, 2014.
http://www.ora-on.pt/.
[17] Global Biodiversity Information Facility (GBIF). http://www.gbif.org/.
[18] Lifewatch: e-science and technology european infrastructure for biodiversity
and ecosystem research. http://www.lifewatch.eu.
[19] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep
networks with stochastic depth, 2016.
[20] Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten.
Densely connected convolutional networks, 2016.
[21] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J.
Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and <0.5mb model size, 2016.
"
17,"1Right for the Right Reason: Training Agnostic
Networks
Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini
Intelligent Systems Laboratory, University of Bristol, Bristol BS8 1UB, UK
{sen.jia, thomas.lansdall-welfare, nello.cristianini}@bris.ac.uk
Abstract—We consider the problem of a neural network being
requested to classify images (or other inputs) without making
implicit use of a “protected concept”, that is a concept that
should not play any role in the decision of the network. Typically
these concepts include information such as gender or race, or
other contextual information such as image backgrounds that
might be implicitly reflected in unknown correlations with other
variables, making it insufficient to simply remove them from the
input features. In other words, making accurate predictions is not
good enough if those predictions rely on information that should
not be used: predictive performance is not the only important
metric for learning systems. We apply a method developed in
the context of domain adaptation to address this problem of
“being right for the right reason”, where we request a classifier
to make a decision in a way that is entirely ‘agnostic’ to a given
protected concept (e.g. gender, race, background etc.), even if
this could be implicitly reflected in other attributes via unknown
correlations. After defining the concept of an ‘agnostic model’, we
demonstrate how the Domain-Adversarial Neural Network can
remove unwanted information from a model using a gradient
reversal layer.
I. INTRODUCTION
Data-driven Artificial Intelligence (AI) is behind the new
generation of success stories in the field, and is predicated
not just on a few technological breakthroughs, but on a
cultural shift amongst its practitioners: namely the belief that
predictions are more important than explanations, and that
correlations count more than causations [4], [8]. Powerful
black-box algorithms have been developed to sift through data
and detect any possible correlation between inputs and in-
tended outputs, exploiting anything that can increase predictive
performance. Computer vision (CV) is one of the fields that
has benefited the most from this choice, and therefore can
serve as a test bed for more general ideas in AI.
This paper targets the important problem of ensuring trust in
AI systems. Consider a case as simple as object classification.
It is true that exploiting contextual clues can be beneficial in
CV and generally in AI tasks. After all, if an algorithm thinks
it is seeing an elephant (the object) in a telephone box (the
context), or Mickey Mouse driving a Ferrari, it is probably
wrong. This illustrates that even though your classifier might
have an opinion about the objects in an image, the context
around it can be used to improve your performance (e.g. telling
you that it is unlikely to be an elephant inside a telephone box),
as shown in many recent works [3], [13], [14].
However making predictions based on context can also lead
to problems and creates various concerns, one of which is the
use of classifiers in “out of domain” situations, a problem that
leads to research questions in domain adaptation [6], [18].
Other concerns are also created around issues of bias, e.g.
classifiers incorporating biases that are present in the data
and are not intended to be used [2], which run the risk of
reinforcing or amplifying cultural (and other) biases [20].
Therefore, both predictive accuracy and fairness are heavily
influenced by the choices made when developing black-box
machine-learning models.
Since the limiting factor in training models is often sourc-
ing labelled data, a common choice is to resort to reusing
existing data for a new purpose, such as using web queries
to generate training data, and employing various strategies to
annotate labels, i.e. using proxy signals that are expected to be
somewhat correlated to the intended target concept [5], [11].
These methods come with no guarantees of being unbiased,
or even to reflect the deployment conditions necessarily, with
any data collected “in the wild” [8], [10] carrying with it the
biases that come from the wild.
To address these issues, a shift in thinking is needed, from
the aforementioned belief that predictions are more important
than explanations, to ideally developing models that make pre-
dictions that are right for the right reason, and consider other
metrics, such as fairness, transparency and trustworthiness, as
equally important as predictive performance. This means that
we want to ensure that certain protected concepts are not used
as part of making critical decisions (e.g. decisions about jobs
should not be based on gender or race) for example, or that
similarly, predictions about objects in an image should not be
based on contextual information (gender of a subject in an
image should not be based on the background).
In this direction, we demonstrate how the Domain-
Adversarial Neural Network (DANN) developed in the context
of domain adaptation [6] can be modified to generate ‘agnos-
tic’ feature representations that do not incorporate any implicit
contextual (correlated) information that we do not want, and is
therefore unbiased and fair. We note that this is a far stronger
requirement than simply removing protected features from the
input that might otherwise implicitly remain in the model due
to unforeseen correlations with other features.
We present a series of experiments, showing how the rele-
vant pixels used to make a decision move from the contextual
information to the relevant parts of the image. This addresses
the problem of relying on contextual information, exemplified
by the Husky/Wolf problem in [15], but more importantly
shows a way to de-bias classifiers in the feature engineering
ar
X
iv
:1
80
6.
06
29
6v
1 
 [c
s.L
G]
  1
6 J
un
 20
18
2step, allowing it to be applied generally for different models,
whether that is word embeddings, support vector machines, or
deep networks etc.
Ultimately, this ties into the current debate about how to
build trust in these tools, whether this is about their predictive
performance, their being right for the right reason, their being
fair, or their decisions being explainable.
II. AGNOSTIC MODELS
Methods have previously been proposed to remove biases,
based on various principles, one of which is distribution
matching [20]: ensuring that the ratio between protected
attributes is the same in the training instances and in the
testing instances. However, this does not avoid using the wrong
reasons in assessing an input but simply enforces a post-hoc
rescaling of scores, to ensure that the outcome matches the
desired statistical requirements of fairness.
In our case, we do not want to have an output distribution
that only looks as if it has been done without using protected
concepts. We actually want a model that cannot even represent
them within its internal representations, where we call such a
model agnostic. This is a model that does not represent a
forbidden concept internally, and therefore cannot use it even
indirectly. Of course this kind of constraint is likely to lead
to lower accuracy. However, we should keep in mind that this
reduction in accuracy is a direct result of no longer using
contextual clues and correlations that we explicitly wish to
prevent.
In this direction, we consider classification tasks where X
is the input space and Y = {0, 1, . . . , L − 1} is the set of L
possible labels. An agnostic model (or feature representation)
Gf : X → RD, parameterized by θf , maps a data example
(xi, yi) into a new D-dimensional feature representation z ∈
RD such that for a given label p ∈ Y , there does not exist an
algorithm Gy : RD → [0, 1]L which can predict p with better
than random performance.
III. DOMAIN-ADVERSARIAL NEURAL NETWORKS
One possible way to learn an agnostic model is to use a
DANN [6], recently proposed for domain adaptation, which
explicitly implements the idea raised in [1] of learning a
representation that is unable to distinguish between training
and test domains. In our case, we wish for the model to be
able to learn a representation that is agnostic to a protected
concept.
DANNs are a type of Convolutional Neural Network
(CNN) that can achieve an agnostic representation using
three components. A feature extractor Gf (·; θf ), a label pre-
diction output layer Gy(·; θy) and an additional protected
concept prediction layer Gp : RD → [0, 1], parameter-
ized by θp. During training, two different losses are then
computed: a target prediction loss for the i-th data instance
Liy(θf , θy) = Ly(Gy(Gf (xi; θf ); θy), yi) and a protected
concept loss Lip(θf , θp) = Lp(Gp(Gf (xi; θf ); θp), pi), where
Ly and Lp are both given by the cross-entropy loss and pi is
the label denoting the protected concept we wish to be unable
to distinguish using the learnt representation.
Training the network then attempts to optimise
E(θf , θy, θp) = (1− α) 1
n
n∑
i=1
Liy(θf , θy)
− α
(
1
n
n∑
i=1
Lip(θf , θp) +
1
n′
N∑
i=n+1
Lip(θf , θp)
)
, (1)
using α as a hyper-parameter for the trade-off between the
two losses and finding the saddle point θˆf , θˆy, θˆp such that
(θˆf , θˆy) = argmin
θf ,θy
E(θf , θy, θˆp), (2)
θˆp = argmax
θp
E(θˆf , θˆy, θp). (3)
As further detailed in [6], introducing a gradient rever-
sal layer (GRL) between the feature extractor Gf and the
protected concept classifier Gp allows (1) to be framed as
a standard stochastic gradient descent (SGD) procedure as
commonly implemented in most deep learning libraries.
The network can therefore be learnt using a simple stochas-
tic gradient procedure, where updates to θf are made in the
opposite direction of the gradient for the maximizing param-
eters, and in the direction of the gradient for the minimizing
parameters. Stochastic estimates of the gradient are made, both
for the target concept and for the protected concept, using the
training set. We can see this as the two parts of the neural
network (target classifier Gy and protected concept classifier
Gp) are competing with each other for the control of the
internal representation. DANN will attempt to learn a model
Gf that maps an example into a representation allowing the
target classifier to accurately classify instances, but crippling
the ability of the protected concept classifier to discriminate
inputs by their label for the protected concept.
IV. EXPERIMENTS
To test the use of DANNs for learning representations
that can be used to make predictions for the right reasons,
we ran two different experiments. In Experiment 1, we first
demonstrate the issue of using contextual information to make
predictions in a cross-domain classification task, before using a
DANN in Experiment 2, showing that the network can learn an
agnostic representation that allows us to make predictions on
a target concept without using information from a correlated
contextual concept, such as the image background.
A. Data Description
In this work, we combine two datasets, making use of
the ‘Jaguar’ and ‘Killer whale’ categories from the ImageNet
dataset [16], as well as the ‘Forest path’ and ‘Coast’ categories
from the Places dataset [21].
A two-part training set was constructed containing 2,524
images from the ‘Jaguar’ category, and the same number for
the ‘Killer whale’ category from ImageNet (the target concept
training set). This was further supplemented with 5,000 images
from each of the two categories (‘Forest path’ and ‘Coast’)
from the Places dataset (the contextual concept training set),
3Fig. 1. Example images taken from the ‘Jaguar’, ‘Killer whale’, ‘Forest path’
and ‘Coast’ categories of the ImageNet and Places datasets respectively (left-
right).
for a total of 15,048 images in the combined training set.
Two separate hold-out sets were also created, one for the
target concept containing 50 hold-out images from each of
the ‘Jaguar’ and ‘Killer whale’ categories, and one for the
contextual concept containing 50 hold-out images from each
of the ‘Forest path’ and ‘Coast’ categories.
Data augmentation was performed on the training set to
increase the number of instances by creating new images
that are multi-crops of 224 × 224 pixels and horizontally
flipping copies of the training set images. All images in our
experiments were also pre-processed to be 256 × 256 pixels
by a process of multi-cropping where each image is resized
before cropping the final size from the centre region, as in
[9], [12]. Example images from the training set used for the
experiments can be seen in Fig. 1.
B. Network structure
The network structure used for our experiments in this paper
are based upon a simplified version of the VGG-net CNN
used in [17], where the feature extraction layers Gf consist of
five convolutional layers: conv3-641, conv3-128, conv3-256,
conv3-512 and conv3-512, with ReLU activation and max-
pooling layers inserted after each convolutional layer. The
output prediction classifiers Gy and Gp are each composed of
four fully connected layers, fc-1024, with ReLU and dropout
layers with a dropout of 0.5 after each fully connected layer.
C. Experiment 1: Cross-domain classification
In this first experiment, we motivate our approach by
demonstrating the problem we wish to address, namely that
contextual information can be used to make classification
decisions about our target concept that is not related to the
target that we actually wish to learn.
We began by training from scratch two independent CNNs
with the same network architecture, one on the target concept
training set and one on contextual concept training set. The
layers of the network are described in Sec IV-B with a single
output prediction classifer Gy per model, i.e. each CNN is
composed of five convolution layers, followed by four fully
connected layers with no shared features across the models.
1conva-b denotes a convolutional layer consisting of b filters of size a×a.
Each model was trained for 10 epochs using the following
model parameters: a batch size of 32, a starting learning rate
of η = 0.01 that decays every three epochs by a factor of 10,
a momentum of 0.5 and a weight decay of 5e−4.
The accuracy of each model was measured on both the target
and contextual test sets after each epoch as shown in Fig. 2.
As one might expect, we can see that the model trained on the
target concept achieves an accuracy of 92% on the target test
set, while the contextual concept model achieves an accuracy
of 91% on the contextual test set. More problematically, we
can see that the target concept model, trained only on images
of animals, also has good performance at classifying images
of forest paths and coastlines from the contextual test set,
with an accuracy of 79%. Similarly, the contextual concept
model, trained only on images of forest paths and coastlines
can correctly identify animals with an accuracy of 88%.
D. Experiment 2: Learning with Domain-adversarial neural
networks
In this next experiment, we show that with our proposed use
of DANNs maximises its performance on the target concept
whilst following the constraint that it should not learn useful
features for the contextual concept. We further examine the
most informative pixels (e.g. those pixels which have the
strongest response in the feature map) used for classification
[7], [19], showing that the most informative pixels are no
longer found in the image background.
Keeping all the model parameters, apart from a new learning
rate (η = 0.001), the same as in Experiment 1, we trained a
single DANN model on the combined training set, with the
network layers outlined in Sec IV-B, with the target predic-
tion output layers Gy predicting the target concept, and the
protected concept prediction layers predicting the contextual
concept. By doing so, we force the model to learn a shared data
representation (feature space) that maximises performance on
the target while incorporating no knowledge of features which
are useful for classifying the contextual concept images. This
process was repeated for different gradient trade-offs in the
range α = [0, 0.1, . . . , 1] using a grid-search procedure, where
α = 0 represents simply training the share feature space on the
target concept, and α = 1 represents training the shared feature
space to maximise the loss for the contextual concept. We
repeated this process 10 times, reporting the average accuracy
for each run, along with the standard deviation.
In Fig. 3, we can see the accuracy of the DANN for
varying gradient trade-off values on the target and contextual
concept test sets. Our results show that as α increases and
is further constrained in its use of information from the
contextual concept, the performance on the target concept
decreases, suggesting that the performance on the intended
target concept was indeed being helped by the contextual
background information. Our results show that once we have
removed features which are useful for predicting the contextual
concept, our target classifier achieves an accuracy of 64%,
while the contextual classifier can only maintain an accuracy
close to random guessing.
We further investigated whether after applying the minimax
procedure of the DANN that the most informative pixels
41 2 3 4 5 6 7 8 9 10
Training epochs
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ac
cu
ra
cy
Target concept
Contextual concept
(a) CNN trained on the target concept training set (animals)
1 2 3 4 5 6 7 8 9 10
Training epochs
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ac
cu
ra
cy
Target concept
Contextual concept
(b) CNN trained on the contextual concept training set (backgrounds)
Fig. 2. Results from Experiment 1, showing that a standard CNN model trained on the target concept will also learn how to classify in the contextual concept
and vice versa.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
0.4
0.5
0.6
0.7
0.8
0.9
1
Ac
cu
ra
cy
Target concept
Contextual concept
(a) Performance of Gy in the DANN trained for the target concept (animals)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
0.4
0.5
0.6
0.7
0.8
0.9
1
Ac
cu
ra
cy
Target concept
Contextual concept
(b) Performance of Gp in the DANN trained for the contextual concept
(backgrounds)
Fig. 3. Accuracy of the two independent classifiers in the DANN using the shared feature space on the test sets for different values of α.
for prediction corresponded with the location of the target
concept in the image, or whether they were focused on the
background scene of the image. Fig. 4 shows activation maps
for the feature representation shared between the independent
classifiers on a set of three images for each target concept
category. Examples were selected as those with the least
correlation between the activation maps for the contrasting α
values of 0 and 0.8 shown, where α values were chosen as
the two extremes in the classification accuracy.
We can observe that for the ‘Killer whale’ category, the
most informative pixels for α = 0 are indeed found in the
background of the image, while for α = 0.8 the activation
maps show that the network is focusing on the actual body
of the animal instead, as desired. For the ‘Jaguar’ category,
analysis of the most informative pixels is less clear, with
activation generally being spread widely across the image.
However, we do see some evidence of a stronger activation
response to parts of the jaguar’s body overall.
V. DISCUSSION
In our experiments, we found that as the model learns the
shared features with increasingly less contextual information,
accuracy of the target classifier decreases. This is exactly
what we expect and directly addresses our main argument,
that previously the classifier was relying on the contextual
5(a) Activation maps for the ‘Killer whale’ category (b) Activation maps for the ‘Jaguar’ category
Fig. 4. Activation maps based on the strongest response of the shared feature representation. Examples selected are those with the least correlation between
the activation maps for α = 0 and α = 0.8 as shown in the images.
background information that should not be used to make its
predictions.
At one extreme, where α = 1, the network is using no
information from the target concept in its data representation,
instead trying to maximise its loss on the contextual concept in
the shared feature space Gf , while minimising its loss in the
contextual classifier Gp. This tension between the two parts
of the network leads to a minimax scenario where if there is
any information which can be exploited to correctly predict in
the contextual concept, it is subsequently removed from the
data representation.
We note that ideally α should be set to 1 for similar
experiments, given that for any other setting the learning
system would still be exploiting forbidden information, and
would not be satisfying the original requirements of the task: to
learn to predict without the contextual information. However,
since in this scenario the shared feature space would not rely
on the target domain at all, α needs to be slowly increased
as training progresses until reaching its maximum. In this
way, the features will be guided by the target domain as well,
forming a saddle point in the exploration of the feature space
as required.
Results from investigating the most informative pixels for
classification at differing levels of α revealed that the con-
straint of the contextual concept appears to have been more
successful for the ‘Killer whale’ and ‘Coast’ images than for
the ‘Jaguar’ and ‘Forest path’ pairing. This can perhaps be
best explained by how closely the contextual concept training
images represent the contextual concept found in the target
concept training images, i.e. the whales are always pictured
next to or in the ocean, whereas jaguars will sometimes be
found outside of the jungle with different backgrounds, and
therefore the ‘Forest path’ category does not match ‘Jaguar’
backgrounds as closely as ‘Coast’ does for the ‘Killer whale’
category.
Further theoretical and experimental analysis of additional
minimax architectures is needed to explain the phenomena
of the target classifier accuracy increasing on both target and
contextual test sets for values of α ≥ 0.8.
VI. CONCLUSIONS
The creation of a new generation of AI systems that can
be trusted to make fair and unbiased decisions is an urgent
task for researchers. As AI rapidly conquers technical chal-
lenges related to predictive performance, we are discovering
a new dimension to the design of such systems that must be
addressed: the fairness and trust in the system’s decisions.
In this paper, we address this critical issue of trust in AI by
not only proposing a new high standard for models to meet,
being agnostic to a protected concept, but also proposing a
method to achieve such models. We define a model to be
agnostic with respect to a set of concepts if we can show
that it makes its decisions without ever using these concepts.
This is a much stronger requirement than in distributional
matching or other definitions of fairness. We focus on the
case where a small set of contextual concepts should not be
used in decisions, and can be exemplified by samples of data.
We have demonstrated how ideas developed in the context of
domain adaptation can deliver agnostic representations that are
important to ensure fairness and therefore trust.
6Our experiments demonstrate that the DANN successfully
removes unwanted contextual information, and makes de-
cisions for the right reasons. While demonstrated here by
ignoring the physical background context of an object in an
image, the same approach can be used to ensure that other
contextual information does not make its way into black-box
classifiers deployed to make decisions about people in other
domains and classification tasks.
ACKNOWLEDGEMENTS
SJ, TLW and NC are support by the FP7 Ideas: European
Research Council Grant 339365 - ThinkBIG.
REFERENCES
[1] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.
Analysis of representations for domain adaptation. In Advances in neural
information processing systems, pages 137–144, 2007.
[2] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics
derived automatically from language corpora contain human-like biases.
Science, 356(6334):183–186, 2017.
[3] Wenqing Chu and Deng Cai. Deep feature based contextual model for
object detection. Neurocomputing, 275:1035–1042, 2018.
[4] Nello Cristianini. On the current paradigm in artificial intelligence. AI
Communications, 27(1):37–43, 2014.
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-
Fei. Imagenet: A large-scale hierarchical image database. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 248–255. IEEE, 2009.
[6] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain,
Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor
Lempitsky. Domain-adversarial training of neural networks. The Journal
of Machine Learning Research, 17(1):2096–2030, 2016.
[7] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
Rich feature hierarchies for accurate object detection and semantic
segmentation. CoRR, abs/1311.2524, 2013.
[8] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24(2):8–12, 2009.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep resid-
ual learning for image recognition. arXiv preprint arXiv:1512.03385,
2015.
[10] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.
Labeled faces in the wild: A database for studying face recognition in
unconstrained environments. Technical report, Technical Report 07-49,
University of Massachusetts, Amherst, 2007.
[11] Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini. Gender
classification by deep learning on millions of weakly labelled images.
In Data Mining Workshops (ICDMW), 2016 IEEE 16th International
Conference on, pages 462–467. IEEE, 2016.
[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet
classification with deep convolutional neural networks. In F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in
Neural Information Processing Systems 25, pages 1097–1105. Curran
Associates, Inc., 2012.
[13] Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi
Feng, and Shuicheng Yan. Attentive contexts for object detection. IEEE
Transactions on Multimedia, 19(5):944–954, 2017.
[14] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You
only look once: Unified, real-time object detection. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages
779–788, 2016.
[15] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should
i trust you?: Explaining the predictions of any classifier. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1135–1144. ACM, 2016.
[16] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015.
[17] Karen Simonyan and Andrew Zisserman. Very deep convolutional
networks for large-scale image recognition. Eprint Arxiv, 2014.
[18] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Addressing ap-
pearance change in outdoor robotics with adversarial domain adaptation.
arXiv preprint arXiv:1703.01461, 2017.
[19] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding
convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele,
and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages
818–833, Cham, 2014. Springer International Publishing.
[20] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei
Chang. Men also like shopping: Reducing gender bias amplification
using corpus-level constraints. arXiv preprint arXiv:1707.09457, 2017.
[21] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio
Torralba. Places: A 10 million image database for scene recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.
"
18,"Adversarial Training For Sketch Retrieval
Antonia Creswell & Anil Anthony Bharath
BICV Group, Bioengineering,
Imperial College London
ac2211@ic.ac.uk
Abstract. Generative Adversarial Networks (GAN) are able to learn
excellent representations for unlabelled data which can be applied to im-
age generation and scene classification. Representations learned by GANs
have not yet been applied to retrieval. In this paper, we show that the
representations learned by GANs can indeed be used for retrieval. We
consider heritage documents that contain unlabelled Merchant Marks,
sketch-like symbols that are similar to hieroglyphs. We introduce a novel
GAN architecture with design features that make it suitable for sketch
retrieval. The performance of this sketch-GAN is compared to a modified
version of the original GAN architecture with respect to simple invariance
properties. Experiments suggest that sketch-GANs learn representations
that are suitable for retrieval and which also have increased stability to
rotation, scale and translation compared to the standard GAN architec-
ture.
Keywords: Deep learning, CNN, GAN, Generative Models, Sketches
1 Introduction
Recently, the UK’s National Archives has collected over 70, 000 heritage docu-
ments that originate between the 16th and 19th centuries. These documents make
up a small part of the “Prize Papers”, which are of gross historical importance,
as they were used to establish legitimacy of ship captures at sea.
This collection of documents contain Merchant Marks (see Fig.4B), symbols
used to uniquely identify the property of a merchant. For further historical re-
search to be conducted, the organisation requires that the dataset be searchable
by visual example (see Fig.1). These marks are sparse line drawings, which makes
it challenging to search for visually similar Merchant Marks between documents.
This dataset poses the following challenges to learning representations that are
suitable for visual search:
1. Merchant marks are line drawings, absent of both texture and colour, which
means that marks cannot be distinguished based on these properties.
2. Many machine learning techniques, and most notably convolutional neural
networks (CNNs), require large amounts of labelled training data, containing
on the order of millions of labelled images [7]. None of the Merchant Marks
are labelled, and in many cases it is not clear what labels would be assigned
to them. This motivates an unsupervised approach to learning features.
ar
X
iv
:1
60
7.
02
74
8v
2 
 [c
s.C
V]
  2
3 A
ug
 20
16
2 Antonia Creswell & Anil Anthony Bharath
3. The marks are not segmented from the dataset, limiting the number of ex-
amples available, and making it difficult to train CNNs.
Fig. 1: An overview of the problem: the circled items contain examples of Mer-
chant Marks; note that although some marks are distinct, they are still visu-
ally similar. We would like to retrieve visually similar examples, and find exact
matches if they exist. Note that the two marks on the left are exact matches,
while the others might be considered to be visually similar.
To perform visual search on the Merchant Marks, a representation for the
marks that captures their structure must be learned. Previous work has demon-
strated that deep convolutional neural networks (CNNs) are able to learn excel-
lent hierarchical representations for data [15]. CNNs have proven useful for tasks
such as classification [7], segmentation [9] and have been applied to retrieval of
art work [2][1]. However, these methods rely on large amounts of labelled data
for learning the weights. In the absence of sufficient labelled training data, we
propose the use of unsupervised techniques with CNN architectures to learn
representations for the Merchant Marks.
Unlike some previous approaches in which feature representations were learned
by using labelled datasets that differ in appearance from the retrieval set, we
used the Merchant Marks dataset itself to learn dataset-specific features. For
example, Crowley et al. [2] trained a network similar to AlexNet [7] on examples
from the photographic scenes of ILSVRC-2012 in order to learn features for the
retrieval of art work; they also trained a network on photographs of faces to
learn features for retrieving paintings of faces [1]. Yu et al. [14] suggested that
features suitable for understanding natural images are not necessarily the most
appropriate for understanding sketches.
Convolutional Auto-encoders (CAE) can be a useful tool for unsupervised
learning of features. They are made up of two networks, an encoder which com-
presses the input to produce an encoding and a decoder, which reconstructs the
input from that encoding. It has been shown [8] that shallow CAEs often learn
Adversarial Training For Sketch Retrieval 3
Fig. 2: Most marks in the Merchant Marks dataset are made of of the above
sub-structures which we refer to as parts.
the delta function (a trivial solution) which is not a useful representation for
the data. Instead, deep encoders are needed with strict regularisation on the
activations. The Winner Take All CAE [8] imposes both spatial and life-time
sparsity on the activations of the CAE in order to learn useful representations.
Other regularisation techniques include the Variational Auto-encoder [6], which
imposes a prior distribution on the encoding.
An alternative method, which learns representations from data without the
need for regularisation, is the Generative Adversarial Network [3] (GAN). Deep
convolutional generative adversarial networks [10] have been shown to learn good
representations for data. In this paper, we propose the use of GANs for learning
a representation of the Merchant Marks that can be used for visual search.
The key contribution is to show that GANs can be used to learn a repre-
sentation suitable for visual search. We apply this novel idea to the Merchant
Mark dataset, and compare two GAN architectures. The first GAN is designed
to learn a representation for sketches, based on reported architectural consid-
erations specific to sketches [14]. The second GAN is a modified version of the
network proposed by Radford et. al [10] often used for learning representations
for natural images. The representations are evaluated by comparing their in-
variance to shift, scale and rotation as well as the top 8 retrieval results for 15
examples.
2 Generative Adversarial Networks
Generative Adversarial Networks (see Fig. 3), (GANs) where first introduced
by Goodfellow et al [3], as a generative model that learned an excellent repre-
sentation for the training dataset. GANs consist of two networks, a generative
network, G and a discriminative network, D. The goal of the generative network
is to learn the distribution of the training data, pdata(x) where x ∈ Rdx and dx
is the dimensions of a data sample.
4 Antonia Creswell & Anil Anthony Bharath
Fig. 3: Generative Adversarial Network: A random sample z is drawn from a prior
distribution and fed into the generator, G to generate a sample. The discrimi-
nator will take a sample either from the generator, G(z), or from the Merchant
Mark dataset, pdata(x), and predict whether the sample is machine or human
generated. The discriminator’s objective is to make the correct prediction, while
the generator’s objective is to generate examples that fool the discriminator.
In a GAN, the generator takes as input a vector, z ∈ Rdz of dz random
values drawn from a prior distribution pz(z), and maps this to the data space,
G : Rdz → Rdx. The discriminator takes examples from both the generator and
real examples of training data and predicts whether the examples are human (or
real) (1) or machine generated (0), D : Rdx → [0, 1].
The objective of the discriminator is to correctly classify examples as human
or machine generated, while the objective of the generator is to fool the discrimi-
nator into making incorrect predictions. This can be summarised by the following
value function that the generator aims to minimise while the discriminator aims
to maximise:
min
G
max
D
Ex∼pdata(x) logD(x) +Ez∼pz(z) log(1−D(G(z)))
Training an adversarial network, both the generator and the discriminator
learn a representation for the real data. The approach considered here will use
the representation learned by the discriminator.
3 Methods
Here, we show how the discriminator, taken from a trained GAN, can be modified
to be used as an encoder for sketch retrieval. An overview of the methods used
can be seen in Fig.4.
Adversarial Training For Sketch Retrieval 5
Fig. 4: Overview: (A) Shows examples of raw Merchant Mark data, photographs
of the documents. (B) Shows examples extracted by hand from the raw Merchant
Mark dataset, a total of 2000 examples are collected. (C) An encoder is simply
a discriminator, taken from a trained GAN with the final layer removed. Rep-
resentations for both query and data samples are obtained by passing examples
through the encoder, the representations are used for retrieval.
3.1 Dataset Acquisition
The raw Merchant Mark dataset that we have been working with consists of 76
photographs of pages from the raw Merchant Mark dataset, similar to the one
shown in Fig.4A, which contain multiple Merchant Marks at different spatial
locations on the page. The focus of this paper is on retrieval of visually similar
examples rather than localisation, so the first step involved defining box regions
from which Merchant Mark training examples could be extracted. The extracted
examples are re-size to be 64× 64 pixels to form a suitable dataset for training
a GAN. In total there are 2000 training examples (see Fig.4B).
3.2 Learning an Encoder
Training A GAN To learn an encoding, the generator and the discriminator of
a GAN are trained iteratively, as proposed by Goodfellow et al. [3]. See pseudo-
code in Alg.1.
Network Architecture Both the generator, and the discriminator are convo-
lutional neural networks [13], using convolutions applied with strides rather than
pooling as suggested by Radford et al. [10]. In the discriminator, the image is
mapped to a single scalar label, so the stride applied in the convolutional layer of
the discriminator must be grater than 1. A stride of 2 is used in all convolutional
layers of the discriminator. In the generator, a vector is mapped to an image,
so a (positive) step size less than 1 is needed to increase the size of the image
after each convolution. A stride of 0.5 is used in all convolutional layers of the
generator.
Encoding Samples Having trained both the generator and the discriminator,
the discriminator can be detached from the GAN. To encode a sample, it is
6 Antonia Creswell & Anil Anthony Bharath
for Number of training iterations do
for k iterations do
sample pz(z) to get m random samples {z1...zm}
sample pdata(x) to get m random samples {x1...xm}
calculate the discriminator error:
JD = − 1
2m
(
m∑
i=1
logD(xi) +
m∑
i=1
log(1−D(G(zi)))
)
update θD using Adam [5] update rule.
end
sample pz(z) to get m random samples {z1...zm}
calculate the generator error:
JG = − 1
m
m∑
i=1
log(D(G(zi)))
update θG using Adam [5] update rule.
end
Algorithm 1: Training a GAN: After Goodfellow et al. [3] with changes to the
optimisation, using Adam [5] instead of batch gradient descent. Note, m is the
batch size and θG,θD are the weights of the generator, G and discriminator,
D.
passed through all but the last layer of the discriminator. The discriminative
network without the final layer is called the encoder. Both the query examples
and all examples in the dataset can be encoded using the this encoder. The
encoding is normalised to have unit length by dividing by the square root of the
sum of squared values in the encoding.
3.3 Retrieval
The objective is to retrieve samples that are visually similar to a query example.
To retrieve examples similar to the query, similarity measures are calculated
between the representation for the query and representations for all samples in
the dataset. Examples with the highest similarity scores are retrieved. The focus
of this paper is on learning a good representation for the data, for this reason a
simple similarity measure is used, the (normalised) dot product.
4 Experiments And Results
The purpose of these experiments is to show that GANs can be used to learn
a representation for our Merchant Mark dataset from only 2000 examples, that
can be used to precisely retrieve visually similar marks, given a query. We com-
pare invariance of feature representations learned and retrieval results from two
different networks to show that there is some benefit to using a network designed
specifically for learning representations for sketches.
Adversarial Training For Sketch Retrieval 7
4.1 GAN Architectures
Two different architectures were compared:
sketch-GAN We propose a novel GAN architecture inspired by Sketch-A-Net
[14], a network achieving state of the art recognition on sketches. Sketch-A-
Net employs larger filters in the shallower layers of the discriminative network
to capture structure of sketches rather than fine details which are absent in
sketches. This motivated our network design, using larger filters in the lower
levels of the discriminator and the deeper levels of the generator. This network
will be referred to as the sketch-GAN. This network has only 33k parameters.
thin-GAN A network similar to that proposed by Radford et al. [10] is used.
This network has very small filters, consistent with most of the state-of-the-
art natural image recognition networks [12]. The original network has 12.4M
parameters which would not compare fairly with the sketch-GAN, instead a
network with 1/16th of the filters in each layer is used, this will be referred to as
the thin-GAN and has 50k parameters. Full details of the architecture are given
in Table.1.
4.2 Details of Training
In adversarial training the generator and discriminator networks are competing
against eachother in a mini-max game, where the optimal solution is a Nash Equi-
librium [11]. Adversarial networks are trained iteratively alternating between the
generator and discriminator using gradient descent which aims to minimise the
individual cost functions of the generator and discriminator, rather than finding
a Nash Equilibrium [11]. For this reason convergence, during adversarial training
cannot be guaranteed [11][4]. During training we found that networks did not
converge, for this reason networks were trained for a fixed number of iterations,
rather than till the networks converged. The networks are trained for 2000 itera-
tions with batch size of 128 according to Alg. 1 [3], with k = 1, dz = 2, learning
rate = 0.002, and pz(z) ∼ U(0, 1). The networks were still able to learn features
useful for retrieval despite not converging.
4.3 Feature Invariance
Merchant Marks are hand drawn, which means that the marks are likely to vary
in both scale and orientation. It is therefore important to consider the rota-
tion and scale invariance of the representations that result from training. When
searching a document for Marks, one approach may be to apply a sliding box
search. The step size in sliding the search box will affect the computational feasi-
bility of the search. If a representation used for search is invariant to larger shifts,
then a sliding box search can be performed with a larger step size, making the
search more efficient. For this reason, shift invariance of the two representations
is also compared.
8 Antonia Creswell & Anil Anthony Bharath
Table 1: A summary of the network architectures used in this study. fc=fully
connected layer, c=convolutional layer with stride 2, d=convolutional layer with
stride 0.5, unless stated otherwise; for all cases, dz, the dimension of the random
valued vector input to the generator is 2. The ReLU activation function is used
in all hidden layers of all networks and the sigmoid activation is used in final
layer of each network.
thin-GAN:G thin-GAN:D
fc: 1024× dz, reshape(64,4,4) c: 8× 1× 3× 3
d: 32× 64× 3× 3 c: 16× 8× 3× 3
batch normalisation batch normalisation
d: 16× 32× 3× 3 c: 32× 16× 3× 3
batch normalisation batch normalisation
d: 8× 16× 3× 3 c: 64× 32× 3× 3
batch normalisation batch normalisation, reshape(1024)
d: 1× 8× 3× 3 fc: 1× 1024
sketch-GAN:G sketch-GAN:D
fc: 128× dz, reshape(8,4,4) c: 8× 1× 9× 9 (stride=1)
d: 16× 8× 3× 3 c: 16× 8× 5× 5
batch normalisation batch normalisation
d: 16× 16× 5× 5 c: 16× 16× 5× 5
batch normalisation batch normalisation
d: 16× 16× 5× 5 c: 16× 16× 5× 5
batch normalisation batch normalisation, reshape(1,1024)
d: 16× 16× 5× 5 fc: 1× 1024
batch normalisation -
d: 1× 16× 9× 9 (stride=1) -
Adversarial Training For Sketch Retrieval 9
Fig. 5: Invariance: Shows invariance of the sketch-GAN and thin-GAN represen-
tations to A) rotation, B) scale and C,D) translation.
10 Antonia Creswell & Anil Anthony Bharath
Invariance To Rotation To assess the degree of rotation invariance within
the two representations, 100 samples were randomly taken from the Merchant
Mark dataset and rotated between the angles of −10 and 10 degrees. At each
0.5 degree increment, the samples were encoded and the similarity score between
the rotated sample and the sample at 0 degrees was calculated. The similarity
score used was the normalised dot product, since this was also the measure used
for retrieval. The results are shown in the top left of Fig.5. It is clear that the
sketch-GAN encoding is more tolerant to rotation than the thin-GAN encoding.
Note that the background of the rotated samples were set to 0 to match the
background of the samples.
Invariance To Scale A similar approach was used to assess the degree of scale
invariance within the two networks. Again, 100 samples were randomly taken
from the Merchant Mark dataset, and scaled by a factor between 0.5 and 1.5.
At each increment of 0.05, the scaled sample was encoded and a similarity score
was calculated between the scaled samples and the sample at scale 1. The results
are shown in the top right of Fig.5. Note, that when the scaling factor is < 1
the scaled image is padded with zeros to preserve the 64× 64 image size. When
scaling with a factor > 1, the example is scaled and cropped to be of size 64×64.
The bounding box of the unscaled marks is tight, which means that at higher
scaling factors parts of the marks are sometimes cropped out. Despite this, the
sketch-GAN encoding is able to cope better with up-scaling compared to down-
scaling. The sketch-GAN encoder generally outperforms the thin-GAN encoder,
particularly for up-scaling.
Invariance To Shift Finally, we compared the shift invariance of the two
encoders. Sampling 100 marks from the merchant mark dataset, and applying
shifts between −10 and 10 pixels in increments of 1 pixel in both the x and y
directions. The results are shown as a heat map in Fig.5, where the sketch-GAN
encoding appears to be more invariant to shift than the thin-GAN encoding.
4.4 Retrieval
For the retrieval experiments, 500 queries were taken at random from the training
dataset and used to query the whole dataset using features from the sketch-GAN.
The top 9 matches were retrieved, where the first retrieval is the example itself
and the rest are examples that the system thinks are similar. The results from
some of these queries are shown in Fig.6b. The same query examples were used
to query the dataset using the features from the thin-GAN, the results of these
queries are shown in Fig.6a.
Retrieval results using trained sketch-GAN encoder Results show that
using the sketch-GAN encoder for Merchant Marks retrieval (Fig.6b) allows
retrieval of examples that have multiple similar parts for example results for
Adversarial Training For Sketch Retrieval 11
(a) thin-GAN (b) sketch-GAN
Fig. 6: Retrieval examples using different GAN architectures. Each sub-figure
shows 15 retrievals where the 1st example, in a purple box, in each row is the
query and the following images on each row are the top 8 retrievals. (a) Shows
retrievals using the thin-GAN encoder and (b) shows retrievals using the sketch-
GAN encoder.
queries #4,#8,#11,#14 and #15 consistently retrieve examples with at least
two similar parts (Fig.2). Specifically, most retrievals for query #15, Fig.6b have
parts 12 and 19 from Fig.2. Exact matches are found for retrievals #4,#5,#8
and #10. Specifically, query #10 finds an exact match despite the most similar
example being shifted upwards and rotated slightly. Retrievals for query #6 finds
an exact match but does not rank the retrieval as high as non-exact matches,
suggesting that there is still room for improvement in the representations that
are learned.
Retrieval results using trained thin-GAN encoder On visual inspection
of the retrieval results that use the thin-GAN encoder, it is clear that they under
perform compared to the sketch-GAN for the same query examples, with fewer
visually similar examples. The thin-GAN encoder fails to find exact matches for
4,5 and 10. Failure to find a match for 10 further suggests that the thin-GAN is
less invariant to rotation.
12 Antonia Creswell & Anil Anthony Bharath
5 Conclusions
Convolutional networks contain, at a minimum, tens of thousands of weights.
Training such networks has typically relied on the availability of large quantities
of labelled data. Learning network weights that provide good image representa-
tions in the absence of class labels is an attractive proposition for many problems.
One approach to training in the absence of class labels is to encourage networks
to compete in coupled tasks of image synthesis and discrimination. The question
is whether such Generative Adversarial Networks can learn feature representa-
tions suitable for retrieval in a way that matches human perception.
We have found that GANs can indeed be used to learn representations that
are suitable for image retrieval. To demonstrate this, we compared the represen-
tation learned by GANs that were trained on Merchant Marks. We compared two
related architectures, sketch-GAN and thin-GAN ; sketch-GAN has an architec-
tural design that is more appropriate for performing generation and discrimina-
tion of sketches. Our experiments showed that GANs are suitable for retrieval
of both visually similar and exact examples. Experiments also showed that the
features that were learned by the sketch-GAN were, on average, more robust
to small image perturbations in scale, rotation and shift than the thin-GAN.
Further, retrieval results when using the sketch-GAN appeared more consistent
than in using thin-GAN.
More generally, the experiments suggest that adversarial training can be used
to train convolutional networks for the purpose of learning good representations
for the retrieval of perceptually similar samples; this can be achieved without the
level of labelling and examples required for non-adversarial training approaches.
This broadens the scope of deep networks to problems of perceptually similar
retrieval in the absence of class labels, a problem that is increasingly of interest
in heritage collections of images.
Acknowledgements We would like to acknowledge Nancy Bell, Head of Col-
lection Care at the National Archives. We would also like to acknowledge the
Engineering and Physical Sciences Research Council for funding through a Doc-
toral Training studentship.
References
1. Crowley, E.J., Parkhi, O.M., Zisserman, A.: Face painting: querying art with pho-
tos. In: British Machine Vision Conference (2015)
2. Crowley, E.J., Zisserman, A.: In search of art. In: Workshop on Computer Vision
for Art Analysis, ECCV (2014)
3. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in Neural
Information Processing Systems. pp. 2672–2680 (2014)
Adversarial Training For Sketch Retrieval 13
4. Goodfellow, I.J.: On distinguishability criteria for estimating generative models.
arXiv preprint arXiv:1412.6515 (2014)
5. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
6. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-
volutional neural networks. In: Advances in neural information processing systems.
pp. 1097–1105 (2012)
8. Makhzani, A., Frey, B.J.: Winner-take-all autoencoders. In: Advances in Neural
Information Processing Systems. pp. 2773–2781 (2015)
9. Noh, H., Hong, S., Han, B.: Learning deconvolution network for semantic segmen-
tation. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 1520–1528 (2015)
10. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434
(2015)
11. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:
Improved techniques for training gans. arXiv preprint arXiv:1606.03498 (2016)
12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
13. Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplic-
ity: The all convolutional net. arXiv preprint arXiv:1412.6806 (2014)
14. Yu, Q., Yang, Y., Song, Y.Z., Xiang, T., Hospedales, T.M.: Sketch-a-net that beats
humans. In: Proceedings of the British Machine Vision Conference (BMVC). pp.
7–1 (2015)
15. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: Computer vision–ECCV 2014, pp. 818–833. Springer (2014)
"
19,"Reduced Memory Region Based Deep
Convolutional Neural Network Detection
Denis Tome´, Luca Bondi, Luca Baroffio, Stefano Tubaro
Dipartimento di Elettronica, Informazione e Bioingegneria
Politecnico di Milano
Milano, Italy
denis.tome@mail.polimi.it, luca.bondi@polimi.it,
luca.baroffio@polimi.it, stefano.tubaro@polimi.it
Emanuele Plebani, Danilo Pau
Advanced System Technology
STMicroelectronics
Agrate Brianza, Italy
emanuele.plebani1@st.com, danilo.pau@st.com
Abstract—Accurate pedestrian detection has a primary role
in automotive safety: for example, by issuing warnings to the
driver or acting actively on cars brakes, it helps decreasing
the probability of injuries and human fatalities. In order to
achieve very high accuracy, recent pedestrian detectors have been
based on Convolutional Neural Networks (CNN). Unfortunately,
such approaches require vast amounts of computational power
and memory, preventing efficient implementations on embedded
systems. This work proposes a CNN-based detector, adapting a
general-purpose convolutional network to the task at hand. By
thoroughly analyzing and optimizing each step of the detection
pipeline, we develop an architecture that outperforms methods
based on traditional image features and achieves an accuracy
close to the state-of-the-art while having low computational
complexity. Furthermore, the model is compressed in order to
fit the tight constrains of low power devices with a limited
amount of embedded memory available. This paper makes two
main contributions: (1) it proves that a region based deep neural
network can be finely tuned to achieve adequate accuracy for
pedestrian detection (2) it achieves a very low memory usage
without reducing detection accuracy on the Caltech Pedestrian
dataset.
Keywords—Window proposals, CNN, Object Detection, fine
tuning, embedded systems
I. INTRODUCTION
Figures form NHTSAs Fatal Analysis Reporting System
(FARS) in 2014 show that 32,675 people died in motor
vehicle crashes and the fatality rate for 2015 is estimated to
reach 1.17 deaths per 100 million vehicle miles traveled [1].
These numbers show the importance of building automated
vision systems for pedestrian and car detection. As the world
urbanizes more and more, accidents involve 33K lives, 250K
disabilities and 2M injuries accounting for $300B of damage.
In 95% of the cases human error is the cause, mostly by
passenger distraction or changes in traffic / road / environ-
mental conditions realized too late. This situation calls for
urgent actions by the automotive industry in order to react and
propose advanced safety measures to the driver, and visual
object detection is instrumental to that need.
The most successful and accurate approaches in object
detection are based on convolutional neural networks (CNN)
which have significantly outperformed methods based on
densely extracted features [2]. CNNs integrate the feature
extraction and feature classification stages of an object detector
in an end-to-end approach by training the model parameters on
a large dataset. However, the resulting models are characterized
by a large computational complexity and number of parame-
ters: for example, the successful AlexNet model [3] requires
one billion floating point operations per classification and 60
million parameters, corresponding to 217 MB of parameters
memory in single precision floating point. The more accurate
VGG networks [4] can require up to 40 billion floating point
operations and 500 MB of parameter memory.
The complexity of CNNs prevents performing classification
at every potential position and scale and thus a widely used
approach, first proposed in [5], is to use a object proposal
mechanism, where only “object-like” regions are processed
by the network. The computation can be further sped-up by
using Graphic Processing Units (GPU) or specialized hardware
(such as [6]). However, the memory required for the parameters
puts severe constraints on embedded platforms, where a larger
amount of on-chip memory increases costs and access to an
external Dynamic Random Access Memory (DRAM) requires
two orders of magnitude more energy than accessing local
Static RAM (SRAM) caches [7]. This motivates developing
a fully embedded memory implementation of CNN, where
suitable compression schemes are applied to the network
weights while minimizing the loss in accuracy.
This paper proposes a set of strategies tailored at signifi-
cantly reducing the amount of space required by the parameters
of a convolutional neural network. Starting from DeepPed, an
optimized pedestrian detection pipeline we previously devel-
oped [8], we reduce the redundancy of the parameters with
two approaches inspired by [7]: by compressing the individual
weights through k-means quantization, and by pruning weights
with small absolute value. We evaluate both approaches sep-
arately and in combination, in order to find the best trade-off
between accuracy and memory requirements.
The rest of this paper is organized as follows. In section
II a review of the state of the art about neural network com-
pression is offered to the reader. Sections III and IV illustrate
respectively the proposed detection pipeline and the model
compression scheme. In section V numerical results from
experiments on the Caltech Pedestrian dataset are presented
and discussed. Finally, in section VI some conclusions are
drawn.
ar
X
iv
:1
60
9.
02
50
0v
1 
 [c
s.C
V]
  8
 Se
p 2
01
6
1Region 
proposal
Feature 
extraction
Region 
classification
• Sliding window
• Selective Search
• ...
• HOG
• ACF / LDCF
• CNN
• ...
• Adaboost
• Support Vector Machines
• ...
(a)
2
ACF / 
LDCF
CNN
SVM 
level 1 SVM 
level 2
Image
regions features scores
scores
Detections
(b)
Fig. 1. Detector pipelines. In (a), a typical pedestrian detection pipeline in the state of the art methods. In (b), proposed pipeline for Region based Convolutional
Neural Networks (R-CNN).
II. REVIEW OF THE STATE OF THE ART
Starting with the introduction of AlexNet in 2012, which
won by a large margin the ImageNet Large Scale Visual
Recognition Challenge [3], deep convolutional neural networks
have become the dominant approach in image classification,
recently achieving human-level performances [9]. The seminal
work by Girschick on Regions with convolutional neural net-
works (R-CNN) [5] extended those results to object detection.
In the R-CNN paradigm, detection is performed in two stages:
first, a separate algorithm generates candidate object proposals
based on e.g. region segmentation or edges; then, a CNN is run
on the proposals to generate the final detection results. While
for generic object detection an object agnostic method such
as Selective Search [10] is necessary, for more focused tasks
like pedestrian detection such proposals are under-performing
compared to the state of the art [2] and by using an accurate
pedestrian detector as proposal method, significant gains can be
achieved. Moreover, the proposals generated by a specialized
object detector have a higher probability of containing the
object of interest, reducing the number of proposal regions
needed: as shown by [2] in the supplemental material, only 3
proposal per images are enough to reach a recall of more than
90%. While methods for neural network compression have
a long history [11], the pace of research has accelerated in
response to the large networks introduced after 2012. Denil et
al. demonstrated that the parameters of deep neural networks
are highly redundant and can be reduced up to 20× with
no appreciable loss of accuracy, giving a strong incentive to
network compression.
Early approaches are based on enforcing weight sparsity,
either through low-rank approximations [12] or by an oppor-
tune regularization term [13] [14], achieving a compression
ratio up to 20× at the price of a 1-2% loss of accuracy. The
convolutional layers of the network can be compressed either
by 1-rank approximation [15], filter decomposition [16] or
Tucker decomposition [17], achieving a compression rate of
5×. The sparsity of convolutional layers has the added benefit
of reducing the number of operations, up to 4× in the case of
[14]. Other approaches replace the fully connected layers of
the network (the ones contributing the most to the number of
parameters) with a different kind of layer with a lower number
of parameters. Examples are kernel machines [18], tensors
[19], circulant matrices [20] or a cascade of diagonalized
matrices [21]; convolutional layers can also be replaced by
separable filters [22]. Alternatively, the network weights can be
compressed by an hashing function and the hashing trick used
to carry out computations directly in the compressed space.
Stages with 1×1 convolutions are an effective way of reducing
the number of parameters in a network and they are often
used in very deep networks such as the Inception architecture
[23] and in Residual Networks [24]; however, when applied
to smaller networks can achieve the same level of accuracy as
AlexNet with 40× less parameters [25].
However, a simple strategy based on scalar quantization of
the weights [26] and connection pruning [27] is surprisingly
effective and with network retraining achieves a 37× com-
pression on AlexNet with almost no loss in performance [7].
The performances of this approach are further improved by
enforcing a layer-wise reconstruction penalty to the quantized
weights [28].
III. PROPOSED PIPELINE
The baseline CNN detector, dubbed DeepPed [8], follows
from [2] in combining R-CNN with an efficient pedestrian
detector used as proposal method. The Aggregated Channel
Features (ACF) detector [29] has been chosen for its speed
and accuracy. An improved version of ACF, known as Locally
Decorrelated Channel Features (LDCF) [30], was also taken
into account, but despite its higher accuracy it was discarded
due to its computational complexity, which is ten times higher
than ACF.
In DeepPed, an input image is analyzed by ACF and several
regions are proposed as potential pedestrians, with a score
associated to each region. A pre-trained AlexNet network,
trained on the 1000 categories of the ImageNet challenge
and publicly available1, is used as starting point. The last
classification layer, which is application-specific, is removed
and and replaced with a Support Vector Machine (SVM). The
model is then retrained by fine-tuning on the Caltech Pedes-
trian training dataset [31] sub-sampled by 3× as suggested
by [29] and using 6-fold cross-validation to select the best
performing network. The training examples are pedestrian and
non-pedestrian windows chosen by ACF and selected among
the highest-scoring proposals. The ACF score and the SVM
score from the CNN are then further combined by stacking
a second SVM trained on the validation set, which gives the
final detection score.
Figure 2 shows the performance of the final DeepPed
pipeline compared with other state-of-the art approaches in
pedestrian detection. The details of the algorithm are discussed
in [8], where each step of the pipeline is analyzed and
optimized in order to increase the final detection accuracy. In
the following sections, we focus on reducing the space required
to store the parameters of this network.
1BVLC AlexNet Model in Caffe: https://github.com/BVLC/caffe/tree/
master/models/bvlc alexnet
10 -3 10 -2 10 -1 100 101
false positives per image
.05
.10
.20
.30
.40
.50
.64
.80
1
m
is
s 
ra
te
0.298 ACF-Caltech+ [17]
0.248 LDCF [17]
0.233 SCF+AlexNet [12]
0.225 Katamari [2]
0.219 SpatialPooling+ [21]
0.209 TA-CNN [27]
0.199 DeepPed
0.185 Checkerboards [31]
0.173 CCF+CF [30]
Fig. 2. Comparison between the proposed DeepPed (solid black line) and
other popular pedestrian detection algorithms. Note that the proposed method
does not make use of multiple frame information, i.e. it does not exploit the
optical flow between contiguous frames.
IV. MODEL COMPRESSION
In order to reduce the parameter memory, two strategies
inspired by [7] have been chosen to compress the network
weights: scalar quantization and weight pruning. In the first
case, we consider for each layer the distribution of individual
weights and we quantize their values with k-means using a
variable number of centroids; in the second case, we set to
zero the weights with the lowest absolute value, using different
values for the threshold in order to change the proportion of
non-zero weights. Finally, the two approaches are combined,
either by quantizing and then pruning the resulting weights, or
by pruning and then quantizing the resulting distribution. As
the experiments in Section V will show, the two approaches
are largely independent one from the other and thus the
compression factors can be composed maintaining roughly the
same accuracy level.
More in detail, the following procedures have been used to
compress the weights:
• Scalar quantization: each CNN layer is compressed
individually. All the weight values in the layer param-
eters are clustered using the k-means algorithm, where
the number of centroids is chosen as a function of the
compression factor. Assuming that the uncompressed
weights are represented each with B bits, the number
of centroids is:
ncentroids = 2
B
fcompr
Henceforth, we will consider a single precision float-
ing point representation (B = 32) for the uncom-
pressed weights. The maximum achievable compres-
sion rate is thus 32 (ncentroids = 2).
• Pruning: each CNN layer is compressed individually.
Weights whose absolute value is smaller than a thresh-
old are zeroed out, i.e.:
wi,j =
{
0, if |wi,j | < threshold
wi,j , otherwise
The threshold is set as the pth percentile of the weight
distribution, where p = 100 · (1 − 1f ) and f is the
compression factor; i.e. the 75th percentile is chosen
for f = 4.
• Quantization and Pruning: The combination of both
quantization and pruning lets the strengths of one
method compensate the shortcomings of the other. As
shown in V, a higher level of compression can be
achieved with respect to the single methods alone,
while reaching the same level of accuracy.
Differently for quantization, the distribution of pruned
connections needs to be stored, increasing the storage require-
ments. For example, if a binary map coding the pruned or not-
pruned status of the connections is used, the fully connected
layers of DeepPed would require a map of 6.8 MB. However,
in a scenario in which a hardware designer can draw on an
Application Specific Integrated Circuit (ASIC) or on an Field
Programmable Gate Array (FPGA) all the connections between
neurons individually, the pruned weights would translate in
missing connections, and since power consumption increases
linearly with wire capacity loads, the pruned connections
would reduce power, space and storage usage.
V. EXPERIMENTAL EVALUATION AND RESULTS
In the experiments, we tested the following approaches:
• scalar quantization alone
• weight pruning alone
• weight pruning followed by scalar quantization
Moreover, we tested the compression separately on two
different set of layers:
• compressing the convolutional layers
• compressing the fully connected layers
Most of the network parameters reside in the fully con-
nected layers, and thus the latter set is the most important;
however, to reach high compression rates, both sets of layers
need to be taken into account. The target size is 4 MB, a typical
size for local SRAM in embedded platforms.
To assess the impact of compression on the network ac-
curacy we resort to the evaluation metrics proposed by Dolla´r
at al. [31] on the Caltech Pedestrian dataset. In particular, the
performance of the different methods is evaluated in terms
of trade-off between the compression factor (CF) and the
log average miss rate (LAMR), as measured on the Caltech
Pedestrian test set. The LAMR metric computes the geometric
mean of the miss rate in the interval between 0.01 false
detections per frame and 1 false detections per frame and can
be interpreted as a smoothed estimate of the miss rate at 0.1
false detections per frame. Besides the initial fine-tuning of the
DeepPed uncompressed model, the methods we tested do not
require additional training, so only the test set has been used
0.280
0.315
0.350
0.385
0.420
0 2.5 5 7.5 10
LO
G	  
AV
ER
AG
E	  
M
IS
S	  
RA
TE
COMPRESSION	  FACTOR
Quantization Pruning
(a)
0.280
0.355
0.430
0.505
0.580
1 10 100 1000
LO
G	  
AV
ER
AG
E	  
M
IS
S	  
RA
TE
COMPRESSION	  FACTOR
Quantization Pruning Quantization	  and	  Pruning
(b)
Fig. 3. DeepPed results. In (a), comparison between compression by scalar quantization and by pruning in convolutional layers: the pruning method has a
dramatic impact on the information content. In (b), comparison between compression by scalar quantization and by pruning in fully connected layers.
TABLE I. FINAL COMPRESSED MODEL. COMPRESSION FACTORS AND SIZES PER LAYER.
Layer CF Quantization CF Pruning Original size (MB) Compressed size (MB)
Conv 1 3.56 - 0.13 0.037
Conv 2 4 - 1.17 0.293
Conv 3 4 2 3.38 0.422
Conv 4 4 2 2.53 0.319
Conv 5 4 - 1.69 0.422
Fully Connected 1 16 4 144.02 0.66
Fully Connected 2 16 4 64.02 1.35
Total 61.92 216.94 3.50
in the experiments. The original DeepPed model with ACF
proposals reaches a LAMR of 28.3%.
We assess the effect of scalar quantization by measuring the
accuracy of the model after compressing the fully connected
(fc) layers at a factor of 8, 10.7, 16 and 32 (4, 3, 2 and 1 bit
per weight) and keeping the convolutional layers unchanged.
Likewise, we assess pruning by reducing the number of con-
nections by 2, 4, 8, 16 and 32 times. The results are shown in
Figure 3(b), with the compression factors in logarithmic scale:
quantization is more efficient than pruning, and compression
up to 8× can be achieved without appreciable loss of accuracy.
In the case of convolutional (conv) layers, we observe that
the first layer is strongly affected by compression, while the
fourth and fifth layers are more resilient. For this reason, the
first layer is always compressed with 9 bits (CF=3.56) and
never pruned; the remaining layers are compressed or pruned
with increasingly high factors. Scalar quantization is tested at
a factor of 3.56, 4, 6, 7, 8.5 and 10; pruning is tested by using
the factors (1, 2, 2, 2), (2, 2, 2, 4) and (2, 2, 4, 4) for the con-
volutional layers from 2 to 5, resulting in overall compression
factors for convolutional layers at 1.74, 2.17 and 2.57. The
results are shown in Figure 3(a): the performances degrade
much faster than in the case of fully connected layers, as
expected from the fact that weight sharing in the convolutional
layers already counts as a form of parameter reduction. As
in the case of fully connected layers, convolutional layers
are more robust to scalar quantization, achieving compression
factor up to 6× with a small cost in accuracy. Pruning instead
leads to a rapid degradation of performances with factors
greater than 2×, showing that throwing away weights with
small coefficients is not desirable in convolutional layers, since
the effect of these small contributions greatly influences the
final accuracy.
The two methods are combined in the case of fully con-
nected layers, where we prune 23 of the connections and we
quantize the remaining weights with 1 bit, resulting in a com-
pression factor of 102×. As Figure 3(b) shows, combining the
two approaches actually helps both, and despite a difference
of an order of magnitude in compression factor, the accuracy
is comparable to scalar quantization at 10×. Pruning even
improves the results of quantization at 1 bit, because now the
centroids need not to fit irrelevant weights. By compressing
only the fully connected weights, the model already reaches a
size of 10.9 MB.
We finally combine compression of convolutional and fully
connected layers. Table II summarizes the performances for
the best model in each scenario and for the final selected
model; for the final model, Table I shows the compression
factors and sizes layer by layer. The result is a model with
a total size of 3.5 MB and an overall compression factor of
61.92. The final model accuracy is only slightly worse than the
accuracy reachable when compressing only the fully connected
layers (LAMR from 28.6% to 28.7%), despite requiring less
than one third the size of the latter. Moreover, when fully
connected compression is applied together with convolutional
TABLE II. BEST PERFORMING COMPRESSED MODELS.
Layers Compression Size (MB) LAMRQuant. Pruning Original Compr.
conv1-5 5.94× 2.23 1.5 29.2% (+0.9%)
fc6-7 102× 208.04 2.04 28.6% (+0.3%)
Total 61.35× 216.93 3.54 28.7% (+0.4%)
compression, the overall accuracy increases with respect to the
case when only convolutional layers are compressed (LAMR
from 29.2% to 28.7%). A possible motivation for this behavior
is related to the “screening” capabilities of quantization applied
on fully connected layers, acting as a filter on the noise
generated in convolutional layers compression. Since 4 MB
of embedded memory are viable in the 28 nm Fully Depleted
Silicon On Insulator (FDSOI) STMicroelectronics fabrication
process, this model enables several low-power and low-cost
embedded applications.
The DeepPed architecture combined with the proposed
compression scheme has been ported on an NVIDIA Jetson
TK1 board and integrated with an optimized implementation
of the ACF detector. The detector is capable of running at 2.4
frames per second (fps) when processing 5 proposal per frame.
Moreover, by applying a tracking-by-detection algorithm such
as [32] and using an average tracking length of 5 frames, the
speed of the detector increases up to 10 fps, most of it spent
in the CNN evaluation stage.
VI. CONCLUSIONS
In this paper we present a detailed study of the effect
of neural network compression in the case of a pedestrian
detector and we show that a combination of simple yet
effective methods allows to significanlty reduce the memory
required to store the network parameters, achieving a final
compression factor of 62×. The behavior analysis of both con-
volutional and fully connected layers under scalar quantization
and connection pruning shows that while the fully connected
layers are the most redundant ones, high compression rates
are achievable on convolutional layers with a small effect on
the final accuracy. Thus, with a proper choice of compression
parameters the accuracy of the system is preserved while
allowing the development of embedded architectures at a small
cost and low power consumption.
REFERENCES
[1] N. C. for Statistics and Analysis, “Early Estimate of Motor Vehicle
Traffic Fatalities for the First Nine Months (JanSep) of 2015,” National
Highway Traffic Safety Administration, Washington DC, Tech. Rep.,
2016.
[2] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a Deeper
Look at Pedestrians,” in CVPR, Boston, MA, jan 2015.
[3] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification
with deep convolutional neural networks,” in NIPS, Lake Tahoe, NV,
2012, pp. 1–9.
[4] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks
for Large-Scale Image Recognition,” Google Research, Tech. Rep., sep
2014.
[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in CVPR, Columbus, OH, nov 2014.
[6] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J.
Dally, “EIE: Efficient Inference Engine on Compressed Deep Neural
Network,” in ISCA, feb 2016.
[7] S. Han, H. Mao, and W. J. Dally, “Deep Compression: Compressing
Deep Neural Networks with Pruning, Trained Quantization and Huff-
man Coding,” in ICLR, San Juan, Puerto Rico, oct 2016.
[8] D. Tome`, F. Monti, L. Baroffio, L. Bondi, M. Tagliasacchi, and
S. Tubaro, “Deep convolutional neural networks for pedestrian detec-
tion,” Politecnico di Milano, Tech. Rep., oct 2015.
[9] K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classification,” in
CVPR, Boston, MA, feb 2015.
[10] J. R. R. Uijlings, K. E. A. Van De Sande, T. Gevers, and A. W. M.
Smeulders, “Selective search for object recognition,” International
Journal of Computer Vision, vol. 104, pp. 154–171, 2013.
[11] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal Brain Damage,” in
NIPS, no. 598-605, Denver, CO, 1990.
[12] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, “Exploiting
Linear Structure Within Convolutional Networks for Efficient Evalua-
tion,” in NIPS, Montreal, Canada, apr 2014.
[13] M. D. Collins and P. Kohli, “Memory Bounded Deep Convolutional
Networks,” Microsoft Research, Tech. Rep., dec 2014.
[14] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Penksy, “Sparse
Convolutional Neural Networks,” in CVPR. Boston, MA: IEEE, jun
2015, pp. 806–814.
[15] M. Jaderberg, A. Vedaldi, and A. Zisserman, “Speeding up Convolu-
tional Neural Networks with Low Rank Expansions,” Arxiv, may 2014.
[16] X. Zhang, J. Zou, K. He, and J. Sun, “Accelerating Very Deep
Convolutional Networks for Classification and Detection,” Arxiv, may
2015.
[17] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compres-
sion of Deep Convolutional Neural Networks for Fast and Low Power
Mobile Applications,” in ICLR, San Juan, Puerto Rico, may 2016.
[18] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song,
and Z. Wang, “Deep Fried Convnets,” in CVPR, Boston, MA, dec 2015,
p. 9.
[19] A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov, “Tensorizing
Neural Networks,” in NIPS, Motreal, Canada, sep 2015.
[20] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-
F. Chang, “An exploration of parameter redundancy in deep networks
with circulant projections,” in ICCV, Santiago, Chile, feb 2015.
[21] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas, “ACDC: A
Structured Efficient Linear Layer,” in ICLR, San Juan, Puerto Rico, may
2016.
[22] J. Jin, A. Dundar, and E. Culurciello, “Flattened Convolutional Neural
Networks for Feedforward Acceleration,” in ICLR, San Diego (CA),
dec 2015.
[23] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,”
in CVPR, Boston, MA, jun 2015.
[24] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for
Image Recognition,” Microsoft Research, Tech. Rep., dec 2015.
[25] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally,
and K. Keutzer, “SqueezeNet: AlexNet-level accuracy with 50x fewer
parameters and <1MB model size,” DeepScale, Tech. Rep., feb 2016.
[26] Y. Gong, L. Liu, M. Yang, and L. Bourdev, “Compressing Deep Convo-
lutional Networks using Vector Quantization,” Facebook AI Research,
Tech. Rep., dec 2014.
[27] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both Weights and
Connections for Efficient Neural Networks,” in NIPS, Montreal, Canada,
jun 2015.
[28] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng, “Quantized Convo-
lutional Neural Networks for Mobile Devices,” Arxiv 2016, p. 11, dec
2016.
[29] P. Dollar, R. Appel, S. Belongie, and P. Perona, “Fast Feature Pyramids
for Object Detection,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, pp. 1–14, 2014.
[30] W. Nam, P. Dollar, and J. H. Han, “Local Decorrelation for Improved
Pedestrian Detection,” in NIPS, Montreal, Canada, 2014, pp. 1–9.
[31] P. Dolla´r, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
an evaluation of the state of the art,” Pattern Analysis and Machine
Intelligence, vol. 34, no. 4, pp. 743–61, 2012.
[32] M. D. Breitenstein, F. Reichlin, B. Leibe, E. Koller-Meier, and L. Van
Gool, “Online Multiperson Tracking-by-Detection from a Single, Un-
calibrated Camera.” IEEE transactions on pattern analysis and machine
intelligence, vol. 33, no. 9, pp. 1820–33, sep 2011.
"
20,"ar
X
iv
:1
50
3.
01
91
9v
1 
 [q
-b
io.
QM
]  6
 M
ar 
20
15
Convolutional LSTM Networks for Subcellular Localization of Proteins
Søren Kaae Sønderby1* SKAAESONDERBY@GMAIL.COM
Casper Kaae Sønderby1* CASPERKAAE@GMAIL.COM
Henrik Nielsen3 HNIELSEN@CBS.DTU.DK
Ole Winther1,2 OLWI@DTU.DK
1 Bioinformatics Centre, Department of Biology, University of Copenhagen, Copenhagen, Denmark
2 Department for Applied Mathematics and Computer Science, Technical University of Denmark, 2800 Lyngby, Denmark
3 Center for Biological Sequence Analysis, Department of Systems Biology, Technical University of Denmark, 2800
Lyngby, Denmark
* These authors contributed equally to the work
Abstract
Machine learning is widely used to analyze biological sequence data. Non-sequential models such as SVMs
or feed-forward neural networks are often used although they have no natural way of handling sequences of
varying length. Recurrent neural networks such as the long short term memory (LSTM) model on the other hand
are designed to handle sequences. In this study we demonstrate that LSTM networks predict the subcellular
location of proteins given only the protein sequence with high accuracy (0.902) outperforming current state of
the art algorithms. We further improve the performance by introducing convolutional filters and experiment with
an attention mechanism which lets the LSTM focus on specific parts of the protein. Lastly we introduce new
visualizations of both the convolutional filters and the attention mechanisms and show how they can be used to
extract biological relevant knowledge from the LSTM networks.
1. INTRODUCTION
Deep neural networks have gained popularity for a wide range of classification tasks in image recognition and speech
tagging (Dahl et al., 2012; Krizhevsky et al., 2012) and recently also within biology for prediction of exon skipping events
(Xiong et al., 2014). Furthermore a surge of interest in recurrent neural networks (RNN) has followed the recent impres-
sive results shown on challenging sequential problems like machine translation and speech recognition (Bahdanau et al.,
2014; Graves & Jaitly, 2014; Sutskever et al., 2014). Within biology, sequence analysis is a very common task used for
prediction of features in protein or nucleic acid sequences. Current methods generally rely on neural networks and sup-
port vector machines (SVM), which have no natural way of handling sequences of varying length. Furthermore these
systems rely on highly hand-engineered input features requiring a high degree of domain knowledge when designing the
algorithms (Emanuelsson et al., 2007; Petersen et al., 2011). This paper uses the long short term memory network (LSTM)
(Hochreiter et al., 1997) to analyze biological sequences and predict to which subcellular compartment a protein belongs.
This prediction task, known as protein sorting or subcellular localization, has attracted large interest in the bioinformat-
ics field (Emanuelsson et al., 2007). We show that an LSTM network, using only the protein sequence information, has
significantly better performance than current state of the art SVMs and furthermore have nearly as good performance as
large hand engineered systems relying on extensive metadata such as GO terms and evolutionary phylogeny, see Figure 4
(Blum et al., 2009; Briesemeister et al., 2009; Ho¨glund et al., 2006). These results show that LSTM networks are efficient
algorithms that can be trained even on relatively small datasets of around 6000 protein sequences. Secondly we investigate
how an LSTM network recognizes the sequence. In image recognition, convolutional neural networks (CNN) have shown
state of the art performance in several different tasks (Cunn et al., 1990; Krizhevsky et al., 2012). Here the lower layers
of a CNN can often be interpreted as feature detectors recognizing simple geometric entities, see Figure 1. We develop a
Convolutional LSTM Networks for Subcellular Localization of Proteins
simple visualization technique for convolutional filters trained on either DNA or amino acid sequences and show that in
the biological setting filters can be interpreted as motif detectors, as visualized in Figure 1. Thirdly, inspired by the work of
Figure 1. Left: First layer convolutional filters learned in (Krizhevsky et al., 2012), note that many filters are edge detectors or color
detectors. Right: Example of learned filter on amino acid sequence data, note that this filter is sensitive to positively charged amino
acids.
Bahdanau et al., we augment the LSTM network with an attention mechanism that learns to assign importance to specific
parts of the protein sequence. Using the attention mechanism we can visualize where the LSTM assigns importance, and
we show that the network focuses on regions that are biologically plausible. Lastly we show that the LSTM network learns
a fixed length representation of amino acids sequences that, when visualized, separates the sequences into clusters with
biological meaning. The contributions of this paper are:
1. We show that LSTM networks combined with convolutions are efficient for predicting subcellular localization of
proteins from sequence.
2. We show that convolutional filters can be used for amino acid sequence analysis and introduce a visualization tech-
nique.
3. We investigate an attention mechanism that lets us visualize where the LSTM network focuses.
4. We show that the LSTM network effectively extracts a fixed length representation of variable length proteins.
2. MATERIALS AND METHODS
2.1. MODEL
This section introduces the LSTM cell and then explains how a regular LSTM (R-LSTM) can produce a single output.
We then introduce the LSTM with attention mechanism (A-LSTM), and describes how the attention mechanism is imple-
mented.
2.1.1. LSTM NETWORK
The LSTM cell is implemented as described in (Graves, 2013) except for peepholes, because recent papers have shown
good performance without (Sutskever et al., 2014; Zaremba & Sutskever, 2014; Zaremba et al., 2014b). Figure 2 shows
Convolutional LSTM Networks for Subcellular Localization of Proteins
the LSTM cell. Equations (1)-(10) state the forward recursions for a single LSTM layer.
it = σ(D(xt)Wxi + ht−1Whi + bi) (1)
ft = σ(D(xt)Wxf + ht−1Whf + bf ) (2)
gt = tanh(D(xtWxg) + ht−1Whg + bg) (3)
ct = ft ⊙ ct−1 + it ⊙ gt (4)
ot = σ(D(xt)Wxo + ht−1Who + bo) (5)
ht = ot ⊙ tanh(ct) (6)
σ(z) =
1
1 + exp(−z)
(7)
⊙ : Elementwise multiplication (8)
D : Dropout, set values to zero with probability p (9)
xt : input from the previous layer: hl−1t (10)
Where all quantities are given as row-vectors and activation and dropout functions are applied element-wise. If dropout is
used it is only applied to non-recurrent connections in the LSTM cell (Zaremba et al., 2014a). In a multilayer LSTM ht is
passed upwards to the next layer.
Figure 2. LSTM memory cell. i: input gate, f : forget gate, o:
output gate, g: input modulation gate, c: memory cell. The
Blue arrow heads refers to ct−1. The notation corresponds
to equations 1 to 10 such that Wxo denotes wights for x to
output gate and Whf denotes weights for ht−1 to forget gates
etc. Adapted from (Zaremba & Sutskever, 2014). Figure 3. A-LSTM network. Each state of the hidden units,
ht are weighted and summed before the output network cal-
culates the predictions.
2.2. REGULAR LSTM NETWORKS FOR PREDICTING SINGLE TARGETS
When used for predicting a single target for each input sequence, one approach is to output the predicted target from the
LSTM network at the last sequence position as shown in Figure 4. A problem with this approach is that the gradient has to
flow from the last position to all previous positions and that the LSTM network has to store information about all previously
seen data in the last hidden state. Furthermore a regular bidirectional LSTM (BLSTM)(Schuster & Paliwal, 1997) is not
useful in this setting because the backward LSTM will only have seen a single position, xT , when the prediction has to be
made. We instead combine two unidirectional LSTMs, as shown in figure 4C, where the backward LSTM has the input
reversed. The prediction from the two LSTMs are combined before predictions.
Convolutional LSTM Networks for Subcellular Localization of Proteins
Figure 4. A: Schematic indicating how MultiLoc combines predictions from several sources to make predictions whereas the LSTM
networks only rely on the sequence (Ho¨glund et al., 2006). B: Unrolled single layer BLSTM. The forwards LSTM (red arrows) starts
at time 1 and the backwards LSTM (blue arrows) starts at time T , then they go forwards and backwards respectively. The errors from
the forward and backward nets are combined and a prediction is made for each sequence position. Adapted from (Graves, 2012). C:
Unidirectional LSTM for predicting a single target. All targets except for the target at the last position are masked. Squares are LSTM
layers.
2.3. ATTENTION MECHANISM LSTM NETWORK
Bahdanau et al. (Bahdanau et al., 2014), have introduced an attention mechanism for combining hidden state information
from a encoder-decoder RNN approach to machine translation. The novelty in their approach is that they use an alignment
function that for each output word finds important input words, thus aligning and translating at the same time. We modify
this alignment procedure such that only a single target is produced for each sequence. The developed attention mechanism
can be seen as assigning importance to each position in the sequence with respect to the prediction task. We use a BLSTM
to produce a hidden state at each position and then use an attention function to assign importance to each hidden state
as illustrated in figure 3. The weighted sum of hidden states is used as a single representation of the entire sequence.
This modification allows the BLSTM model to naturally handle tasks involving prediction of a single target per sequence.
Conceptually this corresponds to adding weighted skip connections (green arrow heads Figure 3) between any ht and the
output network, with the weight on each skip connection being determined by the attention function. Each hidden state ht,
t = 1, . . . , T is used as input to a Feed Forward Neural Network (FFN) attention function:
at = tanh(htWa)v
T
a , (11)
where Wa is an attention hidden weight matrix and va is an attention output vector. From the attention function we form
softmax weights:
αt =
exp(at)
ΣTt′=1 exp (at′)
(12)
that are used to produce a context vector c as a convex combination of T hidden states:
c = ΣTt=1htαt . (13)
The context vector is then used is as input to the classification FFN f(c). We define f as a single layer FFN with softmax
outputs.
2.4. SUBCELLULAR LOCALIZATION DATA
The model was trained and evaluated on the dataset used to train the MultiLoc algorithm published by Hglund et al.
(Ho¨glund et al., 2006)1. The dataset contains 5959 proteins annotated to one of 11 different subcellular locations. To
1http://abi.inf.uni-tuebingen.de/Services/MultiLoc/multiloc_dataset
Convolutional LSTM Networks for Subcellular Localization of Proteins
reduce computational time the protein sequences were truncated to length 1000. We truncated by removing from the
middle of the protein as both the N- and C-terminal regions are known to contain sorting signals (Emanuelsson et al.,
2007). Each amino acid was encoded using 1-of-K encoding, the BLOSUM80 (Henikoff & Henikoff, 1992) and HSDM
(Prlic´ et al., 2000) substitution matrices and sequence profiles, yielding 80 features per amino acids. Sequence profiles
where created with ProfilePro2 using 3 blastpgp3 iterations on UNIREF50 (Magrane et al., 2011).
2.5. VISUALIZATIONS
Convolutional filters for images can be visualized by plotting the convolutional weights as pixel intensities as shown in
figure 1. However a similar approach does not make sense for amino acid inputs due to the 1-of-K vector encoding. Instead
we view the 1D convolutions as a position specific scoring matrix (PSSM). The convolutional weights can be reshaped into
a matrix of lfilter-by-lenc, where the amino acid encoding length is is 20. Because the filters show relative importance we
rescale all filters such that the height of the highest column is 1. Each filter can then be visualized as a PSSM logo, where
the height of each column can be interpreted as position importance and the height of each letter is amino acid importance.
We use Seq2logo with the PSSM-logo setting to create the convolution filter logos (Thomsen & Nielsen, 2012).
We visualize the importance the A-LSTM network assigns to each position in the input by plotting α from equa-
tion 12. Lastly we extract and plot the hidden representation from the LSTM networks. For the A-LSTM network we
use c from equation 13 and for the R-LSTM we use the last hidden state, ht. Both c and ht can be seen as fixed length
representation of the amino acid sequences. We plot the representation using t-SNE (Van Der Maaten & Hinton, 2008).
2.6. EXPERIMENTAL SETUP
All models were implemented in Theano (Bastien et al., 2012) using a modified version of the Lasagne library4 and trained
with gradient descent. The learning rate was controlled with ADAM (α = 0.0002, β1 = 0.1, β2 = 0.001, ǫ = 108 and
λ = 10−8) (Kingma & Ba, 2014). Initial weights were sampled uniformly from the interval [-0.05, 0.05]. The network
architecture is a 1D convolutional layer followed by an LSTM layer, a fully connected layer and a final softmax layer. All
layers use 50% dropout. The 1D convolutional layer uses convolutions of sizes 1, 3, 5, 9, 15 and 21 with 10 filters of
each size. Dense and convolutional layers use ReLU activation (Nair & Hinton, 2010) and the LSTM layer uses hyperbolic
tangent. For the A-LSTM model the size of the first dimension of Wa was 400. Based on previous experiments we trained
for 100 epochs for all models and used 4/5 of the data for training the last 1/5 of the data for testing.
3. RESULTS
Table 1 shows accuracy for the R-LSTM and A-LSTM models and several other models trained on the same dataset.
Comparing the performance of the R-LSTM, A-LSTM and MultiLoc models, utilizing only the sequence information, the
R-LSTM model (0.879 Acc.) performs better than the A-LSTM model (0.854 Acc.) whereas the MultiLoc model (0.767
Acc.) performs significantly worse. Furthermore the 10-ensemble R-LSTM model further increases the performance to
0.902 Acc. Comparing this performance with the other models, combining the sequence predictions from the MultiLoc
model with large amounts of metadata for the final predictions, only the Sherloc2 model (0.930 Acc.) performs better than
the R-LSTM ensemble. Figure 5 shows a plot of the attention matrix from the A-LSTM model. Figure 7 shows examples
of the learned convolutional filters. Figure 6 shows the hidden state of the R-LSTM and the A-LSTM model.
4. DISCUSSION AND CONCLUSION
In this paper we have introduced LSTM networks with convolutions for prediction of subcellular localization. Table 1
shows that the LSTM networks perform much better than other methods that only rely on information from the sequence
(LSTM ensemble 0.902 vs. MultiLoc 0.767). This difference is all the more remarkable given that our method is
biologically naı¨ve, only utilizing the sequences and their localization labels, while MultiLoc incorporates specific domain
knowledge such as known motifs and signal anchors. One explanation for the performance difference is that the LSTM
networks are able to look at both global and local sequence features whereas the SVM based models do not model global
2http://download.igb.uci.edu/
3http://nebc.nox.ac.uk/bioinformatics/docs/blastpgp.html
4https://github.com/skaae/nntools
Convolutional LSTM Networks for Subcellular Localization of Proteins
Table 1. Comparison of results for LSTM models and MultiLoc1/2.
MultiLoc1/2 accuracies are reprinted from (Goldberg et al., 2012)
and the SherLoc accuracy from (Briesemeister et al., 2009).
Model Accuracy
Input: Protein Sequence
R-LSTM 0.879
A-LSTM 0.854
R-LSTM ensemble 0.902
MultiLoc 0.767
Input: Protein Sequence + Metadata
MultiLoc + PhyloLoc 0.842
MultiLoc + PhyloLoc + GOLoc 0.871
MultiLoc2 0.887
SherLoc2 0.930
Table 2. True labels are shown by row and model predictions by col-
umn. E.g. row 4 column 3 means that the actual class was Cytoplas-
mic but the model predicted Chloroplast.
Confusion Matrix
ER 26 1 0 0 8 1 0 0 0 3 0
Golgi 1 28 0 0 0 0 0 0 0 1 0
Chloroplast 0 0 82 3 0 0 5 0 0 0 0
Cytoplasmic 0 0 1 266 0 0 3 12 0 0 0
Extracellular 0 0 0 1 166 0 0 0 0 1 0
Lysosomal 0 0 0 0 5 12 0 0 0 3 0
Mitochondrial 0 0 2 5 0 0 94 1 0 0 0
Nuclear 0 0 0 27 1 0 3 137 0 0 0
Peroxisomal 0 1 0 10 0 0 0 1 18 2 0
Plasma membrane 0 0 0 0 5 0 1 1 0 241 0
Vacuolar 0 0 0 0 7 0 0 0 0 1 5
Figure 5. Importance weights assigned to different regions of the proteins when making predictions. y-axis is true group and x-axis is
the sequence positions. All proteins shorter than 1000 are zero padded from the middle such that the N and C terminals align.
Convolutional LSTM Networks for Subcellular Localization of Proteins
Figure 6. t-SNE plot of hidden representation for Forward and Backward R-LSTM and A-LSTM.
dependencies. The LSTM networks have nearly as good performance as methods that use information obtained from other
sources than the sequence (LSTM ensemble 0.902 vs. SherLoc2 0.930). Incorporating these informations into the LSTM
models could further improve the performance of these models. However, it is our opinion that using sequence alone yields
the biologically most relevant prediction, while the incorporation of, e.g., GO terms limits the usability of the prediction
requiring similar proteins to be already annotated to some degree. Furthermore, as we show below, a sequence-based
method potentially allows for a de novo identification of sequence features essential for biological function.
Figure 5 shows where in the sequence the A-LSTM network assigns importance. Sequences from the compartments ER,
extracellular, lysosomal, and vacuolar all belong to the secretory pathway and contain N-terminal signal peptides, which
are clearly seen as bars close to the left edge of the plot. Some of the ER proteins additionally have bars close to the right
edge of the plot, presumably representing KDEL-type retention signals. Golgi proteins are special in this context, since
they are type II transmembrane proteins with signal anchors, slightly further from the N-terminus than signal peptides
(Ho¨glund et al., 2006). Chloroplast and mitochondrial proteins also have N-terminal sorting signals, and it is apparent
from the plot that chloroplast transit peptides are longer than mitochondrial transit peptides, which in turn are longer than
signal peptides (Emanuelsson et al., 2007). For the plasma membrane category we see that some proteins have signal
peptides, while the model generally focuses on signals, presumably transmembrane helices, scattered across the rest
of the sequence with some overabundance close to the C-terminus. Some of the attention focused near the C-terminus
could also represent signals for glycosylphosphatidylinositol (GPI) anchors (Emanuelsson et al., 2007). Cytoplasmic and
nuclear proteins do not have N-terminal sorting signals, and we see that the attention is scattered over a broader region of
the sequences. However, especially for the cytoplasmic proteins there is some attention focused close to the N-terminus,
presumably in order to check for the absence of signal peptides. Finally, peroxisomal proteins are known to have either
N-terminal or C-terminal sorting signals (PTS1 and PTS2) (Emanuelsson et al., 2007), but these do not seem to have been
picked up by the attention mechanism.
In Figure 7 we investigate what the convolutional filters in the model focus on. Notably the short filters focus on
amino acids with specific characteristics, such as positively or negatively charged, whereas the longer filters seem to focus
on distributions of amino acids across longer sequences. The arginine-rich motif in Figure 7C could represent part of a
nuclear localization signal (NLS), while the longer motif in Figure 7D could represent the transition from transmembrane
helix (hydrophobic) to cytoplasmic loop (in accordance with the ”positive-inside” rule). We believe that the learned filters
can be used to discover new sequence motifs for a large range of protein and genomic features.
Convolutional LSTM Networks for Subcellular Localization of Proteins
Figure 7. Examples of learned filters. Filter A captures proline or trypthopan stretches, B) and C) are sensitive to positively and negatively
charged regions, respectively. Note that for C, negative amino acids seems to suppress the output. Lastly we show a long filter which
captures larger sequence motifs in the proteins.
In Figure 6 we investigate whether the LSTM models are able to extract fixed length representations of variable
length proteins. Using t-SNE we plot the LSTMs hidden representation of the sequences. It is apparent that proteins
from the same compartment generally group together, while the cytoplasmic and nuclear categories tend to overlap. The
corresponds with the fact that these two categories are relatively often confused, see Table 2. The categories form clusters
which make biological sense; all the proteins with signal peptides (ER, extracellular, lysosomal, and vacuolar) lie close
to each other in t-SNE space in all three plots, while the proteins with other N-terminal sorting signals (chloroplasts
and mitochondria) are close in the R-LSTM plots (but not in the A-LSTM plot). Note that the lysosomal and vacuolar
categories are very close to each other in the plots, this corresponds with the fact that these two compartments are
considered homologous (Ho¨glund et al., 2006).
In summary we have introduced LSTM networks with convolutions for subcellular localization. By visualizing the
learned filters we have shown that these can be interpreted as motif detectors, and lastly we have shown that the LSTM
network can represent protein sequences as a fixed length vector in a representation that is biologically interpretable.
References
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural Machine Translation by Jointly Learning to Align and Translate.
arXiv preprint arXiv:1409.0473, September 2014.
Bastien, Fre´de´ric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, Warde-
Convolutional LSTM Networks for Subcellular Localization of Proteins
Farley, David, and Bengio, Yoshua. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590, November
2012.
Blum, Torsten, Briesemeister, Sebastian, and Kohlbacher, Oliver. MultiLoc2: integrating phylogeny and Gene Ontology terms
improves subcellular protein localization prediction. BMC bioinformatics, 10:274, January 2009. ISSN 1471-2105. doi:
10.1186/1471-2105-10-274.
Briesemeister, Sebastian, Blum, Torsten, Brady, Scott, Lam, Yin, Kohlbacher, Oliver, and Shatkay, Hagit. SherLoc2: a high-accuracy
hybrid method for predicting subcellular localization of proteins. J. Proteome Res., 8(11):5363–6, November 2009. ISSN 1535-3907.
doi: 10.1021/pr900665y.
Cunn, Yann Le, Boser, B, Denker, JS, Henderson, D, Howard, RE, W, Hubbard., and Jackel, LD. Handwritten digit recognition with
a back-propagation network. In Lippmann, R.P, Moody, J.E., and Touretzky, D.S (eds.), Advances in neural information processing
systems, pp. 396–404, 1990.
Dahl, GE, Yu, D, Deng, L, and Acero, A. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.
Emanuelsson, Olof, Brunak, Sø ren, von Heijne, Gunnar, and Nielsen, Henrik. Locating proteins in the cell using TargetP, SignalP and
related tools. Nat. Protoc., 2(4):953–971, 2007. ISSN 1754-2189. doi: 10.1038/nprot.2007.131.
Goldberg, Tatyana, Hamp, Tobias, and Rost, Burkhard. LocTree2 predicts localization for all domains of life. Bioinformatics, 28(18):
i458–i465, September 2012. ISSN 1367-4811. doi: 10.1093/bioinformatics/bts390.
Graves, A. Supervised sequence labelling with recurrent neural networks. Springer, 2012. ISBN 978-3-642-24797-2.
Graves, A and Jaitly, N. Towards end-to-end speech recognition with recurrent neural networks. Proceedings of the 31st International
Conference on Machine Learning (ICML-14), pp. 1764–1772, 2014.
Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Henikoff, S and Henikoff, J G. Amino acid substitution matrices from protein blocks. Proc. Natl. Acad. Sci. U. S. A., 89:10915–10919,
1992. ISSN 0027-8424. doi: 10.1073/pnas.89.22.10915.
Hochreiter, S, Schmidhuber, J, and Elvezia, C. LONG SHORT-TERM MEMORY. Neural Computation, 9(8):1735–1780, 1997.
Ho¨glund, Annette, Do¨nnes, Pierre, Blum, Torsten, Adolph, Hans-Werner, and Kohlbacher, Oliver. MultiLoc: prediction of protein
subcellular localization using N-terminal targeting sequences, sequence motifs and amino acid composition. Bioinformatics, 22(10):
1158–65, May 2006. ISSN 1367-4803. doi: 10.1093/bioinformatics/btl002.
Kingma, Diederik and Ba, Jimmy. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980, December 2014.
Krizhevsky, A, Sutskever, I, and Hinton, GE. Imagenet classification with deep convolutional neural networks. In Pereira, F., Burges, C.
J. C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in neural information processing systems, pp. 1097–1105, 2012.
Magrane, Michele, UniProt Consortium, et al. Uniprot knowledgebase: a hub of integrated protein data. Database, 2011:bar009, 2011.
Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), pp. 807–814, 2010.
Petersen, TN, Brunak, Søren, von Heijne, Gunnar, and Nielsen, H. SignalP 4.0: discriminating signal peptides from transmembrane
regions. Nat. Methods, 8(10):785–786, 2011.
Prlic´, A, Domingues, F S, and Sippl, M J. Structure-derived substitution matrices for alignment of distantly related sequences. Protein
Eng., 13:545–550, 2000. ISSN 1741-0126. doi: 10.1093/protein/13.8.545.
Schuster, M and Paliwal, KK. Bidirectional recurrent neural networks. Signal Processing, 45(11):2673–2681, 1997.
Sutskever, I, Vinyals, O, and Le, QVV. Sequence to sequence learning with neural networks. Advances in Neural Information Processing
Systems, pp. 3104–3112, 2014.
Thomsen, M. C. F and Nielsen, M. Seq2Logo: a method for construction and visualization of amino acid binding motifs and sequence
profiles including sequence weighting, pseudo counts and two-sided representation of amino acid enrichment and depletion. Nucleic
Acids Res., 40:W281–W287, 2012.
Van Der Maaten, L J P and Hinton, G E. Visualizing high-dimensional data using t-sne. J. Mach. Learn. Res., 9:2579–2605, 2008. ISSN
1532-4435.
Convolutional LSTM Networks for Subcellular Localization of Proteins
Xiong, H. Y., Alipanahi, B., Lee, L. J., Bretschneider, H., Merico, D., Yuen, R. K. C., Hua, Y., Gueroussov, S., Najafabadi, H. S., Hughes,
T. R., Morris, Q., Barash, Y., Krainer, A. R., Jojic, N., Scherer, S. W., Blencowe, B. J., and Frey, B. J. The human splicing code reveals
new insights into the genetic determinants of disease. Science 347,, 1254806, 2014. ISSN 0036-8075. doi: 10.1126/science.1254806.
Zaremba, W, Sutskever, I, and Vinyals, O. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014a.
Zaremba, Wojciech and Sutskever, Ilya. Learning to Execute. arXiv preprint arXiv:1410.4615, October 2014.
Zaremba, Wojciech, Kurach, Karol, and Fergus, Rob. Learning to Discover Efficient Mathematical Identities. In Ghahramani, Z.,
Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems, pp.
1278–1286, June 2014b.
"
21,"L. Bulwahn, M. Kamali, S. Linker (Eds.): First Workshop on
Formal Verification of Autonomous Vehicles (FVAV 2017).
EPTCS 257, 2017, pp. 19–26, doi:10.4204/EPTCS.257.3
Towards Proving the Adversarial Robustness of
Deep Neural Networks
Guy Katz, Clark Barrett, David L. Dill, Kyle Julian and Mykel J. Kochenderfer
Stanford University
{guyk, clarkbarrett, dill, kjulian3, mykel}@stanford.edu
Autonomous vehicles are highly complex systems, required to function reliably in a wide variety of
situations. Manually crafting software controllers for these vehicles is difficult, but there has been
some success in using deep neural networks generated usingmachine-learning. However, deep neural
networks are opaque to human engineers, rendering their correctness very difficult to provemanually;
and existing automated techniques, which were not designed to operate on neural networks, fail to
scale to large systems. This paper focuses on proving the adversarial robustness of deep neural
networks, i.e. proving that small perturbations to a correctly-classified input to the network cannot
cause it to be misclassified. We describe some of our recent and ongoing work on verifying the
adversarial robustness of networks, and discuss some of the open questions we have encountered and
how they might be addressed.
1 Introduction
Designing software controllers for autonomous vehicles is a difficult and error-prone task. A main cause
of this difficulty is that, when deployed, autonomous vehicles may encounter a wide variety of situations
and are required to perform reliably in each of them. The enormous space of possible situations makes
it nearly impossible for a human engineer to anticipate every corner-case.
Recently, deep neural networks (DNNs) have emerged as a way to effectively create complex soft-
ware. Like other machine-learning generated systems, DNNs are created by observing a finite set of
input/output examples of the correct behavior of the system in question, and extrapolating from them a
software artifact capable of handling previously unseen situations. DNNs have proven remarkably useful
in many applications, including including speech recognition [8], image classification [14], and game
playing [20]. There has also been a surge of interest in using them as controllers in autonomous vehicles
such as automobiles [3] and aircraft [12].
The intended use of DNNs in autonomous vehicles raises many questions regarding the certification
of such systems. Many of the common practices aimed at increasing software reliability — such as code
reviews, refactoring, modular designs and manual proofs of correctness — simply cannot be applied
to DNN-based software. Further, existing automated verification tools are typically ill-suited to reason
about DNNs, and they fail to scale to anything larger than toy examples [18, 19]. Other approaches use
various forms of approximation [2, 9] to achieve scalability, but using approximations may not meet the
certification bar for safety-critical systems. Thus, it is clear that new methodologies and tools for scalable
verification of DNNs are sorely needed.
We focus here on a specific kind of desirable property of DNNs, called adversarial robustness.
Adversarial robustness measures a network’s resilience against adversarial inputs [21]: inputs that are
produced by taking inputs that are correctly classified by the DNN and perturbing them slightly, in a
way that causes them to be misclassified by the network. For example, for a DNN for image recognition
20 Towards Proving Adversarial Robustness of Deep Neural Networks
such examples can correspond to slight distortions in the input image that are invisible to the human eye,
but cause the network to assign the image a completely different classification. It has been observed
that many state-of-the-art DNNs are highly vulnerable to adversarial inputs, and several highly effective
techniques have been devised for finding such inputs [4, 7]. Adversarial attacks can be carried out in the
real world [15], and thus constitute a source of concern for autonomous vehicles using DNNs—making
it desirable to verify that these DNNs are robust.
In a recent paper [13], we proposed a new decision procedure, called Reluplex, designed to solve
systems of linear equations with certain additional, non-linear constraints. In particular, neural networks
and various interesting properties thereof can be encoded as input to Reluplex, and the properties can
then be proved (or disproved, in which case a counter example is provided). We used Reluplex to verify
various properties of a prototype DNN implementation of the next-generation Airborne Collision Avoid-
ance Systems (ACAS Xu), which is currently being developed by the Federal Aviation Administration
(FAA) [12].
This paper presents some of our ongoing efforts along this line of work, focusing on adversarial
robustness properties. We study different kinds of robustness properties and practical considerations for
proving them on real-world networks. We also present some initial results on proving these properties
for the ACAS Xu networks. Finally, we discuss some of the open questions we have encountered and
our plans for addressing them in the future.
The rest of this paper is organized as follows. We briefly provide some needed background on
DNNs and on Reluplex in Section 2, followed by a discussion of adversarial robustness in Section 3.
We continue with a discussion of our ongoing research and present some initial experimental results in
Section 4, and conclude with Section 5.
2 Background
2.1 Deep Neural Networks
Deep neural networks (DNNs) consist of a set of nodes (“neurons”), organized in a layered structure.
Nodes in the first layer are called input nodes, nodes in the last layer are called output nodes, and nodes
in the intermediate layers are called hidden nodes. An example appears in Fig. 1 (borrowed from [13]).
Input #1
Input #2
Input #3
Input #4
Input #5
Output #1
Output #2
Output #3
Output #4
Output #5
Figure 1: A DNN with 5 input nodes (in green), 5 output nodes (in red), and 36 hidden nodes (in blue).
The network has 6 layers.
Nodes are connected to nodes from the preceding layer by weighted edges, and are each assigned
a bias value. An evaluation of the DNN is performed as follows. First, the input nodes are assigned
values (these can correspond, e.g., to user inputs or sensor readings). Then, the network is evaluated
G. Katz et al. 21
layer-by-layer: in each layer the values of the nodes are calculated by (i) computing a weighted sum
of values from the previous layer, according to the weighted edges; (ii) adding each node’s bias value
to the weighted sum; and (iii) applying a predetermined activation function to the result of (ii). The
value returned by the activation function becomes the value of the node, and this process is propagated
layer-by-layer until the network’s output values are computed.
This work focuses on DNNs using a particular kind of activation function, called a rectified linear
unit (ReLU). The ReLU function is given by the piecewise linear formula ReLU(x) = max(0,x), i.e.,
positive values are unchanged and negative values are changed to 0. When applied to a positive value,
we say that the ReLU is in the active state; and when applied to a non-positive value, we say that it is
in the inactive state. ReLUs are very widely used in practice [14, 16], and it has been suggested that the
piecewise linearity that they introduce allows DNNs to generalize well to new inputs [5, 6, 10, 17].
A DNN N is referred to as a classifier if it is associated with a set of labels L, such that each output
node of N corresponds to a specific output label. For a given input~x and label ℓ ∈ L, we refer to the value
of ℓ’s output node as the confidence of N that~x is labeled ℓ, and denote this value byC(N,~x, ℓ). An input
~x is said to be classified to label ℓ ∈ L, denoted N(~x) = ℓ, if C(N,~x, ℓ)>C(N,~x, ℓ′) for all ℓ′ 6= ℓ.
2.2 Verifying Properties of Neural Networks
A DNN can be regarded as a collection of linear equations, with the additional ReLU constraints. Ex-
isting verification tools capable of handling these kinds of constraints include linear programming (LP)
solvers and satisfiability modulo theories (SMT) solvers, and indeed past research has focused on using
these tools [2, 9, 18, 19]. As for the properties being verified, we restrict our attention to properties
that can be expressed as linear constraints over the DNN’s input and output nodes. Many properties of
interest seem to fall into this category, including adversarial robustness [13].
Unfortunately, this verification problem is NP-complete [13], making it theoretically difficult. It is
also difficult in practice, with modern solvers scaling only to very small examples [18, 19]. Because prob-
lems involving only linear constraints are fairly easy to solve, many solvers handle the ReLU constraints
by transforming the input query into a sequence of pure linear sub-problems, such that the original query
is satisfiable if and only if at least one of the sub-problems is satisfiable. This transformation is performed
by case-splitting: given a query involving n ReLU constraints, the linear sub-problems are obtained by
fixing each of the ReLU constraints in either the active or inactive state (recall that ReLU constraints
are piecewise linear). Unfortunately, this entails exploring every possible combination of active/inactive
ReLU states, meaning that the solver needs to check 2n linear sub-problems in the worst case. This
quickly becomes a crucial bottleneck when n increases.
In a recent paper, we proposed a new algorithm, called Reluplex, capable of verifying DNNs that are
an order-of-magnitude larger than was previously possible [13]. The key insight that led to this improved
scalability was a lazy treatment of the ReLU constraints: instead of exploring all possible combinations
of ReLU activity or inactivity, Reluplex temporarily ignores the ReLU constraints and attempts to solve
just the linear portion of the problem. Then, by deriving variable bounds from the linear equations that
it explores, Reluplex is often able to deduce that some of the ReLU constraints are fixed in either the
active or inactive case, which greatly reduces the amount of case-splitting that it later needs to perform.
This has allowed us to use Reluplex to verify various properties of the DNN-based implementation of
the ACAS Xu system: a family of 45 DNNs, each with 300 ReLU nodes.
22 Towards Proving Adversarial Robustness of Deep Neural Networks
3 Adversarial Robustness
A key challenge in software verification, and in particular in DNN verification, is obtaining a specifi-
cation against which the software can be verified. One solution is to manually develop such properties
on a per-system basis, but we can also focus on properties that are desirable for every network. Adver-
sarial robustness properties fall into this category: they express the requirement that the network behave
smoothly, i.e. that small input perturbations should not cause major spikes in the network’s output. Be-
cause DNNs are trained over a finite set of inputs/outputs, this captures our desire to ensure that the
network behaves “well” on inputs that were neither tested nor trained on. If adversarial robustness is
determined to be too low in certain parts of the input space, the DNN may be retrained to increase its
robustness [7].
We begin with a common definition for local adversarial robustness [2, 9, 13]:
Definition 1 A DNN N is δ -locally-robust at point ~x0 iff
∀~x. ‖~x−~x0‖ ≤ δ ⇒ N(~x) = N(~x0)
Intuitively, Definition 1 states that for input ~x that is very close to ~x0, the network assigns to~x the same
label that it assigns to ~x0; “local” thus refers to a local neighborhood around ~x0. Larger values of δ imply
larger neighborhoods, and hence better robustness. Consider, for instance, a DNN for image recognition:
δ -local-robustness can then capture the fact that slight perturbations of the input image, i.e. perturbations
so small that a human observer would fail to detect them, should not result in a change of label.
There appear to be two main drawbacks to using Definition 1: (i) The property is checked for in-
dividual input points in an infinite input space, and it does not necessarily carry over to other points
that are not checked. This issue may be partially mitigated by testing points drawn from some random
distribution thought to represent the input space. (ii) For each point ~x0 we need to specify the minimal
acceptable value of δ . Clearly, these values can vary between different input points: for example, a point
deep within a region that is expected to be labeled ℓ1 should have high robustness, whereas for a point
closer to the boundary between two labels ℓ1 and ℓ2 even a tiny δ may be acceptable. We note that given
a point ~x0 and a solver such as Reluplex, one can perform a binary search and find the largest δ for which
N is δ -locally-robust at ~x0 (up to a desired precision).
In order to overcome the need to specify each individual δ separately, in [13] we proposed an alter-
native approach, using the notion of global robustness:
Definition 2 A DNN N is (δ ,ε)-globally-robust in input region D iff
∀~x1,~x2 ∈ D. ‖~x1−~x2‖ ≤ δ ⇒ ∀ℓ ∈ L. |C(N,~x1, ℓ)−C(N,~x2, ℓ)|< ε
Definition 2 addresses the two shortcomings of Definition 1. First, it considers an input domain D instead
of a specific point ~x0, allowing it to cover infinitely many points (or even the entire input space) in a single
query, with δ and ε defined once for the entire domain. Also, it is better suited for handling input points
that lay on the boundary between two labels: this definition now only requires that two δ -adjacent points
are classified in a similar (instead of identical) way, in the sense that there are no spikes greater than ε
in the levels of confidence that the network assigns to each label for these points. Here it is desirable to
have a large δ (for large neighborhoods) and a small ε (for small spikes), although it is expected that the
two parameters will be mutually dependent.
Unfortunately, global robustness appears to be significantly harder to check, as we discuss next.
G. Katz et al. 23
3.1 Verifying Robustness using Reluplex
Provided that the distance metrics in use can be expressed as a combination of linear constraints and
ReLU operators (L1 and L∞ fall into this category), δ -local-robustness and (δ ,ε)-global-robustness prop-
erties can be encoded as Reluplex inputs. For the local robustness case, the input constraint ‖~x−~x0‖ ≤ δ
is encoded directly as a set of linear equations and variable bounds, and the robustness property is negated
and encoded as ∨
ℓ 6=N(~x0)
N(~x) = ℓ
Thus, if Reluplex finds a variable assignment that satisfies the query, this assignment constitutes a
counter-example ~x that violates the property, i.e., ~x is δ -close to ~x0 but has a label different from that
of ~x0. If Reluplex discovers that the query is unsatisfiable, then the network is guaranteed to be δ -local-
robust at ~x0.
Encoding (δ ,ε)-global-robustness is more difficult because neither ~x1 nor ~x2 is fixed. It is performed
by encoding two copies of the network, denoted N1 and N2, such that ~x1 is the input to N1 and ~x2 is the
input to N2. We again encode the constraint ‖~x1−~x2‖ ≤ δ as a set of linear equations, and the robustness
property is negated and encoded as
∨
ℓ∈L
|C(N1,~x1, ℓ)−C(N2, ~x2, ℓ)| ≥ ε
As before, if the query is unsatisfiable then the property holds, whereas a satisfying assignment consti-
tutes a counter-example.
While both kinds of queries can be encoded in Reluplex, global robustness is significantly harder
to prove than its local counterpart. The main reason is the technique mentioned in Section 3.1, which
allows Reluplex to achieve scalability by determining that certain ReLU constraints are fixed at either
the active or inactive state. When checking local robustness, the network’s input nodes are restricted
to a small neighborhood around ~x0, and this allows Reluplex to discover that many ReLU constraints
are fixed; whereas the larger domain D used for global robustness queries tends to allow fewer ReLUs
to be eliminated, which entails additional case-splitting and slows Reluplex down. Also, as previously
explained, encoding a global-robustness property entails encoding two identical copies of the DNN in
question. This doubles the number of variables and ReLUs that Reluplex needs to handle, leading to
slower performance. Consequently, our implementation of Reluplex can currently verify the local ad-
versarial robustness of DNNs with several hundred nodes, whereas global robustness is limited to DNNs
with a few dozen nodes.
4 Moving Forward
A significant question in moving forward is on which definition of adversarial robustness to focus. The
advantages of using (δ ,ε)-global-robustness are clear, but the present state-of-the-art seems insufficient
for verifying it; whereas δ -local-robust is more feasible but requires a high degree of manual fine tuning.
We suggest to focus for now on the following hybrid definition, which is an enhanced version of local
robustness:
Definition 3 A DNN N is (δ ,ε)-locally-robust at point ~x0 iff
∀~x. ‖~x−~x0‖ ≤ δ ⇒ ∀ℓ ∈ L. |C(N,~x, ℓ)−C(N,~x0, ℓ)|< ε
24 Towards Proving Adversarial Robustness of Deep Neural Networks
The encoding of (δ ,ε)-local-robustness properties as inputs to Reluplex is similar to the previous cases:
the constraint ‖~x−~x0‖ ≤ δ is encoded as a set of linear equations and variable bounds, and the robustness
property is negated and encoded as
∨
ℓ∈L
|C(N,~x, ℓ)−C(N,~x0, ℓ)| ≥ ε
Definition 3 is still local in nature, which means that testing it using Reluplex does not require encoding
two copies of the network. It also allows ReLU elimination, which affords some scalability (see Table 1
for some initial results). Finally, this definition’s notion of robustness is based on the difference in
confidence levels, as opposed to a different labeling, making it more easily applicable to any input point,
even if it is close to a boundary between two labels. Thus, we believe it is superior to Definition 1. An
open problem is how to determine the finite set of points to be tested, and the δ and ε values to test.
(Note that it may be possible to use the same δ and ε values for all points tested, reducing the amount of
manual work required.)
Another important challenge in moving forward is scalability. Currently, Reluplex is able to handle
DNNs with several hundred nodes, but many real-world DNNs are much larger than that. Apart from
improving the Reluplex heuristics and implementation, we believe that parallelization will play a key
role here. Verification of robustness properties, both local and global, naturally lends itself to paral-
lelization. In the local case, testing the robustness of n input points can be performed simultaneously
using n machines; and even in the global case, an input domain D can be partitioned into n sub domains
D1, . . . ,Dn, each of which can be tested separately. The experiment described in Table 1 demonstrates
the benefits of parallelizing (δ ,ε)-local-robustness testing even further: apart from testing each point on
a separate machine, for each point the disjuncts in the encoding of Definition 3 can also be checked in
parallel. The improvement in performance is evident, emphasizing the potential benefits of pursuing this
direction further.
We believe parallelization can be made even more efficient in this context by means of two comple-
mentary directions:
1. Prioritization. When testing the (local or global) robustness of a DNN, we can stop immediately
once a violation has been found. Thus, prioritizing the points or input domains and starting from
those in which a violation is most likely to occur could serve to reduce execution time. Such
prioritization could be made possible by numerically analyzing the network prior to verification,
identifying input regions in which there are steeper fluctuations in the output values, and focusing
on these regions first.
2. Information sharing across nodes. As previously mentioned, a key aspect of the scalability of
Reluplex is its ability to determine that certain ReLU constraints are fixed in either the active or
inactive case. When running multiple experiments, these conclusions could potentially be shared
between executions, improving performance. Of course, great care will need to be taken, as a
ReLU that is fixed in one input domain may not be fixed (or may even be fixed in the other state)
in another domain.
Finally, we believe it would be important to come up with automatic techniques for choosing the
input points (in the local case) or domains (in the global case) to be tested, and the corresponding δ
and ε parameters. These techniques would likely take into account the distribution of the inputs in the
network’s training set. In the global case, domain selection could be performed in a way that would
optimize the verification process, by selecting domains in which ReLU constraints are fixed in the active
or inactive state.
G. Katz et al. 25
Table 1: Checking the (δ ,ε)-local-robustness of one of the ACAS Xu DNNs [13] at 5 arbitrary input
points, for different values of ε (we fixed δ = 0.018 for all experiments). The Seq. columns indicate
execution time (in seconds) for a sequential execution, and the Par. columns indicate execution time (in
seconds) for a parallelized execution using 5 machines.
Point ε = 0.01 ε = 0.02 ε = 0.03
Robust? Par. Seq. Robust? Par. Seq. Robust? Par. Seq.
1 No 5 5 No 785 7548 Yes 9145 38161
2 Yes 277 1272 Yes 248 989 Yes 191 747
3 Yes 103 460 Yes 134 480 Yes 93 400
4 No 17 17 Yes 249 774 Yes 132 512
5 Yes 333 1479 Yes 259 1115 Yes 230 934
5 Conclusion
The planned inclusion of DNNs within autonomous vehicle controllers poses a significant challenge for
their certification. In particular, it is becoming increasingly important to show that these DNNs are robust
to adversarial inputs. This challenge can be addressed through verification, but the scalability of state-of-
the-art techniques is a limiting factor and dedicated techniques and methodologies need to be developed
for this purpose.
In [13] we presented the Reluplex algorithm which is capable of proving DNN robustness in some
cases. Still, additional work is required to improve scalability. We believe that by carefully phrasing the
properties being proved, and by intelligently applying parallelization, a significant improvement can be
achieved.
As a long-term goal, we speculate that this line of work could assist researchers in verifying the
dynamics of autonomous vehicle systems that include a DNN-based controller. In particular, it may be
possible to first formally prove that a DNN-based controller satisfies certain properties, and then use
these properties in analyzing the dynamics of the system as a whole. Specifically, we plan to explore
the integration of Reluplex with reachability analysis techniques, for both the offline [11] and online [1]
variants.
Acknowledgements. We thank Neal Suchy from the FAA and Lindsey Kuper from Intel for their valu-
able comments and support. This work was partially supported by grants from the FAA and Intel.
References
[1] M. Althoff & J. Dolan (2014): Online Verification of Automated Road Vehicles using Reachability Analysis.
IEEE Transactions on Robotics 30, pp. 903–918, doi:10.1109/TRO.2014.2312453.
[2] O Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori & A. Criminisi (2016): Measuring Neural
Net Robustness with Constraints. In: Proc. 30th Conf. on Neural Information Processing Systems (NIPS).
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. Jackel, M. Monfort, U. Muller,
J. Zhang, X. Zhang, J. Zhao & K. Zieba (2016): End to End Learning for Self-Driving Cars. Technical
Report. http://arxiv.org/abs/1604.07316.
[4] N. Carlini & D. Wagner (2017): Towards Evaluating the Robustness of Neural Networks. In: Proc. 38th
Symposium on Security and Privacy (SP), doi:10.1109/SP.2017.49.
[5] X. Glorot, A. Bordes & Y. Bengio (2011): Deep Sparse Rectifier Neural Networks. In: Proc. 14th Int. Conf.
on Artificial Intelligence and Statistics (AISTATS), pp. 315–323.
26 Towards Proving Adversarial Robustness of Deep Neural Networks
[6] I. Goodfellow, Y. Bengio & A. Courville (2016): Deep Learning. MIT Press.
[7] I. Goodfellow, J. Shlens & C. Szegedy (2014): Explaining and Harnessing Adversarial Examples. Technical
Report. http://arxiv.org/abs/1412.6572.
[8] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath & B. Kingsbury (2012): Deep Neural Networks for Acoustic Modeling in Speech Recogni-
tion: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine 29(6), pp. 82–97,
doi:10.1109/MSP.2012.2205597.
[9] X. Huang, M. Kwiatkowska, S. Wang & M. Wu (2016): Safety Verification of Deep Neural Networks. Tech-
nical Report. http://arxiv.org/abs/1610.06940.
[10] K. Jarrett, K. Kavukcuoglu & Y. LeCun (2009): What is the Best Multi-Stage Architecture for Ob-
ject Recognition? In: Proc. 12th IEEE Int. Conf. on Computer Vision (ICCV), pp. 2146–2153,
doi:10.1109/ICCV.2009.5459469.
[11] J.-B. Jeannin, K Ghorbal, Y. Kouskoulas, R. Gardner, A. Schmidt, E. Zawadzki & A. Platzer (2015): A
Formally Verified Hybrid System for the Next-Generation Airborne Collision Avoidance System. In: Proc.
21st Int. Conf. on Tools and Algorithms for the Construction and Analysis of Systems (TACAS), pp. 21–36,
doi:10.1007/978-3-662-46681-0 2.
[12] K. Julian, J. Lopez, J. Brush, M. Owen & M. Kochenderfer (2016): Policy Compression for Air-
craft Collision Avoidance Systems. In: Proc. 35th Digital Avionics Systems Conf. (DASC), pp. 1–10,
doi:10.1109/DASC.2016.7778091.
[13] G. Katz, C. Barrett, D. Dill, K. Julian & M. Kochenderfer (2017): Reluplex: An Efficient SMT Solver for
Verifying Deep Neural Networks. In: Proc. 29th Int. Conf. on Computer Aided Verification (CAV), pp.
97–117, doi:10.1007/978-3-319-63387-9 5.
[14] A. Krizhevsky, I. Sutskever & G. Hinton (2012): Imagenet Classification with Deep Convolutional Neural
Networks. Advances in Neural Information Processing Systems, pp. 1097–1105.
[15] A. Kurakin, I. Goodfellow & S. Bengio (2016): Adversarial Examples in the Physical World. Technical
Report. http://arxiv.org/abs/1607.02533.
[16] A. Maas, A. Hannun & A. Ng (2013): Rectifier Nonlinearities improve Neural Network Acoustic Models. In:
Proc. 30th Int. Conf. on Machine Learning (ICML).
[17] V. Nair & G. Hinton (2010): Rectified Linear Units Improve Restricted Boltzmann Machines. In: Proc. 27th
Int. Conf. on Machine Learning (ICML), pp. 807–814.
[18] L. Pulina & A. Tacchella (2010): An Abstraction-Refinement Approach to Verification of Artificial
Neural Networks. In: Proc. 22nd Int. Conf. on Computer Aided Verification (CAV), pp. 243–257,
doi:10.1007/978-3-642-14295-6 24.
[19] L. Pulina & A. Tacchella (2012): Challenging SMT Solvers to Verify Neural Networks. AI Communications
25(2), pp. 117–135, doi:10.3233/AIC-2012-0525.
[20] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot & S. Dieleman (2016): Mastering the Game of Go with Deep Neural Net-
works and Tree Search. Nature 529(7587), pp. 484–489, doi:10.1038/nature16961.
[21] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow & R. Fergus (2013): Intriguing
Properties of Neural Networks. Technical Report. http://arxiv.org/abs/1312.6199.
"
22,"Scaling Deep Learning on GPU and Knights Landing clusters
Yang You
Computer Science Division
UC Berkeley
youyang@cs.berkeley.edu
Aydın Buluc¸
Computer Science Division
LBNL and UC Berkeley
abuluc@lbl.gov
James Demmel
Computer Science Division
UC Berkeley
demmel@cs.berkeley.edu
ABSTRACT
e speed of deep neural networks training has become a big bot-
tleneck of deep learning research and development. For example,
training GoogleNet by ImageNet dataset on one Nvidia K20 GPU
needs 21 days [11]. To speed up the training process, the current
deep learning systems heavily rely on the hardware accelerators.
However, these accelerators have limited on-chip memory com-
pared with CPUs. To handle large datasets, they need to fetch
data from either CPU memory or remote processors. We use both
self-hosted Intel Knights Landing (KNL) clusters and multi-GPU
clusters as our target platforms. From an algorithm aspect, current
distributed machine learning systems [5] [18] are mainly designed
for cloud systems. ese methods are asynchronous because of
the slow network and high fault-tolerance requirement on cloud
systems. We focus on Elastic Averaging SGD (EASGD) [28] to
design algorithms for HPC clusters. Original EASGD [28] used
round-robin method for communication and updating. e com-
munication is ordered by the machine rank ID, which is inecient
on HPC clusters.
First, we redesign four ecient algorithms for HPC systems to
improve EASGD’s poor scaling on clusters. Async EASGD, Async
MEASGD, and Hogwild EASGD are faster than their existing coun-
terparts (Async SGD, Async MSGD, and Hogwild SGD, resp.) in all
the comparisons. Finally, we design Sync EASGD, which ties for
the best performance among all the methods while being determin-
istic. In addition to the algorithmic improvements, we use some
system-algorithm codesign techniques to scale up the algorithms.
By reducing the percentage of communication from 87% to 14%, our
Sync EASGD achieves 5.3× speedup over original EASGD on the
same platform. We get 91.5% weak scaling eciency on 4253 KNL
cores, which is higher than the state-of-the-art implementation.
CCS CONCEPTS
•Computingmethodologies→Massively parallel algorithms;
KEYWORDS
Distributed Deep Learning, Knights Landing, Scalable Algorithm
ACM acknowledges that this contribution was authored or co-authored by an em-
ployee, or contractor of the national government. As such, the Government retains a
nonexclusive, royalty-free right to publish or reproduce this article, or to allow others
to do so, for Government purposes only. Permission to make digital or hard copies for
personal or classroom use is granted. Copies must bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. To copy otherwise, distribute, republish, or post, requires prior
specic permission and/or a fee. Request permissions from permissions@acm.org.
SC17, Denver, CO, USA
© 2017 ACM. 978-1-4503-5114-0/17/11. . .$15.00
DOI: 10.1145/3126908.3126912
ACM Reference format:
Yang You, Aydın Buluc¸, and James Demmel. 2017. Scaling Deep Learning
on GPU and Knights Landing clusters. In Proceedings of SC17, Denver, CO,
USA, November 12–17, 2017, 12 pages.
DOI: 10.1145/3126908.3126912
1 INTRODUCTION
For deep learning applications, larger datasets and bigger models
lead to signicant improvements in accuracy [1]. However, the com-
putational power for training deep neural networks has become a
big boleneck. e current deep networks require days or weeks to
train, which makes real-time interaction impossible. For example,
training ImageNet by GoogleNet on one Nvidia K20 GPU needs
21 days [11]. Moreover, the neural networks are rapidly becoming
more and more complicated. For instance, state-of-the-art Residual
Nets have 152 layers [9] while the best networks four years ago
(AlexNet [14]) had only 8 layers. To speed up the training process,
the current deep learning systems heavily rely on hardware acceler-
ators because they can provide highly ne-grained data-parallelism
(e.g. GPGPUs) or fully-pipelined instruction-parallelism (e.g. FPGA).
However, these accelerators have limited on-chip memory com-
pared with CPUs. To handle big models and large datasets, they
need to fetch data from either CPU memory or remote processors at
runtime. us, reducing communication and improving scalability
are critical issues for distributed deep learning systems.
To explore architectural impact, in addition to multi-GPU plat-
form, we choose the Intel Knights Landing (KNL) cluster as our tar-
get platform. KNL is a self-hosted chip with more cores than CPUs
(e.g. 68 or 72 vs 32). Compared with its predecessor Knights Corner
(KNC), KNL signicantly improved both computational power (6
Tops vs 2 Tops for single precision) and memory bandwidth
eciency (450 GB/s vs 159 GB/s for STREAM benchmark). More-
over, KNL introduced MCDRAM and congurable NUMA, which
are highly important for applications with complicated memory
access paerns. We design communication-ecient deep learning
methods on GPU and KNL clusters for beer scalability.
Algorithmically, current distributed machine learning systems
[5] [18] are mainly designed for cloud systems. ese methods
are asynchronous because of the slow network and high fault-
tolerance requirement on cloud systems. A typical HPC cluster’s
bisection bandwidth is 66.4 Gbps (NERSC Cori) while the data
center’s bisection bandwidth is around 10 Gbps (Amazon EC2).
However, as mentioned before, the critical issues for current deep
learning system are speed and scalability. erefore, we need to
select the right method as the starting point. Regarding algorithms,
we focus on Elastic Averaging SGD (EASGD) method since it has
a good convergence property [28]. Original EASGD used a round-
robin method for communication. e communication is ordered
by the machine rank ID. At any moment, the master can interact
ar
X
iv
:1
70
8.
02
98
3v
1 
 [c
s.D
C]
  9
 A
ug
 20
17
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
with just a single worker. e parallelism is limited to the pipeline
among dierent workers. Original EASGD is inecient on HPC
systems.
First, we redesign four ecient distributed algorithms to improve
EASGD’s poor scaling on clusters. By changing the round-robin
style to parameter-server style, we got Async EASGD. Aer adding
momentum [24] to Async EASGD, we got Async MEASGD. en we
combine Hogwild method and EASGD updating rule to get Hogwild
EASGD. Async EASGD, Async MEASGD, and Hogwild EASGD are
faster than their existing counterparts (i.e. Async SGD, Async
MSGD, and Hogwild SGD, resp.). Finally, we design Sync EASGD,
which ties for the best performance among all the methods while
being deterministic (Figure 8). Besides the algorithmic renements,
the system-algorithm codesign techniques are important for scaling
up deep neural networks. e techniques we introduce include:
(1) using single-layer layout and communication to optimize the
network latency and memory access, (2) using multiple copies of
weights to speedup the gradient descent, and (3) partitioning the
KNL chip based on data/weight size and reducing communication
on multi-GPU systems. By reducing the communication percent
from 87% to 14%, our Sync EASGD achieves 5.3× speedup over
original EASGD on the same platform. Using ImageNet dataset to
train GoogleNet on 2176 KNL cores, the weak scaling eciency of
Intel Cae is 87% while our implementation is 92%. Using ImageNet
to train VGG on 2176 KNL cores, the weak scaling eciency of
Intel Cae is 62% while our implementation is 78.5%. To highlight
the dierence between existing methods and our methods, we list
our three major contributions:
(1) Sync EASGD and Hogwild EASGD algorithms. We have
documented our process in arriving at these two algorithms, which
ultimately perform beer than existing methods. e existing
EASGD uses round-robin updating rule. We refer to the exist-
ing method as Original EASGD. We rst changed the round-robin
rule to parameter-server rule to arrive at Async EASGD. e dif-
ference between Original-EASGD and Async-EASGD is that the
updating rule of Original-EASGD is ordered while Async-EASGD
is unordered. Adding momentum to that we arrived at Async
MEASGD. Neither Async EASGD nor Async MEASGD were signif-
icantly faster than Original EASGD (Figure 8).
In both Original-EASGD and Async-EASGD, the master only
communicates with one worker at a time. en we relaxed this
requirement to allow the master to communicate with multiple
workers at a time to get Hogwild EASGD. e master rst receives
multiple weights from dierent workers. e master then processes
these weights by the Hogwild (lock-free) Updating rule. We ob-
serve that the lock-free Hogwild makes Hogwild EASGD run much
faster than Original EASGD. For the convex case, we can prove the
algorithm is safe and faster under some assumptions1.
We used tree reduction algorithm to replace round-robin rule
to get Sync EASGD. Sync EASGD is much faster (Θ(logP) vs Θ(P)).
is is highly important because deep learning researchers oen
need to tune many hyperparameters, which is exetremly time-
consuming. While not being one of our major contributions, we
also documented the relative order of performance between inter-
mediate algorithms we have considered. For instance, we observe
1hps://www.cs.berkeley.edu/∼youyang/HogwildEasgdProof.pdf
Figure 1: e KNL Architecture. Our version has 68 cores.
that Async EASGD is faster than Async SGD and Async MEASGD
is faster than Async MSGD.
(2)Algorithm-SystemCo-design formulti-GPU system. Af-
ter the algorithm-level optimization, we need to produce an ecient
design on the multi-GPU system. We reduce the communication
overhead by changing the data’s physical locations. We also design
some strategies to overlap the communication with the computa-
tion. Aer the algorithm-system co-design, our implementation
achieves a 5.3x speedup over the Original EASGD.
(3) Use KNL cluster to speedup DNN training. GPUs are
good tools to train deep neural networks. However, we also want
to explore more hardware options for deep learning applications.
We choose KNL because it has powerful computation and memory
units. Section 6.2 describes the optimization for small dataset DNN
training on KNL platform. In our experiments, using an 8-core CPU
to train CIFAR-10 dataset takes 8.2 hours. However, CIFAR-10 is
only 170 MB, which can not make full use of KNL’s 384 GB memory.
is optimization helps us to nish the training in 10 minutes. e
optimization in Section 5.2 is designed on KNL cluster, but it can
be used on regular clusters.
2 BACKGROUND
We describe the Intel Knights Landing architecture, which is used
in this paper. We review necessary background on deep learning
for readers to understand this paper.
2.1 Intel Knights Landing Architecture
Intel Knights Landing (KNL) Architecture is the latest version of
Intel Xeon Phi. Compared with the previous version, i.e. Knights
Corner (KNC), KNL has slightly more cores (e.g. 72 or 68 vs 60).
Like KNC, each KNL core has 4 hardware threads and supports
512-bit instruction for SIMD data parallelism. e major distinct
features of KNL include the following:
(1) Self-hostedPlatforme traditional accelerators (e.g. FPGA,
GPUs, and KNC) rely on CPU for control and I/O management. For
some applications, the transfer path like PCIE may become a bole-
neck at runtime because the memory on accelerator is limited (e.g.
12 GB GDDR5 on one Nvidia K80 GPU). e KNL does not need a
host. It is self-hosted by an operating system like CentOS 7.
(2) Better Memory KNL’s DDR4 memory size is much larger
than that of KNC (384 GB vs 16 GB). Moreover, KNL is equipped
Scaling Deep Learning on GPU and Knights Landing clusters SC17, November 12–17, 2017, Denver, CO, USA
with 16 GB Multi-Channel DRAM (MCDRAM). MCDRAM’s mea-
sured bandwidth is 475 GB/s (STREAM benchmark). e bandwidth
of KNL’s regular DDR4 is 90 GB/s. MCDRAM has three modes: a)
Cache Mode: KNL uses it as the last level cache; b) Flat Mode: KNL
treats it as the regular DDR; c) Hybrid Mode: part of it is used as
cache, the other is used as the regular DDR memory (Figure 2).
(3) Congurable NUMA KNL supports all-to-all (A2A), quad-
rant/hemisphere (ad/Hemi) and sub-NUMA (SNC-4/2) clustering
modes of cache operation. For A2A, memory addresses are uni-
formly distributed across all tag directories (TDs) on the chip. For
ad/Hemi, the tiles are divided into four parts called quadrants,
which are spatially local to four groups of memory controllers.
Memory addresses served by a memory controller in a quadrant
are guaranteed to be mapped only to TDs contained in that quad-
rant. Hemisphere mode functions the same way, except that the
die is divided into two hemispheres instead of four quadrants. e
SNC-4/2 mode partitions the chip into four quadrants or two hemi-
spheres, and, in addition, expose these quadrants (hemispheres)
as NUMA nodes. In this mode, NUMA-aware soware can pin
soware threads to the same quadrant (hemisphere) that contains
the TD and access NUMA-local memory.
2.2 DNN and SGD
We focus on Convolutional Neural Networks (CNN) [16] in this
section. Figure 3 is an illustration of CNN. CNN is composed of
a sequence of tensors. We refer to these tensors as weights. At
runtime, the input of CNN is a picture X (e.g. X is stored as a
32-by-32 matrix in Figure 3). Aer a sequence of tensor-matrix
operations, the output of CNN is an integer y (e.g. y ∈ {0, 1, 2, ..., 9}
in Figure 3). e tensor-matrix operations can be implemented
by dense matrix-matrix multiplication, FFT, dense matrix-vector
multiplication, dense vector-vector add and non-linear transform
(e.g. Tanh, Sigmoid, and ReLU [7]).
Figure 3 is an example of hand-wrien image recognition. e
input pictureX should be recognized as 3 by a human. Ify is 3, then
the input picture is correctly classied by the CNN framework. To
get the correct classication, we need to get a set of working weights.
e weights needs to be trained by using the real-world datasets.
For simplicity, let us refer to the weights as W , and the training
dataset as {Xi ,yi }, i ∈ {1, 2, ...,n}. n is the number of training
pictures. yi is the correct label for Xi . e training process includes
three parts: 1) Forward Propagation, 2) Backward Propagation, and
3) Weight Update.
Forward Propagation Xi is passed from the rst layer to the
last layer of the neural network (le to right in Figure 3). e output
is the prediction of Xi ’s label, which is referred to as y˜i .
Backward Propagation We get a numerical prediction error E
as the dierence between yi and y˜i . en we pass E from the last
layer to the rst layer to get the gradient ofW , which is ∆W .
Weight Update We rene the CNN framework by updating the
weight: W ←W − η∆W , where η is a number called the learning
rate (e.g. 0.01).
We conduct the above three steps iteratively over all the samples
until the model is optimized (i.e. randomly picks a batch of samples
at each iteration). is method is called Stochastic Gradient Descent
(SGD) [7]. Stochastic means we randomly pick a batch of b pictures
at each iteration. Usually b is an integer chosen from 16 to 2048. If
b is too large, SGD’s convergence rate usually will decrease [19].
2.3 Data Parallelism and Model Parallelism
Let us parallelize the DNN training process on P machines. ere
are two major parallelism strategies for this: Data Parallelism (Fig.
4.1) and Model Parallelism (Fig. 4.2). All the later parallel methods
are the variants of these two methods.
Data Parallelism [5] e dataset is partitioned into P parts
and each machine only gets one part. Each machine has a copy
of the neural network, hence the weights (W ). e communica-
tion includes sum of all the gradients ∆Wi and broadcast of W .
e rst part of communication is conducted between Backward
Propagation and Weights Update. e master updatesW byW ←
W − η∑Pi=1 ∆Wi aer it gets all the sub-gradients ∆Wi from the
workers. en the master machine broadcastsW to all the worker
machines, which is the second part of communication. Figure 4.1 is
an example of data parallelism on 4 machines.
Model Parallelism [3] Data parallelism replicates the neural
network itself on each machine while model parallelism partitions
the neural network into P pieces. Partitioning the neural network
means parallelizing the matrix operations on the partitioned net-
work. us, model parallelism can get the same solution as the
single-machine case. Figure 4.2 shows model parallelism on 3 ma-
chines. ese three machines partition the matrix operation of
each layer. However, because both the batch size (<= 2048) and the
picture size (e.g. 32×32) typically are relatively small, the matrix op-
erations are not large. For example, parallelizing a 2048×1024×1024
matrix multiplication only needs one or two machines. us, state-
of-the-art methods oen use data-parallelism ([1], [2], [5], [22]).
2.4 Evaluating Our Method
e objective of this paper is designing distributed algorithms on
HPC systems to get the same or higher classication accuracy
(algorithm benchmark) in a shorter time. If our optimization may
inuence the convergence of algorithm, we report both the time
and accuracy. Otherwise, we only report the time for experimental
results. All algorithmic comparisons in this paper used the same
hardware (e.g. # CPUs, # GPUs, and # KNLs) and the same hyper-
parameters (e.g. batch size, learning rate). We do not compare
dierent architectures (e.g. KNL vs K80 GPU) because they have
dierent performance, power, and prices.
3 RELATEDWORK
In this section, we review the previous literature about scaling deep
neural networks on parallel or distributed systems.
3.1 Parameter Server (Async SGD)
Figure 5 illustrates the idea of parameter server or Asynchronous
SGD [5]. Under this framework, each worker machine has a copy of
weightW . e dataset is partitioned to all the worker machines. At
each step, i-th worker computes a sub-gradient (∆Wi ) from its own
data and weight. en the i-th worker sends ∆Wi to the master (i ∈
{1, 2, ..., P}). e master receives ∆Wi , conducts the weight update,
and sends weight back to i-th worker machine. All the workers
nish this step asynchronously, using rst come rst serve (FCFS).
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
Figure 2: e threemodes ofMCDRAM.Under CacheMode, MCDRAMacts as the last-level cache. Under FlatMode, MCDRAM
is part of RAM. Under Hybird Mode, part of MCDRAM acts as the last-level cache, and the rest of it acts as RAM.
Figure 3: is gure [20] is an illustration of Convolutional
Neural Network.
4.1 4.2
Figure 4: Methods of parallelism. 4.1 is data parallelism
on 4 machines. 4.2 is model parallelism on 3 machines.
Model parallelism partitions the neural network into P
pieces whereas data parallelism replicates the neural net-
work itself in each processor.
3.2 Hogwild (Lock Free)
e Hogwild method [21] can be presented as a variant of Async
SGD. e master machine is a shared memory system. For Async
SGD, if the sub-gradient from j-th worker arrives during the period
that the master is interacting with i-th worker, then W ←W −
η∆Wj can not be started beforeW ←W − η∆Wi is nished (i , j ∈
{1, 2, ..., P}). is means that there is a lock to avoid weight update
conicts on the shared memory system (master machine). e lock
makes sure the master only processes one sub-gradient at one time.
e Hogwild method, however, removes the lock and allows the
master to process multiple sub-gradients at the same time. e
proof of Hogwild’s lock-free convergence is in its paper [21].
Figure 5: is gure [5] is an illustration of Parameter
Server. Data is partitioned to workers. Each worker com-
putes a gradient and sends it to server for updating the
weight. e updated model is copied back to wokers
3.3 EASGD (Round-Robin)
e Elastic Averaging SGD (EASGD) method [28] can also be pre-
sented as a variant of Async SGD. Async SGD uses a FCFS strategy
for processing the sub-gradients asynchronously. EASGD uses a
round-robin strategy for ordered update, i.e. W ←W − η∆Wi can
not be started beforeW ←W −η∆Wi−1 is nished (i ∈ {2, 3, ...,n}).
Also, EASGD requires the workers to conduct the update locally
(Equation (1)). Before all the workers conduct the local updating,
the master updates the center (or global) weight (Equation (2)). e
ρ in Equation (1) and Equation (2) is a term that connects the global
and local parameters. e framework of Original EASGD method
is shown in Algorithm 1.
W it+1 =W
i
t − η(∆W it + ρ(W it − W¯t )) (1)
W¯t+1 = W¯t + η
P∑
i=1
ρ(W it − W¯t ) (2)
3.4 Other methods
Li et al. [17] is focused on single-node memory optimization. e
idea is included in our implemenation. ere is some work [3], [15]
on scaling up deep neural networks by model parallelism method,
which is out of scope for this paper. is paper is focused on data
parallelism. Low-precision representation of neural networks is
another direction of research. e idea is to use low-precision
oating point to reduce the computation and communication for
Scaling Deep Learning on GPU and Knights Landing clusters SC17, November 12–17, 2017, Denver, CO, USA
Algorithm 1: Original EASGD on Multi-GPU system
master: CPU, workers: GPU1, GPU2, …, GPUP
Input: samples and labels: {Xi ,yi } i ∈ 1, ...,n
#iterations: T , batch size: b, #GPUs: G
Output: model weightW
1 Normalize X on CPU by standard deviation: E(X ) = 0 (mean)
and σ (X ) = 1 (variance)
2 InitializeW on CPU: random and Xavier weight lling
3 for j = 1; j <= G; j++ do
4 create local weightWj on j-th GPU, copyW toWj
5 create global weight W¯1 on 0-th GPU, copyW to W¯1
6 for t = 1; t <= T ; t++ do
7 j = t mod G
8 CPU randomly picks b samples
9 CPU asynchronously copies b samples to j-th GPU
10 CPU sends W¯t to j-th GPU
11 Forward and Backward Propagation on j-th GPU
12 CPU getsW jt from j-th GPU
13 j-th GPU updatesW jt by Equation (1)
14 CPU updates W¯t by W¯t+1 = W¯t + ηρ(W jt − W¯t )
geing the acceptable accuracy ([4], [8], [10], [22]). We reserve this
for future study.
4 EXPERIMENTAL SETUP
4.1 Experimental Datasets
Our test datasets are Mnist [16], Cifar [13], and ImageNet [6], which
are the standard benchmarks for deep learning research. Descrip-
tions can be found in Table 1. e application of Mnist dataset is
hardwrien digits recognition. e images of Mnist were grouped
into 10 classes (0, 1, 2, …, 9). e application of Cifar dataset is
object recognition. Cifar dataset includes 10 classes: airplane, au-
tomobile, bird, cat, deer, dog, frog, horse, ship, truck. Each Cifar
image only belongs to one class. e accuracy of random guess for
Mnist and Cifar image prediction is 0.1.
ImageNet [6] is a computer vision dataset of over 15 million
labeled images belonging to more than 20,000 classes. e images
were collected from the web and labeled by human labelers us-
ing Amazon’s Mechanical Turk crowd-sourcing tool. An annual
competition called the ImageNet Large-Scale Visual Recognition
Challenge (ILSVRC) has been held since 2010. ILSVRC uses a subset
of ImageNet with 1200 images in each of 1000 classes. In all, there
are roughly 1.2 million training images, 50,000 validation images,
and 150,000 testing images. In this paper, the ImageNet dataset
means ILSVRC-2012 dataset. e accuracy of random guess for
ImageNet image prediction is 0.001.
4.2 Neural Network Models
We use the state-of-the-art DNN models to process the datasets in
this paper. e Mnist dataset was processed by LetNet [16], which
is shown in Figure 3. e Cifar dataset is processed by AlexNet
[14], which has 5 convolutional layers and 3 fully-connected layers.
Table 1: e Test Datasets
Dataset Training Images Test Images Pixels Classes
Mnist [16] 60,000 10,000 28×28 10
Cifar [13] 50,000 10,000 3×32×32 10
ImageNet [6] 1.2 million 150,000 256×256 1000
ImageNet dataset is processed by GoogleNet [25] and VGG [25].
GoogleNet has 22 layers and VGG has 19 layers.
4.3 e baseline
In Section 5, the Original EASGD is our baseline. e original
EASGD method (Algorithm 1) uses round-robin approach for sched-
uling the way the master interacts with the workers. At any mo-
ment, the master can interact with just a single worker. Addition-
ally, the interactions of dierent workers are ordered. e (i + 1)-st
worker can not begin before i-th nishes.
5 DISTRIBUTED ALGORITHM DESIGN
5.1 Redesigning the parallel SGD methods
In this section we redesign some ecient parallel SGD methods
based on the existing methods (i.e. Original EASGD, Async SGD,
Async MSGD, and Hogwild SGD). We will use our methods to make
comparisons with the existing methods (i.e. we will plot accuracy
versus time, on the same data sets and computing resources). Since
the existing SGD methods were originally implemented on GPUs,
we also implement our methods on GPUs. ese ideas work in the
same way for KNL chips because these methods are focused on
inter-chip processing rather than intra-chip processing.
Async EASGD e original EASGD method (Algorithm 1) uses
round-robin approach for scheduling. is method is inecient
because the computation and update of dierent GPUs are ordered
(Section 3.3). e (i + 1)-st worker can not begin before i-th nishes.
Although this method has good fault-tolerance and convergence
properties, it is inecient. erefore, our rst optimization is to use
parameter-server update to replace the round-robin update. e
dierence between our Async EASGD and Original EASGD is that
we use rst-come rst-served (FCFS) strategy to process multiple
workers while they use ordered rule to process multiple workers.
We put the global (or center) weight W¯ on the master machine. e
i-th worker machine has its own local weightW i . During the t-th
iteration, there are three steps:
• (1) i-th worker rst sends its local weight W it to master
and master returns W¯t to i-th worker (i ∈ {1, 2, ..., P}).
• (2) i-th worker computes gradient ∆W it and receives W¯t .
• (3) master does the update based on Equation (2) and
worker does the update based on Equation (1).
From Figure 6.1 we can observe that our method Async EASGD
is faster than Async SGD.
Async MEASGD Momentum [24] is an important method to
accelerate SGD. e updating rule of Momentum SGD (MSGD) is
shown in Equations (3) and (4). V is the momentum parameter,
which has the same dimension as the weight and gradient. µ is the
momentum rate. Rule of thumb is µ = 0.9 or a similar value. In our
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
6.1 6.2
6.3 6.4
Figure 6: Original EASGD, Hogwild SGD, Async SGD, and
Async MSGD are the existing methods. All the comparisons
use the same hardware and data. Our methods are faster.
Each point on the gure is a single train and test. For exam-
ple, Hogwild EASGD has 10 points in the gure. It means
we run 10mutually independentHogwild EASGD caseswith
dierent numbers of iterations (e.g. 1k, 2k, 3k, …, 10k). e
experiments are conducted on 4 Tesla M100 GPUs that are
connected with a 96-lane, 6-way PCIe switch.
design, the updating rule of MEASGD master will be the same as
before (Equation (2)). e updating rule of the i-th worker will be
changed to Equations (5) and (6). From Figure 6.2 we can observe
that our method Async MEASGD is faster and more stable than
Async MSGD.
Vt+1 = µVt − η∆Wt (3)
Wt+1 =Wt +Vt+1 (4)
V it+1 = µV
i
t − η∆W it (5)
W it+1 =W
i
t +V
i
t+1 − ηρ(W it − W¯t ) (6)
Hogwild EASGD For Hogwild SGD, the lock for updating W
is removed to achieve a faster convergence. In the same way, for
regular EASGD, there should be a lock between W¯t+1 = W¯t +
ηρ(W it −W¯t ) andW¯t+1 = W¯t +ηρ(W jt −W¯t ) (i, j ∈ {1, 2, ..., P}). e
reason is thatW it andW
j
t may arrive at the same time. us, we
remove this lock to get the Hogwild EASGD method. From Figure
6.3 we clearly observe that Hogwild EASGD is much faster than
Hogwild SGD. e convergence proof of Hogwild EASGD can be
found in the appendix2.
2hps://www.cs.berkeley.edu/∼youyang/HogwildEasgdProof.pdf
Table 2: InniBand Performance under α-β Model
Network α (latency) β (1/bandwidth)
Mellanox 56Gb/s FDR IB 0.7 × 10−6s 0.2 × 10−9s
Intel 40Gb/s QDR IB 1.2 × 10−6s 0.3 × 10−9s
Intel 10GbE NetEect NE020 7.2 × 10−6s 0.9 × 10−9s
Sync EASGD e updating rules of Sync EASGD are Equations
(1) and (2). e Sync EASGD contains ve steps at iteration t :
• (1) the i-th worker computes its sub-gradient ∆W it based
on its data and weightW it (i ∈ {1, 2, ..., P}).
• (2) the master broadcasts W¯t to all the workers.
• (3) the system does a reduce operation to get ∑Pi=1W it and
sends it to master.
• (4) the i-th worker updates its local weightW it based on
Equation (1).
• (5) the master updates W¯t based on Equation (2).
Among them, step (1) and step (2) can be overlapped, step (4)
and step (5) can be overlapped. From Figure 6.4 we observe that
Sync EASGD is faster than Original EASGD. Here, Sync EASGD
means Sync EASGD3 implementation detailed in Section 6.1.
7.1 Sync EASGD1
7.2 Sync EASGD2 and Sync EASGD3
Figure 7: e architecture our Sync EASGD design on multi-
GPU system. C variable means Center Weight or Global
Weight, L variable means Local Weight.
We make an overall comparison by puing these comparisons
together into Figure 8. Among them, Original SGD, Hogwild SGD,
Async SGD, and Async MSGD are the existing methods. Our
method is always faster than its counterpart as already seen in
Figure 6. We also observe that Sync EASGD or Hogwild EASGD is
the fastest method among them. Sync EASGD and Hogwild EASGD
are essentially tied for fastest. Sync EAGSD incorporates a number
of optimizations that we describe in more detail in sections 5.2 and
6. e framework of our algorithm design is shown in Fig. 9, which
shows the dierence between these methods.
Scaling Deep Learning on GPU and Knights Landing clusters SC17, November 12–17, 2017, Denver, CO, USA
Figure 8: To visualize the comparisons, we use error rate
(1.0 − accuracy) as the algorithm benchmark. en we
use loд10 scale of error rate to make the comparisons more
clear. Among thesemethods, Original EASGD,Hogwild SGD,
Async SGD, and Async MSGD are the existing methods. e
rest of them are our methods. Each point on the gure is a
single run. For example, Sync EASGDhas 13 points in theg-
ure. It means we run 13 mutually independent Sync EASGD
cases with dierent numbers of iterations. It also means
longer time ormore iterations will help us to get a higher ac-
curacy, even with dierent initiations. e experiments are
conducted on 4 Tesla M100 GPUs that are connected with a
96-lane, 6-way PCIe switch.
Hogwild
EASGD
Hogwild
SGD
Async
SGD
Async
EASGD
Async
MEASGD
Async
MSGD
Original
EASGD
Sync
EASGD
Momentum Lock-Free
Lock-Free
Elastic Averging Elastic Averging
Tree Reduce Momentum
FCFS
Round-Robin
Existing
Method
New
Method
Figure 9: is framework of our algorithm design. e red
block means the existing method and the blue block means
the new method.
5.2 Single-Layer Communication
Current deep learning systems [11] allocate noncontiguous mem-
ory for dierent layers of the neural networks. ey also conduct
multiple rounds of communication for dierent layers. We allo-
cate the neural networks in a contiguous way and pack all the
layers together and conduct one communication each time. is
signicantly reduces the latency. From Figure 10 we can observe
the benet of this technique. ere are two reasons for the im-
provement: (1) e communication overhead of sending a n-word
message can be formulated as α-β model: (α + β × n) seconds. α is
Figure 10: e benet of packed layer comes from re-
duced communication latency and continuous memory ac-
cess. Since this is Sync SGD, the red triangles and blue
squares should be at identical heights. e reason for dif-
ferent heights is that a dierent random number generator
seed is used for the two runs. e example used Sync SGD
to process AlexNet (Section 4.2).
the network latency and β is the reciprocal of network bandwidth.
β is much smaller than α , which is the major communication over-
head (Table 2). us, for transferring the same volume of data,
sending one big message is beer than multiple small messages. (2)
e continuous memory access has a higher cache-hit ratio than
the non-continuous memory access.
Algorithm 2: Sync EASGD1
master: CPU, workers: GPU1, GPU2, …, GPUP
Input: samples and labels: {Xi ,yi } i ∈ 1, ...,n
#iterations: T , batch size: b, # GPUs: G
Output: model weightW
1 Normalize X on CPU by standard deviation: E(X ) = 0 (mean)
and σ (X ) = 1 (variance)
2 InitializeW on CPU: random and Xavier weight lling
3 for j = 1; j <= G; j++ do
4 create local weightW j1 on GPUj , copyW toW
j
1
5 create global weight W¯1 on CPU, copyW to W¯1
6 for t = 1; t <= T ; t++ do
7 for j = 1; j <= G; j++ do
8 CPU randomly picks b samples
9 CPU asynchronously copies b samples to j-th GPUj
10 Forward and Backward Propagation on all the GPUs
11 CPU broadcasts W¯t to all the GPUs
12 CPU gets
∑G
j=1W
j
t from all the GPUs
13 All the GPUs updateW jt by Equation (1)
14 CPU updates W¯t by Equation (2)
6 ALGORITHM-ARCHITECTURE CODESIGN
6.1 Multi-GPU Optimization
In this section we show how we optimize EASGD step-by-step on
a multi-GPU system. We use Sync EASGD1, Sync EASGD2, and
Sync EASGD3 to illustrate our three-step optimization.
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
6.1.1 Sync EASGD1. Algorithm 1 is the original EASGD algo-
rithm. Multi-GPU system implementation contains 8 potentially
time-consuming parts. For Algorithm 1, they are: (1) data I/O (In-
put and Output); (2) data and weight initialization (lines 1-2); (3)
GPU-GPU parameter communication (none); (4) CPU-GPU data
communication (line 9); (5) CPU-GPU parameter communication
(lines 10 and 12); (6) Forward and Backward propagation (line 11);
(7) GPU weight update (line 13); (8) CPU weight update (line 14). We
ignore parts (1) and (2) because they only cost a tiny percent of time.
GPU-GPU parameter communication means dierent GPUs ex-
change weights. CPU-GPU data communication means GPU copies
a batch of samples each iteration. CPU-GPU parameter communi-
cation means CPU sends global weight W¯ to GPUs and receives
local weightsW i (i ∈ {1, 2, ..., P ) from GPUs. Parts (3), (4), and (5)
are communication. Parts (6), (7), and (8) are computation. Aer
benchmarking the code, we found the major overhead of EASGD is
communication (Figure 11), which costs 87% of the total training
time on an 8-GPU system. If we look deep into the communication,
we observe that CPU-GPU parameter communication costs much
more time than CPU-GPU data communication (86% vs 1%). e
reason is that the size of weights (number of elements in W ) is
much larger than a batch of training data. For example, the weights
of AlexNet are 249 MB while 64 Cifar samples are only 64 × 32 × 32
× 3 × 4B = 768 KB. To solve this problem, we design Sync EASGD1
(Algorithm 2). In Sync EASGD1, P blocking send/receive opera-
tions can be eciently processed by a tree-reduction operation
(e.g. standard MPI reduction), which reduces the communication
overhead from P(α + |W |β) to loдP(α + |W |β). Our experiments
show that Sync EASGD1 achieves a 3.7× speedup over Original
EASGD (Table 3 and Figure 11).
Algorithm 3: Sync EASGD2 & Sync EASGD3
master: GPU1, workers: GPU1, GPU2, …, GPUP
Input: samples and labels: {Xi ,yi } i ∈ 1, ...,n
#iterations: T , batch size: b, # GPUs: G
Output: model weightW
1 Normalize X on CPU by standard deviation: E(X ) = 0 (mean)
and σ (X ) = 1 (variance)
2 InitializeW on CPU: random and Xavier weight lling
3 for j = 1; j <= G; j++ do
4 create local weightW j1 on GPUj , copyW toW
j
1
5 create global weight W¯1 on GPU1, copyW to W¯1
6 for t = 1; t <= T ; t++ do
7 for j = 1; j <= G; j++ do
8 CPU randomly pick b samples
9 CPU asynchronously copy b samples to j-th GPUj
10 Forward and Backward Propagation on all the GPUs
11 GPU1 broadcasts W¯t to all the GPUs
12 GPU1 gets
∑G
j=1W
j
t from all the GPUs
13 All the GPUs updateW jt by Equation (1)
14 GPU1 updates W¯t by Equation (2)
6.1.2 Sync EASGD2. From Table 3 we observe that CPU-GPU
communication is still the major overhead of communication. us,
we want to move either data or weights from CPU to GPU to reduce
the communication overhead. We can not put all the data on the
GPU card because the on-chip memory is very limited compared
with CPU. For example, the training part of ImageNet dataset is 240
GB while the on-chip memory of K80 is only around 12 GB. Since
the algorithm needs to randomly pick samples from the dataset,
we can not predict which part of dataset will be used by a certain
GPU. us, we put all the training and test data on the CPU. We
only copy the required data to the GPUs at runtime each iteration.
On the other hand, the weights are usually smaller than 1 GB,
which can be stored on a GPU card. For example, the large DNN
model VGG-19 [23] is 575 MB. Also, the weight will be reused every
iteration (Algorithm 3). us, we put all the weights on GPU to
reduce communication overhead. We refer to this method as Sync
EASGD2, which achieves 1.3× speedup over Sync EASGD1. e
framework of Sync EASGD2 is shown in Algorithm 3.
6.1.3 Sync EASGD3. We further improve the algorithm by over-
lapping the computation with the communication. We maximize
the overlapping benet inside the steps 7-14 of Algorithm 3. Be-
cause Forward/Backward Propagation uses the data from the CPU,
steps 7-10 are a critical path. e GPU-GPU communication (steps
11-12) is not dependent on steps 7-10. us, we overlap steps 7-
10 and steps 11-12 in Algorithm 3, yielding Sync EASGD3, which
achieves a 1.1× speedup over Sync EASGD2. In all, Sync EASGD3
reduced the communication ratio from 87% to 14% and achieves
5.3× speedup over original EASGD for geing the same accuracy
(Table 3 and Figure 11). us we refer to Sync EASGD3 as Com-
munication Ecient EASGD. We also design similar algorithm
for KNL cluster, which is shown in Algorithm 4, discussed next.
Algorithm4:Communication Ecient EASGD on KNL cluster
master: KNL1, workers: KNL1, KNL2, …, KNLP
Input: samples and labels: {Xi ,yi } i ∈ 1, ...,n
#iterations: T , batch size: b, # KNL Nodes: K
Output: model weightW
1 All nodes read the samples and labels from disk
2 Normalize X on all KNLs by standard deviation: E(X ) = 0
(mean) and σ (X ) = 1 (variance)
3 InitializeW on 1-st KNL: random and Xavier weight lling
4 KNL1 broadcastsW to all KNLs
5 for j = 1; j <= K ; j++ parallel do
6 create local weightW j1 on KNLj , copyW toW
j
1
7 create global weight W¯1 on KNL1, copyW to W¯1
8 for t = 1; t <= T ; t++ do
9 for j = 1; j <= K ; j++ parallel do
10 KNLj randomly pick b samples from local memory
11 Forward and Backward Propagation on all the KNLs
12 KNL1 broadcasts W¯t to all the KNLs
13 KNL1 gets
∑K
j=1W
j
t from all the KNLs
14 All the KNLs updateW jt by Equation (1)
15 KNL1 updates W¯t by Equation (2)
Scaling Deep Learning on GPU and Knights Landing clusters SC17, November 12–17, 2017, Denver, CO, USA
11.1
11.2
Figure 11: Breakdown of time for EASGD variants. para
means parameter or weight. comm means communication.
Computation includes forward/backward, gpu update, and
cpu update. ere is an overlap between for/backward and
cpu-gpu para comm for Original EASGD. Let us refer to the
non-overlap version as Original EASGD*. Sync EASGD3 re-
duced the communication percentage from 87% to 14% and
got 5.3× speedup over original EASGD for the same accuracy
(98.8%). e test is for MNIST dataset on 4 GPUs. More infor-
mation is in Table 3.
6.2 Knights Landing Optimization
Our platform’s KNL chip has 68 cores or 72 cores, which is much
more than that of a regular CPU chip. To make full use of KNL’s
computational power, data locality is highly important. Also, we
need to make the best use of KNL’s cluster mode (Section 2.1) at the
algorithm level. We partition the KNL chip into 4 parts like ad
or SNC-4 mode. e KNL acts like a 4-node NUMA system. In this
way, we also replicate the data into 4 parts and each NUMA node
gets one part. We make 4 copies of weights and each NUMA node
has one copy. Aer all the NUMA nodes compute the gradients,
we conduct a tree-reduction operation to sum these all gradients.
Each NUMA node can get one copy of the gradient sum and use
it to update its own weights. In this way, dierent NUMA nodes
do not need to communicate with each other unless they share the
gradients. is is a divide-and-conquer method. e divide step
includes replicating the data and copying the weights. e conquer
step is to sum up the gradients from all partitions. is can speedup
the algorithm by the faster propagation of gradients.
In the same way, we can partition the chip into 8 parts, 16 parts,
and so on. Let us partition the chip into P parts. e limitation of
this method is that the fast memory (cache and MCDRAM) should
Figure 12: Partitioning a KNL chip into group and making
each group process one local weight can improve the perfor-
mance.
be able to handle P copies of weight and P copies of data. Figure 12
shows that this method works for P ≤ 16 when we use AlexNet to
process Cifar dataset. e reason is that the AlexNet is 249 MB and
one Cifar data copy is 687 MB. us, MCDRAM can hold at most
16 copies of weight and data. Concretely, for achieving the same
accuracy (0.625), 1-part case needs 1605 sec, 4-part case needs 1025
sec, 8-part case needs 823 sec, and 16-part case only needs 490 sec.
We achieve 3.3× speedup by copying weight and data to make full
use of the fast memory and reduce communication.
7 ADDITIONAL RESULTS AND DISCUSSIONS
13.1
13.2
Figure 13: e benets of using more machines and more
data: (1) get the target accuracy in a shorter time, and (2)
achieve a higher accuracy in a xed time. Objective Loss
means Error (lower is better).
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
Table 3: Breakdownof time for EASGDvariants. parameans parameter orweight. commmeans communication. Computation
includes for/backward, gpu update, and cpu update. Sync EASGD3 reduced the communication percentage from 87% to 14%
and got 5.3× speedup over original EASGD for the same accuracy (98.8%). ere is an overlap between for/backward and cpu-
gpu para comm for Original EASGD. Let us refer to the non-overlap version as Original EASGD*. e reason why Original
EASGD*’s for/backward time is larger than the Sync methods (30s vs 6s) is that only one GPU (worker) is working at each
iteration. Original EASGD/EASGD* need more iterations to get the same accuracy. e test is for Mnist dataset on 4 GPUs.
Method accuracy iterations time gpu-gpu para cpu-gpu data cpu-gpu para for/backward gpu update cpu update comm ratio
Original EASGD* 0.988 5,000 69s 0% 0.5% 51% 44% 0.5% 4% 52%
Original EASGD 0.988 5,000 41s 0% 1% 86% 3% 1% 9% 87%
Sync EASGD1 0.988 1,000 11s 1% 3% 21% 55% 4% 16% 25%
Sync EASGD2 0.988 1,000 8.2s 16% 4% 0% 74% 6% 0% 20%
Sync EASGD3 0.988 1,000 7.7s 10% 4% 0% 79% 7% 0% 14%
Table 4: Weak Scaling Time and Eciency for ImageNet Dataset
Models 68 cores 136 cores 272 cores 544 cores 1088 cores 2176 cores 4352 cores
GoogleNet (300 Iters Time) 1533s 1590s 1608s 1641s 1630s 1662s 1674s
GoogleNet (Eciency) 100% 96.4% 95.3% 93.4% 94.0% 92.3% 91.6%
VGG (80 Iters Time) 1318s 1440s 1482s 1524s 1634s 1679s 1642s
VGG (Eciency) 100% 91.5% 89.0% 86.5% 80.7% 78.5% 80.2%
7.1 Comparison with Intel Cae
Intel Cae is the state-of-the-art implementation for both single-
node and multi-node on Xeon and Xeon Phi platforms. Because
this paper is focused on inter-node (distributed) algorithm, we
use Intel Cae for single-node implementation. We only compare
with Intel Cae for scaling because we have the same single-node
performance (baseline) with Intel Cae.
Machine Learning researchers focus on weak scaling because
they need higher accuracy when they use more machines and larger
datasets in a xed time (e.g. draw a vertical line in Figure 13). On
the other hand, weak scaling also means geing the target accuracy
in a shorter time by using more machines and larger data (e.g. draw
a horizontal line in Figure 13). Figure 13 shows the benet of using
more machines and more data. Each node processes one copy of
Cifar dataset and the batch size is 64. In the way, we increase the
total data size as we increase the number of machines.
For large-scale weak scaling study, we use GoogleNet and VGG
to process the ImageNet dataset. Each node has one copy of the
ImageNet dataset (240 GB). We increase the number of cores from
68 to 4352. e data size increases as we increases the number of
machines. e results of our weak scaling study are shown in Table
4. Compared with Intel’s implementation, we have a higher weak
scaling eciency. For GoogleNet on 2176 cores, the weak scaling
of Intel Cae is 87% while that of our implementation is 92%. For
VGG on 2176 cores, the weak scaling of Intel Cae is 62% while
that of our implementation is 78.5%.
7.2 e Impact of Batch Size
When changing the batch size, the users need to change learning
rate and momentum at the same time. For small batch sizes (e.g
from 32 to 1024), increasing the batch size generally speeds up
DNN training because larger batch size makes BLAS functions run
more eciently. Increasing the batch size beyond a threshold (e.g.
4096) generally slows down DNN training because in that regime,
the optimization space around minima becomes sharper, requiring
more epochs to get the same accuracy [12]. For medium batch size
(e.g. from 1024 to 4096), the users need to tune batch size, learning
rate, and momentum together to speed up the training.
8 CONCLUSION
e current distributed machine learning algorithms are mainly
designed for cloud systems. Due to cloud systems’ slow network
and high fault-tolerance requirement, these methods are mainly
asynchronous. However, asynchronous methods are usually unre-
producible, nondeterministic, and unstable. EASGD has a good con-
vergence property. Nevertheless, the round-robin method makes
it inecient on HPC systems. In this paper, we designed ecient
methods for HPC clusters to speedup deep learning applications’
time-consuming training process. Our methods Async EASGD,
Async MEASGD, and Hogwild EASGD are faster than their existing
counterpart methods. Sync EASGD or Hogwild EASGD method is
the fastest one among our competing methods in this paper. Sync
EASGD3 achieves 5.3× speedup over original EASGD for the same
accuracy (98.8%) while being deterministic and reproducible. We
achieve 91.6% weak-scaling eciency, which is higher than the
state-of-the-art implementation.
9 ACKNOWLEDGEMENT
We would like to thank Yang You’s 2016 IBM summer intern man-
ager Dr. David Kung and mentor Dr. Rajesh Bordawekar. Yang You
nished a multi-node multi-GPU EASGD with less global communi-
cation overhead at IBM, which is not included in this paper. We also
want to thank Prof. Cho-Jui Hsieh at UC Davis for reading the proof.
We used resources of the NERSC supported by the Oce of Science
Scaling Deep Learning on GPU and Knights Landing clusters SC17, November 12–17, 2017, Denver, CO, USA
of the DOE under Contract No. DEAC02-05CH11231. Dr. Buluc¸ is
supported by the Applied Mathematics program of the DOE Oce
of Advanced Scientic Computing Research under contract number
DE-AC02- 05CH11231 and by the Exascale Computing Project (17-
SC-20-SC), a collaborative eort of the U.S. DOE Oce of Science
and the National Nuclear Security Administration. Prof. Demmel
and Yang You are supported by the U.S. DOE Oce of Science,
Oce of Advanced Scientic Computing Research, Applied Mathe-
matics program under Award Number DE-SC0010200; by the U.S.
DOE Oce of Science, Oce of Advanced Scientic Computing
Research under Award Numbers DE-SC0008700; by DARPA Award
Number HR0011-12- 2-0016, ASPIRE Lab industrial sponsors and
aliates Intel, Google, HP, Huawei, LGE, Nokia, NVIDIA, Oracle
and Samsung. Other industrial sponsors include Mathworks and
Cray. e funding information in [26] and [27] maybe also relevant.
10 ARTIFACT DESCRIPTION APPENDIX
10.1 e Source Code
We share our source code online3, with everything necessary in-
cluded.
10.2 e dataset
First, due to the limit of le size, we can not upload the datasets.
To run our code, the readers need to download the datasets. For
Mnist dataset, the readers can download it from this link4. For Cifar
dataset, the readers can download it from this link5. For Imagenet
dataset, the readers can download it from this link6.
10.3 Dependent Libraries
All our codes are wrien in C++, and the users need to add -
std=c++11 to compile our codes. For GPU codes, we use CUDA 7.5
and CuBLAS and CuDNN 5.0 libraries. We use Nvidia NCCL for
GPU-to-GPU communication. We use MPI for distributed process-
ing on the multi-GPU multi-node system. For KNL codes, since
this paper is focused on inter-node (distributed) algorithm, we use
Intel Cae for single-node implementation. e Intel Cae depends
on Intel MKL for basic linear algebra functions. To install and use
Intel Cae, we install the follow libraries: (1) protobuf/2.6.1, (2)
boost/1.55.0, (3) gags/2.1.2, (4) glog/0.3.4, (5) snappy/1.1.3, (6) lev-
eldb/1.18, (7) lmdb/0.9.18, and (8) opencv/3.1.0-nogui. We use MPI
for distributed processing on the KNL cluster.
10.4 Experimental Systems
We have two GPU clusters. e rst one has 16 nodes. Each node
has one Intel E5-1680 v2 3.00GHz CPU and two Nvidia Tesla K80
GPUs. e two halves of the K80 are connected by a PLX Technol-
ogy, Inc. PEX 8747 48-lane PCIe switch. e nodes are connected
by 56 Gbit/s Inniband. e second cluster has 4 nodes. Each node
has one E5-2680 v3 2.50GHz CPU and eight Nvidia Tesla M40 GPUs.
Groups of 4 Tesla M100 GPUs are connected with a 96-lane, 6-way
PCIe switch. e nodes are connected by 56 Gbit/s Inniband. We
3hps://www.cs.berkeley.edu/∼youyang/sc17code.zip
4Mnist dataset is at hp://yann.lecun.com/exdb/mnist
5Cifar dataset is at hp://www.cs.toronto.edu/ kriz/cifar-10-binary.tar.gz
6Imagenet dataset is at hp://image-net.org/download
test the Knights Landing and CPU algorithm on NERSC’s Cori su-
percomputer, which has 2,004 Haswell CPU nodes (64,128 cores)
and 9,688 KNL nodes (658,784 cores in total, 68 cores per node) e
CPU version is 16-core Intel Xeon Processor E5-2698 v3 at 2.3 GHz.
e KNL version is Intel Xeon Phi Processor 7250 processor with
68 cores per node @ 1.4 GHz. e interconnect for Cori is Cray
Aries with Dragony topology with 5.625 TB/s global bandwidth
(CPU) and 45.0 TB/s global peak bisection bandwidth (KNL).
10.5 Running our codes
Aer downloading our codes from SC17 submission system and
unzipping it. e readers will get two folders: gpu and knl.
To run the GPU related codes, the readers need to enter the gpu
folder. ere are eight subfolders in gpu folder. Each subfolder
corresponds to one method mentioned in this paper. For example,
aer entering the mnist easgd async subfolder, the readers will
nd a couple of les. readubyte.cpp and readubyte.h are for
reading the dataset. e algorithm is implemented in my nn.cu
and the readers can use Makele to compile it. Aer compiling the
code, the readers can just execute run.sh le to run the program.
e parameter.txt le denes the neural network structure. e
results will be shown on screen and stored in .out les. For running
the distributed code, the readers can enter thempi easgd subfolder.
To compile the code, the readers just need to run compile.sh le.
en the readers can execute run.sh le to run the program.
To run the KNL related codes, the readers need to enter the knl
folder. ere are eight subfolders in knl folder. Each subfolder cor-
responds to one method mentioned in this paper. For example, aer
entering the cifar average sync subfolder, the readers will nd a
couple of les. e solver.prototxt les dene the algorithmic set-
ting (e.g. # iterations, # learning rate, and # testing frequency). e
train test.prototxt les dene the structure of neural networks.
To compile the code, the readers just need to execute compile.sh
le. Aer the compilation, the readers need to submit the program
to job management system. We use the slurm workload manager.
myknlrun.sl le is our submission script. Aer nishing the job,
the readers can use calacc.cpp to sum up the accuracy information
and caltime.cpp to sum up the time information.
REFERENCES
[1] Dario Amodei, Rishita Anubhai, Eric Baenberg, Carl Case, Jared Casper, Bryan
Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, and
others. 2015. Deep speech 2: End-to-end speech recognition in english and
mandarin. arXiv preprint arXiv:1512.02595 (2015).
[2] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. 2016. Revisiting
distributed synchronous SGD. arXiv preprint arXiv:1604.00981 (2016).
[3] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Andrew
Ng. 2013. Deep learning with COTS HPC systems. In International Conference
on Machine Learning. 1337–1345.
[4] Mahieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2014. Train-
ing deep neural networks with low precision multiplications. arXiv preprint
arXiv:1412.7024 (2014).
[5] Jerey Dean, Greg Corrado, Rajat Monga, Kai Chen, Mahieu Devin, Mark Mao,
Andrew Senior, Paul Tucker, Ke Yang, oc V Le, and others. 2012. Large scale
distributed deep networks. In Advances in neural information processing systems.
1223–1231.
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Ima-
genet: A large-scale hierarchical image database. In IEEE Conference on Computer
Vision and Paern Recognition. IEEE, 248–255.
[7] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT
Press.
[8] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep learning with limited numerical precision. In Proceedings of the 32nd
SC17, November 12–17, 2017, Denver, CO, USA Yang You, Aydın Buluc¸, and James Demmel
International Conference on Machine Learning (ICML-15). 1737–1746.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Paern Recognition. 770–778.
[10] Itay Hubara, Mahieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. antized neural networks: Training neural networks with low
precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).
[11] Forrest N Iandola, Mahew W Moskewicz, Khalid Ashraf, and Kurt Keutzer.
2016. FireCae: near-linear acceleration of deep neural network training on
compute clusters. In Proceedings of the IEEE Conference on Computer Vision and
Paern Recognition. 2592–2600.
[12] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. 2016. On large-batch training for deep learning:
Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836 (2016).
[13] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
Masters thesis, Department of Computer Science, University of Toronto (2009).
[14] Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. 2012. Imagenet classica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[15] oc V Le. 2013. Building high-level features using large scale unsupervised
learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on. IEEE, 8595–8598.
[16] Yann LeCun, Le´on Boou, Yoshua Bengio, and Patrick Haner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[17] Chao Li, Yi Yang, Min Feng, Srimat Chakradhar, and Huiyang Zhou. 2016. Op-
timizing memory eciency for deep convolutional neural networks on GPUs.
In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis. IEEE Press, 54.
[18] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed,
Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling
Distributed Machine Learning with the Parameter Server.. In OSDI, Vol. 14.
583–598.
[19] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014. Ecient
mini-batch training for stochastic optimization. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge discovery and data mining. ACM,
661–670.
[20] Maurice Peemen, Bart Mesman, and Henk Corporaal. 2011. Eciency optimiza-
tion of trainable feature extractors for a consumer platform. In International
Conference on Advanced Concepts for Intelligent Vision Systems. Springer, 293–304.
[21] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A lock-free approach to parallelizing stochastic gradient descent. In Advances in
Neural Information Processing Systems. 693–701.
[22] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
DNNs.. In Interspeech. 1058–1062.
[23] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional net-
works for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[24] Ilya Sutskever, James Martens, George E Dahl, and Georey E Hinton. 2013. On
the importance of initialization and momentum in deep learning. (2013).
[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Sco Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going deeper with convolutions. In Proceedings of the IEEE Conference on Com-
puter Vision and Paern Recognition. 1–9.
[26] Yang You, James Demmel, Kenneth Czechowski, Le Song, and Richard Vuduc.
2015. CA-SVM: Communication-avoiding support vector machines on distributed
systems. In Parallel and Distributed Processing Symposium (IPDPS), 2015 IEEE
International. IEEE, 847–859.
[27] Yang You, Xiangru Lian, Ji Liu, Hsiang-Fu Yu, Inderjit S Dhillon, James Demmel,
and Cho-Jui Hsieh. 2016. Asynchronous parallel greedy coordinate descent. In
Advances in Neural Information Processing Systems. 4682–4690.
[28] Sixin Zhang, Anna E Choromanska, and Yann LeCun. 2015. Deep learning with
elastic averaging SGD. In Advances in Neural Information Processing Systems.
685–693.
"
23,"Vision-based Real Estate Price Estimation
Omid Poursaeed,1,3 Toma´sˇ Matera3 and Serge Belongie2,3
1 School of Electrical and Computer Engineering, Cornell University
2 Department of Computer Science, Cornell University 3 Cornell Tech
{op63,sjb344}@cornell.edu, tomas@matera.cz
Abstract
Since the advent of online real estate database compa-
nies like Zillow, Trulia and Redfin, the problem of automatic
estimation of market values for houses has received consid-
erable attention. Several real estate websites provide such
estimates using a proprietary formula. Although these es-
timates are often close to the actual sale prices, in some
cases they are highly inaccurate. One of the key factors that
affects the value of a house is its interior and exterior ap-
pearance, which is not considered in calculating automatic
value estimates. In this paper, we evaluate the impact of vi-
sual characteristics of a house on its market value. Using
deep convolutional neural networks on a large dataset of
photos of home interiors and exteriors, we develop a method
for estimating the luxury level of real estate photos. We also
develop a novel framework for automated value assessment
using the above photos in addition to home characteristics
including size, offered price and number of bedrooms. Fi-
nally, by applying our proposed method for price estima-
tion to a new dataset of real estate photos and metadata, we
show that it outperforms Zillow’s estimates.
1. Introduction
The real estate industry has become increasingly digital
over the past decade. More than 90% of home buyers search
online in the process of seeking a property 1. Homeown-
ers list their properties on online databases like Zillow, Tru-
lia and Redfin. They provide information on characteristics
such as location, size, age, number of bedrooms, number
of bathrooms as well as interior and exterior photos. Home
buyers, owners, real estate agents and appraisers all need a
method to determine the market value of houses.
A core component of real estate websites like Zillow and
Redfin is an automated valuation method (AVM) which es-
1The Digital House Hunt: Consumer and Market Trends in Real
Estate (https://www.nar.realtor/sites/default/files/
Study-Digital-House-Hunt-2013-01_1.pdf)
timates the value of a house based on the user-submitted in-
formation and publicly available data. The Zestimate home
valuation is Zillow’s estimated market value for houses. It is
calculated, using a proprietary formula, for about 110 mil-
lion homes in the United States. It takes into account fac-
tors like physical attributes, tax assessments and prior trans-
actions. The Zestimate has a median error rate of 7.9%2,
which means half of the Zestimates are closer than the error
percentage and half are farther off. Redfin has also released
an estimator tool recently that purportedly outperforms Zes-
timate. It uses massive amounts of data from multiple list-
ing services. Redfin’s estimate considers more than 500
data points representing the market, the neighborhood and
the home itself to arrive at an estimate for 40 million homes
across the United States. It is claimed to have 1.82% me-
dian error rate for homes that are listed for sale, and 6.16%
for off-market homes3.
Neither Redfin nor Zillow consider the impact of inte-
rior and exterior appearance in their automated valuation
methods. However, the visual aspects of a house are key
elements in its market value. Home staging companies use
this fact to make a home more appealing to buyers. In view
of the importance of design and appearance on the value of
a house, in this paper we propose a novel framework for
incorporating the impact of interior and exterior design in
real estate price estimation. By applying our network to a
dataset of houses from Zillow, we evaluate its performance.
Our contributions can be summarized as follows:
• We present the first method which considers the impact
of appearance on real estate price estimation.
• We elicit luxury-related information from real estate
imagery using deep neural networks and crowdsourced
2This value refers to the reported error rate at the time we started col-
lecting data (June 2016). While the latest reported median error rate of Zes-
timate is 5.6% (https://www.zillow.com/zestimate/#acc),
the same approach as what we describe in the paper can be used to de-
crease the error rate.
3About the Redfin Estimate: www.redfin.com/
redfin-estimate
ar
X
iv
:1
70
7.
05
48
9v
3 
 [c
s.C
V]
  3
 O
ct 
20
18
data.
• We release a new dataset of photos and metadata for
9k houses obtained from Zillow. By applying our val-
uation method to this dataset, we show that it outper-
forms Zillow’s estimates.
• We release a new, large-scale dataset of 140k interior
design photos from Houzz. The images are classi-
fied based on their room types: bathroom, bedroom,
kitchen, living room, dining room, interior (miscella-
neous) and exterior.
• We present a qualitative visualization of real estate
photos in which images at similar luxury levels are
clustered near one another.
2. Related Work
We now provide an overview of related work, with a fo-
cus on automated real estate valuation methods and visual
design. We also give a brief overview of machine learning
methods and datasets relevant to our approach.
2.1. Automated Valuation Methods
Real estate price estimation plays a significant role in
several businesses. Home valuation is required for pur-
chase and sale, transfer, tax assessment, inheritance or es-
tate settlement, investment and financing. The goal of au-
tomated valuation methods is to automatically estimate the
market value of a house based on its available information.
Based on the definition of the International Valuation Stan-
dards Committee (IVSC), market value is a representation
of value in exchange, or the amount a property would bring
if offered for sale in the open market at the date of valuation.
A survey of real estate price estimation methods is given in
[22]. In this section, we give an overview of these meth-
ods. To our knowledge, none of these methods consider the
impact of visual features on value estimation.
One of the traditional methods for market valuation is the
“comparables” model, a form of k nearest neighbors regres-
sion. In this model, it is assumed that the value of the prop-
erty being appraised is closely related to the selling prices of
similar properties within the same area. The appraiser must
adjust the selling price of each comparable to account for
differences between the subject and the comparable. The
market value of the subject is inferred from the adjusted
sales prices of the comparables. This approach heavily de-
pends on the accuracy and availability of sale transaction
data [22].
The problem of price estimation can be viewed as a re-
gression problem in which the dependent variable is the
market value of a house and independent variables are home
characteristics like size, age, number of bedrooms, etc.
Given the market value and characteristics for a large num-
ber of houses, the goal is to obtain a function that relates the
metadata of a house to its value. There are many bodies of
work that apply regression methods to the problem of real
estate price estimation. Linear regression models assume
that the market value is a weighted sum of home charac-
teristics. They are not robust to outliers and cannot address
non-linearity within the data. Another model that is used for
price estimation is the hedonic pricing model. It supposes
that the relationship between the price and independent vari-
ables is a nonlinear logarithmic relation. The interested
reader is referred to [22], [3] and [18] for an overview of
regression analysis for price estimation. Other approaches
for price estimation include Artificial Neural Networks [19]
and fuzzy logic [1].
Zillow and Redfin use their own algorithms for real es-
tate price estimation. Home characteristics, such as square
footage, location or the number of bathrooms are given dif-
ferent weights according to their influence on home sale
prices in each specific location over a specific period of
time, resulting in a set of valuation rules, or models that are
applied to generate each home’s Zestimate4. Redfin, hav-
ing direct access to Multiple Listing Services (MLSs), the
databases that real estate agents use to list properties, pro-
vides a more reliable estimation tool5. While Zillow and
Redfin do not disclose how they compute their estimates,
their algorithms are prone to error, and do not consider the
impact of property photos on the market value of residential
properties.
2.2. Convolutional Neural Networks (ConvNets)
Convolutional neural networks (ConvNets) have
achieved state-of-the-art performance on tasks such
as image recognition [14, 9, 10, 26], segmentation
[17, 5, 29, 31], object detection [7, 6, 25] and generative
modeling [8, 24, 11, 13, 23] in the last few years. The
recent surge of interest in ConvNets recently has resulted
in new approaches and architectures appearing on arXiv on
a weekly basis. The interested reader is referred to [16] for
a review of ConvNets and deep learning.
2.3. Scene Understanding
One of the hallmark tasks of computer vision is Scene
Understanding. In scene recognition the goal is to deter-
mine the overall scene category by understanding its global
properties. The first benchmark for scene classification was
the Scene15 database [15], which contains only 15 scene
categories with a few hundred images per class. The Places
dataset is presented in [32], a scene-centric database con-
4What is a Zestimate? Zillow’s Home Value Forecast (http://www.
zillow.com/zestimate/)
5About the Redfin Estimate: www.redfin.com/
redfin-estimate
Figure 1: Examples of correctly and incorrectly classified
pictures. The first row illustrates images classified correctly,
and the second row represents wrongly classified photos.
taining more than 7 million images from 476 place cate-
gories. [30] constructs a new image dataset, named LSUN,
which contains around one million labeled images for 10
scene categories and 20 object categories. Table 1 shows
relevant categories of LSUN, Places and Houzz datasets
as well as the number of images in each category. Sev-
eral categories in the Places dataset are subsumed under the
term “Exterior”: “apartment building (outdoor)”, “build-
ing facade”, “chalet”, “doorway (outdoor)”, “house”, “man-
sion”, “manufactured home”, “palace”, “residential neigh-
borhood” and “schoolhouse”. Miscellaneous indoor classes
such as “stairway” and “entrance hall” are categorized as
“Interior (misc.)”6.
Table 1: Number of images per room category in different
datasets
LSUN Places Houzz
Living Room 1,315,802 28,842 971,512
Bedroom 3,033,042 71,033 619,180
Dining Room 657,571 27,669 435,160
Kitchen/Kitchenette 2,212,277 84,054 1,891,946
Bathroom − 27,990 1,173,365
Exterior − 25,869 868,383
Interior (misc.) − 20,000 368,293
2.4. Visual Design
In spite of the importance of visual design and style, they
are rarely addressed in the computer vision literature. One
of the main challenges in assigning a specific style to an
image is that style is hard to define rigorously, as its inter-
pretation can vary from person to person. In our work, we
are interested in encoding information relevant to the luxury
level of real estate photos.
6While the Houzz dataset contains millions of images in each category,
we download and use 20k images in each category.
An approach for predicting the style of images is de-
scribed in [12]. It defines different types of image styles,
and gathers a large-scale dataset of style-annotated photos
that encompasses several different aspects of visual style. It
also compares different image features for the task of style
prediction and shows that features obtained from deep Con-
volutional Neural Networks (ConvNets) outperform other
features. A visual search algorithm to match in-situ im-
ages with iconic product images is presented in [2]. It also
provides an embedding that can be used for several visual
search tasks including searching for products within a cat-
egory, searching across categories, and searching for in-
stances of a product in scenes. [4] presents a scalable al-
gorithm for learning image similarity that captures both se-
mantic and visual aspects of image similarity. [21] discov-
ers and categorizes learnable visual attributes from a large
scale collection of images, tags and titles of furniture. A
computational model of the recognition of real world scenes
that bypasses the segmentation and the processing of indi-
vidual objects or regions is proposed in [20]. It is based
on a low-dimensional representation of the scene, called the
Spatial Envelope. It proposes a set of perceptual dimensions
that represent the dominant spatial structure of a scene.
3. Our Approach
In order to quantify the impact of visual characteristics
on the value of residential properties, we need to encode
real estate photos based on the value they add to the market
price of a house. This value is tightly correlated with the
concept of luxury. Luxurious designs increase the value of
a house, while austere ones decrease it. Hence, we focus on
the problem of estimating the luxury level of real estate im-
agery and quantifying it in a way that can be used alongside
the metadata to predict the price of residential properties.
3.1. Classifying Photos Based on Room Categories
To make a reasonable comparison, we consider photos
of each room type (kitchen, bathroom, etc.) separately. In
other words, we expect that comparing rooms of the same
type will give us better results than comparing different
room categories. Hence, we trained a classifier to categorize
pictures based on the categories shown in Table 1. In order
to train the classifier, we used data from Places dataset [32],
Houzz and Google image search. Our final dataset contains
more than 200k images.
Using labeled pictures from our dataset, we trained
DenseNet [10] for the task of classifying real estate photos
to the following categories: bathroom, bedroom, kitchen,
living room, dining room, interior (miscellaneous) and ex-
terior. Using this classifier, we achieved an accuracy of 91%
on the test set. After collecting a large dataset of real estate
photos and metadata from Zillow, we applied the classifier
to the images to categorize them based on their room type.
Figure 2: Crowdsourcing user interface for comparing photos based on their luxury level. Each probe image on the left is
compared with 9 other images, uniformly drawn from the dataset. Using these comparisons, we obtain an embedding of real
estate photos based on their luxury level and anchor images which represent different levels of luxury.
Figure 1 shows examples of photos that are classified cor-
rectly and incorrectly. As we can observe from this fig-
ure, the classifier performs well for typical photos, while it
sometimes wrongly classifies empty rooms and those rooms
which combine elements from different categories.
3.2. Luxury Level Estimation
After classifying the images based on their room cate-
gories, we used crowdsourcing for luxury level estimation.
Since our goal is to quantify luxury level of photos, we need
to assign a value to each photo to represent its level of lux-
ury. Hence, we used a classification framework to catego-
rize photos based on their luxury level. However, since it
was not clear how many classes should be used and which
photos should represent each of those classes, we used an-
other crowdsourcing framework to compare images in our
dataset according to their luxury level. Using these compar-
isons, we could obtain a low-dimensional embedding of real
estate photos in which images with the same level of luxury
are clustered near each other. By inspecting the embedding,
we can determine the number of clusters that best represent
variations in luxury, and choose the number of classes for
the classification framework accordingly. We can also sam-
ple images from each cluster to represent different classes
in the classification framework.
3.3. Crowdsourcing Framework
We first discuss our crowdsourcing framework for com-
paring images based on their luxury. Motivated by [28],
we presented a grid user interface to crowd workers, with a
probe image on the left and 9 gallery images on the right.
We asked the workers to select all images on the right that
are similar in terms of luxury level to the image on the left.
In order to extract meaningful comparisons from each grid,
we want it to have images from several different luxury lev-
els. Therefore, for each grid, we need to select images from
our dataset uniformly.
The images from Houzz have a ‘budget’ label which de-
termines the cost of each design. There are 4 different bud-
get levels, and photos with a higher level represent more
luxurious designs. Houses from Zillow are labeled with
their offered price and Zestimates. We expect that houses
with a higher price and Zestimate have more luxurious pho-
tos, and vice versa. Hence, to uniformly divide our dataset,
(a) Bathroom
(b) Living room
(c) Bedroom (d) Kitchen
Figure 3: 2D embedding visualization of real estate photos based on their luxury using the t-STE algorithm. The embedding
is obtained using 10,000 triplet comparisons. More luxurious photos are clustered at the center and more austere ones are
scattered around.
we divided Zillow houses into 2 classes based on the aver-
age value of their offered prices and Zestimates. Moreover,
to add images with low level of luxury to our dataset, we
used Google image search. We searched for terms like ‘ugly
bedroom,’ ‘ugly kitchen,’ etc. In this way, we obtained pho-
tos from ugly and spartan designs which generally decrease
the value of a house.
In order to create each crowdsourcing grid, we sampled
two images from each of the 2 classes of Zillow photos, four
images from each of the 4 ‘budget’ categories of Houzz pic-
tures, two photos from the Places dataset, and two photos
from Google search results. Then we selected one random
picture as the probe and constructed the grid from the other
9 images. In this way, each grid contains photos of several
different luxury levels to help the crowd workers provide
meaningful comparisons among the pictures. A schematic
of our crowdsourcing user interface is shown in figure 2.
We used Amazon Mechanical Turk (AMT) to collect com-
parisons on our images.
Using the crowdsourced data, we obtained triplet com-
parisons based on luxury level for a large-scale dataset of
real estate photos. We then used the t-STE algorithm [27]
to obtain a two-dimensional embedding of the images. The
result is shown in figure 3. By examining the embedding,
we observe that images with similar luxury levels are clus-
tered near one another. This indicates the quality of the
crowdsourced data. Each cluster represents a specific lux-
ury level. We selected one anchor image from each cluster
Figure 4: User interface for classifying real estate photos. Each of the 8 levels of luxury is represented with an anchor image,
and the worker is asked to classify the probe image according to its luxury level.
Figure 5: Examples of bedroom photos classified at different luxury levels. Level 1 represents the least level of luxury, and
level 8 shows the highest.
to represent photos in that cluster. Based on these repre-
sentative pictures, we created another crowdsourcing task
to rank photos according to their degree of luxury. Fig-
ure 4 shows this task for kitchen images. Figure 5 illus-
Figure 6: The price estimation network. After classifying photos based on their room category, a vector representing luxury
is extracted and concatenated with the normalized metadata vector and passed through a regression layer to produce the
estimated price. The loss function is then computed as difference between the estimated price and the purchase price.
trates examples of crowdsourcing results. It demonstrates
that crowd workers generally performed well at categoriz-
ing photos based on their degree of luxury. This is due to
the measures, such as tutorial rounds and catch trials, that
we provided to ensure that workers comprehend the task
and perform it attentively.
3.4. Price Estimation Network
Using the classification framework shown in figure 4, we
obtained luxury levels for a large training set of interior and
exterior photos. We trained DenseNet [10] for the task of
classifying real estate images based on their luxury level
into 8 different categories. Then by using the trained clas-
sifier, we obtained levels of luxury for rooms of the houses
in the Zillow dataset. For each house we obtained the av-
erage level of luxury for each of its room types. In this
way, we obtained 7 values (one for each room type) rep-
resenting luxury of each house. For houses with no pho-
tos of a specific room category, we used the average value
of other categories to represent luxury level of that particu-
lar room. Then we concatenated the metadata vector with
the vector representing the average luxury levels of rooms
of the house. The metadata vector contains all the infor-
mation about home characteristics like offered price, Zes-
timate, size, etc. Since different elements of the metadata
vector (like offered price, age, number of bedrooms, etc.)
are in different ranges, we first normalized the components.
We computed the average value and the standard deviation
of each component of the metadata vector for houses in our
dataset. Then we used z-scoring for normalization: we sub-
tracted the mean from each component and divided it by
its standard deviation. In this way, the mean value of each
entry of the metadata vector is approximately zero, and the
standard deviation is approximately one. This allows us to
obtain the function relating the market price of a house to
its representative vector more easily from a computational
point of view.
The architecture for the price estimation network is
shown in figure 6. In order to train the network, we use
the purchase price of recently sold houses as the ground-
truth labels. As discussed in [22], the price on which the
homeowner and the buyer agree best represents the market
value of a house. As shown in the figure, for each house
in the training dataset, we first classify its photos according
to room type. Pictures of each room type are then classi-
fied based on their luxury level. After extracting and nor-
malizing the metadata vector of each house, we concate-
nate it with the vector that denotes the level of luxury. In
this way, we obtain a representative vector for each house
that captures the impact of its photos and metadata. Then
we use kernel support vector regression (with Radial Ba-
sis Function as the kernel) to relate this vector to the actual
value of the house. The input is the representative vector
of each house and the output is the estimated price of the
house. This price is then compared with the purchase price
as the ground-truth. The difference between them repre-
sents the loss. Using the data from recently sold houses in
our dataset, we obtained regression weights so as to mini-
mize the loss function.
4. Results and Discussion
Using the crowdsourced data, we trained the price esti-
mation network depicted in figure 6. Then we used the net-
work to estimate prices for a separate testing set of recently-
sold houses. The error rate is determined by considering
the difference between the price estimated by our network
with the purchase price of houses in the test set. We evalu-
ated the network’s performance on a testing dataset of 1,000
recently-sold houses from Zillow. We compared the esti-
mated price obtained using our network with the purchase
price to find the error rate. The median error rate of our
network is 5.8 percent which is better than the 7.9 percent
median error rate of the Zestimate. Results are shown in Ta-
ble 2. Our automated valuation method improves upon that
of Zillow by augmenting the input data with images.
4.1. Ablation Studies
We provide ablation analysis to evaluate effectiveness of
different parts of the model.
• We first consider using only the metadata and discard-
ing visual information. The resulting median error rate
is 8.0%, which is very close to the Zestimate error rate.
This is expected as Zestimate is included in the meta-
data. In other words, the regression network almost
ignores all other elements in metadata except the Zes-
timate, which is optimized to be an accurate estimation
of the price.
• In the next experiment, we remove the room classifier
and train a single luxury level estimator for all rooms.
As shown in Table 2, the resulting median error rate is
6.7%. The increase in error rate shows the importance
of room classification as it helps the model focus on the
fine-grained signal relevant to luxury within images of
the same room type.
• We consider directly regressing the price. Instead of
training the network for luxury estimation, we pre-
train it on ImageNet. We use features extracted from
the layer before the final classification layer to repre-
sent images. We train the regression network on these
features and metadata. The resulting median error rate
is 6.6%. If we also fine-tune the feature extraction net-
work, the error rate would be 6.4%. The decreased
performance is due to the fact that the amount of data
containing the housing price is limited. This leads to
features which do not correlate well with the value im-
ages add to the price of the house. However, we have a
larger set of images without housing price values. As
mentioned in the previous section, we annotate these
images with luxury levels, and use that data to train a
network for efficient luxury level estimation. This in
turn helps for the price estimation since luxury is cor-
related with the additive value of images.
• After training each part of the model separately, we
fine-tune the whole model end-to-end. This allows bet-
ter information flow from the regression model to the
luxury estimation network. As demonstrated in Table
2, this leads to slight improvement (0.2%) in the me-
dian error rate.
5. Conclusion
We have presented a novel algorithm to consider the
impact of appearance on the value of residential proper-
Table 2: Median error rate of automated valuation methods
Method Median Error Rate
Zestimate 7.9%
Ours (Vision-based) 5.8%
Only metadata 8.0%
Ours – Room Classifier 6.7%
Direct Regression 6.6%
Direct Regression + Fine-tuning 6.4%
Ours + Fine-tuning 5.6%
ties. After collecting large datasets of real estate photos
and metadata, we used a crowdsourcing pipeline to extract
luxury-related information from interior and exterior pho-
tos. Based on the data we obtained via crowdsourcing, we
trained a convolutional neural network to classify photos
based on their level of luxury. Using purchase price of re-
cently sold houses as their actual value, we trained a net-
work to relate the market value of a house to its photos and
metadata. We used our algorithm to estimate the price of
houses in our dataset, and we showed that it provides a bet-
ter value estimation than Zillow’s estimates. Future avenues
of research include assessing the effect of staging on the
market value of a house, analyzing the impact of different
furniture styles on the luxury level of real estate imagery,
developing a user interface to help users select images that
add more value to their residential properties, evaluating in-
terior and exterior design photos from an aesthetic point of
view, among others.
References
[1] C. Bagnoli and H. Smith. The theory of fuzz logic and its
application to real estate valuation. Journal of Real Estate
Research, 16(2):169–200, 1998.
[2] S. Bell and K. Bala. Learning visual similarity for product
design with convolutional neural networks. ACM Transac-
tions on Graphics (TOG), 34(4):98, 2015.
[3] J. Benjamin, R. Guttery, and C. Sirmans. Mass appraisal:
An introduction to multiple regression analysis for real estate
valuation. Journal of Real Estate Practice and Education,
7(1):65–77, 2004.
[4] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large
scale online learning of image similarity through ranking.
The Journal of Machine Learning Research, 11:1109–1135,
2010.
[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint arXiv:1606.00915, 2016.
[6] R. Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015.
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587,
2014.
[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.
[10] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.
Densely connected convolutional networks. arXiv preprint
arXiv:1608.06993, 2016.
[11] X. Huang, Y. Li, O. Poursaeed, J. E. Hopcroft, and S. J. Be-
longie. Stacked generative adversarial networks. In CVPR,
volume 2, page 3, 2017.
[12] S. Karayev, M. Trentacoste, H. Han, A. Agarwala, T. Darrell,
A. Hertzmann, and H. Winnemoeller. Recognizing image
style. arXiv preprint arXiv:1311.3715, 2013.
[13] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
arXiv preprint arXiv:1710.10196, 2017.
[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012.
[15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing natural
scene categories. In Computer vision and pattern recogni-
tion, 2006 IEEE computer society conference on, volume 2,
pages 2169–2178. IEEE, 2006.
[16] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,
521(7553):436–444, 2015.
[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 3431–3440, 2015.
[18] S. Malpezzi et al. Hedonic pricing models: a selective and
applied review. Section in Housing Economics and Public
Policy: Essays in Honor of Duncan Maclennan, 2003.
[19] S. McGreal, A. Adair, D. McBurney, and D. Patterson. Neu-
ral networks: the prediction of residential values. Journal of
Property Valuation and Investment, 16(1):57–70, 1998.
[20] A. Oliva and A. Torralba. Modeling the shape of the scene: A
holistic representation of the spatial envelope. International
journal of computer vision, 42(3):145–175, 2001.
[21] V. Ordonez, V. Jagadeesh, W. Di, A. Bhardwaj, and R. Pi-
ramuthu. Furniture-geek: Understanding fine-grained furni-
ture attributes from freely associated text and tags. In Ap-
plications of Computer Vision (WACV), 2014 IEEE Winter
Conference on, pages 317–324. IEEE, 2014.
[22] E. Pagourtzi, V. Assimakopoulos, T. Hatzichristos, and
N. French. Real estate appraisal: a review of valuation meth-
ods. Journal of Property Investment & Finance, 21(4):383–
401, 2003.
[23] O. Poursaeed, I. Katsman, B. Gao, and S. Belongie.
Generative adversarial perturbations. arXiv preprint
arXiv:1712.02328, 2017.
[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015.
[25] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Unified, real-time object detection. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 779–788, 2016.
[26] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway
networks. arXiv preprint arXiv:1505.00387, 2015.
[27] L. Van Der Maaten and K. Weinberger. Stochastic triplet
embedding. In Machine Learning for Signal Processing
(MLSP), 2012 IEEE International Workshop on, pages 1–6.
IEEE, 2012.
[28] M. J. Wilber, I. S. Kwak, and S. J. Belongie. Cost-effective
hits for relative similarity comparisons. In Second AAAI
Conference on Human Computation and Crowdsourcing,
2014.
[29] F. Yu and V. Koltun. Multi-scale context aggregation by di-
lated convolutions. arXiv preprint arXiv:1511.07122, 2015.
[30] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and
J. Xiao. Lsun: Construction of a large-scale image dataset
using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.
[31] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene
parsing network. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), pages 2881–2890, 2017.
[32] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Advances in neural information processing sys-
tems, pages 487–495, 2014.
"
24,"Scatteract: Automated extraction of data from
scatter plots
Mathieu Cliche, David Rosenberg, Dhruv Madeka, and Connie Yee
Bloomberg LP
731 Lexington Ave. New York, New York, U.S.A.
{mcliche,drosenberg44,dmadeka1,cyee35}@bloomberg.net
Abstract. Charts are an excellent way to convey patterns and trends in
data, but they do not facilitate further modeling of the data or close in-
spection of individual data points. We present a fully automated system
for extracting the numerical values of data points from images of scatter
plots. We use deep learning techniques to identify the key components of
the chart, and optical character recognition together with robust regres-
sion to map from pixels to the coordinate system of the chart. We focus
on scatter plots with linear scales, which already have several interesting
challenges. Previous work has done fully automatic extraction for other
types of charts, but to our knowledge this is the first approach that is
fully automatic for scatter plots. Our method performs well, achieving
successful data extraction on 89% of the plots in our test set.
Keywords: Computer vision, Information retrieval, Data visualization
1 Introduction
Charts are used in many contexts to give a visual representation of data. How-
ever, there is often reason to extract the data represented by the chart back
into a numeric form, which is easy to use for further analysis or processing. This
problem has been studied for many years, and the solution process may be di-
vided into four steps: chart detection [4,16], chart classification [9,15,17], text
detection and recognition [5,14], and data extraction [14,17]. Our work focuses
on the latter two areas.
We have built Scatteract, a system that takes the image of a scatter plot
as input and produces a table of the data points represented in the plot, in
the coordinate system defined by the plot axes. There are two main steps in this
process. First, we localize three key types of objects in the image: (1) tick marks,
the visual representation of units on an axis (e.g. short lines on the axis), (2)
tick values, the representation of scale on the axis (e.g. numbers written near
the tick marks), and (3) points, the visual elements representing data tuples of
the form1 (x, y). In the chart in Fig. 1, we show the bounding boxes for objects
1 As is traditional, we refer to the horizontal axis as “X” and the vertical axis as “Y”.
ar
X
iv
:1
70
4.
06
68
7v
1 
 [c
s.C
V]
  2
1 A
pr
 20
17
2 Scatteract: Automated extraction of data from scatter plots
of these three types, as produced by our object detection system. The locations
of these bounding boxes are initially described in pixel coordinates, as is natural
for image processing. The second key step in our process is to map the pixel
coordinates of the points into chart coordinates, as defined by the scale on the
chart axes. The end result of our process, a table of data points, is also illustrated
in Fig. 1, along with the ground truth (x, y) coordinates of the data.
Fig. 1. Illustration of input and output of the the end-to-end system. Bounding boxes
are shown to illustrate the result of the object detection and are not present on the
original image. Red bounding boxes identify the tick values, green bounding boxes
identify the tick marks, and blue bounding boxes identify the points.
While information extraction from charts and other infographics has been
widely studied, to our knowledge, our system is the first to extract data from an
image of a scatter plot and represent it in the coordinate system of the chart,
fully automatically. The closest in aim is [3], an unpublished work that describes
an approach to such a system, but only gives test results for the point detection
part. Semi-automatic software tools are available, but they require the user to
manually define the coordinate system of the chart and click on the data points,
or to provide metadata about the axes and data [11,14,19,25].
There exist several systems that attempt to extract key components of charts,
but do not attempt to convert from pixel coordinates to chart coordinates.
Many use heuristics, such as connected component analysis, edge detection and
k-median filtering [8,12,13,15]. Recent work has taken a machine learning ap-
proach. [2] classifies the role of extracted chart components (e.g. bar or legend).
In [24], a deep learning object detection model is trained to detect sub-figures
in compound figures. FigureSeer uses a convolutional neural network to extract
Scatteract: Automated extraction of data from scatter plots 3
features for the localization of lines and heuristics for the localization of tick val-
ues [20]. The end result used for evaluation is the localization of all the relevant
chart components, but does not include the pixel-to-chart coordinate transfor-
mation.
For charts other than scatter plots, there are two systems that aim to con-
vert from pixel coordinates to chart coordinates. The automatic data extraction
in [1] is for bar charts. It extracts the text and numerical values using OCR,
and then recovers the numerical values for each bar by multiplying its height in
pixels by the pixel-per-data ratio. Inaccuracies of the OCR tool resulted in a sig-
nificant number of the charts having incorrectly calculated Y-axis scale values.
The ReVision system proposed in [17] recovers the raw data encoded in bar and
pie charts by extracting the labels with OCR and using a scaling factor to map
from image space to data space. All these systems are highly dependent on the
accuracy of the OCR.
In this paper, we propose Scatteract, an algorithm that leverages deep learn-
ing techniques and OCR to retrieve the chart coordinates of the data points in
scatter plots. Scatteract has three key advantages over the systems mentioned
above. First, to our knowledge, Scatteract is the only system to use deep learn-
ing methods to identify all the key components of a scatter plot. The second
novel aspect of our work is that we created a system for procedurally generating
a large training dataset of scatter plots, for which the ground truth is known,
without requiring any manual labeling. This allows our system to be much more
extensible than heuristic methods built around a set of assumptions. The plot
distribution can be easily modified to accommodate new plot aesthetics, while
heuristic methods may need a complete redesign to accommodate these changes.
The third key advantage of our system is that our method for determining the
mapping from pixel coordinates to chart coordinates is fairly robust to OCR
errors.
This paper is organized as follows. In Sec. 2 we describe the datasets used to
train and test Scatteract. In Sec. 3 we expand on the methodology and present
test results for the building blocks of Scatteract. In Sec. 4 we present a perfor-
mance analysis of the end-to-end system, and we outline our main conclusions
in Sec. 5.
2 Datasets
Scatteract uses an object detection model, which requires a large amount of
annotated data for training. One way to achieve this is to collect a large sample of
scatter plots from the web and manually label the bounding boxes for the objects
of interest (points, tick marks, and tick values). A more efficient approach is to
generate the scatter plots procedurally, so that the bounding boxes are known.
Using this approach, we generated 25,000 train images and 600 test images.
4 Scatteract: Automated extraction of data from scatter plots
Some examples are shown in Fig. 6 and Fig. 8. Besides these artificial charts, we
scraped an additional 50 scatter plots from the web (Fig. 7 and Fig. 9). More
details on how our datasets were obtained are below.
2.1 Procedurally generated scatter plots
To achieve randomness in the scatter plots, we developed a script to randomly
select values that affect the aesthetics and the data of the scatter plot. We used
the Python library Matplotlib [10] to generate the plots since it allows for easy
extraction of the bounding boxes of the points, tick marks, and tick values. The
factors we used to randomize the plot aesthetics and data are listed below.
1. Plot aesthetics
(a) Plot style (default styling values e.g. “classic”, “seaborn”, “ggplot”, etc.)
(b) Size and resolution of the plot
(c) Number of style variations on the points
(d) Point styles (markers, sizes, colors)
(e) Padding between axis and tick values, axis and axis-labels, and plot area and plot title
(f) Axis and ticks locations
(g) Tick sizes if using non-default ticks
(h) Rotation angle of the tick values, if any
(i) Presence and style of minor ticks
(j) Presence and style of grid lines
(k) Font and size of the tick values
(l) Fonts, sizes and contents of the axis-labels and plot title
(m) Colors of the plot area and background area
2. Plot data
(a) Data points distribution (uniformly random, or random around a linear or quadratic dis-
tribution)
(b) Number of points
(c) Order of magnitude of the X and Y coordinates
(d) X and Y coordinates ranges around the selected order of magnitude
(e) Actual values of the X and Y coordinates given the order of magnitude, ranges and distri-
bution
These parameters allow us to build a wide variety of scatter plots, for which
we have full ground-truth labels. Some of the plots generated from this proce-
dure are very difficult and sometimes impossible to read, even for a human. For
example, the tick values can overlap with each other or with the data points, and
the randomly selected font for the tick values can be unreadable. Although we
did not eliminate such unrealistic plots from the training set, we did manually
remove them from the test set.
2.2 Scatter plots from the web
To see how our system, trained on randomly generated scatter plots, generalizes
to real charts, we also collected a small test set of scatter plots that were gen-
erated by humans. We downloaded 50 scatter plots from a Google image search
for “scatter plot”. The only inclusion criteria was the presence of tick values
without units2.
2 It is possible for our system to take units into account, but for simplicity we postpone
this extension to future work.
Scatteract: Automated extraction of data from scatter plots 5
3 Methodology
We take as input the image of a scatter plot, and the output is a set of the
(Xpred, Ypred) chart coordinates of the data points detected. The pipeline is as
follows:
1. Use the object detection model to find bounding boxes for the tick marks, tick
values, and points.
2. Apply OCR on the images inside the bounding boxes of the tick values.
3. Find the closest tick mark to each tick value.
4. Use clustering to assign each (tick mark, tick value) pair to either the X or Y axis.
5. Apply robust regression to determine the mapping from pixel coordinates to chart
coordinates, for each axis.
6. Apply the mapping to the pixel locations of the detected points to build the table
of chart coordinates.
Below we expand on the most important steps.
3.1 Object detection
Object detection is the task of putting bounding boxes around objects that ap-
pear in an image. For our task, we use object detection to localize points, tick
marks, and tick values. There is a wide variety of object detection models, but all
the state-of-the-art methods use deep learning. We chose ReInspect [22] as our
object detection method because it is very effective in crowded scenes, and the
points in scatter plots are often very close together, even overlapping. ReInspect
uses the OverFeat algorithm [18] but adds a long short-term memory network
at the end of it, such that predictions are not made independently of each other.
We used the Tensorflow implementation of ReInspect, TensorBox3. It is standard
practice to initialize the core of the model with a pretrained convolutional net-
work, for which we selected Google Inception V1 [23]. We trained three separate
models: one for tick marks, one for tick values, and one for points. We resized our
scatter plot images to 800x800 pixels for the tick mark and tick value models,
and to 1440x1440 for the point model to help resolve a high density of points.
Each model was trained for 5 epochs on a Geforce GTX Titan X GPU, and the
total training time was about 60 hours. Given an image, a model outputs a set
of bounding boxes for the object it is detecting, along with a confidence score for
each. We only keep detections with a confidence above 0.3, based on validation
experiments.
We can evaluate the performance of the object detection on the procedurally
generated test set. We define a true positive as a predicted bounding box which
has an intersection-over-union ratio with a true bounding box above 50%. We
can then vary the threshold on the confidence of the predicted bounding boxes to
generate recall-precision curves, see Fig. 2. We get an average precision of 87.1%
for the detection of points, 99.1% for the detection of tick values and 93.0% for
3 https://github.com/TensorBox/TensorBox
6 Scatteract: Automated extraction of data from scatter plots
the detection of tick marks. The detection of points is the most difficult, particu-
larly for plots with many overlapping points. However, in the end-to-end system,
failures on the detection of tick marks and tick values can be much more costly,
since they are essential for determining the pixel-to-chart coordinate conversion.
Fig. 2. Recall-Precision curves for the object detection.
3.2 Optical character recognition
To extract the tick values, we apply OCR to the content of the bounding boxes
produced by the tick-value model. We use Google’s open source OCR engine,
Tesseract [21], because of its ease of use. It comes pretrained on several fonts,
and while it is possible to retrain it on custom fonts, we used it as is. However,
we found that performing several preprocessing steps on a tick-value image be-
fore applying Tesseract significantly improves OCR accuracy. We first convert
the image to gray scale and then rescale it so that its height is 130 pixels, while
maintaining its aspect ratio. The most important transformation, however, is a
heuristic procedure to rotate the tick-value images such that they are horizon-
tally aligned, since this is what Tesseract is expecting. This procedure4 finds
the minimum area rectangle that encloses the thresholded tick-value image, and
then rotates the image by an angle that takes into account the angle of the rect-
angle and the number of characters in the tick-value image. The result of this
procedure is illustrated in Fig 3(a). It works well except for tick values that are
rotated by precisely ±90◦ or upside-down.
To test the OCR piece, we used true tick-value images along with their cor-
responding tick values, for each of the plots in the procedurally generated test
4 The procedure is largely inspired by this blog post:
http://felix.abecassis.me/2011/10/opencv-rotation-deskewing/.
Scatteract: Automated extraction of data from scatter plots 7
set. Then, for each tick-value image, we ran our image preprocessing technique,
followed by the Tesseract engine, and compared the predicted tick values with
the ground truth. This gives an accuracy of 91.2%, but without the image pre-
processing the accuracy drops to 63.8%.
(a) Illustration of the results from the
rotation-fixing heuristics. The blue bound-
ing box represents the minimum area rect-
angle that encloses the thresholded image.
(b) Illustration of the clustering problem
used to assign the tick marks to either the
X- or Y-axis.
Fig. 3.
3.3 Axis splitting
Now that we have extracted the tick value associated with each tick-value image,
we need to associate each value with its corresponding tick mark and assign it
to the X-axis or Y-axis. We make use of the observation that if we project the
centers of the tick-mark bounding boxes to the X-axis, the projection of the
ticks from the Y-axis are clustered in one area, while the projection of ticks from
the X-axis are spread out, as illustrated in Fig. 3(b). This therefore effectively
becomes a clustering problem. The algorithm is as follows:
1. Assign each tick value to its closest tick mark.
2. Perform a 1-dimensional DBSCAN [6] clustering algorithm twice: once to
obtain a set of clusters for the X pixel coordinates of the ticks, and another
one to obtain a set of clusters for the Y pixel coordinates of the ticks. The
expected result from the two DBSCANs is one cluster with the X coordinates,
8 Scatteract: Automated extraction of data from scatter plots
these are the ticks that belong on the Y-axis, and one cluster with the Y
coordinates, these are the ticks that belong on the X-axis.
(a) (b)
Fig. 4. In Fig. 4(a) above, the OCR fails to detect the minus sign in front of the
number -5000. While other regression techniques are thrown off by this outlier, Fig.
4(b) shows that RANSAC regression (visualized by the green line) provides a more
robust estimation of the transformation from pixel to chart coordinates.
3.4 RANSAC regression
At this point, we need to find the mapping from pixel to chart coordinates. We
assume the scales are linear, so the conversion is an affine transformation. For
the X-axis, it takes the form Xchart = αxXpixel+βx, and similarly for the Y-axis.
We now create a pair of the form (Xpixel, Xchart) for each tick mark
5/tick value
pair we find on the X-axis, and similarly for the Y-axis. For the chart in Fig.
4(a), we have extracted these pairs for the X-axis and plotted them as black
points in Fig. 4(b). In principle, we can determine our affine transformation
just by performing linear regression on these points. However, standard least-
squares regression is very sensitive to outliers, which means that a single OCR
error can cause a complete failure to estimate the transformation. We therefore
explored alternatives that are more robust to outliers, such as least absolute
deviations (LAD) regression, Theil-Sen Regression, and RANSAC regression.
While all three fared better than linear regression, we obtained the best results
with RANSAC regression [7]. RANSAC regression is an iterative algorithm in
which linear regression is applied to samples of the data points in order to detect
5 We use the center of the tick-mark bounding box as the tick mark location.
Scatteract: Automated extraction of data from scatter plots 9
inliers and outliers from the out-of-sample data points. The model with the most
inliers and the smallest residuals on the inliers is chosen. For RANSAC we need
to choose a loss function, as well as a maximum residual Rmax beyond which a
point is classified as an outlier. We use a square loss function, and choose Rmax
as the median absolute deviation of the tick values squared and divided by 50,
such that for the X-axis:
Rmax,x =
median (Xchart,j −median(Xchart))2
50
. (1)
We apply RANSAC regression for both the X- and Y-axes, such that we end
up with a set of 4 parameters (αx, αy, βx, βy) for each scatter plot. It is then
straightforward to apply these affine transformations to the pixel coordinates of
the center of the bounding box of each point detected, and thus obtain their
location in chart coordinates. Fig. 4 shows how RANSAC improves the estimate
of the affine transformation compared to other regression techniques.
4 Results
4.1 Performance analysis
To evaluate the performance of the end-to-end system, we need to have an eval-
uation metric. We define true positives as predicted points that are within 2%
of true points in chart coordinates:
|Xpred −Xtrue|
∆Xtrue
<= 0.02 and
|Ypred − Ytrue|
∆Ytrue
<= 0.02 (2)
where ∆Xtrue = maxj(Xtrue,j) − minj(Xtrue,j) and ∆Ytrue = maxj(Ytrue,j) −
minj(Ytrue,j) are the true ranges of values for the X and Y chart coordinates.
We compute all pairs of distances between predicted and true points, and we
first select the predicted points closest to each true point in order to find true
positives. If a pair of nearby predicted and true points satisfy the true positive
criteria, both true and predicted points are removed from the true and predicted
set such that we do not overcount true positives. We repeat this process until
either no points are remaining or none of the closest points satisfy the true pos-
itive criteria. With this definition we can evaluate the precision and recall for
each plot. Taking the average across all plots in the procedurally generated test
set, we end up with an average precision of 88% and an average recall of 87%.
We can also compute the F1 score for each plot. Fig. 5 shows the distribution
of F1 scores across all plots in the procedurally generated test set. Based on
Fig. 5, we can see that plots either have a very high F1 score or a very low one.
The latter case indicates that the pixel/chart coordinate transformation was not
determined correctly. We can therefore define a successful data extraction for a
plot if the F1 score is above 80% for that plot. By this definition, our method is
successful for 89% of the plots in the procedurally generated test. We can also use
this definition to evaluate alternative versions of our method. In Table 1, note
10 Scatteract: Automated extraction of data from scatter plots
Fig. 5. F1 score distribution across the plots in the procedurally generated test set.
System Success rate
Scatteract 89.2%
Scatteract without image preprocessing before OCR 43.3%
Scatteract with Theil-Sen instead of RANSAC 83.8%
Scatteract with LAD instead of RANSAC 82.8%
Scatteract with linear regression on 2 tick values instead of RANSAC 70.7%
Table 1. Test results on variations of our system.
that the version that uses linear regression on 2 tick values instead of RANSAC
is analogous to what [3] used to find the conversion factors between pixel and
chart coordinates. RANSAC has a net advantage over the other methods, giving
an absolute performance boost of 5.4% over Theil-Sen, the next best method.
We note that in its current implementation, Scatteract requires 3.5 seconds to
extract the data from a single plot.
Without the ground truth available for the test set from the web, we can-
not evaluate it as systematically as the procedurally generated one. However, a
visual inspection of the plots is sufficient to determine if the data extraction is
successful. To quickly see if the axes were correctly decoded, we can look at a
handful of predicted points and see if they correspond to actual points on the
plots. Moreover, to see if most points were detected correctly, we can inspect
the predicted bounding boxes for the points directly on the plot image. Through
visual inspection we conclude that our method gave a successful data extraction
on 39 plots out of the 50, or 78%. As one might expect, this is lower than on
the procedurally generated test set due to the fact that these plots occasionally
exhibit features which are not present in the training data. These aspects make
identifying all the relevant objects more challenging for the object detection.
Scatteract: Automated extraction of data from scatter plots 11
4.2 Error analysis
Let us now comment on results for individual plots. First, Fig. 6 shows six suc-
cessful data extraction plots from the procedurally generated test set, while Fig.
7 displays six successful data extraction plots from the web test set. Note that
these plots cover a wide range of features occasionally seen in scatter plots. Sec-
ond, Fig. 8 displays two unsuccessful data extraction plots from the procedurally
generated test set, while Fig. 9 displays two unsuccessful data extraction plots
from the web test set. By examining the picture with the predicted bounding
boxes and the result of the OCR we can see which part of the system failed on
the unsuccessful examples. On Fig. 8(a) the tick detection failed on the Y-axis
because it is ambiguous whether the ticks should be on the left or right of the
tick values, and on Fig. 8(b) the OCR failed to pick up the minus sign on several
tick values of the X-axis. Similarly, on Fig. 9(a) our rotation fixing procedure
fails on the 90o-rotated Y tick values, and on Fig. 9(b) the shamrock marker is
detected as three separate points by the object detection. Those errors represent
well the kind of failure that our method can encounter.
5 Conclusion
In this paper we introduced Scatteract, a method used to automatically extract
data from scatter plots. Arguably the most important innovation in Scatteract
is the deep learning object detection models used to locate points, tick marks,
and tick values, combined with the ability to train these models on procedurally
generated scatter plots, for which the ground truth is known. The other main
contribution of Scatteract is the pipeline, from the OCR preprocessing to the
RANSAC regression, for finding the affine transformation between pixel coordi-
nates and chart coordinates. There seems to be relatively little prior work on
this, and we have shown our method to be more robust than the other methods
we are aware of in the literature. The end-to-end system is able to successfully
extract data from 89% of procedurally generated scatter plots and 78% of scatter
plots from the web.
Extending our models to handle new chart aesthetics is straightforward if
we can generate training data with the new aesthetics. While the performance
on the web test set is promising, it is somewhat worse than performance on
the procedurally generated test set. We leave to future work an investigation of
how to improve Scatteract’s generalization ability. We will also pursue a deeper
extraction of information from scatter plots, including point categories, legends,
axis titles, nonlinear scales, scales with units, and non-numeric scales, such as
dates. The ultimate goal is to extend the method to other chart types as well,
so that an end-to-end system would detect charts, classify them (line chart,
bar chart, pie chart, scatter plot, etc.), and then use an appropriate pipeline to
extract the data from the chart.
12 Scatteract: Automated extraction of data from scatter plots
Fig. 6. Successful examples from the procedurally generated test set.
Scatteract: Automated extraction of data from scatter plots 13
(a) https://www.
harrisgeospatial.com/docs/html/
images/SedData_Scatterplot.png
(b) http://d2r5da613aq50s.
cloudfront.net/wp-content/
uploads/460860.image0.jpg
(c) http://www.statisticshowto.
com/wp-content/uploads/2013/08/
spss-scatter-plot-4.png
(d) http://docs.enthought.com/
chaco/_images/scatter_plot.png
(e) https://plot.ly/~RPlotBot/
4326/styled-scatter.png
(f) https://statsmethods.
files.wordpress.com/2013/05/
scatter-plot-2.png
Fig. 7. Successful examples from the web test set.
14 Scatteract: Automated extraction of data from scatter plots
(a) (b)
Fig. 8. Unsuccessful examples from the procedurally generated test set.
(a) http://www.
wekaleamstudios.co.uk/
wp-content/uploads/2010/
04/scatterplot-base.jpeg
(b) https://www.ncl.ucar.edu/
Applications/Images/scatter_
2_1_lg.png
Fig. 9. Unsuccessful examples from the web test set.
Scatteract: Automated extraction of data from scatter plots 15
References
1. Al-Zaidy, R.A., Giles, C.L.: Automatic extraction of data from bar charts. In:
Proceedings of the 8th International Conference on Knowledge Capture. p. 30.
ACM (2015)
2. Al-Zaidy, R.A., Giles, C.L.: A machine learning approach for semantic structur-
ing of scientific charts in scholarly documents. In: Twenty-Ninth IAAI Conference
(2017)
3. Baucom, A., Echanique, C.: Scatterscanner: Data extraction and chart restyling of
scatterplots (2013)
4. Browuer, W., Kataria, S., Das, S., Mitra, P., Giles, C.L.: Segregating and extract-
ing overlapping data points in two-dimensional plots. In: Proceedings of the 8th
ACM/IEEE-CS joint conference on Digital libraries. pp. 276–279. ACM (2008)
5. Chen, Z., Cafarella, M., Adar, E.: Diagramflyer: A search engine for data-driven
diagrams. In: Proceedings of the 24th International Conference on World Wide
Web. pp. 183–186. ACM (2015)
6. Ester, M., Kriegel, H.P., Sander, J., Xu, X., et al.: A density-based algorithm for
discovering clusters in large spatial databases with noise. In: Kdd. vol. 96, pp.
226–231 (1996)
7. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
fitting with applications to image analysis and automated cartography. Communi-
cations of the ACM 24(6), 381–395 (1981)
8. Huang, W., Tan, C.L.: A system for understanding imaged infographics and its ap-
plications. In: Proceedings of the 2007 ACM symposium on Document engineering.
pp. 9–18. ACM (2007)
9. Huang, W., Tan, C.L., Leow, W.K.: Model-based chart image recognition. In: In-
ternational Workshop on Graphics Recognition. pp. 87–99. Springer (2003)
10. Hunter, J.D.: Matplotlib: A 2d graphics environment. Computing In Science &
Engineering 9(3), 90–95 (2007)
11. Jung, D., Kim, W., Song, H., Hwang, J.i., Lee, B., Kim, B., Seo, J.: Chartsense:
Interactive data extraction from chart images. ACM (2017)
12. Kataria, S., Browuer, W., Mitra, P., Giles, C.L.: Automatic extraction of data
points and text blocks from 2-dimensional plots in digital documents. (2008)
13. Lu, X., Kataria, S., Brouwer, W.J., Wang, J.Z., Mitra, P., Giles, C.L.: Automated
analysis of images in documents for intelligent document search. International Jour-
nal on Document Analysis and Recognition (IJDAR) 12(2), 65–81 (2009)
14. Mishchenko, A., Vassilieva, N.: Chart image understanding and numerical data
extraction. In: Digital Information Management (ICDIM), 2011 Sixth International
Conference on. pp. 115–120. IEEE (2011)
15. Nair, R.R., Sankaran, N., Nwogu, I., Govindaraju, V.: Automated analysis of line
plots in documents. In: Document Analysis and Recognition (ICDAR), 2015 13th
International Conference on. pp. 796–800. IEEE (2015)
16. Ray Choudhury, S., Giles, C.L.: An architecture for information extraction from
figures in digital libraries. In: Proceedings of the 24th International Conference on
World Wide Web. pp. 667–672. ACM (2015)
17. Savva, M., Kong, N., Chhajta, A., Fei-Fei, L., Agrawala, M., Heer, J.: Revision:
Automated classification, analysis and redesign of chart images. In: Proceedings of
the 24th annual ACM symposium on User interface software and technology. pp.
393–402. ACM (2011)
16 Scatteract: Automated extraction of data from scatter plots
18. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks.
arXiv preprint arXiv:1312.6229 (2013)
19. Shadish, W.R., Brasil, I.C., Illingworth, D.A., White, K.D., Galindo, R., Nagler,
E.D., Rindskopf, D.M.: Using ungraph to extract data from image files: Verification
of reliability and validity. Behavior Research Methods 41(1), 177–183 (2009)
20. Siegel, N., Horvitz, Z., Levin, R., Divvala, S., Farhadi, A.: Figureseer: Parsing
result-figures in research papers. In: European Conference on Computer Vision.
pp. 664–680. Springer (2016)
21. Smith, R.: An overview of the tesseract ocr engine. In: Document Analysis and
Recognition, 2007. ICDAR 2007. Ninth International Conference on. vol. 2, pp.
629–633. IEEE (2007)
22. Stewart, R., Andriluka, M., Ng, A.Y.: End-to-end people detection in crowded
scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 2325–2333 (2016)
23. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1–9
(2015)
24. Tsutsui, S., Crandall, D.: A data driven approach for compound figure separation
using convolutional neural networks. arXiv preprint arXiv:1703.05105 (2017)
25. Yang, L., Huang, W., Tan, C.L.: Semi-automatic ground truth generation for chart
image recognition. In: International Workshop on Document Analysis Systems. pp.
324–335. Springer (2006)
"
25,"This is a pre-print of the original paper accepted for oral presentation at ICMLA 2016.
Are Accuracy and Robustness Correlated?
Andras Rozsa, Manuel Gu¨nther, and Terrance E. Boult
University of Colorado at Colorado Springs
Vision and Security Technology (VAST) Lab
Email: http://vast.uccs.edu/contact-us
Abstract—Machine learning models are vulnerable to adver-
sarial examples formed by applying small carefully chosen per-
turbations to inputs that cause unexpected classification errors.
In this paper, we perform experiments on various adversarial
example generation approaches with multiple deep convolutional
neural networks including Residual Networks, the best per-
forming models on ImageNet Large-Scale Visual Recognition
Challenge 2015. We compare the adversarial example generation
techniques with respect to the quality of the produced images,
and measure the robustness of the tested machine learning
models to adversarial examples. Finally, we conduct large-scale
experiments on cross-model adversarial portability. We find
that adversarial examples are mostly transferable across similar
network topologies, and we demonstrate that better machine
learning models are less vulnerable to adversarial examples.
I. INTRODUCTION
In the last few years, deep neural networks have become
the most powerful machine learning models and have been
successfully applied to various tasks. Due to the significant
performance improvements on visual recognition tasks, state-
of-the-art machine learning models have managed to obtain
classification accuracies comparable to or even better than
human-level performance [1], [2], [3], [4]. This performance
boost is mainly the result of advances in two technical di-
rections, namely, building more powerful learning models and
designing better strategies to avoid overfitting.
Although deep neural networks provide outstanding perfor-
mance on various recognition tasks, an intriguing property
of these models was revealed by Szegedy et al. [5]. Ma-
chine learning models, including state-of-the-art deep neural
networks, unexpectedly and confidently misclassify inputs
crafted by adding imperceptible, non-random perturbations
to correctly classified images. These perturbed samples that
cause classification errors are called adversarial examples and
their existence reveals at least two problems. First, it demon-
strates that there is a classification inconsistency between
vulnerable machine learning models and human observers, as
adversarial images are generally indistinguishable from their
corresponding original inputs by human perception. Second,
adversarial examples demonstrate that deep neural networks
do not generalize well, in other words, they are not robust to
small perturbations to their inputs.
Szegedy et al. [5] also analyzed the cross-model generaliza-
tion of adversarial examples and concluded that “a relatively
large fraction of examples will be misclassified by networks
trained from scratch with different hyper-parameters (number
(a) VGG-16: white shark
PASS=0.9999, L2=15.84, L∞=1
(b) BVLC-GoogLeNet: hammerhead
PASS=0.9998, L2=60.93, L∞=3
(c) ResNet-50: hammerhead
PASS=0.9992, L2=111.43, L∞=5
(d) ResNet-152: white shark
PASS=0.9985, L2=165.93, L∞=9
Fig. 1: ADVERSARIAL EXAMPLES AND PERTURBATIONS.
This figure shows adversarial examples and their corresponding per-
turbations for a tiger shark image generated by the fast gradient value
(FGV) approach [6] on four deep convolutional neural networks: (a)
VGG-16 [7], (b) Berkeley-trained version of GoogLeNet [1] (BVLC-
GoogLeNet), (c) Residual Network [4] with 50 layers (ResNet-50),
and (d) Residual Network with 152 layers (ResNet-152). The class
label of the adversarially perturbed ImageNet example and corre-
sponding metrics are shown below images: perceptual adversarial
similarity score (PASS) [6] between original and adversarial image
pairs, followed by L2 and L∞ norms of the adversarial perturbation.
For better visualization, perturbations are scaled by a factor of 25
with gray indicating no change. Although all examples are formed
by imperceptible perturbations for a human observer, adversarial
examples originating from better performing models in (c) and (d)
contain perturbations with higher magnitudes.
of layers, regularization or initial weights)”. They quantified
adversarial portability – the ability of adversarial examples
generated on one model to fool another – with respect to a
particular non-convolutional model on the MNIST dataset [8].
Although both Szegedy et al. [5] and Goodfellow et al. [9]
showed examples of adversarial images on ImageNet, they
only performed quantitative evaluation on the MNIST dataset.
Since deep neural networks are vulnerable to attacks by
applying small adversarial perturbations to their inputs, ad-
versarial examples pose a potential security threat for the
application of those machine learning models. The cross-
model portability of adversarial examples indicate a more
severe problem, namely, vulnerable machine learning systems
can be attacked by causing misclassifications without having
1
ar
X
iv
:1
61
0.
04
56
3v
2 
 [c
s.C
V]
  1
 D
ec
 20
16
access to the underlying model.
In this paper, we study different types of adversarial exam-
ples generated on various learning models, and analyze ad-
versarial portability across modern deep convolutional neural
networks. The major contributions of this paper are:
1) We generate adversarial examples on the ImageNet
dataset [10] by using three different low-cost techniques.
We evaluate these techniques by analyzing the produced
adversarial images with respect to various metrics.
2) We evaluate the robustness of eight deep neural networks
– including state-of-the-art models of recent years – to
adversarial examples by analyzing the metrics of the
generated examples.
3) We experimentally evaluate the portability of adversarial
examples across eight deep neural networks. We show
that adversarial images are mostly portable across sim-
ilar network topologies.
4) We find that best performing models are more difficult to
be fooled, i.e., networks with higher accuracies require
greater perturbations to form adversarial examples.
II. RELATED WORK
Adversarial examples in deep neural networks were dis-
covered by Szegedy et al. [5]. The authors demonstrated the
first method to reliably find these perturbations by a box-
constrained optimization technique (L-BFGS) that relies on
internal network state. However, due to the computationally
expensive L-BFGS optimization, this method is impractical.
Furthermore, Szegedy et al. performed experiments on a few
networks and datasets and concluded that a relatively large
fraction of the adversarial examples were misclassified by
different networks trained with varying hyperparameters or on
disjoint training sets.
Goodfellow et al. [9] presented the simpler and compu-
tationally cheaper fast gradient sign (FGS) algorithm for
adversarial generation, and also demonstrated that machine
learning models can benefit from these perturbed inputs. FGS
uses the sign of the gradient of loss with respect to the input
image to form adversarial perturbations. FGS is computa-
tionally efficient as the gradient can be effectively calculated
using backpropagation, and the generated perturbations cause
unexpected classification errors in various machine learning
models. Based upon FGS and by simply using the raw gradient
of loss, Rozsa et al. [6] formalized the fast gradient value
(FGV) method and introduced the hot/cold (HC) approach,
which is capable of efficiently producing multiple adversarial
examples for each input.
Papernot et al. [11] introduced a method to produce adver-
sarial perturbations by leveraging mappings between inputs
and outputs of neural networks. Their experiments on the
MNIST dataset suggest that identifying those mappings is
computationally expensive. Baluja et al. [12] proposed an
approach which generates perturbations, applies them to input
samples, and then observes how models respond to these
perturbed images. They applied small affine image transfor-
mations to form perturbations without utilizing the internal
state of the networks. Also, in order to filter out radical
perturbations and identify useful ones for retraining, Baluja et
al. used peer networks as a control-mechanism. Although these
types of guess and check approaches are capable of finding
adversarial perturbations, they can be prohibitively expensive.
Sabour et al. [13] demonstrated that adversarial images can
be produced by manipulating internal representations to mimic
those that are present in targeted images. Their approach also
relies on the inefficient L-BFGS algorithm. Moosavi-Dezfooli
et al. [14] introduced a method, which can produce adversarial
perturbations with small L2 or L∞ norms, but their approach
is also computationally expensive.
Several approaches have been published to increase the ro-
bustness of vulnerable machine learning models to adversarial
examples [9], [15], [16], [6], [17], [18]. These approaches can
also lead to further improved overall performances as well.
Researchers also try to assess the severity of the security
threat imposed by adversarial examples in the physical world
[19] or even suggest attack strategies that rely on the transfer-
ability of such examples [20], [21]. Specifically, Papernot et
al. [20], [21] suggest a black-box attack by crafting adversarial
samples on one model and apply those on the targeted oracle
to cause misclassification. The authors report that such attacks
can achieve high success rates with samples formed by adver-
sarial perturbations with radically increased magnitudes. Since
the “informal” definition of adversarial examples requires
small perturbations to be applied on original inputs, we note
that FGS examples with visually perceptible modifications
cannot be considered adversarial in nature. Furthermore, recent
advances indicate that those noisy samples would probably be
classified as unknown by open set deep networks [22], and –
depending on the dataset and context – that might eliminate
the threat posed by the introduced attack.
In this paper, we generate various types of adversarial
examples formed by perturbations with minimal magnitudes
that cause misclassifications, and we show on ImageNet that
adversarial portability across deep convolutional neural net-
works is relatively low compared to prior work focusing on
the MNIST dataset.
III. EXPERIMENTS
Szegedy et al. [5] demonstrated that the same adversarial
images are often misclassified by a variety of learning models
with different architectures or trained on varying training data.
In order to have a better understanding of the adversarial
portability problem for deep convolutional neural networks
and to assess the adversarial robustness of those networks,
we perform large-scale experiments with different types of
adversarial images on various machine learning models. In
this section, we introduce the models and the dataset that we
use, and briefly describe our experiments.
A. Models and Dataset
We test adversarial robustness and portability on the
ILSVRC-2014 challenge dataset, which contains 1000 object
categories from the large-scale hierarchical ImageNet [10]
ID MODEL TOP-1 TOP-5
M1 BVLC-AlexNet* 43.230% 20.012%
M2 VGG-16 31.642% 11.556%
M3 VGG-19 31.516% 11.558%
M4 Princeton-GoogLeNet 32.934% 12.104%
M5 BVLC-GoogLeNet 31.070% 10.856%
M6 ResNet-50 27.124% 8.864%
M7 ResNet-101 25.656% 8.054%
M8 ResNet-152 25.090% 7.798%
TABLE I: ERROR RATES. This table lists the top-1 and top-
5 classification error rates of the investigated models on the Ima-
geNet validation dataset. For consistency, we report error rates on
224× 224 pixel center crops from 256× 256 scaled images.
database with approximately 1.28M training images, 50K val-
idation images, and 100K test images (test images are publicly
not available). Performance of machine learning models is
evaluated by top-1 and top-5 accuracies.
The deep neural networks that we use in our experiments
are all publicly available: BVLC-AlexNet is a Berkeley-
trained version of the model introduced by Krizhevsky et
al. [23], VGG-16 and VGG-19 are 16-layer and 19-layer
networks of Simonyan et al. [7], BVLC-GoogLeNet is the
Berkeley-trained version of the network designed by Szegedy
et al. [1], Princeton-GoogLeNet is the GPU implementation
of GoogLeNet by the Princeton Vision Group and, finally,
ResNet-50, ResNet-101, and ResNet-152 are the 50-, 101-,
and 152-layer networks of He et al. [4], respectively.
To be able to test portability, we need all models to
operate on images having the same dimensions. Since BVLC-
AlexNet works with 227× 227 pixel images, while the others
use 224× 224, we fine-tuned the model on the training set
with the smaller crop size (a single epoch with the provided
hyperparameters and a constant learning rate of 10−4). In the
rest of the paper, we refer to this fine-tuned model as BVLC-
AlexNet*.
The performance of the eight models on the ImageNet
validation set is listed in Tab. I. We obtained these error rates
by using a single center crop from each of the 256× 256
scaled training images. Note that for some models, better
performance can be achieved by using 10-crop error (averaging
softmax scores of 10 224× 224 pixel crops) and/or by scaling
images with shorter side to 256 pixels.
For each of the 1000 classes, we selected 10 images from
the training set that were correctly classified by all eight
models. Therefore, the dataset that we use for our experiments
on adversarial images contains 10K images.
B. Adversarial Generation
In order to evaluate the adversarial robustness of the selected
models and quantify adversarial portability across them, we
use three adversarial example generation methods. First, the
fast gradient sign (FGS) method introduced by Goodfellow
et al. [9]. Second, the fast gradient value (FGV) approach
formalized in [6], which is built upon FGS. While FGS takes
steps in the direction defined by the sign of the gradient of loss
with respect to the image in order to reduce the score of the
correct class and cause mislabeling, FGV uses a scaled version
of the raw gradient of loss and produces notably different
adversarial perturbations than FGS. Third, the hot/cold (HC)
approach introduced by Rozsa et al. [6], which is capable of
generating multiple diverse adversarial samples from an input
by not only reducing the score of the original correct class –
denoted as the cold class – but in parallel by increasing the
score of a selected hot class. Specifically, we use the hot/cold
approach with only the most similar class with respect to the
classification score as hot (HC1) and, hence, we generate only
one adversarial example for each input with this technique.
We have selected these three adversarial generation methods
because they are computationally efficient and also able to
produce diverse samples.
We commence our experiments generating adversarial im-
ages on our dataset of 10K images. We use the three afore-
mentioned adversarial generation methods (FGS, FGV, and
HC1) with the previously listed eight deep neural networks and
collect various metrics on the produced samples to compare
the adversarial generation methods. As pointed out by Sabour
el al. [13], L2 and L∞ norms of adversarial perturbations are
not matched well to human perception. Since these measures
are extremely sensitive to even small geometric distortions and
do not map well to psychophysical notions of similarity, we
also use the perceptual adversarial similarity score (PASS) [6]
to better quantify “adversarial” in terms of human perception.
Additionally, we measure the adversarial success rate, i.e., the
relative number of images for which an adversarial example
can be generated. Adversarial image generation can fail in
two ways. First, the adversarial direction (e.g., the gradient
of loss with respect to the input image used by FGS and
FGV methods) can be exactly zero for all pixels. Second, any
arbitrarily large step into the adversarial direction – limited by
the discrete pixel values in range [0, 255] – does not change
the original label.
As we can see in Fig. 2(a), all three adversarial generation
methods maintain high success rates in terms of producing
adversarial images for their inputs on various deep neural net-
works. Particularly, HC1 reaches approximately 100 % success
rate on each model. Based on the metrics shown in Fig. 2, we
can observe that FGV and HC1 methods produce adversarial
perturbations with significantly higher L∞ norms, while the
formed adversarial images still maintain comparable or even
higher PASS scores than others generated by FGS.
C. Adversarial Robustness
By investigating the collected metrics of adversarial im-
ages generated by various methods on given deep neural
networks, we can compare the robustness of those networks
to adversarial examples. Specifically, increased L2 and L∞
norms of adversarial perturbations paired with decreased PASS
scores indicate higher robustness. In other words, models
that require samples formed by adversarial perturbations with
0.95	
0.96	
0.97	
0.98	
0.99	
1.00	
0.75	
0.80	
0.85	
0.90	
0.95	
1.00	
FG
S-M
1	
FG
S-M
2	
FG
S-M
3	
FG
S-M
4	
FG
S-M
5	
FG
S-M
6	
FG
S-M
7	
FG
S-M
8	
FG
V-M
1	
FG
V-M
2	
FG
V-M
3	
FG
V-M
4	
FG
V-M
5	
FG
V-M
6	
FG
V-M
7	
FG
V-M
8	
HC
1-M
1	
HC
1-M
2	
HC
1-M
3	
HC
1-M
4	
HC
1-M
5	
HC
1-M
6	
HC
1-M
7	
HC
1-M
8	
Su
cc
es
s	R
at
e	
(%
)	
PA
SS
	
Adversarial	Type	
PASS	 Success	Rate	(%)	
(a) PASS and success rates for various types of adversarial images
0	
1000	
2000	
3000	
4000	
5000	
6000	
0	
20	
40	
60	
80	
100	
120	
FG
S-M
1	
FG
S-M
2	
FG
S-M
3	
FG
S-M
4	
FG
S-M
5	
FG
S-M
6	
FG
S-M
7	
FG
S-M
8	
FG
V-M
1	
FG
V-M
2	
FG
V-M
3	
FG
V-M
4	
FG
V-M
5	
FG
V-M
6	
FG
V-M
7	
FG
V-M
8	
HC
1-M
1	
HC
1-M
2	
HC
1-M
3	
HC
1-M
4	
HC
1-M
5	
HC
1-M
6	
HC
1-M
7	
HC
1-M
8	
L-
2	
L-
in
f	
Adversarial	Type	
L-inf	 L-2	
(b) L∞ and L2 norms for various types of adversarial images
Fig. 2: METRICS FOR ADVERSARIAL IMAGES. This figure shows various metrics for adversarial images generated by fast gradient
sign (FGS), fast gradient value (FGV), and hot/cold (HC1) approaches on the networks listed in Tab. I using 10K images of the ImageNet
training set. In (a) the mean and standard deviations of PASS scores and the adversarial success rates are displayed, while in (b) the means
and standard deviations of L2 and L∞ norms are presented.
higher magnitudes in terms of L2 and L∞ norms to cause
misclassifications are more robust to adversarial examples.
As shown by the collected metrics in Fig. 2(b), L2 and L∞
norms of adversarial perturbations generally increase as we
generate adversarial examples on better performing machine
learning models. We can see in in Fig. 2(a) that these stronger
perturbations also result in decreased PASS scores. Based on
these results, we can conclude that better performing models
lead to increased robustness to adversarial examples as well.
To better illustrate this phenomenon, we show FGV adver-
sarial examples with corresponding perturbations in Fig. 1
crafted from the same input image on four different models.
Although these samples contain imperceptible modifications
to the human eye, we can observe that a significantly stronger
perturbation is required to cause misclassification on the cur-
rent state-of-the-art Residual Network, denoted as ResNet-152
in Fig. 1(d), compared to VGG-16 shown in Fig. 1(a). Finally,
we can state that not only the magnitudes and structures of
these perturbations vary, but they can lead to different mis-
classifications. Depending on the model, the same adversarial
image generation approach turns the tiger shark into a white
shark or a hammerhead.
D. Adversarial Portability
To evaluate the transferability of adversarial examples
across deep convolutional neural networks, we use the differ-
ent types of adversarial images – formed by using either the
fast gradient sign (FGS) method, the fast gradient value (FGV)
method, or the hot/cold approach with the most similar class
in terms of classification score (HC1) – that we generated on
the dataset described in Sec. III-A. Since FGS, FGV, and HC1
methods all rely on internal network state, the perturbations
that form adversarial images on various networks can be
significantly different – as demonstrated by the FGV examples
shown in Fig. 1 that were generated on a tiger shark.
To quantify adversarial portability, we test the constructed
adversarial examples across the eight deep convolutional neu-
ral networks listed in Tab. I, and calculate the proportion of
transferable samples. The graphically summarized results are
presented in Fig. 3. Apparently, adversarial images originating
from similar models or models sharing the same network
architecture are generally more portable across corresponding
machine learning models. For example, 29.00 % of FGS adver-
sarial images generated on the VGG-16 network (denoted as
FGS-M2 in Fig. 3) remain adversarial on the VGG-19 model,
and 27.59 % of VGG-19 samples (FGS-M3) are transferable to
VGG-16. Similar patterns can be observed between the BVLC-
GoogLeNet and the Princeton-GoogLeNet models: 14.23 %
of the BVLC-GoogLeNet FGS adversarial images (FGS-M5)
are transferable to Princeton-GoogLeNet and 12.88 % remain
portable in the opposite direction (FGS-M4). Although the
portability rates between Residual Networks are lower, they
FG
S
-M
1
FG
S
-M
2
FG
S
-M
3
FG
S
-M
4
FG
S
-M
5
FG
S
-M
6
FG
S
-M
7
FG
S
-M
8
FG
V
-M
1
FG
V
-M
2
FG
V
-M
3
FG
V
-M
4
FG
V
-M
5
FG
V
-M
6
FG
V
-M
7
FG
V
-M
8
H
C
1
-M
1
H
C
1
-M
2
H
C
1
-M
3
H
C
1
-M
4
H
C
1
-M
5
H
C
1
-M
6
H
C
1
-M
7
H
C
1
-M
8
BVLC-AlexNet* (M1)
VGG-16 (M2)
VGG-19 (M3)
Princeton-GoogLeNet (M4)
BVLC-GoogLeNet (M5)
ResNet-50 (M6)
ResNet-101 (M7)
ResNet-152 (M8)
0%
25%
50%
75%
100%
Fig. 3: CROSS-MODEL PORTABILITY OF ADVERSARIAL EXAMPLES. This figure shows the percentage of adversarial examples
generated on one network that are adversarial to other networks. Adversarial examples of types FGS, FGV, and HC1 are generated on the
learning models as listed in Tab. I (shown on the horizontal axis), and tested on the networks displayed in the vertical axis. The diagonals
display the self-portability of adversarial examples, which is 100 % in each case.
exhibit the same behavior: 12.13 % of FGS samples originat-
ing from ResNet-101 (FGS-M7) cause misclassifications on
ResNet-50, while 7.97 % of adversarial samples are portable
backwards (FGS-M6).
Fig. 3 displays that the tested models are more robust to
FGV and HC1 samples than to FGS samples. We believe that
the higher portability of FGS samples is due to the application
of the sign in the fast gradient sign method, which provides
equal importance to pixels regardless their actual raw value in
the gradient of loss. Specifically, FGS applies unnecessarily
large modifications on pixels that do not play an important role
in reducing the loss, and these redundant perturbations make
FGS examples more transferable. Therefore, the portability
rates for FGV and HC1 adversarial images are significantly
lower than for FGS samples. The most transferable adversarial
images among FGV and HC1 are HC1 samples of the ResNet-
152 model (denoted as HC1-M8 in Fig. 3) tested on BVLC-
AlexNet* network with 9.99 % portability rate. In other words,
HC1 adversarial samples generated on the best performing
network cause the most classification errors on the worst
performing model. On the other hand, adversarial examples
generated on BVLC-AlexNet* are nearly never portable to
other networks.
As shown in Fig. 3, adversarial examples generated on the
best performing networks, i.e., the Residual Networks, are
generally more portable to others. Since those networks are
the most robust among the tested models, their adversarial
examples contain the strongest perturbations and, therefore,
we assume that those samples become more transferable.
In summary, the results suggest that FGS examples are
more transferable than FGV or HC1 samples, and adversarial
images are mainly portable across similar networks, e.g., VGG
models (VGG-16, VGG-19), GoogleNets (BVLC-GoogLeNet,
Princeton-GoogLeNet) or Residual Networks (ResNet-50,
ResNet-101, ResNet-152). However, the tranferability rates
that we obtained on the ImageNet dataset are considerably
lower than Szegedy et al. [5] observed when they analyzed
cross-model generalization of adversarial examples on the
MNIST dataset.
IV. DISCUSSION
In this paper, we have generated adversarial examples on
eight deep convolutional neural networks, including state-of-
the-art models of recent years, with three different adversarial
generation methods. We have evaluated these methods with
respect to their success rates, and quantitatively compared
various types of adversarial images. We have found that all
three methods – fast gradient sign (FGS), fast gradient value
(FGV), and hot/cold (HC) – can efficiently produce adversarial
samples on all tested models.
By analyzing the collected metrics – L2 and L∞ norms of
adversarial perturbations, and perceptual adversarial similarity
score (PASS) of original and adversarial image pairs – on
the tested models, we have observed that better performing
networks are more robust to adversarial perturbations. We
believe that more accurate deep convolutional neural networks
learn feature mappings for the given classification task that
make classes more separable and, therefore, these models gen-
eralize better, leading to both higher accuracies and improved
robustness.
We have performed large-scale experiments on the Ima-
geNet dataset to investigate the portability of various types of
adversarial images across multiple deep convolutional neural
networks including the fine-tuned version of BVLC-AlexNet,
the publicly available versions of VGG-16 and VGG-19,
Princeton-GoogLeNet, BVLC-GoogLeNet, and three versions
of Residual Network models. As our results suggest, adver-
sarial images are more transferable between networks sharing
the same or similar network architectures, and adversarial
samples originating from better performing models are more
transferable to more vulnerable networks due to their stronger
adversarial perturbations. Furthermore, our experiments have
highlighted that adversarial images generated by FGS are more
portable than others produced by FGV or HC.
In our cross-model transferability experiments, we have
used adversarial examples formed by perturbations with min-
imal magnitudes that cause misclassifications. We note that
sometimes these perturbations are highly perceptible and,
therefore, the crafted samples cannot be considered adversar-
ial. While those noisy samples are not adversarial in nature,
they still have an effect on our measured portability rates
as, due to their stronger perturbations, they are certainly
more portable across networks. The perceptual adversarial
similarity score (PASS) seems to be the straightforward mea-
sure to differentiate adversarial from noisy samples, i.e., to
quantitatively define adversarialness by measuring the simi-
larity/distinguishability of original and perturbed image pairs.
Specifying an applicable threshold for PASS scores to define
adversarial is beyond the scope of this paper.
There are already applied mechanisms to inadvertently
mitigate the transferability problem of adversarial examples.
To achieve better performances on various visual recognition
tasks, machine learning systems often use multiple crops for
classification. Namely, several crops are extracted from the
input and after classifying them independently, the model
makes the final classification by fusing the results obtained
from the crops. Luo et al. [15] proposed a mechanism to
alleviate the recognition errors caused by adversarial images,
which is basically based upon cropping. Their foveation-based
technique uses only a sub-region of the image during classi-
fication. The authors demonstrated that the negative effect of
foveated perturbations to classification scores can be signif-
icantly reduced compared to entire perturbations, suggesting
that foveation approaches can improve the robustness of neural
networks to adversarial examples. Another popular approach
to obtain better classification performances is the application
of multiple models by using an ensemble of networks. For
instance, He et al. [4] applied an ensemble of three Residual
Networks. As we demonstrated, the proportion of portable
adversarial images among Residual Networks is relatively low.
Therefore, the application of ensembles can further improve
the robustness to adversarial examples.
In summary, considering our experimental results and the
aforementioned techniques applied in real-world applications,
we conclude that the security threat posed by adversarial
portability is moderate, at most, but this area is still important
for applications and future research.
ACKNOWLEDGMENT
This research is based upon work funded in part by NSF
IIS-1320956 and in part by the Office of the Director of
National Intelligence (ODNI), Intelligence Advanced Research
Projects Activity (IARPA), via IARPA R&D Contract No.
2014-14071600012. The views and conclusions contained
herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements,
either expressed or implied, of the ODNI, IARPA, or the
U.S. Government. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
REFERENCES
[1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2015, pp. 1–9. 1, 3
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:
Surpassing human-level performance on ImageNet classification,” in
IEEE International Conference on Computer Vision (ICCV), 2015, pp.
1026–1034. 1
[3] J.-C. Chen, V. M. Patel, and R. Chellappa, “Unconstrained face ver-
ification using deep CNN features,” in IEEE Winter Conference on
Applications of Computer Vision (WACV), 2016. 1
[4] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 1, 3, 6
[5] C. J. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-
fellow, and R. Fergus, “Intriguing properties of neural networks,” in
International Conference on Learning Representation (ICLR), 2014. 1,
2, 5
[6] A. Rozsa, E. M. Rudd, and T. E. Boult, “Adversarial diversity and
hard positive generation,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, 2016. 1, 2, 3
[7] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” in Proceedings of the International
Conference on Learning Representations (ICLR), 2015. 1, 3
[8] Y. LeCun, C. Cortes, and C. J. Burges, “The mnist database of
handwritten digits,” 1998. 1
[9] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in International Conference on Learning Repre-
sentation (ICLR), 2015. 1, 2, 3
[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
A large-scale hierarchical image database,” in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248–255.
2
[11] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in IEEE European Symposium on Security and Privacy (Euro S&P),
2016, arXiv preprint arXiv:1511.07528. 2
[12] S. Baluja, M. Covell, and R. Sukthankar, “The virtues of peer pressure:
A simple method for discovering high-value mistakes,” in Computer
Analysis of Images and Patterns (CAIP). Springer, 2015, pp. 96–108.
2
[13] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet, “Adversarial manipula-
tion of deep representations,” in International Conference on Learning
Representations (ICLR), 2016. 2, 3
[14] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: a simple
and accurate method to fool deep neural networks,” in IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016. 2
[15] Y. Luo, X. Boix, G. Roig, T. Poggio, and Q. Zhao, “Foveation-based
mechanisms alleviate adversarial examples,” 2015, under review. 2, 6
[16] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in IEEE Symposium on Security and Privacy (SP), 2015. 2
[17] T. Miyato, A. M. Dai, and I. Goodfellow, “Virtual adversarial training
for semi-supervised text classification,” 2016, under review. 2
[18] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the
robustness of deep neural networks via stability training,” 2016, under
review. 2
[19] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
physical world,” Google, Tech. Rep., 2016. 2
[20] N. Papernot, P. McDaniel, and I. J. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” 2016, under review. 2
[21] N. Papernot, P. McDaniel, I. J. Goodfellow, S. Jha, Z. Berkay Celik, and
A. Swami, “Practical black-box attacks against deep learning systems
using adversarial examples,” 2016, under review. 2
[22] A. Bendale and T. Boult, “Towards open set deep networks,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
2
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classifica-
tion with deep convolutional neural networks,” in Advances in Neural
Information Processing Systems (NIPS), 2012, pp. 1097–1105. 3
"
26,"IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2016 1
Action Recognition based on Efficient Deep Feature
Learning in the Spatio-Temporal Domain
Farzad Husain, Babette Dellen and Carme Torras
Abstract—Hand-crafted feature functions are usually designed
based on the domain knowledge of a presumably controlled
environment and often fail to generalize, as the statistics of real-
world data cannot always be modeled correctly. Data-driven
feature learning methods, on the other hand, have emerged
as an alternative that often generalize better in uncontrolled
environments. We present a simple, yet robust, 2D convolutional
neural network extended to a concatenated 3D network that
learns to extract features from the spatio-temporal domain of
raw video data. The resulting network model is used for content-
based recognition of videos. Relying on a 2D convolutional neural
network allows us to exploit a pretrained network as a descriptor
that yielded the best results on the largest and challenging
ILSVRC-2014 dataset. Experimental results on commonly used
benchmarking video datasets demonstrate that our results are
state-of-the-art in terms of accuracy and computational time
without requiring any preprocessing (e.g., optic flow) or a priori
knowledge on data capture (e.g., camera motion estimation),
which makes it more general and flexible than other approaches.
Our implementation is made available.
Index Terms—Computer vision for automation, recognition,
visual learning.
I. INTRODUCTION
BUILDING personal robots for tasks involving assistanceand interaction with humans carries several challenges.
One key challenge is to perceive and interpret dynamic human
environments. This is necessary for the active engagement of
the robot. Several attempts have been made to address the
different perception aspects of such dynamic environments
where the robot is meant to assist, such as tracking a hand-held
object for grasping [1], capturing human motion [2], activity
recognition [3] and sensing the human behaviors [4].
One important objective is the detection and recognition of
daily human activities. Actions such as brushing hair, eating,
drinking, chewing, sitting, walking, standing, etc., implicitly
encompass the structure of a particular human environment.
Successful recognition of these actions simplifies several tasks
that are aimed for such robotic assistants. For example, assist-
ing the elderly in timely caregiving [5], [6], in the situation of
accidents [7] or in the daily life activities [8].
Manuscript received: August 31, 2015; Revised December 18, 2015;
Accepted January, 28, 2016. This paper was recommended for publication
by Editor Jana Kosecka upon evaluation of the reviewers’ comments. This
research is partially funded by the CSIC project TextilRob (201550E028),
and the project RobInstruct (TIN2014-58178-R).
F. Husain and C. Torras are with the Institut de Robo`tica i Informa`tica
Industrial, CSIC-UPC, Llorens i Artigas 4-6, 08028, Barcelona, Spain (e-mail:
{shusain, torras}@iri.upc.edu).
B. Dellen is with the RheinAhrCampus der Hochschule Koblenz, Joseph-
Rovan-Allee 2, 53424 Remagen, Germany (e-mail: dellen@hs-koblenz.de).
Digital Object Identifier (DOI): see top of this page.
Recognizing human activities for robots is conventionally
tackled using a pipeline approach, by first (i) modeling the dy-
namics of changing environments using a graphical model [9],
[10], [11] or identifying descriptive features [12], [13], [14],
[15], and then (ii) performing classification [16], [17]. The first
part requires extraction of motion information through some
mechanism. Possible approaches include the computation of
optic flow or motion modeling. However, recent benchmarks
have revealed that there is no universally accepted model that
could outperform others for all datasets [18]. The reason is that
the statistics of datasets can be considerably different, and a
particular model might perform better for one dataset than for
another. Many spatio-temporal descriptors are extensions from
single image descriptors such as SIFT3D [17], HOG3D [12]
and SURF3D [19]. However, such extensions also inherit the
limitations in performance generalization as shown in [20],
making clear the advantage of learned features over hand-
crafted ones.
Deep Convolutional Neural Networks (CNNs) [21] have
emerged as a state-of-the-art solution for a wide range
of computer vision problems, such as image segmenta-
tion/labeling [22], [23], object detection/localization [24] and
pose recovery [25], [26]. The main advantage over the con-
ventional pipeline approaches is that CNNs can be trained
end-to-end (from raw pixels to labels) in a fully supervised
way. One drawback of fully supervised deep learning is that
it requires a huge number of labeled training examples [27].
Recently, it has been shown that a CNN model trained from
a large dataset can be transferred to other visual recognition
tasks with limited training data and thereby leading to higher
accuracy and shorter training period [24], [28]. Since single-
image input-based models that have been trained over a million
labeled images are now readily available [29], we see attempts
to exploit these networks in the video domain [30], [31].
However we observe limited success when learning directly
in the temporal domain.
We also notice that weakly annotated video data is becom-
ing prevalent as time goes by. For example, 300 hours of video
are uploaded to Youtube every minutea. Such abundance of
video data opens up the opportunity to exploit the infinite
space of possible actions in the context of human action recog-
nition [31]. Visual recognition methods have to interpret video
data displaying a large degree of variability and complexity in
order to arrive at a semantic description, i.e., the action class
of the recorded scene.
We propose to recognize human actions using the trans-
fer learning technique. A pretrained single-image recognition
ahttps://www.youtube.com/yt/press/statistics.html
2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2016
model is adapted for videos by temporally concatenating the
output of its deepest spatial convolution layer. The input to
the model are the individual frames of the video. Afterwards,
the concatenated output is used as an input to a network
comprising 3D convolutions that we train.
The feature representation becomes more abstract as we go
deeper in a network thereby obscuring the locally occurring
temporal changes in a video. This poses a limitation to the tem-
poral features that the network learns from the concatenated
output. We overcome this limitation by combining the output
of our learned network with another pretrained model [32]
which employs 3D convolutions from the beginning. The
complementary nature of the two features becomes evident
from the improved recognition accuracy in our experiments.
The combined output contains fewer trainable parameters
thereby allowing us to use a more efficient optimization
method (L-BFGS) [33]. Our model does not require any
pre-computation of features such as optic flow or any other
domain-specific processing, thereby making it generic and
computationally efficient.
Our main contributions are:
• the introduction of a concatenation scheme in the tem-
poral domain to extend the usage of pretrained models
learned from a single image to the video domain,
• combining our learned network with another action recog-
nition model, which yields improved results as compared
to the individual networks, and
• evaluation and comparison with commonly used bench-
marking video datasets.
II. RELATED WORK
Several action recognition methods have been proposed in
the past. We roughly group them into two categories. First is
the conventional pipeline approach (descriptor followed by a
classifier) [34], [35], [17], [12], [19], [36] and second is the
convolutional model [20], [37], [38], [39], [31] which is the
basis of our approach.
In [34], improved dense trajectories are produced by re-
ducing the camera motion effect, which is estimated using
the SURF descriptor [40]. However, for recognizing human
actions, inconsistent matches from SURF are removed by ex-
ploiting domain knowledge, i.e., by adding a human detector.
A higher-level representation of activities, named as “action
bank,” combined with a linear SVM classifier is proposed
in [35].
Another way of representing actions is through spatio-
temporal segmentation of dynamic scenes. The segmented
surfaces and how they change over time gives cues about
the kind of manipulation actions which are being carried out.
The manipulation actions can be encoded in the form of
“Semantic Event Chains“ [36]. These event chains represent
the spatial relations between objects in the scene. Any change
in the spatial relation serves as a decisive key point through
which a manipulation could be defined. Similarly, temporal
segmentation of a video into multiple events is proposed
in [41].
An unsupervised learning method based on convolution and
stacking has also been proposed [20]. The convolved output of
arbitrarily sized videos is made constant by dimensionality re-
duction using principal component analysis. The time-efficient
dimensionality reduction for long video clips has a relatively
larger memory requirement (up to 32 GB). In [37], a spatio-
temporal sparse auto-encoder is trained in an unsupervised
way for classifying video sequences. The convolutional gated
Restricted Boltzmann Machine architecture [42] has been
extended to 3D to learn relations between pairs of adjacent
images in videos and is used to extract features for activity
recognition [38].
A 3D CNN model has also been previously proposed [39].
In this model, features are learned simultaneously in the spatial
and temporal dimensions by performing 3D convolutions. The
model is applied to real-world environments to recognize
human actions. However, other than the raw images, a set of
hardwired kernels is created to generate the gradients and optic
flow which should be learned by the proposed convolutional
network. In addition, a human detector is introduced and
foreground extraction is performed. On the contrary, we feed
the raw image data directly to our network and do not compute
any handcrafted feature.
In [30], a two-stream network is proposed, where each
frame of the video is used as an individual image during
training. One stream is trained on raw images and the other is
trained with optical flow fields computed from the consecutive
video frames. Recognition is attained using a score aggregation
strategy across all the video frames of both streams.
Using pretrained models is also proposed in [31], [43],
[32]. Our model could be categorized as the late fusion
model from [31], in which multiple networks are fused in
the final fully connected layers. However, we use a different
network architecture which is pretrained on a single-image
database instead of a video database, thereby reducing the
computational cost. We get significantly better results without
requiring fine tuning of the pretrained models. Along the same
lines, different aggregation strategies for per frame image-
based features is investigated in [43].
Our approach is closely related to [32]. A 3D convnet is
defined with convolutional kernels up to the first 8 layers.
However, we use 3D convolutional kernels after extracting the
output from a very deep pretrained model and thereby learn
features in the temporal domain at a higher level of abstraction.
III. PROBLEM FORMULATION
Given a set of videos, obtain a label for each video charac-
terizing its content. The video can be of arbitrary spatial and
temporal dimensions.
IV. APPROACH
We use a single-image convolution model for individual
frames of video data and perform volumetric convolution
at a higher level of abstraction by temporally concatenating
the output. The method is illustrated in Fig. 1. Here, K is
the number of action categories. In this way we are able to
initialize our network with the parameters learned from the
ImageNet dataset [44]. Additionally, we freeze the learned
network parameters up to the second-last fully-connected
HUSAIN et al.: ACTION RECOGNITION BASED ON DEEP LEARNING 3
layer, combine the output with another pretrained network and
build a new softmax model. We refer to the CNN architecture
(19-layer network in [45]) as VGG-Net.
A. Feature map concatenation
We train a network which takes as input 3D feature maps.
These feature maps are the outputs from layer-16 of the 19-
layer network defined in [45] and trained on the ImageNet
dataset. Layer-16 is the last spatial convolution layer in [45].
This gives us a high-level feature descriptor in the spatial
domain. Afterwards we add one 3D convolutional layer fol-
lowed by three fully-connected layers. The K-way softmax
function is applied to the output of the last fully-connected
layer, where K is again the number of action categories. In
total, our network contains 20 layers and we train only the
last 4 layers. We use a dropout regularization ratio of 0.5 for
the fully-connected layers.
We take N = 30 uniformly spaced frames from each video
as input to the network. Each image is assigned the same
label as its corresponding video. The network is trained using
stochastic gradient descent and use the same momentum as
in [44]. The learning rate is adjusted to get the maximum
accuracy in a minimum number of iterations on a held-out
validation set from the training set.
B. Combining multiple networks
A deep network learns different features at each level of the
layer hierarchy. The activations in the initial layers tend to be
more sensitive to edge-like patterns and corners within their
receptive field, whereas activations at deeper levels have larger
receptive fields and capture more complex invariances [46].
Our network learns changes that occur in the temporal
domain at a more abstract level because we temporally con-
catenate the output of the convolutional layer-16 from the
pretrained network of [45]. Hence, our model lacks learning
in the temporal domain from locally occurring changes. In
order to palliate this deficiency, we concatenate the fully con-
nected layer-9 feature vectors with a length of 4096, extracted
from another deep network that was trained in 3D from the
beginning [32]. The model contains 8 3D-convolution, 5 max-
pooling, and 2 fully-connected layers. Deeper 3D convolution
layers are not possible, due to GPU memory restrictions. The
model is trained on the Sports-1M dataset [31] which contains
about 1 million videos of different sports action categories.
Figure 2 shows the combination scheme of the two feature
maps. We concatenate the output of the fully-connected layers
for the same video, hence having the same action category. We
also augment the data by cropping M = 10 patches from each
frame of the video for the VGG-3D network, hence the output
feature dimension is 4096+(1024×M). After concatenation,
we perform max-pooling to reduce the feature dimension and
afterwards build a new softmax model. Since we are learning
the parameters for the softmax layer only, we can use a more
efficient optimization approach instead of stochastic gradient
descent. We use an off-the-shelf implementationb of L-BFGS
bhttp://www.cs.ubc.ca/∼schmidtm/Software/minFunc.html
which has been shown to yield better results when the number
of trainable parameters is small [33].
V. EXPERIMENTS
We evaluate our approach on two publicly available bench-
marking datasets, UCF-101 [47] and HMDB [48]. These
datasets are challenging because many video samples include
camera motion as well as a dynamic background. We use the
same evaluation protocol as proposed by the respective authors
and provide an in-depth analysis of our approach using the
UCF-101 dataset as a test case. Additional qualitative results
for both the datasets are available at http://www.iri.upc.edu/
people/shusain/actionrecognition.html
The network is able to take only fixed-size input frames,
hence we resize all the videos so that the maximum dimension
is 256 pixels and crop 10 patches of size 224×224 pixels ac-
cording to the data augmentation scheme as proposed in [44].
Furthermore, we separate 10% percent of the samples from
the training data and use them as validation data. Such data
are needed to determine the number of iterations needed for
stochastic gradient descent.
A. UCF-101 dataset
The UCF-101 [47] dataset contains 13,320 labeled video
samples with 101 action categories. We use the 3-way train/test
split as provided by the authors. Table I shows a comparison
with other approaches. Compared with the baseline [31] we
observe a considerable improvement. Not surprisingly, we see
improved results as also compared to [32]. This shows the
complementary nature of the high-level (layer-19) and low-
level (layer-6) features. It should be noted that we use the
output from the layer-9 activation (C3D 1 net) and concate-
nate it with our trained model as described in Fig. 2, i.e.,
concatenating two networks, as opposed to [32], where the
output from 3 networks that have been trained differently is
combined. Our results are closer to [30], where optical flow
needs to be computed. However, the calculation of optical
flow leads to a significant computational overhead. As shown
in [32], Brox optical flow used in [30] takes 0.85-0.95s per
image pair which is 274x slower than C3D. Additionally,
storing the raw flow fields for this dataset requires a disk
space of 1.5 TB which needs data compression [30]. Figure 3
shows the confusion matrix accumulated for all the three
splits. Comparing the confusion matrix with that resulting
from the approach in [30] (Fig. 5 in [30]), it can be seen that
the actions “CricketBowling” and “CricketShot” have similar
levels of confusion, whereas our approach shows better results
for the action “YoYo”. Figure 6 shows the top-5 predictions for
selected test sequences from the UCF-101 dataset [47] with
101 action categories.
B. Evaluating different scenarios
We measure the performance of our spatial and spatio-
temporal learning framework in different scenarios using the
split-1 of UCF-101 dataset. Table II presents the evaluation
under different settings along with the comparison to other
approaches.
4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2016
Fig. 1. Illustration of the network. We use the output from layer 16 of the VGG-Net (Table 1 in [45]), as a descriptor. The output is concatenated to form
512, 3D feature maps. The 3D feature maps are used as input for the network consisting of a volumetric convolutional layer followed by two fully-connected
layers.
Fig. 2. Illustration of how the different network outputs are combined, where VGG-3D-fc2 refers to the fc2 layer in Fig. 1
Fig. 3. Confusion matrix for the UCF-101 dataset accumulated for all three splits
HUSAIN et al.: ACTION RECOGNITION BASED ON DEEP LEARNING 5
TABLE I
AVERAGE ACCURACY ON THE UCF-101 DATASET (3-FOLD).
Algorithm Accuracy
CNN with transfer learning [31] 65.4%
LRCN (RGB) [49] 71.1%
Spatial stream ConvNet [30] 72.6%
LSTM composite model [50] 75.8%
Our approach (VGG-3D) 79.1%
C3D (1 net) [32] 82.3%
Temporal stream ConvNet [30] 83.7%
C3D (3 nets) [32] 85.2%
Combined ordered and improved trajectories [51] 85.4%
Stacking classifiers and CRF smoothing [52] 85.7%
Improved dense trajectories [34] 85.9%
Improved dense trajectories with human detection[53] 86.0%
Our approach (VGG-3D + C3D-fc6-1 net) 86.7%
Spatial and temporal stream fusion [30] 88.0%
TABLE II
CONVNET ACCURACY UNDER DIFFERENT SETTINGS FOR UCF-101
DATASET.
Scenario Accuracy
Fine tune top 3 layers (Sports 1M - pretrained) [31] 65.4% (3 fold)
Fine tune all layers (Sports 1M - pretrained) [31] 62.2% (3 fold)
Spatial AlexNet-stream (pretrained and last layer) [30] 72.7% (1 fold)
Spatial AlexNet-stream (pretrained and fine tuned) [30] 72.8% (1 fold)
Spatial VGG-stream (pretrained and fine tuned) 71.4% (1 fold)
VGG-3D (pretrained and fine tuned) 75.5% (1 fold)
Spatial VGG-stream (pretrained and adaptation layers) 76.3% (1 fold)
VGG-3D (pretrained and adaptation layers) 80.0% (1 fold)
VGG-3D (pretrained and fine tuned) + C3D-fc6-1 net 83.5% (1 fold)
VGG-3D (pretrained and adaptation layers) + C3D-fc7-1 net 84.8% (1 fold)
VGG-3D (pretrained and adaptation layers) + C3D-fc6-1 net 86.7% (1 fold)
In our spatial VGG-stream we obtained the label for a video
after averaging the scores for all the frames belonging to that
video. All the layers were pretrained on the ImageNet dataset
and fine tuned on the UCF-101 dataset, except the last layer
which was initialized randomly because of different number
of classes. We observed results similar to the spatial AlexNet-
stream [30]. We found better results for spatial VGG-stream
and VGG-3D when training only the adaptation, i.e., the newly
added layers. Similar behavior was observed in [31], i.e., a
drop in the accuracy when fine tuning all the layers. This
is because training such a huge network with a small dataset
results in overfitting. We observed the best result when training
the adaptation layers only combined with the fc6 layer from
C3D.
C. Learning from temporal information
Due to the concatenation of the feature maps in the temporal
domain, the 3D kernels should also be able to exploit the
temporal information in the video. Hence, if we randomly
shuffle the video frames while training, we should see a drop
in the accuracy due to temporal inconsistency. Figure 4 shows
the drop in the accuracy averaged for two training sessions
of the method described in Sec. IV-A for split-1 of UCF-101
dataset.
D. HMDB dataset
The HMDB dataset [48] contains 6,849 labeled video sam-
ples with 51 action categories. We use the 3-way train/test split
Fig. 4. Comparing accuracy for shuffling video sample frames.
TABLE III
AVERAGE ACCURACY ON THE HMDB DATASET (3-FOLD).
Algorithm Accuracy
Spatio-temporal HMAX network [54] 22.8%
Spatial stream ConvNet [30] 40.5%
Trajectory-Based Modeling [55] 40.7%
Our approach (VGG-3D) 46.9%
Decomposing visual motion [56] 52.1%
Our approach (VGG-3D + C3D-fc6-1 net) 53.9%
Temporal stream ConvNet [30] 54.6%
Improved dense trajectories [34] 57.2%
Spatial and temporal stream fusion [30] 59.4%
as provided by the authors. Table III shows a comparison with
other approaches. The methods from [30] and [34] perform
better than ours, however, both require computation of dense
per frame optical flow for each video. In addition, the method
in [34] also requires camera motion estimation. Figure 5 shows
the confusion matrix accumulated for all three splits. It can be
seen that similar actions such as “throw” and “swing baseball”
are the most confused. Figure 7 shows the top-5 predictions
for selected test sequences.
E. Qualitative analysis
Since we do not preprocess the data using techniques
such as background subtraction or tracking a bounding box,
our feature-learning approach is agnostic to such domain-
specific information. For this reason, wrong labels can be
seen, in Figs. 6 and 7, when different activities are performed
in visually similar environments. For example, Fig. 6(c6)
vs. Fig. 6(b3) and Fig. 6(b6) vs. Fig. 6(c2), share similar
environments and we see a high confidence of “HairCut” in
the “ShavingBeard” action and “PlayingFlute” got confused
with “PlayingViolin”. Similar observations can be made in
Fig. 7(b3) vs. Fig. 7(c6). However, sometimes background
plays an important role in correctly recognizing certain ac-
tions, for instance, “SkyDiving” (Fig. 6(e3)) and “Surfing”
(Fig. 6(e4)).
Other than similar background, actions may themselves be
also visually confusing, which can affect feature learning. For
example, Fig. 7(a2) vs. Fig. 7(c1). Both activities “cartwheel”
and “handstand” entail performing a similar motion.
6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2016
Fig. 5. Confusion matrix for the HMDB dataset accumulated for all three
splits
These are inherent problems of feature learning when using
only raw data and the resulting mislabelings have been named
reasonable mistakes in [31].
VI. CONCLUSIONS
We tackled the problem of action recognition by using a
spatio-temporal feature learning scheme. This scheme allowed
us to exploit the record-breaking pretrained single image
classification model [45]. We report extensive experimental
evaluations using challenging action recognition datasets. Our
results are competitive with the state-of-the-art convolutional
and strong feature-based baselines.
We are using the publicly available Torch7 library for our
implementation which is optimized for fast processing on a
CPU as well as a GPU. In our timing experiments we found
that for a batch size of 60 videos, it takes ∼ 1.6 seconds on a
modern GPU to perform a single forward and backward pass
through the network in Fig. 1 and about 6 hours (14 epochs)
to train on complete UCF-101 training set. We expect to get
further speed up by a more efficient implementation of 3D
convolution.
So far, we concatenated the feature maps in the last con-
volutional layer. In the future, we plan to explore possible
modifications in the network design to further exploit learning
in the temporal domain. One possibility would be to grad-
ually increase the number of temporal connections along the
sequence of layers. This kind of adaptation is proposed in [31].
We also plan to investigate the effect on performance of
gradually clipping the top layers of the network and evaluation
on the recently introduced Sports-1M dataset [31] which
contains over 1 million labeled sample videos.
REFERENCES
[1] F. Husain, A. Colome, B. Dellen, G. Alenya, and C. Torras, “Realtime
tracking and grasping of a moving object from range video,” in ICRA,
2014, pp. 2617–2622.
[2] J. Chan and G. Nejat, “A learning-based control architecture for an as-
sistive robot providing social engagement during cognitively stimulating
activities,” in ICRA, 2011, pp. 3928–3933.
[3] A. Chrungoo, S. Manimaran, and B. Ravindran, “Activity recognition
for natural human robot interaction,” in Social Robotics, ser. Lecture
Notes in Computer Science, 2014, vol. 8755, pp. 84–94.
[4] T. Kanda, M. Shiomi, Z. Miyashita, H. Ishiguro, and N. Hagita,
“A communication robot in a shopping mall,” IEEE Transactions on
Robotics, vol. 26, no. 5, pp. 897–913, 2010.
[5] R. Liu, X. Zhang, J. Webb, and S. Li, “Context-specific intention
awareness through web query in robotic caregiving,” in ICRA, 2015,
pp. 1962–1967.
[6] C. Schroeter, S. Mueller, M. Volkhardt, E. Einhorn, C. Huijnen,
H. van den Heuvel, A. van Berlo, A. Bley, and H.-M. Gross, “Realization
and user evaluation of a companion robot for people with mild cognitive
impairments,” in ICRA, 2013, pp. 1153–1159.
[7] S. Nakagawa, P. Di, Y. Hasegawa, T. Fukuda, I. Kondo, M. Tanimoto,
and J. Huang, “Tandem stance avoidance using adaptive and asymmetric
admittance control for fall prevention,” in ICRA, 2015, pp. 5898–5903.
[8] K.-H. Park, H.-E. Lee, Y. Kim, and Z. Bien, “A steward robot for human-
friendly human-machine interaction in a smart house environment,”
IEEE Transactions on Automation Science and Engineering, vol. 5,
no. 1, pp. 21–25, 2008.
[9] H. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
object affordances from rgb-d videos,” International Journal of Robotics
Research, vol. 32, no. 8, pp. 951–970, 2013.
[10] N. Hu, G. Englebienne, Z. Lou, and B. Krose, “Learning latent structure
for activity recognition,” in ICRA, 2014, pp. 1048–1053.
[11] J. Sung, C. Ponce, B. Selman, and A. Saxena, “Unstructured human
activity detection from rgbd images,” in ICRA, 2012, pp. 842–849.
[12] A. Klaeser, M. Marszalek, and C. Schmid, “A spatio-temporal descriptor
based on 3d-gradients,” in Proc. BMVC, 2008, pp. 99.1–99.10.
[13] C. S. Stefan Mathe, “Dynamic eye movement datasets and learnt saliency
models for visual action recognition,” in ECCV, vol. 7573, 2012, pp.
842–856.
[14] I. Laptev, “On space-time interest points,” Int. J. Comput. Vision, vol. 64,
no. 2-3, pp. 107–123, 2005.
[15] H. Wang, H. Zhou, and A. Finn, “Discriminative dictionary learning via
shared latent structure for object recognition and activity recognition,”
in ICRA, 2014, pp. 6299–6304.
[16] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a
local svm approach,” in ICPR, vol. 3, 2004, pp. 32–36 Vol.3.
[17] P. Scovanner, S. Ali, and M. Shah, “A 3-dimensional sift descriptor
and its application to action recognition,” in Proceedings of the 15th
International Conference on Multimedia, 2007, pp. 357–360.
[18] H. Wang, M. M. Ullah, A. Klaser, I. Laptev, and C. Schmid, “Evaluation
of local spatio-temporal features for action recognition,” in Proc. BMVC,
2009, pp. 124.1–124.11.
[19] G. Willems, T. Tuytelaars, and L. Gool, “An efficient dense and scale-
invariant spatio-temporal interest point detector,” in ECCV, 2008, pp.
650–663.
[20] Q. Le, W. Zou, S. Yeung, and A. Ng, “Learning hierarchical invariant
spatio-temporal features for action recognition with independent sub-
space analysis,” in CVPR, 2011, pp. 3361–3368.
[21] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.
[22] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in CVPR, 2014, pp. 580–587.
[23] P. Pinheiro and R. Collobert, “Recurrent convolutional neural networks
for scene labeling,” in ICML, T. Jebara and E. P. Xing, Eds. JMLR
Workshop and Conference Proceedings, 2014, pp. 82–90.
[24] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring
mid-level image representations using convolutional neural networks,”
in CVPR, Columbus, OH, United States, Nov 2014.
[25] W. Ouyang, X. Chu, and X. Wang, “Multi-source deep learning for
human pose estimation,” in CVPR, June 2014, pp. 2337–2344.
[26] A. Jain, J. Tompson, Y. LeCun, and C. Bregler, “Modeep: A deep
learning framework using motion features for human pose estimation,”
in ACCV, 2014, pp. 302–315.
HUSAIN et al.: ACTION RECOGNITION BASED ON DEEP LEARNING 7
Fig. 6. Top-5 predictions using our approach for selected test sequences from the UCF-101 dataset [47] with 101 action categories. First row (green color)
shows the ground-truth followed by predictions in decreasing level of confidence. Blue and red show correct and incorrect predictions, respectively.
[27] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, “Synthetic
data and artificial neural networks for natural scene text recognition,”
arXiv preprint arXiv:1406.2227, 2014.
[28] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell, “Decaf: A deep convolutional activation feature for generic
visual recognition,” in ICML, 2014.
[29] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” arXiv preprint arXiv:1408.5093, 2014.
[30] K. Simonyan and A. Zisserman, “Two-stream convolutional networks
for action recognition in videos,” in NIPS. Curran Associates, Inc.,
2014, pp. 568–576.
[31] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and
L. Fei-Fei, “Large-scale video classification with convolutional neural
networks,” in CVPR, 2014, pp. 1725–1732.
[32] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and M. Paluri,
“Learning spatiotemporal features with 3d convolutional networks,” in
ICCV, 2015, pp. 4489–4497.
[33] Q. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Ng, “On
optimization methods for deep learning,” in ICML. New York, NY,
USA: ACM, 2011, pp. 265–272.
[34] H. Wang and C. Schmid, “Action recognition with improved trajecto-
ries,” in ICCV, 2013, pp. 3551–3558.
[35] S. Sadanand and J. Corso, “Action bank: A high-level representation of
activity in video,” in CVPR, 2012, pp. 1234–1241.
[36] E. E. Aksoy, A. Abramov, J. Do¨rr, K. Ning, B. Dellen, and F. Wo¨rgo¨tter,
“Learning the semantics of object-action relations by observation,”
International Journal of Robotics Research, vol. 30, no. 10, pp. 1229–
1249, Sep 2011.
[37] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, “Spatio-
temporal convolutional sparse auto-encoder for sequence classification,”
in BMVC, J. C. R. Bowden and K. Mikolajczyk, Eds. BMVA Press,
2012, pp. 124.1–124.1.
[38] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler, “Convolutional
learning of spatio-temporal features,” in ECCV. Berlin, Heidelberg:
Springer-Verlag, 2010, pp. 140–153.
[39] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks for
human action recognition,” IEEE TPAMI, vol. 35, no. 1, pp. 221–231,
2013.
[40] H. Bay, A. Ess, T. Tuytelaars, and L. V. Gool, “Speeded-up robust
features (surf),” Computer Vision and Image Understanding, vol. 110,
no. 3, pp. 346–359, 2008, similarity Matching in Computer Vision and
Multimedia.
[41] H. Zhang, W. Zhou, and L. Parker, “Fuzzy segmentation and recognition
8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY, 2016
Fig. 7. Top-5 predictions using our approach for selected test sequences from the HMDB dataset [48] with 51 action categories. First row (green color) shows
the ground-truth followed by predictions in decreasing level of confidence. Blue and red show correct and incorrect predictions, respectively.
of continuous human activities,” in ICRA, 2014, pp. 6305–6312.
[42] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, “Convolutional
deep belief networks for scalable unsupervised learning of hierarchical
representations,” in ICML. ACM, 2009, pp. 609–616.
[43] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and R. Salakhutdinov,
“Exploiting image-trained cnn architectures for unconstrained video
classification,” in Proc. BMVC, 2015, pp. 60.1–60.13.
[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in NIPS. Curran Associates,
Inc., 2012, pp. 1097–1105.
[45] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.
[46] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-
tional networks,” in ECCV. Springer International Publishing, 2014,
pp. 818–833.
[47] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of
101 human actions classes from videos in the wild,” CoRR, vol.
abs/1212.0402, 2012. [Online]. Available: http://arxiv.org/abs/1212.0402
[48] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB:
a large video database for human motion recognition,” in ICCV, 2011,
pp. 2556–2563.
[49] J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venu-
gopalan, T. Darrell, and K. Saenko, “Long-term recurrent convolutional
networks for visual recognition and description,” in CVPR, 2015, pp.
2625–2634.
[50] N. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsupervised learn-
ing of video representations using lstms,” CoRR, vol. abs/1502.04681,
2015.
[51] O. R. Murthy and R. Goecke, “Combined ordered and improved
trajectories for large scale human action recognition,” in ICCV Workshop
on Action Recognition with a Large Number of Classes, 2013.
[52] S. Karaman, , L. Seidenari, A. Bagdanov, and A. Bimbo, “L1-regularized
logistic regression stacking and transductive crf smoothing for action
recognition in video,” in ICCV Workshop on Action Recognition with a
Large Number of Classes, 2013.
[53] H. Wang, D. Oneata, J. Verbeek, and C. Schmid, “A robust and efficient
video representation for action recognition,” International Journal of
Computer Vision, pp. 1–20, 2015.
[54] H. Jhuang, T. Serre, L. Wolf, and T. Poggio, “A biologically inspired
system for action recognition,” in ICCV, 2007, pp. 1–8.
[55] Y.-G. Jiang, Q. Dai, X. Xue, W. Liu, and C.-W. Ngo, “Trajectory-based
modeling of human actions with motion reference points,” in ECCV, ser.
Lecture Notes in Computer Science, 2012, vol. 7576, pp. 425–438.
[56] M. Jain, H. Jegou, and P. Bouthemy, “Better exploiting motion for better
action recognition,” in CVPR, 2013, pp. 2555–2562.
"
27,"Meta-Learning by the Baldwin Effect
Chrisantha Fernando, Jakub Sygnowski, Simon Osindero, Jane Wang, Tom Schaul,
Denis Teplyashin, Pablo Sprechmann, Alexander Pritzel, Andrei A. Rusu
Google Deepmind,
London, UK
chrisantha@google.com
Abstract
The scope of the Baldwin effect was recently called into question by two papers
that closely examined the seminal work of Hinton and Nowlan. To this date there
has been no demonstration of its necessity in empirically challenging tasks. Here
we show that the Baldwin effect is capable of evolving few-shot supervised and re-
inforcement learning mechanisms, by shaping the hyperparameters and the initial
parameters of deep learning algorithms. Furthermore it can genetically accom-
modate strong learning biases on the same set of problems as a recent machine
learning algorithm called MAML ”Model Agnostic Meta-Learning” which uses
second-order gradients instead of evolution to learn a set of reference parameters
(initial weights) that can allow rapid adaptation to tasks sampled from a distri-
bution. Whilst in simple cases MAML is more data efficient than the Baldwin
effect, the Baldwin effect is more general in that it does not require gradients to
be backpropagated to the reference parameters or hyperparameters, and permits
effectively any number of gradient updates in the inner loop. The Baldwin effect
learns strong learning dependent biases, rather than purely genetically accommo-
dating fixed behaviours in a learning independent manner.
1 Introduction
There is a growing interest in the machine learning community in meta-learning [26], i.e. learning to
learn. Recently an influential model-agnostic meta-learning (MAML) algorithm was proposed for
the fast adaptation of parameters in neural networks [9]. It works by using gradient descent to learn
the reference (initial) parameter values of a neural network from which new parameters can most
rapidly be learned to solve a sample of tasks from a distribution of tasks. It requires a differentiable
learning procedure to backpropagate into the reference parameter values, and even then it is limited
in the number of gradient steps in the inner learning loop that can be made before second order
gradient calculations become intractable. Meta-learning in this way achieves state of the art in few
shot learning, for example by allowing a reinforcement learning algorithm to (within a few gradient
updates) learn the optimal speed or direction of a simulated cheetah or four-legged robot based only
on reward.
Over a hundred years ago, a similar effect was proposed by Mark Baldwin [2] to explain how evolu-
tion could deal with irreducibly complex adaptations without the need for Lamarckian information
flow [14]. John Maynard Smith described the effect as follows: “If individuals vary genetically in
their capacity to learn, or to adapt developmentally, then those most able to adapt will leave most
descendants, and the genes responsible will increase in frequency. In a fixed environment, when the
best thing to learn remains constant, this can lead to the genetic determination of a character that,
in earlier generations, had to be acquired afresh in each generation” [19]. In this formulation the
Baldwin effect is really two effects, or a trade-off between two factors: initially genetically speci-
fied phenotypic plasticity (variance), followed by genetic accommodation of the induced trait (bias).
Turney writes ”...the Baldwin effect has two aspects. First, lifetime learning in individuals can, in
ar
X
iv
:1
80
6.
07
91
7v
2 
 [c
s.N
E]
  2
2 J
un
 20
18
Figure 1: Baldwinian evolution (left) versus Lamarckian evolution (right). In Baldwinian evolution
the initial parameters P and hyperparameters h of a learning algorithm are evolved and subsequently
evaluated by training on multiple independent learning trials (which adjust the weights P ) to obtain
the fitness. However, these learned parameters are not inherited by the next generation; only the
original initial parameters P and hyperparameters h are inherited and then mutated to obtain P ∗
and h∗. In contrast, in Lamarckian evolution the parameters are learned over multiple sequential
learning trials and the final parameters P ′′′ (in the case of 3 learning trials shown in the diagram)
are inherited along with the hyperparameters h, which are then mutated to produce P ∗ and h∗.
some situations, accelerate evolution. Second, learning is expensive. Therefore, in relatively stable
environments, there is a selective pressure for the evolution of instinctive behaviors.” [29].
Here we compare these two algorithms – MAML and the Baldwin effect – on the same tasks. Note
that unlike in MAML, our evolutionary experiments show no learning of the hyperparameters and
initial parameters, only standard Darwinian evolution of these elements. We show that the Baldwin
effect is competitive with MAML, biasing a learning algorithm to fit the distribution of tasks en-
countered during evolution, without some of the restrictions encountered in [9] (e.g. having direct
access to gradients).
In the framework of deep reinforcement learning (RL), evolution can be complementary to gradient
descent by specifying and evolving the initial neural network parameters P and hyperparameters h
of a learning algorithm [5, 15]. Throughout the course of learning, phenotypic plasticity is expressed
as gradient updates are made to the parameters and the model learns to perform the task. This has the
effect of smoothing the fitness landscape [29]. In the case of Baldwinian evolution, these updated
weights are forgotten by the next generation, which instead inherit the initial weights P and hyper-
parameters h, with possible mutation, whereas in Lamarckian evolution, these final, learned weights
are evolved and passed on (see Figure 1). We refer to Darwinian evolution as the case where there is
no learning within a lifetime. Lamarckianism closely resembles Population Based Training (PBT)
[15]: a method for online hyperparameter evolution with the exception that in PBT we do not mutate
the learned parameters P but inherit them unchanged, only mutating hyperparameters: h to h∗. This
method, while highly successful on a number of supervised, unsupervised and reinforcement learn-
ing tasks, has no incentive to learn a representation that can be easily evolved to solve a number of
different tasks in a meta-learning setup. Control experiments in [9] suggest that sequentially training
(fine-tuning) a model on different tasks doesn’t lead to competitive performance in meta-learning.
There is already evidence that the Baldwin effect has a role to play in machine learning because it is
capable of evolving inductive bias in the form of the initial parameters P and hyperparameters h of a
learning algorithm [5, 29]. Here we test the more specific hypothesis that the Baldwin effect provides
a way to evolve agents for few-shot data-efficient fast learning on a task distribution (previously
examined mainly in classification settings [21, 30, 22], but see also [31, 8] for applications to RL),
across a wide variety of learning domains. The Baldwin effect is thus used here as an algorithm for
meta-learning, which results in a representation that is fit to a distribution of tasks, i.e. learning the
structure of the various problems to be encountered rather than the specifics [26]. The genome to be
evolved is shaped by the task distribution, whereas the learning algorithm itself learns task specifics.
The effect arises whenever there is a cost to learning imposed by the speed at which learning must
occur. Such costs often arise in nature, for example in a co-evolutionary ecosystem where a newly
born organism must rapidly learn to run so it can escape predators.
2
Our main experimental contributions are as follows: First we show that the Baldwin effect and
MAML are comparable on a supervised learning task. Secondly we demonstrate that the Baldwin
effect can be used in cases where MAML cannot be used, for instance in cases where the genotype is
non-differentiable, e.g. where we evolve the macro-actions used by a discrete action RL algorithm,
or the algorithms’ discrete hyperparameters themselves. Thirdly we examine how genetic accom-
modation takes place in real deep neural networks undergoing the Baldwin effect. Fourthly, we
examine two task distributions; one where Baldwinian learning is superior to Lamarckian learning
and vice versa.
2 Related Work
While deep learning systems trained with traditional supervised or reinforcement learning meth-
ods have achieved remarkable success in a variety of tasks, they perform poorly when only a
small amount of data is available. Meta-learning aims to mitigate this limitation by broadening
the learner’s scope from a single task to a distribution of related tasks [25, 26]. The goal of meta-
learning is then to learn a learning strategy that generalizes to similar but unseen tasks from a given
task distribution. A lot of interest in meta-learning comes from the problem of one-shot learn-
ing in image classification, which consists of learning a new class from a single labelled example
[18]. Several approaches address this problem by using specialized neural network architectures
that learn an embedding space that allows to effectively compare new examples. For instance, em-
ploying Siamese networks [17] or recurrence with attention mechanisms [30]. These approaches
achieve very good results in one-shot visual learning but cannot be easily employed in other tasks,
such as reinforcement learning. Another approach to meta-learning is to train a recurrent memory
augmented learner to quickly adapt to new tasks of a given task distribution. Such networks have
been applied to few-shot image recognition [22] and reinforcement learning [8, 31]. More recent
approaches propose to include the inductive bias of optimization-based learning into a meta-learner
[13, 21, 9]. Particularly related to this work is model-agnostic meta-learning (MAML) approach [9],
that aims to learn the initial set of parameters such that they can be rapidly adapted (via gradient
descent) to solve a given task from the task distribution. We describe this method in detail in section
3.1.
Hinton and Nowlan showed in their 1987 paper that the Baldwin effect works in the toy example of
a needle-in-a-haystack binary optimization problem of 20 alleles (bits) using random search, where
the random search distribution is encoded by evolution [12]. The emphasis in their paper was to
show how learning can smooth a (single task) rugged fitness landscape. The generality of that work
has recently been called into question by a paper which claims that the scope of the effect is severely
limited [23]. Specifically, Santos et al showed that under certain task parameter settings (i.e. initial
ratios of correct alleles P (1) = 0.5, and incorrect alleles P (0) = 0.5) then standard Darwinian
evolution finds the needle in a haystack in the same number of generations as the Baldwin effect on
average. However, a more recent paper by the same authors has shown that, when actually using
Hinton and Nowlan’s original conditions, P (1) = 0.25, P (0) = 0.25, and P (?) = 0.5, (where
? refers to an allele that does random search) the Baldwin effect is indeed significantly faster than
Darwinian evolution [10]. In short, we know the Baldwin effect is possible in this toy task, only in a
subset of the parameter conditions, but we wish here to provide convincing empirical support for the
necessity of the Baldwin effect in more substantial and contemporary learning tasks than L = 20 bit
learning problems using random search.
The Baldwin effect in neural network learning has been investigated in several papers following
the original work of Hinton and Nowlan [7]. Notably, Keesing and Stork used a genetic algorithm
to evolve the initial weights of a neural network for digit classification, and found that the extent
of genetic accommodation by the Baldwin effect depended on the amount of learning; too much
learning and evolution was slowed down because there was too little selection pressure on the initial
weights (as learning can do well form any starting position); too little learning and evolution was
slowed down because fitness landscapes were not sufficiently smoothed [16]. Interestingly, they
found that randomly sampling the number of gradient update steps from a distribution rather than
using a fixed number significantly increased the rate of accommodation because that way the cost of
learning was always felt by selection. Bullinaria evolved learning rate schedules during a lifetime,
showing the evolution of developmental critical periods tailored to specific problems [4]. Neural
network learning is only one kind of phenotypic plasticity. Turney used a genetic algorithm to evolve
3
a population of biases for a decision tree induction algorithm for classification [28]. Cecconi et al
evolved a hyperparameter determining how much learning, by imitation of a parent, an offspring
will do in a co-evolutionary system [6]. Anderson modelled how the adaptive immune system
could facilitate natural antibody production by the Baldwin effect [1]. Bull argued that the Haploid-
Diploid cycle was a primitive example of learning and so subject to the Baldwin effect. [3]. This
paper extends the existing literature by applying the Baldwin effect to deep learning reinforcement
learning algorithms in task distributions.
3 Methods
In this section, we begin by describing the three algorithm families that we compare. Secondly we
describe the three tasks that we solve, before finally outlining the details of the models that we train
and evolve.
3.1 Algorithms
Model-Agnostic Meta-Learning. Given a distribution over tasks p(T ) and a neural network with
parameters collectively denoted θ, MAML aims to learn an set of reference parameters θ∗ such that
one or a small number of gradient decent steps computed using a small amount of data for a given
task from the distribution, Ti ∼ p(T ), leads to effective generalization on that task.
The objective function for MAML is given by
min
θ
ETi∼p(T ) LTi(θ′i) (1)
where the expectation is taken over the task distribution, LTi represents the loss corresponding to
task Ti and the parameters θ′i are the parameters adapted to fit K representative training examples
for this task. The task-specific learning is obtained via gradient descent,
θ′i = θ − α∇θLTi(θ) (2)
where α is the learning rate. We used a single gradient descent step for ease of notation, but multiple
steps can be used insted. The outer loss in (1) evaluates the generalization of θ′i on a small amount
of validation data for the i−th task. The reference set of parameters θ∗ are found by minimizing
(1) via stochastic gradient descent. The procedure is given in Algorithm 1. Note that high order
gradients are required to compute the parameter update.
Genetic Algorithm (GA) is a general-purpose optimization algorithm inspired by the biological
processes of mutation and selection. In our work, we use two flavors of Genetic Algorithms: a
Steady State Genetic Algorithm and Generational Genetic Algorithm [11]. In section 3.2 we intro-
duce a sinusoid fitting task and a physics task domain. In the sinusoid fitting experiments we use
a generational GA of population size 100, and rank-based selection. The physics-based RL experi-
ments use an asynchronous parallel steady state GA with population size 500, and tournament size
10. The Baldwinian evolution algorithm hybridizes the GA and gradient-based learning as shown
in Algorithm 2. We compare the Baldwinian algorithm with to two baselines: standard Darwinian
evolution, and Lamarckian evolution.
With some small modifications to Algorithm 2 we can obtain a learning process for single task or
for a continual/multi-task learning setting by allowing all gradient updates to act successively on the
same set of parameters. In this setting we can also consider a further modification/variant in which
we use Lamarckian inheritance (changing line 15 to update the population using the parameters
from the end of the short run of gradient optimization).
Natural Evolution Strategies (NES) are a family of continuous black-box optimization algorithms
that maintain and adapt a (Gaussian) search distribution in order to maximize the expected fitness
under that distribution [33, 32]; they update the distribution parameters 〈µ,Σ〉 in the direction of
the (natural) policy gradient, as estimated from the fitnesses fg of a population of samples θg ∼
N (µ,Σ). In our case specifically, we employ the variant called separable NES (SNES [24]) that
models only the element-wise variances σ2, instead of the full covariance matrix Σ, because its
linear complexity enables it to scale the high-dimensional spaces required for our experiments.
4
Algorithm 1 Model-Agnostic Meta-Learning (from [9])
Require: p(T ): distribution over tasks
Require: α, β: step size hyperparameters
1: randomly initialize parameters θ
2: while not done do
3: Sample batch of tasks Ti ∼ p(T )
4: for all Ti do
5: Evaluate∇θLTi(θ) with respect to K examples
6: Compute adapted parameters with gradient descent:
θ′i = θ − α∇θLTi(θ)
7: end for
8: Update θ ← θ − β∇θ
∑
Ti∼p(T ) LTi(θ′i)
9: end while
Algorithm 2 Baldwinian Meta-Learning
Require: p(T ): distribution over tasks
Require: P: initial population-representation of individuals
Require: S: procedure to obtain a batch of individuals (i.e. parameters and hyper-parameters)
given a population-representation
Require: U : procedure to update a population-representation given a batch of fitness-scored indi-
viduals
Require: F : fitness scoring function
Require: N : number of gradient steps to take during per-task gradient training
1: while not done do
2: Generate batch of individuals from population:
θg,∅, αg ∼ S(P)
3: for all θg,∅ do
4: Sample batch of tasks Ti ∼ p(T )
5: for all Ti do
6: θg,i ← θg,∅
7: for k=1...N do
8: Evaluate∇θg,iLTi(θg,i)
9: Update adapted parameters with gradient descent:
θg,i ← θg,i − αg∇θg,iLTi(θg, i)
10: end for
11: Compute fitness-score for current task: fgi = F(θg,i)
12: end for
13: Compute overall fitness estimate: fg =
∑
i f
g
i
14: end for
15: Update population based on fitness of individuals:
P ← U(P, {(1, θ1, α1, f1) , ..., (g, θg, αg, fg)})
16: end while
3.2 Tasks
A supervised regression problem distribution and two physics-based reinforcement learning task
distributions are used to compare the algorithms.
Sinusoid-fitting task: In this supervised task, in any one lifetime the agent must fit by regression a
single sinusoid drawn from a distribution of phases and amplitudes. If evolution were to encode a
non-plastic neural network, it would be impossible for it to do more than evolve the mean sinusoid
for the distribution; whereas with lifetime learning, the initial function at birth could be modified to
fit the sampled sinusoid encountered in any particular lifetime. In this case, the Baldwin effect would
be expected to take place. This leads to a different perspective on the Baldwin effect to that taken by
Hinton and Nowlan and others; whereby its role is not primarily for smoothing fitness landscapes
in otherwise unsolvable adaptive problems, but instead for meta-learning distributions of adaptive
5
problems that would be entirely unsolvable by evolution alone without phenotypic plasticity, and
then encoding these distributions genetically to produce faster learning.
We compare the performance of MAML, NES, and generational GA on fitting sinusoids. In each
generation, we sample 25 different sine waves, out of which we select 10 points for training and
10 points for testing. The amplitude of sine waves was sampled uniformly from [0.1, 5.0] and the
phase from [0, pi]. In one fitness evaluation, we perform 5 gradient descent steps for each sine wave
using training points, evaluate performance as mean-squared error on test data, and average the
results for different sine waves. For NES and the GA, the fitness is the final MSE for that task after
the gradient updates obtained over a separate sample of data than what was trained on. In both our
models (generational Genetic Algorithms and Natural Evolution Strategies) the different genotypes
in a given generation are evaluated using the same data, so that the amount of data our models see
after some fixed number of generations is equal to the data baseline MAML sees after doing that
number of meta-updates. In NES the population size was 25 and in GA it was 100.
Physics simulation reinforcement learning tasks. In reinforcement learning, the goal of few-shot
meta-learning is to enable an agent to quickly acquire a policy for a new task based on training on
the same distribution of tasks. For example, an agent might learn to quickly run at a certain desired
target speed or direction. We constructed two sets of tasks based on those used in Finn et al [9].
One fitness evaluation consisted of 10 independent episodes with different task parameters. For
the Baldwinian training condition, the parameters were reset to the inherited parameters at the start
of each episode, and before inheritance. With Lamarckian training, the parameters were not reset
between episodes and were inherited at the end of the final episode. In the Darwinian case there was
no learning (gradient updates) in any of the 10 episodes. Fitness was defined as the sum of rewards
obtained over all 10 episodes, providing an implicit selection pressure to learn quickly.
Two types of high-dimensional locomotion tasks were investigated using the MuJoCo simulator
[27], specifically using the Planar Cheetah task, requiring it to run in either a particular direction
(Goal Direction) or at a particular velocity (Goal Velocity). In the goal velocity experiments, the
reward was the negative absolute value between the current velocity of the agent and a target velocity.
The target velocity for each episode was chosen exhaustively in steps of 0.2, in the range 0.0 to 2.0.
There were 10 such episodes for one fitness evaluation. The fitness was the sum of the rewards over
these 10 episodes. In the goal direction experiments, the reward was the magnitude of the velocity in
either the forward or backward direction. The fitness was the summed reward over 10 episodes with
each episode alternating in whether backwards or forwards movement was required. In both cases
the length of one episode was 3000 time steps (30 seconds) with a rollout size of 40 simulation time
steps per gradient step, unless otherwise noted.
3.3 Models
Sinusoid regression network: The model architecture we used for the sinusoid fitting task (see
Tasks) was the same as in [9]: a neural network with two hidden layers with 40 neurons each. We
used Gaussian noise with mean 0 and std 0.01 to initialize the weights and biases of the network
in GA and NES. The mean squared error loss is used to train the parameters of the network using
stochastic gradient descent.
A2C Controller: For the RL tasks described in section 3.2 we use the a2c algorithm (policy gradient
with a trainable baseline) [20] to estimate the gradient for training the controller of the running
cheetah agent. It consists of a shared torso which is a feed-forward neural network with rectified-
linear transfer functions and two hidden layers of size 100. A policy readout from this final layer
outputs a softmax over 12 possible discrete actions. A value function readout from the final layer
outputs a single scalar value which is used as a baseline for the policy gradient algorithm. On
replication, Gaussian noise of mean zero and standard deviation 0.02 is added to each weight and
bias of the neural network. Additional noise is added to the hyperparameters which are the learning
rate, entropy lost scale, and discount of the a2c algorithm.
For the physics-based RL tasks (see Tasks), macro-actions are evolved by the Baldwin effect; i.e.
the 12x7 action primitive matrix which determines the 7 motor torques produced for each of the
12 discrete actions the cheetah controller can execute at each time step. The use of second order
6
gradients to modify such hyperparameters is known to be notoriously unstable, whilst the Baldwin
effect allows meta-optimization of these hyperparameters as well as the initial weights as in [9].
In the a2c experiments we additionally explored the use of a genetically encoded binary vector of
hyperparameters of the same length as the number of parameters in the model. This vector (which
we call a mask) determines whether or not each parameter should be learnable or not. Bit flip
mutation is used to evolve this mask hyperparameter vector. Thus is done in order to emulate the
setup in the original Hinton and Nowlan paper.
4 Results
Firstly, MAML is compared with genetic algorithms and natural evolution strategies on a supervised
learning task; fitting sinusoids drawn from a distribution. Secondly, genetic algorithms are used to
evolve the hyperparameters and initial parameters of a policy gradient algorithm with an adaptive
critic, on two reinforcement learning problems.
4.1 Rapid fitting of sinusoids
The performance of MAML, NES and GA on fitting sinusoids is shown in Figure 2a. As our methods
are based on a population of genotypes, we plot both the median and the best fitness achieved in each
generation.
0 5000 10000 15000 20000
Generation
0
1
2
3
4
5
M
ea
n
 s
q
u
ar
ed
 e
rr
or
GA median
GA best
NES median
NES best
MAML
(a) Sinusoid fitting task learning curves show a compar-
ison of mean-squared errors of the Baldwin effect oper-
ating over evolution in NES and GA, vs during MAML
training. Despite using only final fitness as a training
signal, our methods achieve results on par to MAML.
−6 −4 −2 0 2 4 6
−4
−2
0
2
4
Ground truth
Prediction after 5 updates
Prediction before updates
Training data
(b) Example fitting of a sine wave using trained NES
model. The parameters correspond to the mean of the
distribution after 20000 generations. The red curve
shows the initial function of the regression network
prior to learning – note that it has evolved to be a sine
wave. The blue line shows the function learned after
five gradient steps, and the green curve shows the target
sine wave to fit in this particular lifetime.
Figure 2: Results of training on the sine wave task.
Figure 3 shows the rate at which the neural network fits a particular sine wave presented during a
lifetime. Similar to MAML, our methods’s adaptation speed is superior to the one of the baseline
approach (pretrained), which was trained to predict sine waves using a standard supervised-learning
approach.1
4.2 Reinforcement Learning in Physics Environments
Goal Velocity Task. The Baldwin effect evolves a model that can quickly adapt its velocity to the
target velocity within a single episode lasting only 30 simulated seconds. Figure 4a shows that
Lamarckian evolution outperforms Baldwinian evolution, which in turn outperforms Darwinian
evolution. Figure 5 shows that Lamarckian evolution achieves the target velocity in each episode
1Plots for MAML and the pretrained approach come from [9].
7
0 1 2 3 4 5
Number of gradient steps
0
1
2
3
4
5
6
M
ea
n
 s
q
u
ar
ed
 e
rr
or
GA
NES
MAML
Pretrained
Figure 3: Comparison of the speed of fitting of the sine wave during validation time, with MAML,
NES, and GA.
(a) Baldwin (Green), Lamarck (Red), Darwin (Blue) fit-
ness on the Cheetah Goal Velocity task, showing that
Lamarckian evolution works best when evaluated on
tasks very similar to the previously learned on.
(b) Baldwin (Green), Lamarck (Red), Darwin (Blue) fit-
ness on the Cheetah Goal Direction task.
Figure 4: Results of training on the Cheetah tasks.
better than Baldwinian evolution. Both the Baldwin effect and Lamarckian learning are superior to
pure Darwinian learning in this case.
Goal Direction Task. The Baldwin effect evolves a model that can quickly adapt its direction to
the target direction within a single episode lasting only 30 simulated seconds. Figure 4b shows
the best agent fitness recorded over five independent evolutionary runs; two that use Baldwinian
evolution (green), two that use Lamarckian evolution (red) and one that uses Darwinian evolution
(blue), on the goal direction task. Best performance is obtained by Baldwinian evolution without an
explicit plasticity mask, and second best with Baldwinian evolution with an explicit plasticity mask,
followed by Darwinian evolution, with Lamarckian evolution a very clear loser in this task. The
horizontal velocity of the cheetah over the course of one fitness evaluation is shown in Figure 6.
The contrast between the goal velocity and goal direction tasks is interesting. The goal direction
task requires a radical change in policy for moving forwards or backwards in different episodes.
Lamarckian evolution gets stuck in a local optimum of only being able to go backwards. Baldwinian
8
0 10000 20000 30000
Episode step
−2
−1
0
1
2
3
4
5
V
el
oc
it
y
Baldwin mask
(a) Baldwin, mask
0 10000 20000 30000
Episode step
−2
−1
0
1
2
3
4
5
V
el
oc
it
y
Baldwin no mask
(b) Baldwin, no mask
0 10000 20000 30000
Episode step
−2
−1
0
1
2
3
4
5
V
el
oc
it
y
Lamarck mask
(c) Lamarck, mask
0 10000 20000 30000
Episode step
−2
−1
0
1
2
3
4
5
V
el
oc
it
y
Lamarck no mask
(d) Lamarck, no mask
0 10000 20000 30000
Episode step
−2
−1
0
1
2
3
4
5
V
el
oc
it
y
Darwin
(e) Darwin
Figure 5: Velocities obtained in the Cheetah Goal Velocity task for (a,b) Baldwinian, (c,d) Lamar-
ckian, and (e) Darwinian evolution. Baldwinian evolution tends to revert back to low velocities
during training, while Lamarckian maintains a steadily increasing forward velocity, as a result of
sequence of tasks trained on (target velocity incremented by 0.2 every episode). Video1Supp shows
the Lamarckian agent running forwards at different speeds.
0 10000 20000 30000
Episode step
−8
−6
−4
−2
0
2
4
6
8
V
el
oc
it
y
Baldwin mask
(a) Baldwin, mask
0 10000 20000 30000
Episode step
−8
−6
−4
−2
0
2
4
6
8
V
el
oc
it
y
Baldwin no mask
(b) Baldwin, no mask
0 10000 20000 30000
Episode step
−8
−6
−4
−2
0
2
4
6
8
V
el
oc
it
y
Lamarck mask
(c) Lamarck, mask
0 10000 20000 30000
Episode step
−8
−6
−4
−2
0
2
4
6
8
V
el
oc
it
y
Lamarck no mask
(d) Lamarck, no mask
0 10000 20000 30000
Episode step
−8
−6
−4
−2
0
2
4
6
8
V
el
oc
it
y
Darwin
(e) Darwin
Figure 6: Velocities obtained in the Cheetah Goal Direction task for (a,b) Baldwinian, (c,d) Lamar-
ckian, and (e) Darwinian evolution. Each fitness evaluation consists of 10 episodes with alternating
requirements for backwards or forwards velocity. In the Baldwinian case, parameter values are reset
at the start of each episode. In the Lamarckian case there is no resetting. The Baldwinian agents are
capable of learning to go forwards and backwards as desired, but the Lamarckian agents evolve/learn
only to go backwards. They have got stuck on that local optimum in this task. Video2Suppl shows
the Baldwinian agent running forwards and backwards.
evolution is able to cope with these two diverse tasks. In the goal velocity task, Lamarckian evolution
is superior because the final velocity achieved in task T − 1 is a suitable starting point for the target
velocity required in task T (note we increment the target velocity by 0.2 in each episode).
How do the hyperparameters of the a2c algorithm evolve during the goal direction task? Figure 7
shows histograms of the distribution of hyperparameters for 5 evenly spaced time-points during the
runs. The main points to note are that in Baldwinian evolution we see learning rates evolve to quite
high values, e.g. 0.005 to 0.01, whereas in Lamarckian evolution we see learning rates drop to the
lowest values i.e. 0.00001. In Baldwinian evolution the entropy loss scale evolves to high values
0.1, but experiences little directed selection in Lamarckian evolution. In Baldwinian evolution the
discounts become as small as we allow, i.e. 0.92, but in Lamarckian evolution they become as large
as we allow i.e. 0.9999. The Baldwin effect does not abolish learning in this task – instead, it
increases the rate of learning, but evolves strong learning biases. This is something that would not
be possible in the Hinton and Nowlan task.
5 Discussion and Conclusion
In conclusion, in supervised learning tasks there was genetic accommodation of the initial function
prior to learning, i.e. the regression network’s prior was initially sinusoidal. Rapid learning con-
tinued to be selected for throughout evolution. In learning RL task distributions using the Baldwin
effect, we observed that learning hyperparameters also evolved high learning rates and low discount
factors, with the initial behaviour ‘at birth’ providing strong biases to the learning algorithm which
continued to show rapid learning throughout evolution. There is no complete genetic accommo-
dation because that can never achieve high fitness by construction. Instead, it is the biases of the
learning algorithm which are accommodated. The Baldwin effect is superior to Lamarckian learning
when the distribution of tasks is broad or quickly changing, whereas Lamarckian learning is superior
when the task distribution is narrow. The use of an explicit Hinton and Nowlan type mask did not
speed up learning or final performance in task distributions.
We have demonstrated that the Baldwin effect is capable of producing learning algorithms and mod-
els capable of few shot learning when combined with deep learning in supervised and reinforcement
9
Figure 7: Hyperparameter evolution shown at 25 evenly spaced timepoints in each evolutionary
run. Green lines denote Baldwinian training, red lines denote Lamarckian training, averaged over a
population of 500. Stars show with mask, and solid shows without mask. High learning rates evolve
with Baldwin, but low learning rates evolve with Lamarck. Low discounts evolve with Baldwin, but
high discounts evolve with Lamarck. There is no selection for entropy loss scale when a mask is
used.
learning tasks. Further work is to show that this principle can be used to achieve state of the art
results in machine learning on more complex task distributions.
Remarkably, meta-learning through evolution enables the use of non-differentiable fitness functions,
in contrast to popular meta-learning approaches. For example, the fitness function can be defined on
different, potentially multi-modal data distributions, making it a prime candidate for multi-objective
optimization, even when data from one or several objectives is not always available to the low level
optimization process.
References
[1] Russell Wayne Anderson. 1996. How adaptive antibodies facilitate the evolution of natural
antibodies. Immunology and cell biology 74, 3 (1996), 286.
[2] J Mark Baldwin. 1896. A new factor in evolution. The american naturalist 30, 354 (1896),
441–451.
[3] Larry Bull. 2016. The Evolution of Sex through the Baldwin Effect. CoRR abs/1607.00318
(2016). arXiv:1607.00318 http://arxiv.org/abs/1607.00318
[4] John A Bullinaria. 2002. The evolution of variable learning rates. In Proceedings of the 4th
Annual Conference on Genetic and Evolutionary Computation. Morgan Kaufmann Publishers
Inc., 52–59.
[5] PA Castillo, MG Arenas, JG Castellano, JJ Merelo, A Prieto, V Rivas, , and G Romero. 2006.
Lamarckian evolution and the Baldwin effect in evolutionary neural networks. arXiv preprint
arXiv:cs/0603004 (2006).
[6] Federico Cecconi, Filippo Menczer, and Richard K Belew. 1995. Maturation and the evolution
of imitative learning in artificial organisms. Adaptive Behavior 4, 1 (1995), 29–50.
10
[7] Keith L Downing. 2010. The Baldwin effect in developing neural networks. In Proceedings of
the 12th annual conference on Genetic and evolutionary computation. ACM, 555–562.
[8] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel.
2016. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv preprint
arXiv:1611.02779 (2016).
[9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast
adaptation of deep networks. arXiv preprint arXiv:1703.03400 (2017).
[10] Jose´ F Fontanari and Mauro Santos. 2017. The revival of the Baldwin effect. The European
Physical Journal B 90, 10 (2017), 186.
[11] David E Goldberg and Kalyanmoy Deb. 1991. A comparative analysis of selection schemes
used in genetic algorithms. In Foundations of genetic algorithms. Vol. 1. Elsevier, 69–93.
[12] Geoffrey E Hinton and Steven J Nowlan. 1987. How learning can guide evolution. Complex
systems 1, 3 (1987), 495–502.
[13] Michael Husken and Christian Goerick. 2000. Fast learning for problem classes using knowl-
edge based network initialization. In Neural Networks, 2000. IJCNN 2000, Proceedings of the
IEEE-INNS-ENNS International Joint Conference on, Vol. 6. IEEE, 619–624.
[14] Eva Jablonka and Marion J Lamb. 2014. Evolution in four dimensions, revised edition: Ge-
netic, epigenetic, behavioral, and symbolic variation in the history of life. MIT press.
[15] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. 2017. Population
Based Training of Neural Networks. arXiv preprint arXiv:1711.09846 (2017).
[16] Ron Keesing and David G Stork. 1991. Evolution and learning in neural networks: the number
and distribution of learning trials affect the rate of evolution. In Advances in Neural Information
Processing Systems. 804–810.
[17] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. 2015. Siamese neural networks for
one-shot image recognition. In ICML Deep Learning Workshop, Vol. 2.
[18] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. 2011. One shot
learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive
Science Society, Vol. 33.
[19] John Maynard Smith. 1987. Natural selection: when learning guides evolution. (01 1987),
455–457.
[20] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep
reinforcement learning. In International Conference on Machine Learning. 1928–1937.
[21] Sachin Ravi and Hugo Larochelle. 2016. Optimization as a model for few-shot learning.
(2016).
[22] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lilli-
crap. 2016. One-shot learning with memory-augmented neural networks. arXiv preprint
arXiv:1605.06065 (2016).
[23] Mauro Santos, Eo¨rs Szathma´ry, and Jose´ F Fontanari. 2015. Phenotypic plasticity, the baldwin
effect, and the speeding up of evolution: The computational roots of an illusion. Journal of
theoretical biology 371 (2015), 127–136.
[24] Tom Schaul, Tobias Glasmachers, and Ju¨rgen Schmidhuber. 2011. High Dimensions and
Heavy Tails for Natural Evolution Strategies. In Genetic and Evolutionary Computation Con-
ference (GECCO).
[25] Juergen Schmidhuber, Jieyu Zhao, and MA Wiering. 1996. Simple principles of metalearning.
Technical report IDSIA 69 (1996), 1–23.
11
[26] Sebastian Thrun and Lorien Pratt. 1998. Learning to learn: Introduction and overview. In
Learning to learn. Springer, 3–17.
[27] Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. MuJoCo: A physics engine for model-
based control. (10 2012), 5026-5033 pages.
[28] Peter D Turney. 1995. Cost-sensitive classification: Empirical evaluation of a hybrid genetic
decision tree induction algorithm. Journal of artificial intelligence research 2 (1995), 369–
409.
[29] Peter D Turney. 2002. Myths and legends of the Baldwin effect. arXiv preprint cs/0212036
(2002).
[30] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. 2016. Matching networks
for one shot learning. In Advances in Neural Information Processing Systems. 3630–3638.
[31] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. 2017. Learning to Reinforcement
Learn. Cognitive Science, CogSci (2017). arXiv:1611.05763
[32] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Ju¨rgen Schmidhuber.
2014. Natural Evolution Strategies. Journal of Machine Learning Research 15 (2014), 949–
980. http://jmlr.org/papers/v15/wierstra14a.html
[33] Daan Wierstra, Tom Schaul, Jan Peters, and Ju¨rgen Schmidhuber. 2008. Natural Evolution
Strategies. In Proceedings of the Congress on Evolutionary Computation (CEC08), Hongkong.
IEEE Press.
12
"
28,"Artificial Intelligence Review
https://doi.org/10.1007/s10462-018-9641-3
Recent progress in semantic image segmentation
Xiaolong Liu1 · Zhidong Deng1 · Yuhan Yang2
© The Author(s) 2018
Abstract
Semantic image segmentation, which becomes one of the key applications in image pro-
cessing and computer vision domain, has been used in multiple domains such as medical
area and intelligent transportation. Lots of benchmark datasets are released for researchers
to verify their algorithms. Semantic segmentation has been studied for many years. Since the
emergence of Deep Neural Network (DNN), segmentation has made a tremendous progress.
In this paper, we divide semantic image segmentation methods into two categories: tradi-
tional and recent DNN method. Firstly, we briefly summarize the traditional method as well
as datasets released for segmentation, then we comprehensively investigate recent methods
based on DNN which are described in the eight aspects: fully convolutional network, up-
sample ways, FCN joint with CRF methods, dilated convolution approaches, progresses in
backbone network, pyramid methods, Multi-level feature and multi-stage method, supervised,
weakly-supervised and unsupervised methods. Finally, a conclusion in this area is drawn.
Keywords Image semantic segmentation · DNN · CNN · FCN
1 Introduction
Semantic image segmentation, also called pixel-level classification, is the task of clustering
parts of image together which belong to the same object class (Thoma 2016).
Two other main image tasks are image level classification and detection. Classification
means treating each image as an identical category. Detection refers to object localization and
This work was supported in part by the National Key Research and Development Program of China under
Grant No. 2017YFB1302200 and by research fund of Tsinghua University - Tencent Joint Laboratory for
Internet Innovation Technology.
B Zhidong Deng
michael@mail.tsinghua.edu.cn
Xiaolong Liu
xllau@126.com
1 State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for
Information Science and Technology, Department of Computer Science, Tsinghua University,
Beijing 100084, China
2 Department of Economics and Management, Tsinghua University, Beijing 100084, China
123
X. Liu et al.
recognition. Image segmentation can be treated as pixel-level prediction because it classifies
each pixel into its category. Moreover, there is a task named instance segmentation which
joints detection and segmentation together. More details can refer to literature (Lin et al.
2014; Li et al. 2017a).
Semantic image segmentation has multiple applications, such as detecting road signs
(Maldonado-Bascon et al. 2007), colon crypts segmentation (Cohen et al. 2015), land use
and land cover classification (Huang et al. 2002). Also, it is widely used in medicine field,
such as detecting brains and tumors (Moon et al. 2002), and detecting and tracking medical
instruments in operations (Wei et al. 1997). Several applications of segmentation in medicine
are listed in Dzung et al. (1999). In Advanced Driver Assistance Systems (ADAS) or self-
driving car area, scene parsing is of great significance and it heavily relies on semantic image
segmentation (Fritsch et al. 2013; Menze and Geiger 2015; Cordts et al. 2016).
Since the re-rising of DNN (Deep Neural Network), the segmentation accuracy has been
significantly enhanced. In general, the methods before DNN are called traditional method.
we also comply with this convention in the following sections. Traditional segmentation
methods are briefly reviewed in this paper. More importantly, it will focus on the recent
progress made by adopting DNN and organize them in several aspects. Moreover, we has
carried out a survey on datasets of image segmentation and evaluation metrics.
This paper is organized as follows: Sect. 2 reviews the semantic image segmentation on
datasets and evaluation metrics. Section 3 makes a brief summary of traditional methods.
Section 4 introduces details of the recent progress. Finally, Sect. 5 makes a brief summary.
2 Datasets and evaluationmetrics
This section reviews the the datasets related to semantic segmentation and evaluation metrics.
2.1 Datasets
At present, there are many general datasets related to image segmentation, such as, PASCAL
VOC (Everingham et al. 2010), MS COCO (Lin et al. 2014), ADE20K (Zhou et al. 2017),
especially in autonomous driving area Cityscapes (Cordts et al. 2016), and KITTI (Fritsch
et al. 2013; Menze and Geiger 2015).
The PASCAL Visual Object Classes (VOC) Challenge (Everingham et al. 2010) consists
of two components: (1) dataset of images and annotation made publicly available; (2) an
annual workshop and competition. The main challenges have run each year since 2005. Until
2012, the challenge contains 20 classes. The train/val data has 11,530 images containing
27,450 ROI annotated objects and 6929 segmentations. In addition, the dataset has been
widely used in image segmentations.
Microsoft COCO dataset (Lin et al. 2014) contains photos of 91 objects types which would
be recognized easily by a 4-year-old person with a total of 2.5 million labeled instances in
328k images. They also present a detailed statistical analysis of the dataset in comparison
to PASCAL (Everingham et al. 2010), ImageNet (Deng et al. 2009), and SUN (Xiao et al.
2010).
ADE20K (Zhou et al. 2017) is another scene parsing benchmark with 150 objects and
stuff classes. Unlike other datasets, ADE20K includes object segmentation mask and parts
segmentation mask. Also, there are a few images with segmentation showing parts of the
heads (e.g. mouth, eyes, and nose). There are exactly 20,210 images in the training set, 2000
123
Recent progress in semantic image segmentation
Fig. 1 An example of ADE20K image. From left to right and top to bottom, the first segmentation shows
the object masks. The second segmentation corresponds to the object parts (e.g. body parts, mug parts, table
parts).The third segmentation shows parts of the heads (e.g. eyes, mouth, and nose)
Fig. 2 A fine annotated image from Cityscapes
images in the validation set, and 3000 images in the testing set (Zhou et al. 2017). A group
of images are shown in Fig. 1.
The Cityscapes Dataset (Cordts et al. 2016) is a benchmark which focuses on semantic
understanding of urban street scenes. It consists of 30 classes in 5000 fine annotated images
that are collected from 50 cities. Besides, the collection time spans over several months,
which covers season of spring, summer, and fall. A fine-annotated image is shown in Fig. 2.
KITTI dataset (Fritsch et al. 2013; Menze and Geiger 2015), as another dataset for
autonomous driving, captured by driving around mid-size city of Karlsruhe, on highways,
and in rural areas. Averagely, in every image, up to 15 cars and 30 pedestrians are visible.
123
X. Liu et al.
The main tasks of this dataset are road detection, stereo reconstruction, optical flow, visual
odometry, 3D object detection, and 3D tracking (http://www.cvlibs.net/datasets/kitti/).
In addition to the above datasets, there are also many others, such as SUN (Xiao et al.
2010), Shadow detection/Texture segmentation vision dataset (https://zenodo.org/record/
59019#.WWHm3oSGNeM), Berkeley segmentation dataset (Martin and Fowlkes 2017),
and LabelMe images database (Russell et al. 2008). More details about the dataset can refer
to http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm.
2.2 Evaluationmetrics
Regular performance evaluation metrics for image segmentation and scene parsing include:
pixel accuracy Pacc, mean accuracy Macc, region intersection upon union (IU) MIU , and
frequency weighted IU FWIU . Let ni j indicates the number of pixels of class i predicted
correctly to belong to class j, where there are ncl different classes, and let ti = ∑ j ni j
indicates the number of pixels of class i. All of the four metrics are described as below (Long
et al. 2014):
Pacc =
∑
i nii∑
i ti
(1)
Macc = 1
ncl
∑
i
nii
ti
(2)
MIU = 1
ncl
∑
i
nii
ti + ∑ j n ji − nii
(3)
FWIU = 1∑
k tk
∑
i
ti nii
ti + ∑ j n ji − nii
(4)
3 Traditional methods
Before DNN is proposed, features and classification methods refer to the most important
topics. In the computer vision and image processing area, feature is a piece of information
which is relevant for solving the computational tasks. In general, this is the same sense as
feature in machine learning and pattern recognition. Variety of features are used for semantic
segmentation, such as Pixel color, Histogram of oriented gradients (HOG) (Dalal and Triggs
2005; Bourdev et al. 2010), Scale-invariant feature transform (SIFT) (Lowe 2004), Local
Binary Pattern (LBP) (He and Wang 1990), SURF (Bay et al. 2008), Harris Corners (Derpanis
2004), Shi-Tomasi (Shi et al. 1994), Sub-pixel Corner (Medioni and Yasumoto 1987), SUSAN
(Smith and Brady 1997), Features from Accelerated Segment Test (FAST) (Rosten and
Drummond 2005), FAST- ER (Rosten et al. 2010), AGAST (Mair et al. 2010) and Multi-
scale AGAST (Leutenegger et al. 2011) Detector, Bag-of-visual-words (BOV) (Csurka et al.
2004), Pselets (Brox et al. 2011), and Textons (Zhu et al. 2005), just to name a few.
Approaches in image semantic segmentation include unsupervised and supervised ones.
To be specific, the simple one is thresholding methods which are widely used in gray images.
Gray images are very common in medical area where the collection equipment is usually
X-ray CT scanner or MRI (Magnetic Resonance Imaging) equipment (Zheng et al. 2010; Hu
et al. 2001; Xu et al. 2010). Overall, thresholding methods are quite effective in this area.
123
Recent progress in semantic image segmentation
K-means clustering refers to an unsupervised method for clustering. The k-means algo-
rithm requires the number of clusters to be given beforehand. Initially, k centroids are
randomly placed in the feature space. Furthermore, it assigns each data point to the near-
est centroid, successively moves the centroid to the center of the cluster, and continues the
process until the stopping criterion is reached (Hartigan and Hartigan 1975).
The segmentation problem can be treated as an energy model. It derives from compression-
based method which is implemented in Mobahi et al. (2010).
Intuitively, edge is important information for segmentation. There are also many edge-
based detection researches (Kimmel and Bruckstein 2003; Osher and Paragios 2003;
Barghout 2014; Pedrycz et al. 2008; Barghout and Lee 2003; Lindeberg and Li 1997). Besides,
edge-based approaches and region-growing methods (Nock and Nielsen 2004) are also other
branches.
Support vector machine (SVMs): SVMs are well-studied binary classifiers which preform
well on many tasks. The training data is represented as (xi , yi ) where xi is the feature vector
and yi ∈ {−1, 1} the binary label for training example i ∈ {1, . . . , m}. Where w is a weight
vector and b is the bias factor. Solving SVM is an optimization problem described as Eq. 5.
min
w,b
= 1
2
||w||2
s.t . ∀mi=1 yi · (< w, xi > +b) ≥ 1
(5)
Slack variables can solve linearly inseparable problems. Besides, kernel method is adopted
to deal with inseparable tasks through mapping current dimensional features to higher dimen-
sion.
Markov Random Network (MRF) is a set of random variables having a Markov property
described by an undirected graph. Also, it is an undirected graphical model. Let x be the
input, and y be the output. MRF learns the distribution P(y, x). In contrast to MRF, A CRF
(Russell et al. 2009) is essentially a structured extension of logistic regression, and it models
the conditional probabilities P(Y |X). These two models and their variations are widely used
and have reached the best performance in segmentation (http://host.robots.ox.ac.uk/pascal/
VOC/voc2010/results/index.html; He et al. 2004; Shotton et al. 2006).
4 Recent DNN in segmentation
Artificial Neural Network (ANN) is inspired by biologic neurons. The basic element of
ANN is artificial neuron. Each single artificial neuron has some inputs which are weighted
and summed up. Followed by a transfer function or activation function, the neuron outputs a
scale value. An example of neural model is illustrated in Fig. 3.
Based on artificial neuron, different stacking of the neurons forms Auto-encoder (Bengio
2009), Restricted Boltz- mann Machine (RBM) (Larochelle and Bengio 2008), Recurrent
Neural Network or Recursive Neural Network (RNN), Convolutional Neural Network (CNN)
(LeCun and Bengio 1995), Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber
1997) and other types of ANNs. The basic architecture is illustrated in Fig. 4.
Convolutional Neural Network (CNN) (LeCun and Bengio 1995) uses shared-weight
architecture, which is inspired by biological processes. The connectivity pattern between
neurons is mimic of the organization of the animal visual cortex. Another important concept
is receptive field, and it means that individual cortical neurons respond to stimuli only in a
123
X. Liu et al.
Fig. 3 Artificial neuron model
Fig. 4 An example of artificial neural network model
restricted region of the visual field. Also, they have the property of shift invariant or space
invariant, based on their shared-weight architecture and translation invariance characteristics.
Due to the excellent structure, CNN has obtained remarkable results on image classifica-
tion, segmentation, and detection. The following part will present the recent progresses by
applying CNNs in image semantic segmentation.
4.1 Fully convolutional network (FCN)
The paper (Long et al. 2014) is the first work that introduces ANNFCN to image segmentation
area. The main insight is the replacement of fully connected layer by fully convolutional
layer. With the use of the interpolation layer, it realizes that the size of output is the same as
the input, which is essential in segmentation. To enhance the segmentation evidence, skips
is adopted. More importantly, the network is trained end to end, takes arbitrary size, and
produces correspondingly-sized output with efficient inference and learning.
FCN is implemented in VGG-Net and achieves the state of art on segmentation of PASCAL
VOC (20% relative improvement to 62.2% mean IU in 2012) at that time, while the inference
takes less than one fifth of a second for a typical image. The main architecture is shown in
Fig. 5.
123
Recent progress in semantic image segmentation
Fig. 5 Fully convolutional network (FCN) architecture
4.2 Up-samplemethod: interpolation versus deconvolution
In addition to the FCN architecture, deconvolution layer is also adopted in semantic segmen-
tation. The deconvolution network used in Noh et al. (2015) consists of deconvolution and
un-pooling layers, which identify pixel-wise class labels and predict segmentation masks.
Unlike FCN in paper (Noh et al. 2015), the network is applied to individual object proposals
so as to obtain instance-wise segmentations combined for the final semantic segmentation.
Up-sample stage adopts bi-linear interpolation, which can refer to Long et al. (2014). Due
to its computation efficiency and good recovery of the original image, the up-sample stage
adopts bi-linear interpolation broadly. Deconvolution is the reverse calculation of convolution
123
X. Liu et al.
Fig. 6 Deconvolution network architecture
operation, which can also recover the input size. Thus, it can be applied into segmentation
to recover the feature map size to original input size. The architecture implemented in Noh
et al. (2015) is illustrated in Fig. 6. Also, other researchers implement semantic segmentation
by deconvolution layer in different versions, which can refer to Mohan (2014), Monvel et al.
(2003), Saito et al. (2016).
4.3 FCN joint with CRF and other traditional methods
According to the research of Deeplab, the responses at the final layer of Deep Convolutional
Neural Networks (DCNNs) are not sufficiently localized for accurate object segmentation
(Chen et al. 2016b). They overcome this poor localization property by combining a fully
connected Conditional Random Field (CRF) at the final DCNN layer. Their method reaches
71.6% IOU accuracy in the test set at the PASCAL VOC-2012 image semantic segmentation
task. After this work, they carry out another segmentation architecture by combining domain
transform (DT) with DCNN (Chen et al. 2016a) because dense CRF inference is compu-
tationally expensive. DT refers to a modern edge-preserving filtering method, in which the
amount of smoothing is controlled by a reference edge map. Domain transform filtering is
several times faster than dense CRF inference. Lastly, through experiments, it not only yields
comparable semantic segmentation results but also accurately captures the object boundaries.
Researchers also exploit segmentation by using super-pixels (Mostajabi et al. 2015; Sharma
et al. 2015).
Paper (Liu et al. 2015) addresses image semantic segmentation by combining rich infor-
mation into Markov Random Field (MRF), including mixture of label contexts and high-order
relations (Figs. 7, 8, 9).
4.4 Dilated convolution
Most semantic segmentations are based on the adaptations of Convolutional Neural Net-
works (CNNs) that had originally been devised for image classification task. However, dense
prediction, such as image semantic segmentation tasks, is structurally different from classi-
fication.
123
Recent progress in semantic image segmentation
Fig. 7 An example of dilated convolution (Atrous convolution or hole convolution). Convolution layer with
kernel size 3 × 3, a normal convolution operation with parameter dilation = 1; b dilated convolution with
parameter dilation = 2; c dilated convolution with parameter dilation = 3
Paper (Chen et al. 2016b) has already applied this strategy in their work. It is called
‘Atrous Convolution’ or ‘Hole Convolution (Chen et al. 2016b)’ or ‘dilated convolution (Yu
and Koltun 2015)’ . Atrous convolution is originally developed for the efficient computation of
the undecimated wavelet transform in the “algorithme à trous” scheme of paper (Holschneider
et al. 1989). In Yu and Koltun (2015), they have presented a module using dilated convolutions
to aggregate multi-scale contextual information systematically. The architecture is based
on dilated convolutions that support exponential receptive field expansion without loss of
resolution or coverage. Since the dilated convolution has griding artifacts, paper (Yu et al.
2017) develops an approach named dilated residual networks (DRN) to remove these artifacts
and further increase the performance of the network.
4.5 Progress in backbone network
The backbone network refers to the main structure of the network. As is known to all, the
backbone used in semantic segmentation is derived from image classification tasks. The
FCN (Long et al. 2014) adopts VGG-16 net (Simonyan and Zisserman 2014) which did
exceptionally well in ILSVRC14. Also, they consider AlexNet architecture (Krizhevsky
et al. 2012) that won ILSVRC12 as well as GoogLeNet (Szegedy et al. 2015) that also did
well in ILSVRC14. VGG net is adopted in many literatures, such as in Chen et al. (2016b)
Liu et al. (2015).
After the release of ResNet (Deep residual network) (He et al. 2016) which Deeplab
implement their work on which won the first place on the ILSVRC 2015 classification task,
the semantic segmentation has made a new breakthrough. To find out the best configuration,
paper (Wu et al. 2016a) evaluates different variations of a fully convolutional residual network,
including the resolution of feature maps, the number of layers, and the size of field-of-
view. Furthermore, paper (Wu et al. 2016b) studies the deep residual networks and explains
some behaviors that have been observed experimentally. As a result, they derive a shallower
architecture of residual network which significantly outperforms much deeper models on the
ImageNet classification dataset.
Recently, ResNeXt (Xie et al. 2016) have been brought up as the next generation of
ResNet. It is the foundation of our entry to the ILSVRC 2016 classification task in which we
secured the 2nd place. GoogleNet also obtains development as Inception-v2, Inception-v3
123
X. Liu et al.
Fig. 8 Three level image pyramid
(Szegedy et al. 2016), Incetion-v4 and Inception-ResNet (Szegedy et al. 2017), which has
already been adopted in the paper (Li et al. 2017b).
4.6 Pyramidmethod in segmentation
Apart from adopting stronger backbone networks, researchers also attempt to combine pyra-
mid strategy to CNN. The typical one is pyramid method.
1. Image pyramid
An image pyramid (Adelson et al. 1984) is a collection of images which are successively
downsampled until some desired stopping criteria are reached. There are two common kinds
of image pyramids: Gaussian pyramid which is used to downsample images and Laplacian
pyramid which is used to reconstruct an upsampled image from an image lower in the pyramid
(with less resolution).
In semantic image segmentation area, paper (Lin et al. 2016a) devises a network with tra-
ditional multi-scale image input and sliding pyramid pooling that can effectively improve the
performance. This architecture captures the patch-background context. Similarly, Deeplab
implements an image pyramid structure (Chen et al. 2016c) which extracts multi-scale fea-
tures by feeding multiple resized input images to a shared deep network. At the end of each
deep network, the resulting features are merged for pixel-wise classification.
Laplacian pyramid is also utilized in semantic image segmentation which can refer to paper
(Ghiasi and Fowlkes 2016). They bring out a multi-resolution reconstruction architecture
based on a Laplacian pyramid, which uses skip connections from higher-resolution feature
maps and multiplicative gating to progressively refine boundaries reconstructed from lower-
resolution feature maps. Paper (Farabet et al. 2013) presents a scene parsing system. The raw
input image is transformed through a Laplacian pyramid. Meanwhile, each scale is fed to a
two-stage CNN that produces a set of feature maps.
123
Recent progress in semantic image segmentation
Fig. 9 Image pyramid used in CNN
2. Atrous spatial pyramid pooling
Inspired by image pyramid strategy, (Chen et al. 2016b) proposes Atrous Spatial Pyramid
Pooling (ASPP) to segment objects robustly at multiple scales. ASPP probes effective fields-
of-views (FOV) and convolutional feature layer with filters at multiple sampling rates, and
then captures objects image context at multiple scales. The architecture is shown in Fig. 10.
3. Pooling pyramid
Through pyramid pooling module illustrated in Fig. 11, paper (Zhao et al. 2016) exploits
the capability of global context information by different-region based context aggregation
and names their pyramid scene parsing network (PSPNet). Through experiments they report
their outstanding results: with pyramid pooling, a single PSPNet yields new record of mIoU
score as 85.4% on PASCAL VOC 2012 and 80.2% on Cityscapes.
The pyramid pooling adopts different scales of pooling size, then does up-sample process
on the outputs to the original size, and finally concatenates the results to form a mixed feature
representation. In Fig. 11, different scales of pooling sizes are marked with different colors.
Generally speaking, the pyramid pooling can be applied to any feature map. For example,
the application in Zhao et al. (2016) applies pyramid pooling in pool5 layer.
4. Feature pyramid
As pointed out by literature (Lin et al. 2016b), feature pyramid is a basic component in
image tasks for detecting objects at different scales. In fact, recent deep learning object detec-
tors have avoided pyramid representation because it is compute and memory intensive. In Lin
et al. (2016b), they exploit the multi-scale, pyramidal hierarchy of CNN to construct feature
pyramids with marginal extra cost. Also, Feature Pyramid Network (FPN) is developed for
building high-level semantic feature maps at all scales.
123
X. Liu et al.
Fig. 10 The atrous spatial pyramid pooling. (The distance in conv does not represent real rate)
4.7 Multi-level feature andmulti-stagemethod
CNN can be treated as a feature extractor (Hariharan et al. 2015). Typically speaking, recog-
nition algorithms based on convolutional networks (CNNs) use the output of the last layer
as a feature representation. However, the information in this layer is too coarse for dense
prediction. On the contrary, earlier layers may be precise in localization, but they will not
capture semantics. To get the best of both advantages, they define the hypercolumns as the
vector of activations of all CNN units above that pixel.
Indeed, skips have already been adopted in FCN (Long et al. 2014) which is depicted in
Fig. 5. It seems that the multi-level method has been used in their work.
Multi-model is an ensemble way to deal with image tasks (Li et al. 2015; Viola and Jones
2001). Apart from multi-level strategy, a multi-stage method is used in semantic segmentation
123
Recent progress in semantic image segmentation
Fig. 11 Illustration of pyramid pooling structure
(Li et al. 2017b). They propose deep layer cascade (LC) method to improve the accuracy
and speed of semantic segmentation. Unlike the conventional model cascade (MC) (Li et al.
2015; Viola and Jones 2001) that consists of multiple independent models. LC treats a single
deep model as a cascade of several sub-models and classifies most of the easy regions in
123
X. Liu et al.
Fig. 12 Structure adopted in Hariharan et al. (2015) as hypercolumns
the shallow stage and makes deeper stage focus on a few hard regions. It not only improves
the segmentation performance but also accelerates both training and testing of deep network
(Fig. 12).
4.8 Supervised, weakly-supervised and unsupervisedmethods
Most of the progress in semantic image segmentation are done under supervised scheme.
However, researchers also dedicate to semi-supervised or non-supervised learning. More
details can refer to Papandreou et al. (2015), Xia et al. (2013), Zhu et al. (2014), Xu et al.
(2015).
123
Recent progress in semantic image segmentation
5 Conclusion
Semantic image segmentation is a key application in image processing and computer vision
domain. Besides briefly reviewing on traditional semantic image segmentation, this paper
comprehensively lists recent progress in semantic image segmentation, especially based on
DCNN, in the following aspects: 1. fully convolutional network, 2. up-sample ways, 3.
FCN joint with CRF methods, 4. dilated convolution approaches, 5. progresses in backbone
network, 6. pyramid methods, 7. Multi-level feature and multi-stage method, 8. supervised,
weakly-supervised and unsupervised methods.
Till now, more and more methods are emerging to make semantic image segmentation
more accurate or faster or both on accuracy and speed. We hope this review on recent progress
of semantic image segmentation can make some help to researchers related to this area.
OpenAccess This article is distributed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and repro-
duction in any medium, provided you give appropriate credit to the original author(s) and the source, provide
a link to the Creative Commons license, and indicate if changes were made.
References
Adelson EH, Anderson CH, Bergen JR, Burt PJ, Ogden JM (1984) Pyramid methods in image processing.
RCA Eng 29(6):33–41
Barghout L (2014) Visual taxometric approach to image segmentation using fuzzy-spatial taxon cut yields
contextually relevant regions. In: IPMU , vol 2, pp 163–173
Barghout L, Lee L (2003) Perceptual information processing system. US Patent App. 10/618,543
Bay H, Ess A, Tuytelaars T, Van Gool L (2008) Speeded-up robust features (surf). Comput Vis Image Underst
110(3):346–359
Bengio Y et al (2009) Learning deep architectures for ai. Foundations and trends®. Mach Learn 2(1):1–127
Bourdev L, Maji S, Brox T, Malik J (2010) Detecting people using mutually consistent poselet activations.
Comput Vis ECCV 2010:168–181
Brox T, Bourdev L, Maji S, Malik J (2011) Object segmentation by alignment of poselet activations to image
contours. In: Proceedings of the 2011 IEEE conference on computer vision and pattern recognition
(CVPR). IEEE, pp 2225–2232
Chen LC, Barron JT, Papandreou G, Murphy K, Yuille AL (2016a) Semantic image segmentation with task-
specific edge detection using CNNS and a discriminatively trained domain transform. In: Proceedings
of the IEEE conference on computer vision and pattern recognition, pp 4545–4554
Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL (2016b) Deeplab: semantic image segmen-
tation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint
arXiv:1606.00915
Chen LC, Yang Y, Wang J, Xu W, Yuille AL (2016c) Attention to scale: scale-aware semantic image segmenta-
tion. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3640–3649
Cohen A, Rivlin E, Shimshoni I, Sabo E (2015) Memory based active contour algorithm using pixel-level
classified images for colon crypt segmentation. Comput Med Imaging Graph 43:150–164
Cordts M, Omran M, Ramos S, Rehfeld T, Enzweiler M, Benenson R, Franke U, Roth S, Schiele B (2016)
The cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp 3213–3223
Csurka G, Dance C, Fan L, Willamowski J, Bray C (2004) Visual categorization with bags of keypoints. In:
Workshop on statistical learning in computer vision, ECCV, vol 1. Prague, pp 1–2
CVonline: Image databases. http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm
Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: IEEE computer society
conference on computer vision and pattern recognition, 2005. CVPR 2005, vol 1. IEEE, pp 886–893
de Monvel JB, Scarfone E, Le Calvez S, Ulfendahl M (2003) Image-adaptive deconvolution for three-
dimensional deep biological imaging. Biophys J 85(6):3991–4001
123
X. Liu et al.
Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet: A large-scale hierarchical image database.
In: IEEE conference on computer vision and pattern recognition, 2009. CVPR 2009. IEEE, pp 248–255
Derpanis KG (2004) The harris corner detector. York University, Toronto
Dzung L, Chenyang X, Jerry L (1999) A survey of current methods in medical image segmentation. Technical
report
Everingham M, Van Gool L, Williams CKI, Winn J, Zisserman A (2010) The pascal visual object classes
(voc) challenge. Int J Comput Vis 88(2):303–338
Farabet C, Couprie C, Najman L, LeCun Y (2013) Learning hierarchical features for scene labeling. IEEE
Trans Pattern Anal Mach Intell 35(8):1915–1929
Fritsch J, Kuehnl T, Geiger A (2013) A new performance measure and evaluation benchmark for road detection
algorithms. In: International conference on intelligent transportation systems (ITSC)
Ghiasi G, Fowlkes CC (2016) Laplacian pyramid reconstruction and refinement for semantic segmentation.
arXiv preprint arXiv:1605.02264
Hariharan B, Arbeláez P, Girshick R, Malik J (2015) Hypercolumns for object segmentation and fine-grained
localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp
447–456
Hartigan JA, Hartigan J (1975) Clustering algorithms, vol 209. Wiley, New York
He DC, Wang L (1990) Texture unit, texture spectrum, and texture analysis. IEEE Trans Geosci Remote Sens
28(4):509–512
He X, Zemel RS, Carreira-Perpiñán MÁ (2004) Multiscale conditional random fields for image labeling. In:
Proceedings of the 2004 IEEE computer society conference on computer vision and pattern recognition,
2004. CVPR 2004, vol 2. IEEE, pp II–II
He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of the
IEEE conference on computer vision and pattern recognition, pp 770–778
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780
Holschneider M, Kronland-Martinet R, Morlet J, Tchamitchian P (1989) A real-time algorithm for signal
analysis with the help of the wavelet transform. In: Combes J, Grossmann A, Tchamitchian P (eds)
Wavelets, pp. 286–297. Springer, Berlin
Hu S, Hoffman EA, Reinhardt JM (2001) Automatic lung segmentation for accurate quantitation of volumetric
x-ray ct images. IEEE Trans Med Imaging 20(6):490–498
Huang C, Davis L, Townshend J (2002) An assessment of support vector machines for land cover classification.
Int J Remote Sens 23(4):725–749
Kimmel R, Bruckstein AM (2003) Regularized laplacian zero crossings as optimal edge integrators. Int J
Comput Vis 53(3):225–243
Kitti vision benchmark suite. http://www.cvlibs.net/datasets/kitti/
Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural networks.
In: Advances in neural information processing systems, pp 1097–1105
Larochelle H, Bengio Y (2008) Classification using discriminative restricted boltzmann machines. In: Pro-
ceedings of the 25th international conference on machine learning. ACM, pp 536–543
LeCun Y, Bengio Y (1995) Convolutional networks for images, speech, and time-series. In: Arbib M (ed) The
handbook of brain theory and neural networks. MIT Press
Leutenegger S, Chli M, Siegwart RY (2011) Brisk: binary robust invariant scalable keypoints. In: 2011 IEEE
international conference on computer vision (ICCV). IEEE, pp 2548–2555
Li H, Lin Z, Shen X, Brandt J, Hua G (2015) A convolutional neural network cascade for face detection. In:
Proceedings of the IEEE conference on computer vision and pattern recognition, pp 5325–5334
Li Y, Qi H, Dai J, Ji X, Wei Y (2017a) Fully convolutional instance-aware semantic segmentation. In: Computer
vision and pattern recognition (CVPR). IEEE, pp 4438–4446
Li X, Liu Z, Luo P, Loy CC, Tang X (2017b) Not all pixels are equal: difficulty-aware semantic segmentation
via deep layer cascade. arXiv preprint arXiv:1704.01344
Lin TY, Maire M, Belongie S, Bourdev L, Girshick R, Hays J, Perona P, Ramanan D, Zitnick CL, Dollar P
(2014) Microsoft coco: common objects in context. arXiv preprint arXiv:1405.0312
Lin G, Shen C, van den Hengel A, Reid I (2016a) Efficient piecewise training of deep structured models
for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern
recognition, pp 3194–3203
Lin TY, Dollár P, Girshick R, He K, Hariharan B, Belongie S (2016b) Feature pyramid networks for object
detection. arXiv preprint arXiv:1612.03144
Lindeberg T, Li MX (1997) Segmentation and classification of edges using minimum description length
approximation and complementary junction cues. Comput Vis Image Underst 67(1):88–98
Liu Z, Li X, Luo P, Loy CC, Tang X (2015) Semantic image segmentation via deep parsing network. In:
Proceedings of the IEEE international conference on computer vision, pp 1377–1385
123
Recent progress in semantic image segmentation
Long J, Shelhamer E, Darrell T (2014) Fully convolutional networks for semantic segmentation. IEEE Trans
Pattern Anal Mach Intell 79(10):1337–1342
Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110
Mair E, Hager G, Burschka D, Suppa M, Hirzinger G (2010) Adaptive and generic corner detection based on
the accelerated segment test. Comput Vis ECCV 2010:183–196
Maldonado-Bascon S, Lafuente-Arroyo S, Gil-Jimenez P, Gomez-Moreno H, López-Ferreras F (2007) Road-
sign detection and recognition based on support vector machines. IEEE Trans Intell Transp Syst 8(2):264–
278
Martin D, Fowlkes C (2017) The berkeley segmentation database and benchmark. Computer Science Depart-
ment, Berkeley University. http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds
Medioni G, Yasumoto Y (1987) Corner detection and curve representation using cubic b-splines. Comput Vis
Graph Image Process 39(3):267–278
Menze M, Geiger A (2015) Object scene flow for autonomous vehicles. In: Proceedings of the conference on
computer vision and pattern recognition (CVPR)
Mobahi H, Rao SR, Yang AY, Sastry SS, Ma Y (2010) Segmentation of natural images by texture and boundary
compression. arXiv preprint arXiv:1006.3679
Mohan R (2014) Deep deconvolutional networks for scene parsing. arXiv preprint arXiv:1411.4101
Moon N, Bullitt E, Van Leemput K, Gerig G (2002) Automatic brain and tumor segmentation. Med Image
Comput Comput Assist Interv MICCAI 2002:372–379
Mostajabi M, Yadollahpour P, Shakhnarovich G (2015) Feedforward semantic segmentation with zoom-out
features. In: Proceedings of the computer vision and pattern recognition, pp 3376–3385
Nock R, Nielsen F (2004) Statistical region merging. IEEE Trans Pattern Anal Mach Intell 26(11):1452–1458
Noh H, Hong S, Han B (2015) Learning deconvolution network for semantic segmentation. In: Proceedings
of the IEEE international conference on computer vision, pp 1520–1528
Osher S, Paragios N (2003) Geometric level set methods in imaging, vision, and graphics. Springer, Berlin
Papandreou G, Chen LC, Murphy KP, Yuille AL (2015) Weakly-and semi-supervised learning of a deep convo-
lutional network for semantic image segmentation. In: Proceedings of the IEEE international conference
on computer vision, pp 1742–1750
Pedrycz W, Skowron A, Kreinovich V (2008) Handbook of granular computing. Wiley, New York
Rosten E, Drummond T (2005) Fusing points and lines for high performance tracking. In: Proceedings of the
tenth IEEE international conference on computer vision, 2005. ICCV 2005, vol 2. IEEE, pp 1508–1515
Rosten E, Porter R, Drummond T (2010) Faster and better: a machine learning approach to corner detection.
IEEE Trans Pattern Anal Mach Intell 32(1):105–119
Russell BC, Torralba A, Murphy KP, Freeman WT (2008) Labelme: a database and web-based tool for image
annotation. Int J Comput Vis 77(1–3):157–173
Russell C, Kohli P, Torr PH et al (2009) Associative hierarchical crfs for object class image segmentation. In:
Proceedings of the 2009 IEEE 12th international conference on computer vision. IEEE, pp 739–746
Saito S, Li T, Li H (2016) Real-time facial segmentation and performance capture from RGB input. arXiv
preprint arXiv:1604.02647
Sharma A, Tuzel O, Jacobs DW (2015) Deep hierarchical parsing for semantic segmentation. In: Proceedings
of the computer vision and pattern recognition, pp 530–538
Shadow detection/texture segmentation computer vision dataset. https://zenodo.org/record/59019#.
WWHm3oSGNeM
Shi J et al (1994) Good features to track. In: Proceedings of the 1994 IEEE computer society conference on
CVPR’94 computer vision and pattern recognition. IEEE, pp. 593–600
Shotton J, Winn J, Rother C, Criminisi A (2006) Textonboost: joint appearance, shape and context modeling
for multi-class object recognition and segmentation. Comput Vis ECCV 2006:1–15
Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556
Smith SM, Brady JM (1997) Susana new approach to low level image processing. Int J Comput Vis 23(1):45–78
Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015)
Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern
recognition, pp 1–9
Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for computer
vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 2818–2826
Szegedy C, Ioffe S, Vanhoucke V, Alemi AA (2017) Inception-v4, inception-resnet and the impact of residual
connections on learning. In: AAAI, pp 4278–4284
Thoma M (2016) A survey of semantic segmentation. arXiv preprint arXiv:1602.06541
VOC2010 preliminary results. http://host.robots.ox.ac.uk/pascal/VOC/voc2010/results/index.html
123
X. Liu et al.
Viola P, Jones M (2001) Rapid object detection using a boosted cascade of simple features. In: Proceedings of
the 2001 IEEE computer society conference on computer vision and pattern recognition, CVPR 2001,
vol 1. IEEE, pp I–I
Wei GQ, Arbter K, Hirzinger G (1997) Automatic tracking of laparoscopic instruments by color coding. In:
CVRMed-MRCAS’97. Springer, Berlin, pp 357–366
Wu Z, Shen C, Hengel A (2016a) High-performance semantic segmentation using very deep fully convolutional
networks. arXiv preprint arXiv:1604.04339
Wu Z, Shen C, Hengel A (2016b) Wider or deeper: revisiting the resnet model for visual recognition. arXiv
preprint arXiv:1611.10080
Xia W, Domokos C, Dong J, Cheong LF, Yan S (2013) Semantic segmentation without annotating segments.
In: Proceedings of the IEEE international conference on computer vision, pp 2176–2183
Xiao J, Hays J, Ehinger KA, Oliva A, Torralba A (2010) Sun database: large-scale scene recognition from
abbey to zoo. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR). IEEE, pp
3485–3492
Xie S, Girshick R, Dollr P, Tu Z, He K (2016) Aggregated residual transformations for deep neural networks.
arXiv preprint arXiv:1611.05431
Xu A, Wang L, Feng S, Qu Y (2010) Threshold-based level set method of image segmentation. In: 2010 3rd
international conference on intelligent networks and intelligent systems (ICINIS). IEEE, pp 703–706
Xu J, Schwing AG, Urtasun R (2015) Learning to segment under various forms of weak supervision. In:
Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3781–3790
Yu F, Koltun V (2015) Multi-scale context aggregation by dilated convolutions. arXiv preprint
arXiv:1511.07122
Yu F, Koltun V, Funkhouser T (2017) Dilated residual networks. arXiv preprint arXiv:1705.09914
Zhao H, Shi J, Qi X, Wang X, Jia J (2016) Pyramid scene parsing network. arXiv preprint arXiv:1612.01105
Zheng L, Li G, Bao Y (2010) Improvement of grayscale image 2D maximum entropy threshold segmentation
method. In: 2010 international conference on logistics systems and intelligent management, vol 1. IEEE,
pp 324–328
Zhou B, Zhao H, Puig X, Fidler S, Barriuso A, Torralba A (2017) Scene parsing through ade20k dataset. In:
Proceedings of the IEEE conference on computer vision and pattern recognition
Zhu SC, Guo CE, Wang Y, Xu Z (2005) What are textons? Int J Comput Vis 62(1):121–143
Zhu J, Mao J, Yuille AL (2014) Learning from weakly supervised data by the expectation loss svm (e-svm)
algorithm. In: Advances in neural information processing systems, pp 1125–1133
123
"
29,"Disclaimer:
This work has been accepted for publication in the IEEE Robotics and Automation Letters.
Copyright:
c© 2017 IEEE. Personal use of this material is permitted. Permission from IEEE must
be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective
works, for resale or redistribution to servers or lists, or reuse of any copyrighted component
of this work in other works.
ar
X
iv
:1
70
2.
07
89
8v
2 
 [c
s.R
O]
  4
 M
ay
 20
17
Learning Deep NBNN Representations for Robust Place Categorization
Massimiliano Mancini1,2, Samuel Rota Bulo`2,3, Elisa Ricci2,4, Barbara Caputo1
Abstract— This paper presents an approach for semantic
place categorization using data obtained from RGB cameras.
Previous studies on visual place recognition and classification
have shown that, by considering features derived from pre-
trained Convolutional Neural Networks (CNNs) in combination
with part-based classification models, high recognition accuracy
can be achieved, even in presence of occlusions and severe view-
point changes. Inspired by these works, we propose to exploit
local deep representations, representing images as set of regions
applying a Naı¨ve Bayes Nearest Neighbor (NBNN) model for
image classification. As opposed to previous methods where
CNNs are merely used as feature extractors, our approach
seamlessly integrates the NBNN model into a fully-convolutional
neural network. Experimental results show that the proposed
algorithm outperforms previous methods based on pre-trained
CNN models and that, when employed in challenging robot
place recognition tasks, it is robust to occlusions, environmental
and sensor changes.
I. INTRODUCTION
Recent years have seen the breakthrough of mobile
robotics into the consumer market. Domestic robots have
become increasingly common, as well as vehicles making
use of cameras, radar and other sensors to assist the driver.
An important aspect of human-robot interaction, is the ability
of artificial agents to understand the way humans think and
talk about abstract spatial concepts. For example, a domestic
robot may be asked to clean the bathroom, while a car may be
asked to stop at the parking area. Hence, a robot’s definition
of bathroom, or parking area should point to the same set of
places that a human would recognize as such.
The problem of assigning a semantic spatial label to an
image has been extensively studied in the computer and robot
vision literature [1], [2], [3], [4], [5]. The most important
challenges in identifying places come from the complexity
of the concepts to be recognized and from the variability of
the conditions in which the images are captured. Scenes from
the same category may differ significantly, while images
corresponding to different places may look similar. The
historical take on these issues has been to model the visual
appearance of scenes considering a large variety of both
global and local descriptors [1], [2], [3], [6] and several
(shallow) learning models (e.g. SVMs, Random Forests).
Since the (re-)emergence of Convolutional Neural Net-
works (CNNs), approaches based on learning deep represen-
This work was partially supported by the ERC grant 637076 -
RoboExNovo (B.C. ), and the CHIST-ERA project ALOOF (B.C.).
1M. Mancini and B. Caputo are with University of Rome La Sapienza,
Rome, Italy. {mancini,caputo}@dis.uniroma1.it
2M. Mancini, S. Rota Bulo` and E. Ricci are with Fondazione Bruno
Kessler, Trento, Italy. {rotabulo,eliricci}@fbk.eu
3S. Rota Bulo`’ is with Mapillary, Graz, Austria.
4E. Ricci is with University of Perugia, Perugia, Italy.
Multiscale patches extractor
CNN features extractor
NBNN-based 
classifier
?
(a) Standard NBNN pipeline
Scaling
Fully Convolutional 
CNN-NBNN
?
(b) FullyConv-NBNN pipeline
Fig. 1: The standard NBNN classification pipeline (a) versus
the proposed model (b). The orange boxes indicate modules
which involve a learning phase. Instead of extracting patches
in a preprocessing step, we employ a fully-convolutional
neural network, which automatically computes local features
from the image. Moreover, features extraction and classifier
modules are merged, allowing end-to-end training.
tations have become mainstream. Several works exploited
deep models for visual-based scene classification and place
recognition tasks, showing improved accuracy over tradi-
tional methods based on hand-crafted descriptors [7], [8],
[9], [10], [11]. Some of these studies [7], [8], [11] demon-
strated the benefit of adopting a region-based approach (i.e.
considering only specific image parts) in combination with
descriptors derived from CNNs, such as to obtain models
which are robust to viewpoint changes and occlusions. With
a similar motivation, lately several works in computer vision
have attempted to bring back the notion of localities into deep
networks, e.g. by designing appropriate pooling strategies
[12] or by casting the problem within the Image-2-Class
(I2C) recognition framework [13], with a high degree of
success. All these works decouple the choice of the signifi-
cant localities from the learning of deep representations, as
the CNN feature extraction and the classifier learning are
implemented as two separate modules. This leads to two
drawbacks: first, choosing heuristically the relevant localities
means concretely cropping parts of the images before feeding
them to the chosen features extractor.This is clearly sub-
optimal, and might turn out to be computationally expensive.
Second, it would be desirable to fully exploit the power of
deep networks by directly learning the best representations
for the task at hand, rather than re-use architectures trained
on general-purpose databases like ImageNet and passively
processing patches from the input images without adapting
its weights. Ideally, a fully-unified approach would guaran-
tee more discriminative representations, resulting in higher
recognition accuracy.
This paper contributes to this last research thread by
addressing these two issues. We propose an approach for
semantic place categorization which exploits local represen-
tations within a deep learning framework. Our method is
inspired by the recent work [13], which demonstrates that,
by dividing images into regions and representing them with
CNN-based features, state-of-the-art scene recognition accu-
racy can be achieved by exploiting an I2C approach, namely
a parametric extension of the Naı¨ve Bayes Nearest Neighbor
(NBNN) model. Following this intuition, we propose a deep
architecture for semantic scene classification which seam-
lessly integrates the NBNN and CNN frameworks (Fig. 1).
We automatize the multi-scale patch extraction process by
adopting a fully-convolutional network [14], guaranteeing a
significant advantage in terms of computational cost over
two-steps methods. Furthermore, a differentiable counterpart
of the traditional NBNN loss is considered to obtain an
error that can be back-propagated to the underlying CNN
layers, thus enabling end-to-end training. To the best of our
knowledge, this is the first attempt to fully unify NBNN
and CNN, building a deep version of Naı¨ve Bayes Nearest
Neighbor. We extensively evaluate our approach on several
publicly-available benchmarks. Our results demonstrate the
advantage of the proposed end-to-end learning scheme over
previous works based on a two-step pipeline and the effec-
tiveness of our deep network over state-of-the-art methods
on challenging robot place categorization tasks.
II. RELATED WORK
In this section we review previous works on (i) visual-
based place recognition and categorization and (ii) Naı¨ve
Bayes Nearest Neighbor classification.
A. Visual-based Place Recognition and Categorization
In the last decade several works in the robotic community
addressed the problem of developing robust place recognition
[15], [16], [17], [8], [9] and semantic classification [4], [18],
[7] approaches using visual data. In particular, focusing on
place categorization from monocular images, earlier works
adopted a two-step pipeline: first, hand-crafted features, such
as GIST [1], CENTRIST [2], CRFH [4] or HOUP [3],
are extracted from the query image, and then the image
is classified into one of the predefined categories using a
previously-trained discriminative model (e.g., Support Vector
Machines). Similarly, earlier studies on visual-based place
recognition and loop closing also considered hand-crafted
feature representations [15], [16], [19].
More recently, motivated by the success of deep learning
models in addressing visual recognition tasks [20], robotic
researchers have started to exploit feature representations
derived from CNNs for both place recognition [8], [9], [11]
and semantic scene categorization [7] tasks. Sunderhau et
al. [8] analyzed the performance of CNN-based descriptors
with respect to viewpoint changes and time variations, pre-
senting the first real-time place recognition system based
on convolutional networks. Arroyo et al. [9] addressed the
problem of topological localization across different seasons
and proposed an approach which fuses information derived
from multiple convolutional layers of a deep architecture.
Gout et al. [21] evaluated the representational power of deep
features for analyzing images collected by an autonomous
surface vessel, studying the effectiveness of CNN descriptors
in case of large seasonal and illumination changes. Ursˇicˇ
et al. [7] proposed an approach for semantic room catego-
rization: first, images are decomposed in regions and CNN-
based descriptors are extracted for each region; then, a part-
based classification model is derived for place categorization.
Interestingly, they showed that their method outperforms
traditional CNN architectures based on global representa-
tions [20], as the part-based model guarantees robustness
to occlusions and image scaling. Our work develops from
a similar idea, but differently from [7] the deep network
is not merely used as feature extractor and a novel CNN
architecture, suitable to end-to-end training, is proposed.
B. Naı¨ve Bayes Nearest Neighbor Classification
The NBNN approach has been widely adopted in the
computer and robot vision community, as an effective method
to overcome the limitations of local descriptor quantization
and Image-2-Image recognition [22]. Several previous stud-
ies have demonstrated that the I2C paradigm implemented
by NBNN models is especially beneficial for generalization
and domain adaptation [23] and that, by adding a learning
component to the non-parametric NBNN, performance can
be further boosted [24].
Recent works have also shown that the NBNN can be suc-
cessfully employed for place recognition and categorization
tasks [13], [16], [10]. Kanji [16] introduced a NBNN scene
descriptor for cross-seasonal place recognition. In a later
work [10], Kanji extended this approach by integrating CNN-
based features and PCA, deriving a PCA-NBNN model for
addressing the problem of self-localization in case of images
with small view overlap. Kuzborskij et al. [13] proposed a
multi-scale parametric version of the NBNN classifier and
demonstrated its effectiveness in combination with precom-
puted CNN descriptors for scene recognition. Our work is
inspired by [13]. However, the proposed learning model is
based on a fully-convolutional network which can be trained
in an end-to-end manner. Therefore, it is significantly faster
and more accurate than [13].
III. FULLY-CONVOLUTIONAL CNN-NBNL
In this section we describe the proposed approach for
semantic place categorization. As illustrated in Fig. 1, our
method develops from the same idea of previous models
based on local representations and CNN descriptors [13], [7]:
NBNL
Fig. 2: Simplified architecture of the proposed framework.
The image is re-scaled to different sizes. The obtained images
are fed in parallel to multiple FC-CNNs with shared weights.
From the networks, descriptors are extracted and used as
input to the NBNL classifier.
images are decomposed into multiple regions (represented
with CNN features) and a part-based classifier is used to
infer the labels associated to places. However, differently
from previous works, our approach unifies the feature ex-
traction and the classifier learning phases, and we propose
a novel CNN architecture which implements a part-based
classification strategy. As demonstrated in our experiments
(Sect. IV), our deep network guarantees a significant boost
in performance, both in term of accuracy and computational
cost. Since our framework is derived from previous works
on NBNN-based methods [22], [24], [13], we first provide a
brief description of these approaches (Sect. III-A-III-B) and
then we introduce the proposed fully-convolutional NBNN-
based network (Sect. III-C).
A. Naı¨ve Bayes Non-Linear Learning
Let X denote the set of possible images and let Y be
a finite set of class labels, indicating the different scene
categories. The goal is to estimate a classifier f : X → Y
from a training set T ⊂ X × Y sampled from the under-
lying, unknown data distribution. The NBNN method [22]
works under the assumption that there is a an intermediate
Euclidean space Z and a set-valued function φ that abstracts
an input image x ∈ X into a set of descriptors in Z ,
i.e. φ(x) ⊂ Z . For instance, the image could be broken
into patches and a descriptor in Z could be computed for
each patch. Given a training set T , let Φy(T ) be the set of
descriptors computed from images in T having labels y ∈ Y ,
i.e. Φy(T ) = {φ(x) : x ∈ X , (x, y) ∈ T }. The NBNN
classifier fNBNN is given as follows:
fNBNN(x; T ) = arg min
y∈Y
∑
z∈φ(x)
d(z,Φy(T ))2 , (1)
where d(x,S) = inf{‖z−s‖2 : s ∈ S} denotes the smallest
Euclidean distance between z and an element of S ⊂ Z , or
in other terms it is the distance between z and its nearest
neighbor in S.
Despite its effectiveness in terms of classification perfor-
mance [22], fNBNN has the drawback of being expensive at
test time, due to the nearest-neighbor search. A possible way
to reduce the complexity of this step consists in learning a
small, finite set Wy ⊂ Z of representative prototypes for
each class y ∈ Y to replace Φy(T ). This idea was pursued by
Fornoni et al. [24] with a method named Naı¨ve Bayes Non-
Linear Learning (NBNL). NBNL is developed from Eq. (1)
by replacing Φy(T ) with the set of prototypes Wy and by
assuming Z to be restricted to the unit ball. Under the latter
assumption the bound d(z,S)2 ≥ 2−ω(z,S) can be derived
[24], where:
ω(z,S) =
(∑
s∈S
|〈z, s〉|q+
)1/q
. (2)
Here, 〈·〉 denotes the dot product, q ∈ [1,+∞] and |x|+ =
max(0, x). The NBNL classifier is finally obtained in the
form given below by using the bound as a replacement of
d()2 in Eq.(1) (and after simple algebraic manipulations):
fNBNL(x;W) = arg max
y∈Y
∑
z∈φ(x)
ω(z,Wy) , (3)
where W = {Wy}y∈Y encompasses all the prototypes.
In order to learn the prototypes Wy for each y ∈ Y ,
Fornoni et al. did not consider fNBNL as classifier and T
as training set, but they considered (only at training time)
a classifier having the form f(x) = arg maxy∈Y ω(z,Wy)
and an extended training set {(z, y) : z ∈ Φy(T ), y ∈ Y},
where each descriptor extracted from an image is promoted
to a training sample. In this way they derived the equivalent
of a Multiclass Latent Locally Linear SVM (ML3) that is
trained using the algorithm in [25].
B. CNN-NBNL
Motivated by the robustness of NBNN/NBNL models and
by the recent success of deep architectures in addressing
challenging visual tasks, Kuzborskij et al. [13] introduced an
approach, named CNN-NBNL, which combines the NBNL
and CNN frameworks. Their method is an implementation
of NBNL, where φ(x) is obtained by dividing an image
x ∈ X into patches at different scales and by employing
a pre-trained CNN-based feature extractor [26] to compute
a descriptor for each patch. In formal terms, if gCNN : X →
Z is the CNN-based feature extractor that takes an input
image/patch and returns a single descriptor, then φ(x) (see,
Sect. III-A) is given by
φCNN(x) = {gCNN(xˆ) : xˆ ∈ patches(x)} , (4)
where patches(x) ⊂ X returns a set of patches extracted
from x at multiple scales and reshaped to be compatible in
terms of resolution with the input dimensionality required
by the implementation of gCNN (e.g. CaffeNet [26] requires
227 × 227). To learn the prototypes Wy in [13] a training
objective similar to [24] is adopted, but the optimization is
performed using a stochastic version of ML3 (STOML3) that
better scales to larger datasets. At test time, fNBNL defined as
in Eq. 3 is used with φ replaced by φCNN.
By moving from hand-crafted features to CNN-based
features, the performance of the NBNL classifier improves
considerably. Nonetheless, the approach proposed in [13] has
two limitations: 1) it requires the extraction of patches for
each image as a pre-processing step, and CNN-features are
extracted sequentially from each patch; 2) the CNN archi-
tecture is used as a mere feature extractor and the method
lacks the advantage of an end-to-end trainable system. The
first limitation has a negative impact on the computation
time of the method, while the latter makes way for further
performance boosts.
C. Fully-Convolutional CNN-NBNL
To overcome the two limitations of CNN-NBNL men-
tioned above, in this work we introduce a fully-convolutional
version of CNN-NBNL that is end-to-end trainable (Fig. 2).
Fully-convolutional extension. Extracting patches at multi-
ple scales and extracting CNN features independently for
each of them is a very costly operation, which severely
impacts training and test time. In order to perform a similar
operation but with a limited impact on computation time,
we propose to employ a Fully-Convolutional CNN (FC-
CNN) [14] to simulate the extraction of descriptors from
multiple patches over the entire image. A FC-CNN can be
derived from a standard CNN by replacing fully-connected
layers with convolutional layers. By doing so, the network
is able to map an input image of arbitrary size into a set
of spatially-arranged output values (descriptors). To cover
multiple scales, we simply aggregate descriptors that are
extracted with the FC-CNN from images at different resolu-
tions. In this way, as the receptive fields of the FC-CNN
remain the same, changing the scale of the input image
induces an implicit change in the scale of the descriptors.
The number of obtained descriptors per image depends on the
image resolution and can in general be controlled by properly
shaping the convolutional layers: for instance, by increasing
the stride of the last convolutional layer it is possible to
reduce the number of descriptors that the FC-CNN returns.
In the following, we denote by gFCN(x; θ) ⊂ Z the output
of a FC-CNN parametrized by θ applied to an input image
x ∈ X . As opposed to gCNN defined in Sect. III-B, which
returns a single descriptor, gFCN(x; θ) outputs a set of descrip-
tors, one for each spatial location in the final convolutional
layer of the FC-CNN. Each descriptor has a dimensionality
that equals the number of output convolutional filters. We
will also denote by η(x) the number of descriptors that the
FC-CNN generates for an input image x. Note that this
number does not depend on the actual parametrization of
the network, but only on its topology, which is assumed to
be fixed, and on the resolution of the input image.
End-to-end architecture. The NBNL classifier that we
propose and detail below can be implemented using layers
that are commonly found in deep learning frameworks and
can thus be easily stacked on top of a FC-CNN (see, Fig. 3).
By doing so, we obtain an architecture that can be trained
end-to-end.
Given an input image x ∈ X , we create a set of m
scaled versions of x, which we denote by scale(x) ⊂ X .
Each scaled image xˆ ∈ scale(x) is fed to the FC-CNN
described before, yielding a set of descriptors gFCN(xˆ; θ).
Instead of aggregating the descriptors from each scale, as
done in Eq. (4), we keep them separated because they un-
dergo a normalization step which avoids biasing the classifier
towards scales that have a larger number of descriptors. The
final form of our NBNL classifier is given by:
fFCN NBNL(x;W, θ) = arg max
y∈Y
h(x;Wy, θ) , (5)
where h defined below measures the likelihood of x given
prototypes in Wy:
h(x;Wy, θ) = 1
m
∑
xˆ∈scale(x)
ω¯(xˆ;Wy, θ) (6)
and ω¯ is the scale-specific normalized score:
ω¯(xˆ;Wy, θ) = 1
η(xˆ)
∑
z∈gFCN(xˆ;θ)
ω(z;Wy) . (7)
This normalization step is necessary to prevent scales that
generate many descriptors to bias the final likelihood.
To train our network we define the following regularized
empirical risk with respect to both the classifiers’ parameters
W and the FC-CNN’s parameters θ:
R(W, θ; T ) = 1T
∑
(x,y)∈T
`(h(x;W, θ), y) + λΩ(W, θ) .
Here, h(x;W, θ) = {h(x;Wy, θ)}y∈Y , Ω is a `2-regularizer
acting on all the networks’ parameters, and `(u, y) with u =
{uy}y∈Y , uy ∈ R, is the following loss function:
`(u, y) = −uy + log
∑
y∈Y
euy ,
obtained from the composition of the log-loss with the soft-
max operator.
Following [24], [13] we actually do not minimize directly
R(W, θ; T ) as defined above, but replace the loss terms with
the following upper-bound, which is obtained by application
of Jensen’s inequality:
`(h(x;W, θ), y) ≤ 1
m
∑
xˆ∈scale(x)
1
η(xˆ)
∑
z∈gFCN(xˆ;θ)
`(ω(z,W), y) ,
with ω(z,W) = {ω(z,Wy)}y∈Y . This is equivalent to
promoting descriptors to training samples, as in [24], [13].
IV. EXPERIMENTAL RESULTS
In this section, we evaluate the performance of our ap-
proach. In Sect. IV-A we compare against the method in [13],
demonstrating the advantages of our end-to-end learning
framework. In Sect. IV-B.1 we assess the effectiveness of
the proposed approach for the place categorization task,
considering images acquired from different robotic platforms
in various indoor environments, comparing with state-of-the-
art approaches. Finally, we demonstrate the robustness of
our model to different environmental conditions and sensors
(Sect. IV-B.2) and to occlusions and image perturbations
xˆixˆ1 xˆm
scale(x)
FCN conv[1, kp] relu pow[q] gconv[k, 1, k] pow
[
1
q
]
reduce[avg] sum argmax
softmax logloss
[
1
η(xˆi)
]
Fig. 3: Architecture of our fully-convolutional CNN-NBNL. We scale an input image x ∈ X and obtain {xˆ1, . . . , xˆm} =
scale(x). The scaled versions of x are forwarded in parallel through the net. The green block represents the FC-CNN. The
gray blocks implement the NBNL classifier. The red blocks (top-left) are active only during training. Parameter k represents
the number of classes, p the number of prototypes per class and q the parameter in Eq.(2). Further details about the building
blocks are given hereafter. FCN is a FC-CNN. conv[W,C] is a W ×W convolutional layer with C filters. relu applies the
ReLu non-linearity to each element. pow[E] raises each element to the power of E. gconv[G,W,C] is a grouped W ×W
convolutional layer with G groups and C filters (the filters are filled and fixed with 1s; biases are omitted). reduce[avg]
averages out the spatial dimensions. sum performs the element-wise sum of the incoming lines. argmax returns the index
of the maximum element. softmax applies the softmax operator along the input channels, for each spatial entry of each
input line. logloss[ 1η(xˆi) ] sums up the log-loss computed along the input channels of each spatial entry of each input
line, and each input line i is weighted by 1η(xˆi) .
(Sect. IV-B.3). Our evaluation has been performed using
NVIDIA GeForce 1070 GTX GPU, implementing our ap-
proach with the popular Caffe framework [26].
A. Comparison with Holistic and Part-based CNN models
In a first series of experiments we demonstrate the ad-
vantages of the proposed part-based model and compare it
with (i) its non end-to-end counterpart (i.e, the CNN-NBNL
method in [13]) and (ii) traditional CNN-based approaches
not accounting for local representations. To implement [13]
following the original paper, we split the input image into
multiple patches, extracting features from the last fully-
connected layer of a pre-trained CNN. The patches were
extracted at three different scales (32,64,128 pixels) after the
original image was rescaled (longest side 200 pixels). We
adopted the sparse protocol in [13], based on which features
from 100 random patches are extracted. The features are
equally distributed between the three scales and an additional
descriptor representing the full image is considered. As rep-
resentative for deep models based on holistic representations,
we chose the successful approach of Zhou et al. [27], [28]:
they pre-train a CNN on huge datasets (i.e., ImageNet [29],
Places [27], [28] or both in the hybrid configuration) and
used it as features extractor for learning a linear SVM
model. Note that this is a strong baseline, widely used in
the computer vision community for scene recognition tasks.
To demonstrate the generality of our contribution, we
tested all models considering three different base networks:
the Caffe [26] version of AlexNet [20], VGG-16 [30] and
GoogLeNet [31]. For AlexNet and VGG-16 we considered
the networks pre-trained on both Places [27], [28] and
ImageNet [29] datasets (i.e, the hybrid configuration). For
GoogLeNet no pre-trained hybrid network was available,
thus we took the model pre-trained on Places365. In order to
fairly compare our model with the baseline method in [13],
our fully-convolutional network was designed to match the
resolution of local patches adopted in [13]. To accomplish
this, since a 128x128 patch covers 64% of a 200x200 image,
we rescaled the input image such that the receptive fields
correspond to approximately 64% of the input (i.e. 355
pxls for CaffeNet and 350 pxls for VGG and GoogLeNet).
The other scale features were obtained by upsampling the
image twice with a deconvolutional layer. We extracted 25
local features for the larger scale (128x128 pxls), 36 for
the medium and 49 for the smallest, for a total of 110
local descriptors. These number of features were obtained
by regulating the stride of the last layers of the network.
As in [13], we extracted features at the last fully-connected
layer level, applying batch normalization [32] before the
classifier. Since the datasets considered in our evaluation
have small/medium dimensions, fine-tuning was performed
only in the last two layers of the network. The networks were
trained with a fixed learning rate which was decreased twice
of a factor 0.1. To decide the proper learning rate schedule
and number of epochs, we performed parameters tuning on a
separate validation set. As parameters of the NBNL classifier,
we chose k = 10 and p = 2, applying a weight decay of
10−5 on the prototypes. Notice that in our model we consid-
ered 110 descriptors, while 100 were used for the baseline
method in [13]. However, we experimentally verified that a
difference of 10 descriptors does not influence performance.
This confirms previous findings in [13], where Kuzborskij
et al. also tested their approach with a dense configuration
employing 400 patches without significant improvements in
accuracy over the sampling protocol.
We performed experiments on three different datasets,
previously used in [13]: Sports8 [33], Scene15 [6] and
MIT67 [34]. The Sports8 dataset [33] contains 8 different
indoor and outdoor sport scenes (rowing, badminton, polo,
bocce, snowboarding, croquet, sailing and rock climbing).
The number of images per category ranges from 137 to 200.
We followed the common experimental setting, taking 70
images per class for training and 60 for testing. The Scene15
dataset [6] is composed by different categories of outdoor
and indoor scenes. It contains a maximum of 400 gray scale
images per category. We considered the standard protocol,
taking 100 images for training and 100 for testing for each
TABLE I: Comparing global and part-based CNN models.
Network Method Sports8 Scene15 MIT67
AlexNet
Hybrid
[27] 94.22±0.78 91.59±0.48 70.8
[13] 95.29± 0.61 92.42± 0.64 73± 0.36
Ours 95.58 ± 0.58 93.63 ± 0.90 74.98 ± 0.78
GoogLeNet
Places365
[28] 91.00 91.25 73.30
[13] 93.08± 1.78 92.29± 0.59 73.14± 1.43
Ours 94.46 ± 0.86 93.68 ± 0.57 80.55 ± 0.70
VGG
Hybrid
[28] 94.17 92.12 77.63
[13] 94.79± 0.42 92.97± 0.68 77.62± 0.97
Ours 97.04 ± 0.27 95.12 ± 0.41 82.49 ± 1.35
(a) [13] (b) Ours
Fig. 4: t-SNE visualization of features extracted from 4
classes of the Scene15 dataset.
class. The MIT67 [34] is a common benchmark for indoor
scene recognition. It contains images of 67 indoor scenes,
with at least 100 images per class. We adopted the common
experimental setting, using 80 images per class for training
and 20 for testing. For each dataset we took 5 random splits
reporting the results as mean and standard deviation.
Tab. I shows the results of our evaluation. Mean and
standard deviation are provided for our approach and [13],
while for the CNN models in [27], [28] we report results
from the original papers. From the table it is clear that, for
all base networks and datasets, our method outperforms the
baselines. These results confirm the significant advantage
of the proposed part-based approach over traditional CNN
architectures which do not consider local representations.
Moreover, our results show that our end-to-end training
model guarantees an improvement in performance compared
to its non end-to-end counterpart CNN-NBNL. This im-
provement is mostly due to the proposed end-to-end training
strategy. A pre-trained network is able to extract powerful
features, but they are not always discriminative when applied
to specific tasks. On the other hand, end-to-end training
allows to overcome this limitation by adapting the pre-trained
features to a new target task, producing class discriminative
representations. This is shown in Fig. 4 where we plot
the fc7 features extracted at scale 64x64 pixels (t-SNE
visualizations [35]) with CNN-NBNL (Fig. 4.a) and with
our approach (Fig. 4.b): while a pre-trained network fails at
creating discriminative local features, our model is able to
learn representations that cluster accordingly to class labels.
To further compare our approach and CNN-NBNL [13]
we also analyzed the computational time required during
the test phase to process an increasing number of patches.
Fig. 5 report the results of our analysis: as expected, our
fully-convolutional architecture is greatly advantageous over
the CNN-NBNL model which extract local features indepen-
0 300 600 900 1200 1500
30
60
90
120
150
n. of descriptors
tim
e 
(m
s)
 
 
[13]
Ours
Fig. 5: Computational time at varying number of descriptors.
dently patch-by-patch. We remark that reduced classification
time is a fundamental for the adoption of the proposed model
in robotic platforms operating in real environments.
B. Robot Place Categorization
In this section we show the results of our evaluation when
testing the proposed approach on publicly available robot
vision datasets. These experiments aim at verifying the effec-
tiveness of our fully-convolutional network and its robustness
to varying environmental conditions and occlusions.
1) COLD dataset: We first tested our method on the COsy
Localization Database (COLD) database [5]. This database
contains three datasets of indoor scenes acquired in three dif-
ferent laboratories from different robots. The COLD-Freiburg
contains 26 image sequences collected in the Autonomous
Intelligent Systems Laboratory at the University of Freiburg
with a camera mounted on an ActivMedia Pioneer-3 robot.
COLD-Ljubljana contains 18 image sequences acquired from
the camera of an iRobot ATRV-Mini platform at the Visual
Cognitive Systems Laboratory of University of Ljubljana. In
the COLD-Saarbru¨cken an ActivMedia PeopleBot has been
employed to gather 29 image sequences inside the Language
Technology Laboratory at the German Research Center for
Artificial Intelligence in Saarbru¨cken.
In our experiments we followed the protocol described
in Rubio et al. [36], considering images of path 2 of each
laboratory. These data depict significant changes with respect
to illumination conditions and time of data acquisition.
Using path 2, there are 9 categories for COLD-Freiburg,
6 for COLD-Ljubljana and 8 for COLD-Saarbru¨cken. We
trained and tested on data collected on the same laboratory,
considering 5 random splits and reporting the average values.
We compared our model with the methods proposed in [36],
since this work is one of the most recent studies adopting
this dataset. In [36], Rubio et al. proposed to extract HOG
features and to apply a dimensionality reduction technique
before providing the features as input to different classifiers.
As classifiers they considered linear SVM, Naı¨ve Bayes
(NB), Bayesian Network (BN) and the Tree Augmented
Naı¨ve Bayes (TAN). In our experiments, to train our model
we adopted the same setting described in Sect. IV-A, fine-
tuning the last two layers of the network.
The results are shown in Tab. II. Our model outper-
forms all the baselines in [36], confirming the advantage
of CNN-based approaches over traditional classifiers and
hand crafted features. The high accuracy of our method also
demonstrates that the proposed fully-convolutional network
TABLE II: Results on COLD dataset.
Method Freiburg Saarbrcken Ljubljana
HOG+SVM [36] 46.5 44.9 66.2
HOG+NB [36] 54.6 52.9 62.6
HOG+TAN [36] 69.5 72.6 75.2
HOG+BN [36] 82.3 84.4 88.5
Ours 95.2 97.3 99.2
is highly effective at discerning among different rooms, even
with significant lighting and environmental changes.
2) KTH-IDOL dataset: To further assess the ability of
the proposed method to generalize across different robotics
platforms and illumination conditions, we performed exper-
iments on the KTH Image Database for rObot Localization
(KTH-IDOL) [37]. This dataset contains 12 image sequences
collected by two robots (Dumbo and Minnie) on 5 different
rooms. The image sequences were collected along several
days on three different illumination conditions: sunny, cloudy
and night. Following [2] we considered the first two se-
quences for each robot and weather condition, performing
three different type of tests. First, we trained and tested
using the same robot and same weather condition with one
sequence used for training and the other for testing and vice-
versa. As a second experiment, we used the same robot for
training and testing, varying the weather conditions of the
two sets. In the last experiment we trained the classifier with
the same weather condition but testing it on a different robot.
Notice that, differently from Sect. IV-B.1, in this case the
illumination changes are not present in the training set. Our
model is trained with the same setting of Sect. IV-A. In this
case, to reduce overfitting and improve the capability of our
network, we apply data augmentation to the RGB channels,
following the standard procedure introduced in [20].
We compared our method with three state-of-the-art ap-
proaches: (i) [4] which used high dimensional histogram
global features as input for a χ2 kernel SVM; (ii) [2] which
proposed the CENTRIST descriptor and performed nearest
neighbor classification and (iii) [3] which used again the
nearest neighbor classifier but with Histogram of Oriented
Uniform Patterns (HOUP) as features.
Tab. III shows the results of our evaluation. Our method
outperforms all the baselines in the first and third series
of experiments (same lighting). In particular, the large im-
provements in performance in the third experiment clearly
demonstrates its ability to generalize over different input
representations of the same scene, independently of the
camera mounted on the robot. These results suggest that it
should be possible to train offline our model and apply it on
arbitrary robotic platforms. On the second experiment, while
the high classification accuracy demonstrates a significant
robustness to lighting variations, our model achieves com-
parable performance with previous works, showing a small
advantage of CNN representations over traditional methods
in case of illumination changes.
3) Household room dataset: In the last series of experi-
ments we tested the robustness of our model with respect to
occlusions. We evaluate the performance of our approach on
the recently introduced household room (or MIT8) dataset
TABLE III: Results on KTH-IDOL dataset (D and M denotes
the names of the robot platforms Dumbo and Minnie).
Train Test Lighting [4] [2] [3] Ours
D D Same 97.26 97.62 98.24 98.61
M M Same 95.51 95.35 96.61 97.32
D D Diff 80.55 94.98 95.76 94.17
M M Diff 71.90 90.17 92.01 93.62
D M Same 66.63 77.78 80.05 87.05
M D Same 62.20 72.44 75.43 88.51
[7]. This dataset is a subset of MIT67 which contains 8
room categories: bathroom, bedroom, children room, closet,
corridor, dining room, kitchen and living room. We used the
setting provided in [7], with 641 images for training and
155 for testing. The challenge proposed by Ursˇicˇ et al. [7]
is to train the model on the original images and test its
performances on various noisy conditions. The conditions
are: occlusion in the center of the image, occlusion on the
right border, occlusions from a person, addition of an outside
border, upside down rotation and cuts on the top or right part
of the image (inducing aspect ratio changes). All the test sets
were produced following the protocol in [7], apart from the
person occlusions set provided directly by the authors.
We compare our approach with the part-based model
developed by Ursˇicˇ et al. [7] and the global CNN-based
model in [27]. In [7] selective search is used to extract infor-
mative regions inside the image, which are then provided as
input to a pre-trained CNN. From these features, exemplar
parts are learned for each category and used by a part-
based mixture model for the final classification. The standard
hybrid CaffeNet [27] is employed as CNN architecture. For
a fair comparison we adopted the same base architecture,
extracting features at the last fully-connected layer before the
classifier. In this case we used images rescaled to 256x256 as
input, upsampling them twice to obtain descriptors at mul-
tiple scales. We extracted 45 descriptors, 4 for the smallest
scale (256x256), 16 for the medium and 25 for the largest.
The training procedure is the one described in Sect. IV-A
and the same parameters are used for the NBNL classifier,
with batch normalization applied to the last layer. We trained
our model 10 times, computing the average accuracy.
The results of the evaluation are reported in Tab. IV. As
shown in the table, both our approach and the method in [7]
achieve higher classification accuracy than the CNN model
in [27], confirming the benefit of part-based modeling. It
is interesting to compare our approach with [7]: while our
framework guarantees better performances in certain con-
ditions (e.g. original frames, person occlusion), the method
in [7] is more robust to changes of the aspect ratio (e.g.
cuts in the image) and scale (e.g. outside border addition).
Interestingly, when the occlusion is not created artificially
obscuring patches (person occluder), our model achieves
higher performance than [7]. Oppositely, in the case of the
outside border experiments, almost half of the image is black
and the real content reduces to a very small scale. In this
(artificial) setting, [7] outperforms our model. For sake of
completeness, we also report the confusion matrix associated
with our results on the original frames (Fig. 6).
TABLE IV: Results on MIT8 dataset.
Experiment [27] [7] Ours
original 86.45 85.16 89.10
outside border 62.58 85.16 74.65
black occluder, right 78.71 80.00 80.53
black occluder, central 61.94 69.68 67.74
person occluder, central 59.35 68.39 72.45
cut right half 62.58 64.52 65.16
cut top half 52.26 68.39 63.16
upside down 52.26 59.35 63.94
bat
hro
om
bed
roo
m
chil
dre
n ro
om
clos
et
cor
rido
r
dini
ng 
roo
m
kitc
hen
livin
g ro
om
living room
kitchen
dining room
corridor
closet
children room
bedroom
bathroom
0 1 1 0 0 2 0 22
0 0 0 0 0 0 20 1
2 0 0 0 0 10 0 1
1 0 0 0 18 2 0 0
0 0 0 17 1 0 0 0
0 0 18 0 0 0 0 0
1 15 2 0 0 0 0 3
17 0 0 0 0 0 0 0
Fig. 6: Confusion matrix obtained with our model classifying
the original images of the MIT8 dataset.
V. CONCLUSIONS
We presented a novel deep learning architecture for ad-
dressing the semantic place categorization task. By seam-
lessly integrating the CNN and NBNN frameworks, our
approach permits to learn local deep representations, en-
abling robust scene recognition. The effectiveness of the
proposed method is demonstrated on various benchmarks.
We show that our approach outperforms traditional CNN
baselines and previous part-based models which use CNNs
purely as features extractors. In robotics scenarios, our deep
network achieves state-of-the-art results on three different
benchmarks, demonstrating its robustness to occlusions, en-
vironmental changes and different sensors. As future work,
we plan to extend this model in order to handle multimodal
inputs (e.g. considering range sensors in addition to RGB
cameras).
REFERENCES
[1] A. Oliva and A. Torralba, “Modeling the shape of the scene: A
holistic representation of the spatial envelope,” International journal
of computer vision, vol. 42, no. 3, pp. 145–175, 2001.
[2] J. Wu and J. M. Rehg, “Centrist: A visual descriptor for scene
categorization,” IEEE TPAMI, vol. 33, no. 8, pp. 1489–1501, 2011.
[3] E. Fazl-Ersi and J. K. Tsotsos, “Histogram of oriented uniform patterns
for robust place recognition and categorization,” IJRR, vol. 31, no. 4,
pp. 468–483, 2012.
[4] A. Pronobis, B. Caputo, P. Jensfelt, and H. I. Christensen, “A discrim-
inative approach to robust visual place recognition,” in IROS, 2006.
[5] A. Pronobis and B. Caputo, “COLD: COsy Localization Database,”
IJRR, vol. 28, no. 5, pp. 588–594, May 2009.
[6] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:
Spatial pyramid matching for recognizing natural scene categories,”
in CVPR, 2006.
[7] P. Ursˇicˇ, A. Leonardis, M. Kristan, et al., “Part-based room catego-
rization for household service robots,” in ICRA, 2016.
[8] N. Su¨nderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford,
“On the performance of convnet features for place recognition,” in
IROS, 2015.
[9] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera, “Fusion
and binarization of cnn features for robust topological localization
across seasons,” in IROS, 2016.
[10] T. Kanji, “Self-localization from images with small overlap,” in IROS,
2016.
[11] P. Neubert and P. Protzel, “Local region detector+ cnn based landmarks
for practical place recognition in changing environments,” in ECMR,
2015.
[12] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless
pooling of deep convolutional activation features,” in ECCV, 2014, pp.
392–407.
[13] I. Kuzborskij, F. Maria Carlucci, and B. Caputo, “When naive bayes
nearest neighbors meet convolutional neural networks,” in CVPR,
2016.
[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in CVPR, 2015.
[15] M. Cummins and P. Newman, “Fab-map: Probabilistic localization
and mapping in the space of appearance,” IJRR, vol. 27, no. 6, pp.
647–665, 2008.
[16] T. Kanji, “Cross-season place recognition using nbnn scene descrip-
tor,” in IROS, 2015.
[17] S. Lowry, N. Su¨nderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke,
and M. J. Milford, “Visual place recognition: A survey,” IEEE Trans-
actions on Robotics, vol. 32, no. 1, pp. 1–19, 2016.
[18] G. Costante, T. A. Ciarfuglia, P. Valigi, and E. Ricci, “A transfer
learning approach for multi-cue semantic place recognition,” in IROS,
2013, pp. 2122–2129.
[19] T. A. Ciarfuglia, G. Costante, P. Valigi, and E. Ricci, “A discriminative
approach for appearance based loop closing,” in IROS, 2012.
[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in NIPS, 2012.
[21] A. Gout, Y. Lifchitz, T. Cottencin, Q. Groshens, S. Griffith, J. Fix, and
C. Pradalier, “Evaluation of off-the-shelf cnns for the representation
of natural scenes with large seasonal variations,” Ph.D. dissertation,
UMI 2958 GeorgiaTech-CNRS, 2017.
[22] O. Boiman, E. Shechtman, and M. Irani, “In defense of nearest-
neighbor based image classification,” in CVPR, 2008.
[23] T. Tommasi and B. Caputo, “Frustratingly easy nbnn domain adapta-
tion,” in ICCV, 2013.
[24] M. Fornoni and B. Caputo, “Scene recognition with naive bayes non-
linear learning,” in ICPR, 2014.
[25] M. Fornoni, B. Caputo, and F. Orabona, “Multiclass latent locally
linear support vector machines.” in ACML, 2013, pp. 229–244.
[26] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in ACMMM, 2014.
[27] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning
deep features for scene recognition using places database,” in NIPS,
2014.
[28] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva, “Places:
An image database for deep scene understanding,” arXiv preprint
1610.02055, 2016.
[29] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet: A large-scale hierarchical image database,” in CVPR, 2009.
[30] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” arXiv preprint 1409.1556, 2014.
[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in CVPR, 2015.
[32] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” arXiv preprint
1502.03167, 2015.
[33] L.-J. Li and L. Fei-Fei, “What, where and who? classifying events by
scene and object recognition,” in ICCV, 2007.
[34] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in CVPR,
2009.
[35] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal
of Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605, 2008.
[36] F. Rubio, J. Martı´nez-Go´mez, M. J. Flores, and J. M. Puerta, “Com-
parison between bayesian network classifiers and svms for semantic
localization,” Expert Systems with Applications, vol. 64, pp. 434–443,
2016.
[37] J. Luo, A. Pronobis, B. Caputo, and P. Jensfelt, “The KTH-IDOL2
Database,” KTH Royal Institute of Technology, CVAP/CAS, Stock-
holm, Sweden, Tech. Rep. CVAP304, Oct. 2006.
"
30,"Evaluating Two-Stream CNN for Video Classification
Hao Ye, Zuxuan Wu, Rui-Wei Zhao, Xi Wang, Yu-Gang Jiang∗, Xiangyang Xue
School of Computer Science, Shanghai Key Lab of Intelligent Information Processing,
Fudan University, Shanghai, China
{haoye10, zxwu,rwzhao14, xwang10, ygj, xyxue}@fudan.edu.cn
ABSTRACT
Videos contain very rich semantic information. Traditional
hand-crafted features are known to be inadequate in analyz-
ing complex video semantics. Inspired by the huge success
of the deep learning methods in analyzing image, audio and
text data, significant efforts are recently being devoted to
the design of deep nets for video analytics. Among the many
practical needs, classifying videos (or video clips) based on
their major semantic categories (e.g., “skiing”) is useful in
many applications. In this paper, we conduct an in-depth
study to investigate important implementation options that
may affect the performance of deep nets on video classifica-
tion. Our evaluations are conducted on top of a recent two-
stream convolutional neural network (CNN) pipeline, which
uses both static frames and motion optical flows, and has
demonstrated competitive performance against the state-of-
the-art methods. In order to gain insights and to arrive
at a practical guideline, many important options are stud-
ied, including network architectures, model fusion, learn-
ing parameters and the final prediction methods. Based
on the evaluations, very competitive results are attained on
two popular video classification benchmarks. We hope that
the discussions and conclusions from this work can help re-
searchers in related fields to quickly set up a good basis for
further investigations along this very promising direction.
Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Design Methodology; H.3.1
[Information Storage and Retrieval]: Content Analysis
and Indexing
General Terms
Algorithms, Measurement, Experimentation.
Keywords
Video Classification, Deep Learning, CNN, Evaluation.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
ICMR’15, June 23 - 26, 2015, Shanghai, China
c© 2015 ACM. ISBN 978-1-4503-3274-3/15/06...$15.00
DOI: http://dx.doi.org/10.1145/2671188.2749406.
1. INTRODUCTION
With the popularity of video recording devices and con-
tent sharing activities, there is a strong need for techniques
that can automatically analyze the huge scale of video data.
Video classification serves as a fundamental and essential
step in the process of analyzing the video contents. For ex-
ample, it would be extremely helpful if the massive consumer
videos on the Web could be automatically classified into pre-
defined categories. Learning semantics from the complicated
video contents is never an easy task, and methods based on
traditional hand-crafted features and prediction models are
known to be inadequate [1].
In recent years, deep learning based models have been
proved to be more competitive than the traditional methods
on solving complex learning problems in various domains.
For example, the deep neural network (DNN) has been suc-
cessfully used for acoustic modeling in the large vocabulary
speech recognition problems [2]. Moreover, the deep learning
based methods have been shown to be extremely powerful in
the image domain. In 2012, Krizhevsky et al. were the first
to use a completely end-to-end deep convolutional neural
network (CNN) model to win the famous ImageNet Large
Scale Visual Recognition Challenge (ILSVRC), outperform-
ing all the traditional methods by a large margin [13]. In the
most recent 2014 edition of the ILSVRC, Szegedy et al. de-
veloped an improved and deeper version of the CNN, which
further reduced the top-5 label error rate to just 6.7% over
one thousand categories [30]. In the text domain, deep mod-
els have also been successfully used for sentence parsing,
sentiment prediction and language translation problems [27,
18, 11, 29].
On video data, however, deep learning often demonstrated
worse results than the traditional techniques [8, 12]. This is
largely due to the difficulties in modeling the unique char-
acteristics of the videos. On one hand, the spatial-temporal
nature demands more complex network structures and maybe
also advanced learning methods. One the other hand, so far
there is very limited amount of training data with manual
annotations in the video domain, which limits the progress
of developing new methods as neural networks normally re-
quire extensive training. Very recently, Simonyan et al. pro-
posed two-stream CNN, an effective approach that trains
two CNNs using static frame and temporal motion sepa-
rately [24]. The temporal motion stream is converted to
successive optical flow images so that the conventional CNN
designed for images can be directly deployed.
Although promising results were observed in [24], we un-
derline that the performance of deep learning in video clas-
ar
X
iv
:1
50
4.
01
92
0v
1 
 [c
s.C
V]
  8
 A
pr
 20
15
sification is subject to many implementation options, and
there are no in-depth and systematic investigations on this
in the field. In this paper, we conduct extensive experiments
on two popular benchmarks to evaluate several important
options, including not only network structures and learn-
ing parameters, but also model fusion that combines results
from different networks and prediction strategies that map
network outputs to classification labels. The two-stream
CNN approach is adopted as the basic pipeline for the eval-
uations in this work. By evaluating the implementation op-
tions, we intend to answer the question of what network
settings and implementation options are likely to produce
good video classification results. As implementing a deep
learning based system for video classification is a very dif-
ficult task, we hope that the discussions and conclusions
from this work are helpful for researchers in this field and
can stimulate future studies.
The rest of this paper is organized as follows. Section 2
reviews related works. In Section 3, we briefly introduce the
two-stream CNN approach. Section 4 discusses the evalu-
ated implementation options and Section 5 reports and an-
alyzes experimental results. Finally, Section 6 summarizes
the findings in this work.
2. RELATEDWORKS
Extensive studies have been conducted on video classi-
fication in the multimedia and computer vision communi-
ties. State-of-the-art video classification systems are usu-
ally built on top of multiple discriminative feature repre-
sentations. To achieve better performance, various features
have been developed. For instance, Laptev et al. extended
the traditional SIFT features to obtain the Space-Time In-
terest Points (STIP) by finding representative tubes in 3D
space [15]. Wang et al. proposed the dense trajectory fea-
tures, which densely sample local patches from each frame
at different scales and then track them in a dense optical
flow field over time [32]. Besides the feature descriptors, one
can obtain further improvements by adopting advantageous
feature encoding strategies like the Fisher Vector [19] or uti-
lizing fusion techniques [26, 34, 33] to integrate information
from different features.
The aforementioned hand-crafted features like the dense
trajectories have demonstrated state-of-the-art performance
on many video classification tasks. However, these features
are still unsatisfying and the room for further improvements
may be limited. In contrast to the hand-crafted features,
there is a growing trend of learning features directly from
raw data using deep learning methods, among which the
CNN [16] has attracted wide attentions due to their great
success in image classification [13, 30], visual object detec-
tion [4], etc.
Compared with the extensive studies on using deep learn-
ing for image analysis, only a few works have exploited
this approach for video analysis. Ji et al. and Karparthy
et al. extended the CNN into temporal domain by stack-
ing static frames, upon which convolution can be performed
with space-temporal filters [8, 12]. However, the learned
representations from these methods produced worse results
than the state-of-the-art hand-crafted features like the dense
trajectories [32]. More recently, Simonyan et al. [24] achieved
very competitive performance by training two CNNs on spa-
tial (static frames) and temporal (optical flows) streams sep-
arately and then fusing the two networks.
Most CNN-based approaches rely on the neural networks
to perform the final class label prediction, normally using
a softmax layer [12, 24] or a linear layer [8]. Instead of
direct prediction by the network, Jain et al. conducted ac-
tion recognition using support vector machines (SVMs) with
features extracted from off-the-shelf CNN models [6]. Their
impressive results in the THUMOS action recognition chal-
lenge [9] indicate that CNN features are very powerful. In
addition, a few works attempted to apply the CNN repre-
sentations with Recurrent Neural Network (RNN) models
to capture the temporal information in videos and perform
classification within the same network. Donahue et al. lever-
aged the Long-Short Term Memory (LSTM) RNN model for
action recognition [3] and Venugopalan et al. proposed to
translate videos directly to sentences with the LSTM model
by transferring knowledge from image description tasks [31].
RNN shares the same motivation with the temporal pathway
in the two-stream framework [24].
In this paper, we provide an in-depth study on various
implementation choices of deep learning based video classi-
fication. On top of the two-stream CNN pipeline [24], we ex-
amine the performance of the spatial and temporal streams
separately and jointly with different network architectures
under various sets of parameters. In addition, we also ex-
amine the effect of different prediction options, including di-
rectly using a CNN with a softmax layer for end-to-end clas-
sification and adopting SVMs on the features learned from
the CNNs. The RNN models are not considered since they
have not been fully explored and only demonstrated limited
performance gain in the context of video classification. This
paper fills the gap in the existing works on deep learning
for video classification, where most people have focused on
designing a new classification pipeline or a deeper network
structure without systematically evaluating and discussing
the implementation options.
3. TWO-STREAM CNN
According to the findings in neuroscience, human visual
system processes what we see through two kinds of streams,
namely the ventral pathway and the dorsal pathway respec-
tively. The ventral pathway is responsible for processing
the spatial information, such as shape and color, while the
dorsal pathway is responsible for processing the motion in-
formation [14, 5]. Mimicking the human vision mechanism,
the video data, i.e., sequential image frames, can be natu-
rally decomposed into the spatial and temporal components
in a similar way. More specifically, image content including
scenes and objects belong to the spatial component. The
complementary temporal component contains motion infor-
mation across video frames. As an early attempt, Schindler
et al. extracted spatial and temporal features on still images
and optical flows by using the Gabor filters [22]. With the
recent success of the CNN, as briefly mentioned earlier, Si-
monyan et al. promoted this framework by the two-stream
CNN structure that models both the spatial and the tempo-
ral information [24]. Figure 1 shows the processing pipeline
of this approach. One CNN is applied to process the spa-
tial stream of the video data, and the other CNN is used to
handle the temporal stream.
The processing data flow of the spatial stream is shown
on top of Figure 1. This CNN has the same structure as
a general deep CNN for image classification [35, 13, 30].
It directly takes individual video frames as network inputs
Conv.
Layer
Pooling
Layer
Conv.
Layer
Pooling
Layer
Softmax
Layer
FC
Layer
FC
Layer
Visual Frame
Stacked Optical 
Flow Image
Input Video
Figure 1: The processing pipeline of the two-stream CNN [24], which has demonstrated competitive video classification
performance. “Conv.” indicates “Convolution” and “FC” is the abbreviation of “Fully Connected”. Our evaluations in this
work are conducted on top of this pipeline.
followed by several convolutional layers, pooling layers and
fully connected (FC) layers. Finally, after a softmax layer,
the network outputs in the range of [0, 1] are taken as the
predicted probabilities of the video classes. Multiple frames
are sampled from each input video. The network processes
one frame each time and the predictions on individual frames
are merged simply by average probability fusion.
For the temporal stream, in order to capture the mo-
tion information, the temporal CNN takes stacked optical
flows as input, rather than the frames in the spatial stream.
Specifically, dense optical flow can be seen as a set of dis-
placement vector fields between consecutive frames. For a
given pair of consecutive frames, the horizontal and verti-
cal components of the calculated displacement vector fields
between them can be used to generate two “optical flow im-
ages”, respectively. To further consider temporal informa-
tion, one can stack the optical flow images of each frame
at time t and its L subsequent frames into a stacked 2L-
channel optical flow image (in contrast to the traditional 3-
channel RGB images). The network architecture and train-
ing process of the temporal CNN are basically the same as
the spatial counterpart, except that the input images have
a different number of channels. There are multiple stacked
2L-channel optical flow images in a video. The way of fus-
ing predictions on these individual images is also the same
as that of the spatial channel.
We now have predictions from the two CNNs, based on
the spatial and temporal streams separately. The last step
is to combine the two streams to produce the final output.
For this, linear fusion with fixed weights was used in [24].
4. IMPLEMENTATION OPTIONS
In this section, we discuss several important implementa-
tion options that can affect the performance of deep learning
for video classification, including network architectures, fu-
sion strategies, learning parameters and prediction options.
4.1 Network Architectures
Network architecture plays a very important role in the
performance of a deep learning model. The current surge of
the CNNs in many tasks heavily relies on the use of supe-
rior network architectures, such as the AlexNet [13] and the
GoogLeNet [30]. Popular CNN architectures usually have
alternating convolutional layers and auxiliary layers (e.g.,
pooling layers), and are toped by a few fully connected (FC)
layers. Recent studies demonstrate that better recognition
accuracies can be achieved by deepening the CNN architec-
tures [25, 30], which means that deeper architectures can
lead to progressively more discriminative features at higher
layers. To evaluate the performance of different network ar-
chitectures, in this work, we adopt and evaluate a medium
network structure, CNN M [24], and a very recent deeper
network architecture, VGG 19 [25]. See Table 1 for the de-
tailed configurations of CNN M and VGG 19.
CNN_M Network.
CNN M basically follows the same spirit as the widely
adopted AlexNet [13]. It contains five convolutional layers
followed by three FC layers and the input image is fixed to
the size of 224×224. Compared with AlexNet [13], CNN M
possesses more convolutional filters. On the first convolu-
tional layer, the size and stride are both smaller (7× 7 and
2 respectively) than those in AlexNet, while the remaining
convolutional layers have the same filter size and stride. By
increasing the number of filters and reducing the filter size
and the stride step, CNN M can discover more subtle in-
formation from input images, and hence can obtain more
robust feature representations and better predictions. Note
that CNN M offers a 13.5% top-5 error on the ILSVRC-2012
validation set [24] (a famous image recognition benchmark),
which is generally considered as a good result.
VGG_19 Network.
VGG 19 not only further reduces the size of convolutional
filters and the stride, but more importantly, it also extends
the depth of the network. More precisely, VGG 19 consists
of nineteen layers, including sixteen convolutional layers and
three fully connected layers. In addition, the size of all the
convolutional filters decreases to 3 × 3 and the stride re-
duces to only 1 pixel, which enables the network to explore
finer-grained details from the feature maps. With this much
deeper architecture, VGG 19 possess strong capabilities of
learning more discriminative features and the high-level fi-
nal predictions. It can produce a 7.1% top-5 error rate on
the ILSVRC-2012 validation set [25].
4.2 Fusion Strategies
Fusing multiple clues is a standard technique in video
analysis, which can often lead to better performance. We
divide this part into model fusion and modality fusion.
CNN M VGG 19
C
o
n
v
o
lu
ti
o
n
a
l
L
ay
er
s
96 × 7 × 7 64 × 3 × 3
(stride: 2, padding: 0) 64 × 3 × 3
LRN ×2 pooling
×2 pooling 128 × 3 × 3
256 × 5 × 5 128 × 3 × 3
(stride: 2, padding: 1) ×2 pooling
LRN 256 × 3 × 3
×2 pooling 256 × 3 × 3
512 × 3 × 3 256 × 3 × 3
(stride: 1, padding: 1) 256 × 3 × 3
512 × 3 × 3 ×2 pooling
(stride: 1, padding: 1) 512 × 3 × 3
512 × 3 × 3 512 × 3 × 3
(stride: 1, padding: 1) 512 × 3 × 3
×2 pooling 512 × 3 × 3
×2 pooling
512 × 3 × 3
512 × 3 × 3
512 × 3 × 3
512 × 3 × 3
×2 pooling
F
C
L
ay
er
s
4,096 neurons 4,096 neurons
2,048 neurons 4,096 neurons
softmax softmax
Table 1: Configurations of two networks: CNN M and
VGG 19. The convolutional kernel parameter is denoted in
the form “# filters × kernel size x × kernel size y”. For both
networks, max-pooling with a sampling factor of 2 is used
(“×2 pooling”). In VGG 19, both stride and padding are set
to 1 for all the convolutional layers. The VGG 19 does not
contain the Local Response Normalization (LRN), which re-
quires considerable computation but contributes little to the
performance.
4.2.1 Model Fusion
Combining various deep learning models can usually pro-
duce better performance than using just a single model, be-
cause models using different architectures or trained with
different parameters may contain complementary informa-
tion. For instance, a model trained with larger convolu-
tional filters may focus more on large patterns, while one
with smaller filters may be more sensitive to finer-grained
details. Since both the two CNN architectures, CNN M and
VGG 19, can be trained with multiple parameters, there are
consequently several candidate models that can be exploited
in this experiment. We examine different combinations to
integrate information from these candidate models.
4.2.2 Modality Fusion
As aforementioned, videos are naturally multimodal, and
hence the integration of the spatial and the temporal streams
is very important. Simonyan et al. [24] adopted a simple
linear fusion method and fixed weights without explana-
tions. We will examine the effect of this fusion weight on two
very different datasets. Notice that another very important
modality, the audio channel, is not considered in this work
because we focus on examining options of training models
using only the visual data following the two-stream pipeline.
However, we strongly believe that better performance can be
achieved by further including the audio information, because
this has been observed in many recent works [10].
4.3 Learning Parameters
Although deep learning has achieved promising results on
many tasks, training and fine-tuning a good model normally
require significant efforts as there are several important pa-
rameters that need to be evaluated, such as learning rate,
dropout ratio and the number of training iterations. These
seemingly arbitrarily chosen parameters can influence the
performance significantly. For instance, a small learning rate
may demand much more iterations to converge, while a large
value may accelerate the convergence but can possibly result
in oscillation. In addition, a larger dropout ratio may lead
to a better model but could probably slow down the con-
vergence. There is no universal rule for parameter selection.
With the goal of providing some insights on parameter se-
lection specially for the problem of video classification, we
study different sets of parameters using the two aforemen-
tioned network architectures and two datasets.
In particular, the dropout ratio and the number of itera-
tions are jointly evaluated and discussed. For learning rate,
we set it to 10−2 initially, and then decreased to 10−3 after
100K iterations, then to 10−4 after 200K iterations. In the
fine-tuning case, the rate starts from 10−3 and decreases to
10−4 after 14K iterations, then to 10−5 after 20K iterations.
This setting is similar to [24], but we start from a smaller
rate of 10−3 instead of 10−2. Note that other choices on
learning rate are not evaluated as we find that the final per-
formance is less sensitive to this parameter as long as it is
set following the suggested rules in [23].
4.4 Prediction Options
While neural networks can act as end-to-end classifiers
by using a final softmax layer, traditional classifiers like the
SVMs can also be deployed on the features extracted by
the CNN, which are generally the outputs of the last sev-
eral fully connected layers. Recently, Razavian et al. [21]
adopted features extracted from a CNN model pre-trained
on ImageNet to perform classification with SVMs. They
demonstrated strong performance on image analysis tasks
like scene recognition, object detection, etc. In addition,
Jain et al. [6] leveraged the CNN features using SVMs for
action recognition in videos, and achieved superior perfor-
mance on the THUMOS action recognition benchmark [9].
The results suggest that the CNN features may be used in
combination with traditional classifiers for improved perfor-
mance, but these existing works were performed only on
images or the spatial frames. This paper investigates the
performance of features extracted from different layers of
both the spatial and the temporal CNNs using SVMs for
classification. Results are compared with that of the end-
to-end neural network based approach.
5. EXPERIMENTS
5.1 Datasets and Evaluation Criteria
UCF-101 [28]. The UCF-101 dataset is a widely adopted
benchmark for action recognition in videos, which consists
of 13,320 video clips (27 hours in total). There are 101 an-
notated classes that can be divided into five types: Human-
Object Interaction, Body-Motion Only, Human-Human In-
teraction, Playing Musical Instruments and Sports. We per-
form evaluation according to the popular 3 train/test splits
following [9]. Results are measured by classification accu-
racy on each split and mean accuracy over the 3 splits. For
some evaluations, we only report results on the first split
due to computation limitation.
Columbia Consumer Videos (CCV) [10]. The CCV
dataset contains 9,317 YouTube videos annotated accord-
ing to 20 classes, such as “wedding ceremony”, “birthday
party”, “skiing” and “playground”. We follow the protocol
defined in [10] to use a training set of 4,659 videos and a
test set of 4,658 videos. Results are measured by average
precision (AP) for each class and mean AP (MAP) across
all the classes [10]. Note that, different from the UCF101
actions, most classes in CCV are social events, sports, ob-
jects and scenes. In addition, the average duration of this
dataset is around 80 seconds, which is over ten times longer
than that of the UCF-101. We hope that using the two
datasets with different characteristics can help lead to more
generalizable conclusions.
For both datasets, we adopt the same data augmentation
strategies as [24].
5.2 Network Options
Using what network structure is the first decision we have
to make in the implementation of a deep learning based
video classification system. There are numerous options. In
this work, we evaluate and compare the two popular struc-
tures CNN M and VGG 19.
Results of the spatial stream are summarized in Table 2.
As can be seen, VGG 19 produces consistently better results
on both datasets, indicating that larger (deeper) networks
are generally better. This is consistent with the observa-
tions on the large scale image classification tasks. Results
of different dropout rates are listed in this table because
this can ofter a more complete understanding of the power
of the networks under different settings of learning parame-
ters. Detailed discussions on the effect of dropout rates will
be given later.
For the temporal stream, we also tried to use the VGG 19
network under a few parameter settings, but observed clearly
worse results than the CNN M. As the gap is clear, we did
not proceed to finish all the parameters to fully compare the
two networks. The key reason is that the temporal stream
has to be trained from scratch, which is different from the
spatial stream that can use fine-tuning to adjust the pre-
trained network based on millions of images [24]. In this
case, learning a smaller temporal network is more feasible
with limited training data. We expect that better results
can be achieved by VGG 19 for the temporal stream when
there is sufficient training data.
5.3 Model and Modality Fusion
Next, we discuss results by fusing models and modalities.
For the combination of different models, we use the spatial
stream and 10 network models (2 structures each trained
with 5 dropout rates). We tried all the possible modal com-
binations by averaging their prediction scores and identified
the top 3 results in order to learn which model is more reli-
able and what combinations are good. Results are shown in
Table 3. We see that VGG 19 models are more “popular” in
the top combinations on both datasets, confirming the fact
that fusing good models generally offers good results. How-
Models UCF-101 (split-1) CCV
CNN M dr1 71.58% 68.78%
CNN M dr3 68.65% 68.81%
CNN M dr5 68.25% 68.64%
CNN M dr7 68.15% 67.40%
CNN M dr9 60.85% 51.81%
VGG 19 dr1 75.87% 74.66%
VGG 19 dr3 79.59% 74.47%
VGG 19 dr5 80.41% 75.04%
VGG 19 dr7 76.66% 74.90%
VGG 19 dr9 76.39% 73.09%
Table 2: Spatial stream results of two network architectures
on UCF-101 and CCV under different dropout rates (“dr1”
indicates the 0.1 dropout rate). See texts for discussions.
Models
UCF-101 (split-1) CCV
1 2 3 1 2 3
CNN M dr1 X X X
CNN M dr3
CNN M dr5
CNN M dr7
CNN M dr9 X X X
VGG 19 dr1 X
VGG 19 dr3 X X X X
VGG 19 dr5 X X X X X
VGG 19 dr7 X X X
VGG 19 dr9 X X X
Perf. (%) 80.46 80.41 80.33 75.31 75.31 75.30
Table 3: Top-3 spatial stream model fusion results on both
datasets. The “X” sign indicates the used models in each of
the top combinations. A general observation is that fusing
good models like the VGG 19 based ones tend to generate
good results, but the improvement is fairly limited.
ever, comparing the model fusion results with the individual
model results in Table 2, it is clear that fusing models does
not improve results significantly. For instance, on the UCF-
101, the 2nd best results by fusing five models is actually the
same with just using the best single model (VGG 19 dr5).
Therefore, one conclusion is that fusing a strong network
(VGG 19) with a relatively weaker network (CNN M) does
not help much (if not becoming worse). Notice that aver-
age model prediction fusion is adopted here. Using dynamic
fusion weights may lead to better results, but the gain is
unlikely to be significant.
For modality fusion, we use the best spatial network con-
figurations to fuse with a temporal network trained using
the CNN M architecture. Results of different fusion weights
are plotted in Figure 2. Comparing the temporal stream
with the spatial counterpart, the latter produces better re-
sults on both datasets. The gap is larger on CCV as its
categories are easier to be recognized by viewing just one
or a few frames, e.g., the sports classes “basketball” and
“skiing”. Fusing the two modalities is effective, leading to
significant improvement on UCF-101 (best fusion: 86.7%;
spatial: 80.5%; temporal: 78.3%). The gain on CCV is lim-
ited as the result of the temporal stream is not good (best
fusion: 75.9%; spatial: 75.3%; temporal: 59.4%), which is
generally consistent with the results of the hand-crafted fea-
Spatial (VGG 19) Temporal (CNN M) Spatial-Temporal Fusion
Feature Early Fusion Late Fusion Early Fusion Late Fusion Early Fusion Late Fusion
UCF-101 (split-1)
FC1 75.84% 70.71% 78.22% 76.74% 87.55% 82.34%
FC2 72.75% 64.42% 77.85% 76.24% 85.94% 80.52%
FC1&FC2 75.73% 70.29% 78.30% 76.63% 87.71% 82.00%
CCV
FC1 70.75% 67.34% 58.04% 54.08% 73.25% 68.87%
FC2 70.45% 68.85% 55.86% 52.52% 72.76% 70.06%
FC1&FC2 71.15% 69.25% 58.79% 54.40% 73.27% 69.90%
Table 4: Prediction results of SVMs classifiers on the CNN features. FC1 (FC2) indicates the output feature of the first
(second) fully connected layer. “FC1&FC2” is the concatenation of the FC1 and FC2 features.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Weight of Spatial Stream
55
60
65
70
75
80
85
90
UCF-101 (Accuracy)
CCV (MAP)
Figure 2: Performance (%) of combining spatial and tem-
poral streams using linear fusion with different weights.
Temporal stream weight is set as “1−spatial weight”. Thus
the left-end points of the curves indicate the performance of
using the temporal stream alone, while the right-end is the
spatial stream performance.
tures on this dataset [10]1. As for the suitable fusion weight,
the results indicate that similar or higher temporal weight is
preferred for classes that can be better recognized by view-
ing a clip (not just a frame), even when the temporal stream
performs worse than the spatial stream.
5.4 Effect of Learning Parameters
We jointly evaluate the effect of two learning parameters:
the dropout ratio and the number of training iterations. We
first study the spatial stream. Figure 3 and Figure 4 visu-
alize the results on UCF-101 and CCV respectively. Notice
that the spatial networks are fine-tuned based on the mod-
els pre-trained on the Image-Net (only the last three FC
layers are fine-tuned), and therefore they start from a fairly
good initialization and become stable quickly after just 10-20
thousands of iterations. With the uniform settings on learn-
ing rate (cf. Section 4.3), the number of iterations required
to reach convergence is similar across different network ar-
chitectures and dropout ratios. One interesting observation
is that large dropout ratios (e.g., 0.9) are especially unsuited
for small networks like the CNN M. This is probably because
a small network can hardly learn anything if as high as 90%
of the information are dropped at each iteration, particu-
larly for the case of fine-tuning where only the last three
layers are adjusted.
1On CCV, it was reported that static frame features are
significantly better than spatial-temporal features.
Figure 5 plots the temporal stream results on both datasets,
using the CNN M architecture with various learning param-
eters. We observe that a large dropout ratio requires more
iterations to reach a high level of performance, which is easy
to understand. Different from the spatial stream observa-
tions on CNN M, a large dropout ratio can also lead to com-
parable results. This may be because the temporal stream
networks are trained from scratch, and, even using the same
architecture, training the entire framework is more complex
than fine-tuning (only tuning three FC layers). Further-
more, some researchers expressed the view that dropout can
be considered as a form of training set augmentation [23].
Complex networks may be more suitable to learn from highly
augmented input data.
5.5 Softmax vs. SVMs
As discussed in Section 4.3, neural networks can be used
as an end-to-end classifier or a feature extractor. In this sec-
tion, we discuss results of using SVMs on the CNN features,
and compare with softmax. We train linear SVMs using out-
puts from the first and the second FC layers. VGG 19 and
CNN M are adopted for the spatial and temporal stream
respectively. As each video has multiple spatial frames and
stacked optical flow images, there are two ways to train the
SVMs classifiers. One is early fusion by averaging features
from all the frames first before classifier training and test-
ing. The other is late fusion, which takes all the frames as
inputs separately and uses average prediction scores as the
final video-level score.
Table 4 summarizes the results of spatial, temporal, and
their fusion on both datasets. Compared with the softmax
based prediction, SVMs is only slightly better on the UCF-
101 under the spatial-temporal modality fusion setting using
the early frame fusion method. On CCV, the performance
is significantly lower than softmax. Comparing early frame
fusion with late fusion, early fusion is consistently good, in-
dicating that averaging frame features before classification
may help suppress noises that affect SVMs training. Inter-
estingly, the neural networks take individual frames as in-
puts like the late fusion based SVMs, but are quite robust.
5.6 Comparison with the State of the Arts
Finally, we compare our results with the state of the arts
on both datasets. For UCF-101, we report average accu-
racies over the three official train-test splits. As shown in
Table 5, our results are competitive on both datasets. The
performance on CCV is significantly better than the state
of the arts, all of which adopted traditional features like the
dense trajectories [32]. On UCF-101, our results are com-
parable to a very recent work [20], which uses an extensive
2000 6000 10000 14000 18000 22000
#Iteration
50
55
60
65
70
75
80
A
cc
ur
ac
y
(%
)
CNN M
2000 6000 10000 14000 18000 22000
#Iteration
50
55
60
65
70
75
80
A
cc
ur
ac
y
(%
)
VGG 19
0.1
0.3
0.5
0.7
0.9
Figure 3: Spatial stream results on UCF-101 (split-1), under different dropout ratios (from 0.1 to 0.9) and iteration numbers.
2000 6000 10000 14000 18000 22000
#Iteration
45
50
55
60
65
70
75
M
A
P
(%
)
CNN M
2000 6000 10000 14000 18000 22000
#Iteration
45
50
55
60
65
70
75
M
A
P
(%
)
VGG 19
0.1
0.3
0.5
0.7
0.9
Figure 4: Spatial stream results on CCV, under different dropout ratios and iteration numbers.
30 70 110 150 190 230 270
#Iteration (k)
20
30
40
50
60
70
80
A
cc
ur
ac
y
(%
)
UCF-101 (split-1)
0.1
0.3
0.5
0.7
0.9
30 50 70 90 110 130 150 170 210
#Iteration (k)
35
40
45
50
55
60
M
A
P
(%
)
CCV
0.1
0.3
0.5
0.7
0.9
Figure 5: Temporal stream results on both UCF-101 (split-1) and CCV, using CNN M with different dropout ratios and
iteration numbers.
fusion approach on top of state-of-the-art hand-crafted fea-
tures. Our results are also similar to that of [24]. Note
that the 88.0% reported in [24] was attained by using exter-
nal training data from another human action benchmark. If
only trained on UCF-101, the performance is lower.
6. SUMMARY AND DISCUSSION
Building a deep learning system for video classification is
not an easy task. We have evaluated several important im-
plementation options. The major findings are summarized
in the following. Note that knowing what works and what
does not work is equally important.
For network architectures, one observation is that deeper
networks like the VGG 19 are better, but a sufficient amount
of training data is required. This is fine for the spatial
stream, as the image annotations like the ImageNet can be
used to pre-train the network. For the temporal stream, as
we cannot use the image collections for model pre-training,
the results of very deep networks are not good. We envision
that the they would work well on the temporal stream once
we have a large amount of training data in the video domain.
UCF-101 CCV
Simonyan et al. [24] 88.0% Ye et al. [34] 64.0%
Peng et al. [20] 87.9% Jhuo et al. [7] 64.0%
Wang et al. [32] 85.9% Liu et al. [17] 68.2%
Karpathy et al. [12] 65.4% Wu et al. [33] 69.3%
Ours - Softmax 86.7% Ours - Softmax 75.9%
Ours - SVM 87.7% Ours - SVM 73.3%
Table 5: Comparison with the state-of-the-art results.
Results indicate that fusing multiple network models is
not very helpful, especially when combining a strong net-
work with a weak one. Fusing two networks with a simi-
lar performance level but different architectures might help,
but this is not verified based on our experiments. In addi-
tion, combining predictions from the spatial and the tem-
poral streams is useful. This is important for the classifica-
tion of long-term procedural actions, which benefits signifi-
cantly from the temporal clues. The linear weighted spatial-
temporal fusion method works well but is not ideal. This
aspect deserves more investigations.
We also observe that a moderate dropout ratio (0.5 for
spatial fine-tuning and 0.7 for temporal training) is consis-
tently good. Large dropout ratios like 0.9 may be unsuited
for small networks with less layers, particularly under the
fine-tuning setting that only adjusts the last several layers.
Finally, we find that softmax seems a better choice with
consistently good results.
Deep learning based approaches are already showing bet-
ter results than the traditional techniques for video classifi-
cation. We believe that the room for further improvement
is huge. First, the temporal stream results might be signifi-
cantly boosted if we could have sufficient labeled video data
to train a deeper network. Second, although the two-stream
framework is adopted in this work, future approaches do
not necessarily need to follow this pipeline. After all, it is
all about the way of modeling the temporal dimension of the
videos, which can be achieved by using alternative solutions
like the RNN or devising new network architectures.
Acknowledgments
The project is supported in part by the National 863 Program
(#2014AA015101), a grant from NSF China (#61201387), and
two grants from the STCSM (#13PJ1400400, #13511504503).
We thank Jie Shao and Xiaoteng Zhang for their help.
7. REFERENCES
[1] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. IEEE TPAMI, 2013.
[2] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-Dependent
Pre-Trained Deep Neural Networks for Large-Vocabulary
Speech Recognition. IEEE TASLP, 2012.
[3] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term
recurrent convolutional networks for visual recognition and
description. CoRR, 2014.
[4] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.
[5] M. A. Goodale and A. D. Milner. Separate Visual Pathways for
Perception and Action. Trends in neurosciences, 1992.
[6] M. Jain, J. van Gemert, and C. G. M. Snoek. University of
amsterdam at thumos challenge 2014. In ECCV THUMOS
Challenge 2014, 2014.
[7] I.-H. Jhuo, G. Ye, S. Gao, D. Liu, Y.-G. Jiang, D. T. Lee, and
S.-F. Chang. Discovering joint audio-visual codewords for video
event detection. Machine Vision and Applications, 2014.
[8] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. In ICML, 2010.
[9] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Action
recognition with a large number of classes.
http://crcv.ucf.edu/THUMOS14/, 2014.
[10] Y.-G. Jiang, G. Ye, S.-F. Chang, D. Ellis, and A. C. Loui.
Consumer video understanding: A benchmark database and an
evaluation of human and machine performance. In ACM
ICMR, 2011.
[11] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A
Convolutional Neural Network for Modelling Sentences. CoRR,
2014.
[12] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classification with
convolutional neural networks. In CVPR, 2014.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
NIPS, 2012.
[14] N. Kru¨ger, P. Janssen, S. Kalkan, M. Lappe, A. Leonardis,
J. Piater, A. J. Rodr´ıguez-Sa´nchez, and L. Wiskott. Deep
Hierarchies in the Primate Visual Cortex: What Can We Learn
for Computer Vision? IEEE TPAMI, 2013.
[15] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.
Learning realistic human actions from movies. In CVPR, 2008.
[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. In
Intelligent Signal Processing, 2001.
[17] D. Liu, K.-T. Lai, G. Ye, M.-S. Chen, and S.-F. Chang.
Sample-specific late fusion for visual category recognition. In
CVPR, 2013.
[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.
Distributed Representations of Words and Phrases and Their
Compositionality. In NIPS, 2013.
[19] D. Oneata, J. Verbeek, C. Schmid, et al. Action and event
recognition with fisher vectors on a compact feature set. In
ICCV, 2013.
[20] X. Peng, L. Wang, X. Wang, and Y. Qiao. Bag of visual words
and fusion methods for action recognition: Comprehensive
study and good practice. CoRR, 2014.
[21] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. CNN
features off-the-shelf: an astounding baseline for recognition.
CoRR, 2014.
[22] K. Schindler and L. Van Gool. Action snippets: How many
frames does human action recognition require? In CVPR, 2008.
[23] J. Schmidhuber. Deep learning in neural networks: An
overview. CoRR, 2014.
[24] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In NIPS, 2014.
[25] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR, 2014.
[26] C. G. Snoek, M. Worring, and A. W. Smeulders. Early versus
late fusion in semantic video analysis. In ACM Multimedia,
2005.
[27] R. Socher, C. C.-Y. Lin, A. Y. Ng, and C. D. Manning. Parsing
Natural Scenes and Natural Language with Recursive Neural
Networks. In ICML, 2011.
[28] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of
101 human actions classes from videos in the wild. CoRR, 2012.
[29] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence
Learning with Neural Networks. In NIPS, 2014.
[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going Deeper
with Convolutions. CoRR, 2014.
[31] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. J.
Mooney, and K. Saenko. Translating videos to natural language
using deep recurrent neural networks. CoRR, 2014.
[32] H. Wang and C. Schmid. Action recognition with improved
trajectories. In ICCV, 2013.
[33] Z. Wu, Y.-G. Jiang, J. Wang, J. Pu, and X. Xue. Exploring
inter-feature and inter-class relationships with deep neural
networks for video classification. In ACM Multimedia, 2014.
[34] G. Ye, D. Liu, I.-H. Jhuo, and S.-F. Chang. Robust late fusion
with rank minimization. In CVPR, 2012.
[35] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive
Deconvolutional Networks for Mid and High Level Feature
Learning. In ICCV, 2011.
"
31,"Cross-Domain Image Retrieval
with Attention Modeling
Xin Ji, Wei Wang
National University of Singapore
{jixin,wangwei}@comp.nus.edu.sg
Meihui Zhang∗
Singapore University of Technology
and Design
meihui_zhang@sutd.edu.sg
Yang Yang
University of Electronic Science and
Technology of China
dlyyang@gmail.com
ABSTRACT
With the proliferation of e-commerce websites and the ubiquitous-
ness of smart phones, cross-domain image retrieval using images
taken by smart phones as queries to search products on e-commerce
websites is emerging as a popular application. One challenge of
this task is to locate the attention of both the query and database
images. In particular, database images, e.g. of fashion products, on
e-commerce websites are typically displayed with other accessories,
and the images taken by users contain noisy background and large
variations in orientation and lighting. Consequently, their attention
is difficult to locate. In this paper, we exploit the rich tag informa-
tion available on the e-commerce websites to locate the attention
of database images. For query images, we use each candidate image
in the database as the context to locate the query attention. Novel
deep convolutional neural network architectures, namely TagYNet
and CtxYNet, are proposed to learn the attention weights and then
extract effective representations of the images. Experimental results
on public datasets confirm that our approaches have significant
improvement over the existing methods in terms of the retrieval
accuracy and efficiency.
CCS CONCEPTS
• Information systems→ Image search; •Computingmethod-
ologies→ Image representations;
KEYWORDS
Cross-Domain Image Retrieval; AttentionModeling; Deep Learning;
Fashion Product
1 INTRODUCTION
With the ubiquitousness of smart phones, it is convenient for people
to take photos anytime and anywhere. For example, we usually
take photos of beautiful sceneries in our outings, candid photos of
funny moments, and well-presented dishes in restaurants. These
photos can be used as queries to search visually similar images on
the Internet. With the wide acceptance of e-commerce, product
∗Meihui Zhang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’17, October 23–27, 2017, Mountain View, CA, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4906-2/17/10. . . $15.00
https://doi.org/10.1145/3123266.3123429
(a) User Image 1 (b) User Image 2 (c) Shop Image 
Tags:
Crew neck
No button
Short sleeves
Rectangle-shaped
Regular thickness
...
Figure 1: Example images of the same product.
image search becomes part-and-parcel of online shopping, where
users submit a photo taken via their cell phones to look for visually
similar products [11].
Product image search is a challenging problem as it involves
images from two heterogeneous domains, namely the user domain
and the shop domain. The user domain consists of the query images
taken by users, and the shop domain consists of the database images
taken by professional photographers for the e-commerce websites.
Images from the two domains exhibit different characteristics. For
instance, images from user domain (see Figure 1(a), 1(b)) typically
have large variations in orientation and lighting, whereas images
from shop domain are taken under good condition(see Figure 1(c)).
How to model the domain-specific and domain-invariant features
is a challenge. Traditional image retrieval approaches designed
for a single domain cannot model the domain-specific features
of different domains. DARN[11] and MSAE [28, 29] create differ-
ent deep learning branches for each domain separately to model
domain-specific features. However, they cannot capture the com-
mon features shared by both domains.
Another challenge comes from the difficulty of attention model-
ing for the user domain and the shop domain. Attention modeling
is to find the focus of images to extract effective image representa-
tions from the salient areas, i.e. spatial locations. For images from
user domain, however, they exhibit large variations and have noisy
background as shown by the example in Figure 1(a) and 1(b). For
shop images, it is common that the whole-body clothes along with
some accessories are displayed in an image as shown in Figure 1(c),
whereas the target is the upper clothes. Without external informa-
tion, it is difficult to find the attention of the query image (from the
user domain) and the database image (from the shop domain). Ex-
isting approaches, including DARN [11] and FashionNet [16], reply
on human annotated bounding boxes and landmarks for attention
modeling. However, it is costly to do such annotations for large
image databases from e-commerce websites.
In this paper, we propose a neural network architecture to extract
effective image representations for cross-domain product image
ar
X
iv
:1
70
9.
01
78
4v
1 
 [c
s.M
M
]  
6 S
ep
 20
17
search. Inspired by [35], which shows that the bottom layers of a
convolutional neural network (CNN) learn domain-invariant fea-
tures and the top layers learn domain-specific features, we propose
a ‘Y-shape’ network architecture that shares a set of convolution
layers at the bottom for both domains, and has separate branches
on the top for each domain. Each branch on the top contains an at-
tention modeling layer to learn the spatial attention weights of the
feature maps from the convolution layer. The attention modeling
layer of the shop branch exploits the tag information of shop im-
ages, which are widely available on e-commerce websites. Tags like
product category and attributes can help us to locate the attention
easily. For instance, in Figure 1(c), from the tag ‘Flare Sleeves’, we
know that the focus of this image is sleeves rather than trousers.
DARN and FashionNet also use tags to train their models. Different
from them, we use tags as inputs instead of prediction outputs. For
the attention modeling layer of the user branch, since user-provided
tags are not widely available and usually contain much noise, we
use the candidate shop image as the context to help locate the atten-
tion. For example, in Figure 1(b), the focus of the user image is not
clear (T-shirt or dress) until we compare it with the shop image. The
final image representation is generated by aggregating the features
at each spatial location according to the attention weight.
We adapt the triplet loss [20] as the training objective function.
A training triple consists of an anchor image from the user domain,
a positive image from the shop domain that has the same product
as the anchor image, and a negative image from the shop domain
that does not have the same product as the anchor image. We first
forward them through the shared layers to get a set of feature maps
for each image. Second, we forward the feature maps of the positive
(resp. negative) image and its tags into the shop branch. Third,
the generated representation of the positive (resp. negative) image
and the feature maps of the anchor image are fed together into
the user branch to learn the representation of the anchor image
w.r.t the positive (resp. negative) image. We extend the triplet loss
to accept four inputs, namely the representations for the positive
image, the negative image, and the anchor image w.r.t the positive
and negative image. The training procedure updates the network
parameters to move the similar pairs closer than dissimilar pairs.
During querying, our approach needs to extract the query repre-
sentation w.r.t each database image, which is expensive. To improve
the search efficiency, we follow [4] to do an initial search with the
query representation generated by aggregating its feature maps
equally across all spatial locations (i.e. all locations have the same
attention weight [1]). The returned top-K candidate images are
re-ranked using the query representation extracted by attention
modeling.
Our contributions include:
(1) A novel neural network architecture to extract effective fea-
tures for cross-domain product image retrieval.
(2) Two attention modeling approaches for the user and shop
domain respectively, i.e., exploiting tag information to help
locate the attention of the database images (denoted as TagY-
Net) and exploiting the candidate database images to locate
the attention of the query images (denoted as CtxYNet).
(3) Comprehensive experimental study, which confirms that
our approaches outperform existing solutions[11, 16] signif-
icantly.
2 RELATEDWORK
Our cross-domain product image retrieval is one type of Content
Based Image Retrieval (CBIR). CBIR has been studied for decades [7]
to search for visually similar images, and it consists of two major
steps. First, a feature vector is extracted to represent each image,
including the query image and images in the database. Second,
the similarity (e.g. cosine similarity) of the query image and the
database images are computed based on the feature vectors. Top-
K similar images are usually returned to the user. Many research
works have been proposed to improve the search performance from
the feature extraction and similarity measurement perspectives.
For instance, local descriptors such as SIFT [17], and feature ag-
gregation algorithm such as VLAD [12], have been proposed for
improving the feature representation. Metric learning [3] is a re-
search area which focuses on learning mapping functions to project
features into an embedding space for better similarity measurement.
With the resurgence of deep learning [13, 30], many approaches
based on deep learning models have been proposed towards learn-
ing semantic-rich features and similarity metric. We review some
related works as follows.
2.1 Feature Learning
Feature learning, in contrast to feature engineering, learns repre-
sentation of data (i.e. images) directly. It becomes popular due to its
superior performance over hand-crafted features, such as SIFT [17],
for many computer vision tasks such as image classification [13]
and retrieval[26].
2.1.1 Deep Convolutional Neural Network. Deep convolutional
neural networks (DCNN) [10, 13] consists of multiple convolutions,
pooling, and other layers. These layers are tuned over a large train-
ing dataset to extract semantic-rich features for the tasks of interest,
e.g. image classification. Rather than training a DCNN from scratch,
recent works [24] have shown that transfer learning, by introducing
knowledge from large-scale well-labeled dataset such as ImageNet
[8], can achieve competitive performance with less training time.
Apart from transferring knowledge from domain to domain [33, 34],
Ji et al. [26] shows that introducing knowledge from one task to
another, i.e., from image classification to image retrieval, also works
well. For example, DARN [11] and FashionNet [16] fine-tune the
DCNNs to extract features for attribute prediction and cross-domain
image retrieval. These work incorporate the attribute information
into the model to capture more semantics in feature representation.
In addition, FashionNet trains a subnetwork to predict the land-
marks (a.k.a. attention areas) by extracting features from local areas.
Different from these methods, we exploit the attributes of database
images, which are easier to collect than landmarks, to locate the spa-
tial attention areas of images directly and then extract features from
these areas to construct our image feature for retrieval. Moreover,
for the query image, we extract context-dependent attention when
calculating its similarity with database images (see Section 4.3).
2.1.2 Attention Modeling. Attention modeling differentiates in-
puts with different weights. It has been exploited in computer vision
to extract features from salient areas of an image [32]. Recently,
it is applied in machine translation models [2] to locate the at-
tention of source words for generating the next target word, and
in trajectory prediction to track the attention of dynamic objects
like pedestrians and bikers [25]. Typically, external information
is required to infer the attention weights. For example, the image
caption generation model [32] uses the previous word to infer the
attention weights for features from different locations of the image,
and then generates the next word using the feature from weighted
aggregation. There are different approaches to compute the atten-
tion weights by aligning the external information with the visual
feature of each location, e.g., via a multi-layer perceptron (MLP) [2],
inner-product [23], outer-product [9], etc. We explicitly exploit the
image attributes as the external information to locate the atten-
tion of images in the database, and exploit the database image as
the context to infer the attention of the query image. Attention
modeling ignores noisy background (occlusion), and thus extracts
discriminative features for retrieval.
2.2 Deep Metric Learning
Deep metric learning trains the neural networks to project the
images into metric space for similarity measurement. Contrastive
loss based on Siamese network [6] is the most widely used pairwise
loss for metric learning. Wang et al. [31] optimizes the contrastive
loss by adding a constraint on the penalty to maintain robustness
on noise positive pairs. Unlike pairwise loss that considers the
absolute distance of pairs, triplet loss [20] computes the relative
distance between a positive pair and a negative pair of the same
anchor. Yuan et al. [36] changes the original triplet loss into a
cascaded way in order to mine hard examples. Song et al. [18]
designs a mini-batch triplet loss that considered all possible triplet
correlations inside a mini-batch. Liu et al. [15] proposes a cluster-
level triplet loss that considered the correlation of cluster center,
positive samples and the nearest negative sample. Query adaptive
matching proposed in [4] computes the similarity based on the
aggregated local features of each candidate image. The aggregation
weights are determined by solving an optimization problem using
the query as the context. However, it incurs extra computation time.
In this paper, we generate different representations for the query
image to compute its similarity with different database images. In
other words, the query representation is adaptive to the compared
database image.
3 PROBLEM STATEMENT
In this paper, we study the problem of cross-domain product image
retrieval where the query image comes from user domain Iu (i.e.
taken by users) and the database images are from shop domain Is (i.e.
from the e-commerce websites). A database image is matched with
the query image if they contain the same product. Our task is to train
a model to extract effective image representations where matched
pairs have smaller distance. We adopt CNN in our model for image
feature extraction as CNN has shown outstanding performance in
learning features for variant computer vision tasks [5]. Denote the
feature vector at location l of the convolution layer as xl ∈ RC ,
Table 1: Notations.
Iu user domain images taken by users
Is shop domain images from e-commerce websites
o ∈ Iu anchor image or query image
p ∈ Is positive image (matched image to o)
q ∈ Is negative image (unmatched image to o)
op (resp. oq ) feature of o using the attention w.r.t to p (resp. q)
xl ∈ RC feature at location l , x ∈ {o,p,q}
xt ∈ {0, 1}T tags vector, x ∈ {p,q}
T total number of tags
al ∈ R attention weight at location l
W ∈ RT ∗C tag embedding matrix
v ∈ RC weight vector in Equation 4
U ∈ RL∗C weight matrix in Equation 4
L total number of locations, i.e. height*width
дu (), дs () feature alignment function in Equation 1 and 4
whereC is the number of channels, i.e. the number of feature maps.
To aggregate the features across all locations effectively, we train
an attention modeling subnetwork for each domain to learn the
spatial weights (i.e. attention). Once we get the weight of each
location, denoted as al ∈ R, the final image feature is aggregated
as
∑
l al ∗ xl .
Our training dataset consists of user images and shop images,
where each image has a product ID. The product ID is used for
constructing triples for metric learning, where each triple consists
of an anchor image o ∈ Iu , a positive image p ∈ Is which contains
the same product as the anchor image, and a negative image q ∈ Is
which does not contain the same product as the anchor image.
In addition, each shop image x has a set of tags (e.g. category or
attributes) represented using a binary vector xt ∈ {0, 1}T 1, where
T is the total number of tags. These tags are exploited for attention
modeling (see Section 4.2).
The notations used in this paper is summarized in Table 1. Scalar
values are in normal fonts, whereas bold fonts are for vectors and
matrices. We refer to o,p,q,x as images and o, p, q, x as image
feature maps (i.e. output of convolution layers) or vectors. We reuse
the notation o, p, q, x as the input and output of a subnetwork.
4 OUR APPROACH
In this section, we introduce our approach for inferring the spatial
attention of both the query and database images to extract effective
features for product image search. The network architecture, atten-
tion models, training and retrieval algorithms shall be discussed.
4.1 Overview
Our network architecture is illustrated in Figure 2, which is like
the character ’Y’ in landscape. It consists of convolution layers
(subnetwork I, II and III) for image feature extraction, attention
layers for spatial attention modeling (subnetwork IV and V), and a
triplet ranking loss layer for metric learning.
During training, each training triple < o,p,q > is forwarded
through a shared subnetwork I and then passed to two separate
1with the value 1 indicting the presence of the tag and 0 otherwise
……
…
𝑎𝑎
𝑥𝑥𝑡𝑡, 𝑥𝑥 ∈ {𝑝𝑝, 𝑞𝑞}
𝑥𝑥𝑙𝑙 𝑔𝑔𝑠𝑠(, )
𝑔𝑔𝑢𝑢(, )𝑜𝑜𝑙𝑙
𝑝𝑝 𝑞𝑞
𝑜𝑜𝑝𝑝
𝑜𝑜𝑞𝑞
𝑝𝑝
𝑞𝑞
𝑜𝑜 𝑎𝑎
Subnetwork I Subnetwork II
Subnetwork III
Subnetwork IV
Subnetwork V
𝑓𝑓𝑉𝑉()
𝑓𝑓𝐼𝐼𝑉𝑉()
𝑜𝑜
𝑝𝑝
𝑞𝑞
Figure 2: Illustration of our neural network architecture.
subnetwork II and III. This design is to learn domain-invariant
features by I and learn domain-specific features by subnetwork
II and III. The shared subnetwork I also saves memory compared
with the dual network architecture in [11] which needs to store
the parameters for both branches. We denote the subnetwork I II
III together as YNet. Subnetwork IV infers the spatial attention
weights by exploiting the tags (e.g. attributes) of shop products.
With the attention weights, it aggregates the features over all lo-
cations to get the feature vector of p (resp. q). The subnetwork I II
III IV is denoted as TagYNet. By using p (resp. q) as the context,
subnetwork V computes the attention of the anchor image o w.r.t
the p (resp. q). The whole network is denoted as CtxYNet. The
image representations are fed into an adapted triplet-loss function.
After training, we extract the representation of each database
image via subnetwork I, II and IV. When a query arrives, we ex-
tract a simple representation of the query by forwarding the image
through subnetwork I and III, and then averaging features at all
spatial locations. This simple representation is used to conduct an
initial retrieval. For each candidate image in the top-K (K=256 in
our experiments) list, we use it to infer the query’s attention via
subnetwork V and then compute the similarity. These images are
finally re-ranked based on the new similarity score.
Specifications of the CNN layers in subnetwork I, II and III will
be given in Section 5. We next introduce the subnetwork IV, V for
attention modeling and the adapted triplet loss function.
4.2 Attention Modeling for Shop Images
Shop images are usually taken in good condition by professional
photographers. Hence, they have better quality than user images.
However, some images, especially for fashion products, are typically
presentedwith accessories or other items as shown in Figure 1(c). To
get the attention, we exploit the tags (or attributes) associated with
the shop image, which can be collected easily from the shopping
websites. Take the shop image in Figure 1(c) as an example, the tags
‘Crew neck’, ‘Short sleeves’ and ‘Rectangle-shaped’ are useful for
locating the attention of the image.
дs (xl , xt ) = xl · (Wt · xt ) (1)
al =
eдs (xl ,xt )∑L
k e
дs (xk ,xt ) (2)
fIV (x, a) =
L∑
l
al ∗ xl (3)
Given a shop image associated with the tag vector xt ∈ {0, 1}T ,
we forward the image’s raw feature (i.e. the RGB feature) through
subnetwork I and II to get the feature maps x. Subnetwork IV
computes the attention of each spatial location through Equation 1-
2. The intuition is to find a projection matrix that embeds (viaWt ∈
RT ∗C ) the tags into a common latent space as the image features. If
the image feature at one position matches (i.e. aligned well with) the
embedded tag feature, we assign a larger attention weight for that
position to let it contribute more in the final image representation.
We adopt inner-product (Equation 1) as the alignment function
and the Softmax function (Equation 2) to generate the weights.
The output of the subnetwork is a C-dimensional feature vector
aggregated across all spatial locations according to Equation 3.
4.3 Attention Modeling for User Images
For user images, i.e. the query images, they are usually taken occa-
sionally using smart phones. Consequently, the focus of the image
may not be clear, e.g. when there are multiple (background) objects
in the image. In addition, for some kinds of products like clothes,
the images are subject to deformations. It is thus important to locate
the attention of the image for extracting effective features. However,
the attention is not easy to get without any context information
(the tags of the query images are usually not available).
Given a query image and a shop image, we infer the spatial
attention of the query image using the shop image as the context.
Denote x ∈ RC as the output of IV, which could be either a positive
image p or a negative image. Following Equation 5-6, we infer the
attention of the anchor image o in subnetwork V.
дu (ol , x) = v · ol + Ul · · x (4)
al =
eдu (ol ,x)∑L
k e
дu (ok ,x) (5)
fV (o, a) =
L∑
l
al ∗ ol (6)
where Equation 4 is a linear alignment function, which has better
performance than the inner-product alignment function for our
experiment. v ∈ RC , U ∈ RL∗C are weights to learn.
4.4 Loss Function
We adapt the triplet loss function as the training objective, which
is shown in Equation 7.
L(o,p,q) = max(0,d(op , p) − d(oq , q) + α) (7)
where d(·, ·) measures the distance of two images based on their
final representation. Euclidean distance is used for d(·, ·). Following
[20], in order to make the training converge stable, we normalize
the output of subnetwork IV and V via L2 norm before feeding
them into the loss function. The margin value α is tuned on a vali-
dation dataset (0.5 for our experiments). Different to the existing
approaches [11, 16] that use the same representation for o in Equa-
tion 7, i.e.d(o, p) andd(o, q). We have different representations foro,
i.e. op and oq ford(o, p) andd(o, q) respectively. This is because two
sets of attention weights are generated against p and q respectively.
The loss function penalizes the triples if d(op , p) + α > d(oq , q) by
updating the model parameters to make matched images close and
unmatched images far away in the embedded Euclidean space.
5 EXPERIMENTS
In this section, we conduct experimental study by comparing our
proposed approaches in Section 4 with baseline methods in terms
of search effectiveness and efficiency on two real-life datasets.
5.1 Dataset
In our experimental study, we use the DARN dataset [11] and Deep-
Fashion dataset [16].
The DARN dataset is collected for street-to-shop retrieval, i.e.
matching street images taken by users with professional photos pro-
vided by online shopping sites. After removing corrupted images,
we get a subset of 62,812 street images and 238,499 shop images of
13598 distinct products distributed over 20 categories. Each street
image has a matched shop image. This dataset also provides seman-
tic attributes for the products. Detailed information of the attributes
is available in [11]. We use 7 types of attributes except the color
attribute since we observe that same product may be displayed
with different colors in this dataset. We partition the dataset into
three subsets for training, validation and test, with no overlap of
products (see Table 2).
The DeepFashion dataset includes over 800,000 images with
various labeled information in terms of categories, clothes attributes,
landmarks, and image correspondences for cross-domain/in-shop
scenario. We do not explore the landmark information since it is
beyond the scope of this paper. Additionally, we only use a subset
Table 2: Dataset Partition
Dataset DARN DeepFashion
Distinct Training Products 10,979 15,494
Training Street Photos 50,528 84,161
Training Shop Photos 32,194 21,377
Distinct Validation Products 9,635 1,948
Validation Street Photos 6,318 1,948
Validation Shop Photos 23,828 2,694
Distinct Test Products 9,636 1,945
Test Street Photos 5,966 1,945
Test Shop Photos 23,773 2,637
of images from street2shop set, i.e. 19,387 distinct upper clothes of
over 130,000 images. We select 11 types of tags that are related to
the upper clothes of the product images. Similar partition method is
applied for this dataset (see Table 2). Different from DARN dataset,
DeepFashion dataset has more street images (130,000+) than shop
images (21,377).
5.2 Approaches
5.2.1 Baseline Approaches.
(1) DARN[11]2 has two branches of NIN (Network in Network)
networks, one for the street domain and one for the shop
domain. It is different to YNet which shares the same bottom
layers for both domains. On top of the NIN networks, there
are several fully connected layers for category and attribute
prediction. The training loss is a weighted combination of
the prediction losses and the triplet loss. The triplet loss
is calculated using a long feature vector by concatenating
the features from convolution layers and fully connected
layers. We adjust the shapes of lower convolution layers to
be the same as the original NIN model [14] in order to use
the pretrained parameters of NIN over ImageNet.
(2) FashionNet[16] FashionNet shares the convolution layers
for both domains. It has a landmark prediction subnetwork
whose results are used to subsample the feature maps of the
last convolution layer. The top branches are for different
tasks (including tag prediction and landmark prediction) in-
stead of for different domains as in YNet. In other words,
all images are passed through the same set of layers in Fa-
sionNet, whereas street and shop images are passed through
different top layers in YNet. Due to the memory limit, we
replace the VGG-16 model [22] used in the DeepFashion
paper with VGG-CNN-S [5] 3. We also remove the landmark
prediction subnetwork since exploring the effect of landmark
is out of the scope of this paper.
(3) TripletNIN and TripletVGG By removing the category
and attribute prediction subnetworks of DARN and Fashion-
Net, we get two networks whose loss function only includes
triplet loss.
2We use the notation DARN for both the dataset and the method.
3https://gist.github.com/ksimonyan/fd8800eeb36e276cd6f9
(4) NIN and VGG-CNN-S We use the NIN and VGG-CNN-S
trained on ImageNet to extract the feature for both user
images and shop images.
5.2.2 Network Configuration of Our Approaches.
(1) TagYNIN and CtxYNIN based on NIN. TagYNIN uses the
first 4 convolution layer blocks of NIN as subnetwork I. The
5-th convolution layer block is used for subnetwork II and III.
Subnetwork IV uses inner-product as the attention alignment
function. The output of subnetwork II and IV are fed into
the triplet loss. CtxYNIN adds the subnetwork V on top of
TagYNIN.
(2) TagYVGG andCtxYVGG based on VGG-CNN-S. TagYVGG
is similar to TagYNIN except the NIN layers are replaced with
layers from VGG-CNN-S. CtxYVGG is similar to CtxYNIN
except the NIN layers are replaced with layers from VGG-
CNN-S.
5.2.3 Implementation Details. We use backpropagation for pa-
rameter gradient calculation and mini-batch Stochastic Gradient
Descent (SGD) for parameter value updating. The batch size is 32
and momentum is 0.9. Learning rate is set to 0.01 at the initial state
and decays by 0.1 for every 30 epochs. Margin in the loss function is
set to 0.5 for the baseline experiments. We set the margin to 0.3 for
YNet and TagYNet, and 0.5 for CtxYNet. We train our networks in
the order of YNet, TagYNet, and CtxYNet by using the parameters
trained from the previous network to initialize the subsequent net-
work. The pretrained parameters over ImageNet (e.g. using NIN) is
used to initialize the corresponding YNet (e.g. YNIN). For YNet and
TagYNet, we use the standard triplet loss function; For CtxYNet, we
use the adapted triplet loss function (Equation 7). When training
TagYNet and CtxYNet, we freeze the parameters in subnetwork
I. Considering that bounding boxes and landmarks are costly to
annotate for large image databases in real applications, we do not
use these information for training. In contrast, the attribute and
category information is typically available on e-commerce websites.
Therefore, we use them in training DARN, FashionNet and our
approaches.
5.3 Evaluation Metric
Following [11, 16], we evaluate the retrieval performance by top-K
precision, which is defined as follows:
P@K =
∑
q∈Q hit(q,K)
|Q | (8)
where Q is the total number of queries; hit(q,K) = 1 if at least
one image of the same product as the query image q appears in
the returned top-K ranking list; otherwise hit(q,K) = 0. For most
queries, there is only one matched database image in both the
DARN and DeepFashion datasets.
5.4 Comparison on DARN Dataset
Figure 3 shows the top-K precision of the baseline approaches and
our approaches on DARN dataset. We can see that the pretrained
NIN performs worst since the task and dataset are different to our
product image retrieval application. TripletNIN is better than NIN
as it is fine-tuned with metric learning over the DARN dataset to
0 10 20 30 40 50 60 70 80 90 100
K
0.0
0.1
0.2
0.3
0.4
0.5
P
@
K
NIN (P@20 = 0.047)
TripletNIN (P@20 = 0.079)
DARN (P@20 = 0.112)
FashionNet (P@20 = 0.208)
YNIN (P@20 = 0.153)
TagYNIN (P@20 = 0.222)
CtxYNIN (P@20 = 0.242)
Figure 3: Comparison of P@K on DARN dataset.
distinguish the inner-category images (e.g. different T-shirts). How-
ever, it is not as good as DARN which considers the tag information
during training through attribute and category prediction. Mean-
while, we can observe that our TagNIN and CtxNIN outperform a
lot than YNIN which does not incorporate tag information. These
prove the effectiveness of exploring tag information for image re-
trieval.
We further compare DARN, FashionNet and our attention mod-
eling based approaches. From Figure 3, we observe that both of
our attention modeling approaches significantly outperform DARN
(over 10% improvement on top-20 accuracy) and FashionNet (1.4%
and 3.4% improvement on top-20 accuracy). DARN and FashionNet
only use tags during training, which serves as a regularization on
the extracted feature (require the feature to capture tag informa-
tion). Our TagYNet uses tags in both training and querying phases
for the shop images, which helps to locate the attention of the shop
images especially when noisy background or multiple products
occur in one image. Consequently, it captures more discriminative
features. In addition, we do not need to tune the weights of predic-
tion loss and triplet loss as in DARN and FashionNet. In terms of
network architecture, our YNet architecture involves fewer param-
eters compared with the dual network architecture of DARN, and
thus is more robust to overfitting.
CtxYNet performs slightly better than TagYNet by reranking
the top-256 list retrieved from TagYNet.It indicates that attention
modeling of the query images indeed helps to obtain better feature
representations.
5.5 Comparison on DeepFashion Dataset
Figure 4 shows the detailed top-k precision of baseline approaches
and our approaches on DeepFashion dataset. We notice that the
retrieval performance of all approaches on this dataset is much
better than those on DARN’s dataset, mainly due to the smaller size
of the database.
We observe that our attention modeling approaches again per-
form best over the other baselines approaches. Compared with
DARN, FashionNet achieves significantly better performance than
DARN, which is consistent with results shown in the paper [16].
0 10 20 30 40 50 60 70 80 90 100
K
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
P
@
K
VGG-CNN-S (P@20 = 0.123)
TripletVGG (P@20 = 0.333)
FashionNet (P@20 = 0.430)
DARN (P@20 = 0.239)
YVGG (P@20 = 0.396)
TagYVGG (P@20 = 0.450)
CtxYVGG (P@20 = 0.479)
Figure 4: Comparison of P@K on DeepFashion dataset.
DARN DARN-PCA YNet FashionNet YNet
Method
0
1
2
3
4
5
6
7
8
Ti
m
e
pe
r1
00
0
Q
ue
ry
DARN
DeepFashion
Figure 5: Querying efficiency with different feature dimen-
sions and different size of databases.
However, our proposed network performs even better than Fash-
ionNet. Our tag-based attention mechanism (TagYNet) and context-
based attention mechanism (CtxYnet) gain 2% and 4% improvement
on top-20 precision over FashionNet. There are still some noisy
tags (not visually relevant to the image content, e.g. ’thickness of
clothes’) in DeepFashion dataset, therefore, we expect to see better
performance if these tags/attributes are filtered out.
5.6 Query Efficiency
Our retrieval system runs on a server with Intel i7-4930K CPU
and three GTX Titan X GPU cards. We develop the system using
SINGA [19, 27] — a deep learning library.
We measure the query efficiency on the two datasets as shown
in Figure 5. The feature extracted by our approaches is aggregated
from the features across spatial locations of the convolution layer.
Therefore, the feature dimension is the number of the feature maps
in the last convolution layer, which is 1000 and 512 for NIN-based
and VGG-based networks respectively. DARN and FashionNet[11,
15] concatenate the local features from convolution layers and
global features from fully connected layers. Consequently, their
feature dimension is larger than ours. Even after applying PCA,
the feature dimension is still up to 8196-D, which is 8x and 16x
2. max attention 3. min attention1. input 2. max attention 3. min attention1. input
(a)
2. max attention 3. min attention1. input 2. max attention 3. min attention1. input
(b)
Figure 6: Images and their attention maps.
larger than our NIN-based and VGG-based features, respectively.
As a result, our approaches achieve better efficiency as shown in
Figure 5.
5.7 Attention Visualization
For better understanding of the attention modeling mechanism, we
visualize the attention maps of two sample images from DARN in
Figure 6. The ‘max attention’ (resp. min attention) map is generated
following [21]: first, setting the gradients of the final convolution
layer as 0 except those for the location (denoted as l ) with the max-
imum (resp. minimum) attention weight computed from TagNIN,
whose gradients are set to 1; second, back-propagating the gradients
to get the gradients of the input data, which reflect the activations
of location l and are used to plot the attention map. From Figure 6a,
we can see that the activations of the maximum weighted position
matches the attention of the input image better than that of the
minimum weighted position. In addition, the noisy background is
filtered in the attention maps, where the activations mainly cover
the clothes. In other words, the attention weights have semantic
explanations.
5.8 Example Queries and Results
We analyze some sample queries and corresponding results for
better understanding the task and feature extraction models. The
DARN dataset and TagYNet are used for this experiment. Four
queries with matched results in the top-5 list are shown In Figure7.
First, we can see that the result images (i.e. shop images in column
2-6) have much better quality than the query images (i.e. user
image in the first column). Therefore, we need separate branches
for extracting the features from the two domains. Second, some
shop images, e.g. the matched result of the third query, also have
noisy background. It is necessary to incorporate extra information
like tags to locate the image attention. Third, the query image and
the matched result may look quite different from the global view,
Figure 7: Example queries (the first column) with matched results (with border) in top-5 list.
Figure 8: Example queries (the first column) without matched results in top-5 list.
as shown in the second row. In spite of such difference, our model
is able to focus on local pattens (e.g. the collar and pocket) to find
matched results.
We also sample some queries for which our model fails to find
matched images in the top-5 list, as shown in Figure 8. First, we
can see that these query images are either cropped (the first query)
or taken under insufficient lighting (the second query). Second, the
evaluation criteria is very strict. As shown in the third row, some
images are not considered as matched results although they are
visually very similar to the query images. In fact, only images of the
exact same product as the query are considered matched results.
6 CONCLUSION
To tackle the problem of cross-domain product image search, we
have presented a novel neural network architecture which shares
bottom convolutional layers to learn domain-invariant features
and separates top convolutional layers to learn domain-specific
features. Different from other approaches, we introduce a tag-based
attention modeling mechanism denoted as TagYNet by exploiting
product tag information, and a context-based attention modeling
mechanism denoted as CtxYNet, using the candidate image as the
context. Experiments on two public datasets confirm the efficiency
and effectiveness of the features extracted using our attention based
approaches.
7 ACKNOWLEDGMENT
This work is supported by the National Research Foundation, Prime
Minister’s Office, Singapore, under its Competitive Research Pro-
gramme (CRP Award No. NRF-CRP8-2011-08), and FY2017 SUG
Grant, National University of Singapore.
REFERENCES
[1] Artem Babenko and Victor Lempitsky. 2015. Aggregating local deep features for
image retrieval. In 2015 IEEE International Conference on Computer Vision (ICCV).
1269–1277.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-
chine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).
[3] Aurélien Bellet, Amaury Habrard, and Marc Sebban. 2013. A survey on metric
learning for feature vectors and structured data. arXiv preprint arXiv:1306.6709
(2013).
[4] Jiewei Cao, Lingqiao Liu, Peng Wang, Zi Huang, Chunhua Shen, and Heng Tao
Shen. 2016. Where to Focus: Query Adaptive Matching for Instance Retrieval
Using Convolutional Feature Maps. arXiv preprint arXiv:1606.06811 (2016).
[5] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014.
Return of the devil in the details: Delving deep into convolutional nets. arXiv
preprint arXiv:1405.3531 (2014).
[6] Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric
discriminatively, with application to face verification. In 2005 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 1.
539–546.
[7] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image retrieval:
Ideas, influences, and trends of the new age. ACM Computing Surveys (Csur) 40,
2 (2008), 5.
[8] Jia Deng,Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. In 2009 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). 248–255.
[9] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach. 2016. Multimodal compact bilinear pooling for visual question
answering and visual grounding. arXiv preprint arXiv:1606.01847 (2016).
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 770–778.
[11] Junshi Huang, Rogerio S Feris, Qiang Chen, and Shuicheng Yan. 2015. Cross-
domain image retrieval with a dual attribute-aware ranking network. In 2015
IEEE International Conference on Computer Vision (ICCV). 1062–1070.
[12] Hervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. 2010. Ag-
gregating local descriptors into a compact image representation. In 2010 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR).
3304–3311.
[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[14] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv
preprint arXiv:1312.4400 (2013).
[15] Hongye Liu, Yonghong Tian, Yaowei Yang, Lu Pang, and Tiejun Huang. 2016.
Deep relative distance learning: Tell the difference between similar vehicles.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
2167–2175.
[16] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deep-
fashion: Powering robust clothes recognition and retrieval with rich annotations.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
1096–1104.
[17] David G Lowe. 1999. Object recognition from local scale-invariant features.
In Proceedings of the Seventh IEEE International Conference on Computer Vision
(ICCV), Vol. 2. 1150–1157.
[18] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. 2016. Deep
metric learning via lifted structured feature embedding. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 4004–4012.
[19] Beng Chin Ooi, Kian-Lee Tan, Sheng Wang, Wei Wang, Qingchao Cai, Gang
Chen, Jinyang Gao, Zhaojing Luo, Anthony KH Tung, Yuan Wang, et al. 2015.
SINGA: A distributed deep learning platform. In Proceedings of the 23rd ACM
international conference on Multimedia. ACM, 685–688.
[20] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A
unified embedding for face recognition and clustering. In 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 815–823.
[21] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside
convolutional networks: Visualising image classification models and saliency
maps. arXiv preprint arXiv:1312.6034 (2013).
[22] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[23] Sainbayar Sukhbaatar, JasonWeston, Rob Fergus, et al. 2015. End-to-end memory
networks. In Advances in neural information processing systems. 2440–2448.
[24] Jinhui Tang, Xiangbo Shu, Zechao Li, Guo-Jun Qi, and Jingdong Wang. 2016.
Generalized deep transfer networks for knowledge propagation in heterogeneous
domains. ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM) 12, 4s (2016), 68.
[25] Daksh Varshneya and G Srinivasaraghavan. 2017. Human Trajectory Prediction
using Spatially aware Deep Attention Models. arXiv preprint arXiv:1705.09436
(2017).
[26] Ji Wan, Dayong Wang, Steven Chu Hong Hoi, Pengcheng Wu, Jianke Zhu, Yong-
dong Zhang, and Jintao Li. 2014. Deep learning for content-based image retrieval:
A comprehensive study. In Proceedings of the 22nd ACM international conference
on Multimedia. ACM, 157–166.
[27] Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao,
Beng Chin Ooi, Kian-Lee Tan, Sheng Wang, and Meihui Zhang. 2016. Deep
learning at scale and at ease. ACM Transactions on Multimedia Computing, Com-
munications, and Applications (TOMM) 12, 4s (2016), 69.
[28] WeiWang, Beng Chin Ooi, Xiaoyan Yang, Dongxiang Zhang, and Yueting Zhuang.
2014. Effective multi-modal retrieval based on stacked auto-encoders. Proceedings
of the VLDB Endowment 7, 8 (2014), 649–660.
[29] WeiWang, Xiaoyan Yang, Beng Chin Ooi, Dongxiang Zhang, and Yueting Zhuang.
2016. Effective deep learning-based multi-modal retrieval. The VLDB Journal 25,
1 (2016), 79–101.
[30] WeiWang, Meihui Zhang, Gang Chen, HV Jagadish, Beng Chin Ooi, and Kian-Lee
Tan. 2016. Database Meets Deep Learning: Challenges and Opportunities. ACM
SIGMOD Record 45, 2 (2016), 17–22.
[31] Xi Wang, Zhenfeng Sun, Wenqiang Zhang, Yu Zhou, and Yu-Gang Jiang. 2016.
Matching user photos to online products with robust deep features. In Proceedings
of the 2016 ACM on International Conference on Multimedia Retrieval. ACM, 7–14.
[32] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural
image caption generation with visual attention. In International Conference on
Machine Learning. 2048–2057.
[33] Yang Yang, Yadan Luo, Weilun Chen, Fumin Shen, Jie Shao, and Heng Tao Shen.
2016. Zero-shot hashing via transferring supervised knowledge. In Proceedings
of the 2016 ACM on Multimedia Conference. ACM, 1286–1295.
[34] Yang Yang, Zheng-Jun Zha, Yue Gao, Xiaofeng Zhu, and Tat-Seng Chua. 2014.
Exploiting web images for semantic video indexing via robust sample-specific
loss. IEEE Transactions on Multimedia 16, 6 (2014), 1677–1689.
[35] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transfer-
able are features in deep neural networks?. In Advances in neural information
processing systems. 3320–3328.
[36] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. 2016. Hard-Aware Deeply Cascaded
Embedding. arXiv preprint arXiv:1611.05720 (2016).
"
32,"Object Detection in Videos with Tubelet Proposal Networks
Kai Kang1,2 Hongsheng Li2,? Tong Xiao1,2 Wanli Ouyang2,5 Junjie Yan3 Xihui Liu4 Xiaogang Wang2,?
1Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China
2The Chinese University of Hong Kong 3SenseTime Group Limited 4Tsinghua University 5The University of Sydney
{kkang,hsli,xiaotong,wlouyang,xgwang}@ee.cuhk.edu.hk
yanjunjie@sensetime.com xh-liu13@mails.tsinghua.edu.cn
Abstract
Object detection in videos has drawn increasing atten-
tion recently with the introduction of the large-scale Im-
ageNet VID dataset. Different from object detection in
static images, temporal information in videos is vital for
object detection. To fully utilize temporal information,
state-of-the-art methods [15, 14] are based on spatiotem-
poral tubelets, which are essentially sequences of associ-
ated bounding boxes across time. However, the existing
methods have major limitations in generating tubelets in
terms of quality and efficiency. Motion-based [14] meth-
ods are able to obtain dense tubelets efficiently, but the
lengths are generally only several frames, which is not
optimal for incorporating long-term temporal information.
Appearance-based [15] methods, usually involving generic
object tracking, could generate long tubelets, but are usu-
ally computationally expensive. In this work, we propose
a framework for object detection in videos, which consists
of a novel tubelet proposal network to efficiently generate
spatiotemporal proposals, and a Long Short-term Mem-
ory (LSTM) network that incorporates temporal informa-
tion from tubelet proposals for achieving high object de-
tection accuracy in videos. Experiments on the large-scale
ImageNet VID dataset demonstrate the effectiveness of the
proposed framework for object detection in videos.
1. Introduction
The performance of object detection has been signifi-
cantly improved recently with the emergence of deep neu-
ral networks. Novel neural network structures, such as
GoogLeNet [29], VGG [27] and ResNet [8], were pro-
posed to improve the learning capability on large-scale
computer vision datasets for various computer vision tasks,
such as object detection [5, 24, 23, 21], semantic segmen-
tation [20, 2, 16], tracking [31, 1, 33], scene understanding
[25, 26, 19], person search [18, 32], etc. State-of-the-art
?Corresponding authors
(a)
(b)
(c)
(d)
Figure 1. Proposals methods for video object detection. (a) orig-
inal frames. (b) static proposals have no temporal association,
which is hard to incorporate temporal information for proposal
classification. (c) bounding box regression methods would focus
on the dominant object, lose proposal diversity and may also cause
recall drop since all proposals tend to aggregate on the dominant
objects. (d) the ideal proposals should have temporal association
and have the same motion patterns with the objects while keeping
their diversity.
object detection frameworks for static images are based on
these networks and consist of three main stages [6]. Bound-
ing box proposals are first generated from the input image
based on how likely each location contains an object of in-
terest. The appearance features are then extracted from each
box proposal to classify them as one of the object classes.
Such bounding boxes and their associated class scores are
refined by post-processing techniques (e.g., Non-Maximal
Suppression) to obtain the final detection results. Multiple
frameworks, such as Fast R-CNN [5] and Faster R-CNN
[24], followed this research direction and eventually for-
mulated the object detection problem as training end-to-end
deep neural networks.
Although great success has been achieved in detecting
objects on static images, video object detection remains
a challenging problem. Several factors contribute to the
difficulty of this problem, which include the drastic ap-
pearance and scale changes of the same object over time,
object-to-object occlusions, motion blur, and the mismatch
1
ar
X
iv
:1
70
2.
06
35
5v
2 
 [c
s.C
V]
  1
0 A
pr
 20
17
between the static-image data and video data. The new
task of detecting objects in videos (VID) introduced by the
ImageNet challenge in 2015 provides a large-scale video
dataset, which requires labeling every object of 30 classes
in each frame of the videos. Driven by this new dataset,
multiple systems [7, 14, 15] were proposed to extend static-
image object detectors for videos.
Similar to the bounding box proposals in the static ob-
ject detection, the counterpart in videos are called tubelets,
which are essentially sequences of bounding boxes propos-
als. State-of-the-art algorithms for video object detection
utilize the tubelets to some extend to incorporate temporal
information for obtaining detection results. However, the
tubelet generation is usually based on the frame-by-frame
detection results, which is extremely time consuming. For
instance, the tracking algorithm used by [14, 15] needs 0.5
second to process each detection box in each frame, which
prevents the systems to generate enough tubelet proposals
for classification in an allowable amount of time, since the
video usually contains hundreds of frames with hundreds
of detection boxes on each frame. Motion-based methods,
such as optical-flow-guided propagation [14], can generate
dense tubelets efficiently, but the lengths are usually lim-
ited to only several frames (e.g., 7 frames in [14]) because
of their inconsistent performance for long-term tracking.
The ideal tubelets for video object detection should be long
enough to incorporate temporal information while diverse
enough to ensure high recall rates (Figure 1).
To mitigate the problems, we propose a framework for
object detection in videos. It consists of a Tubelet Proposal
Network (TPN) that simultaneously obtains hundreds of di-
verse tubelets starting from static proposals, and a Long
Short-Term Memory (LSTM) sub-network for estimating
object confidences based on temporal information from the
tubelets. Our TPN can efficiently generate tubelet propos-
als via feature map pooling. Given a static box proposal at
a starting frame, we pool features from the same box loca-
tions across multiple frames to train an efficient multi-frame
regression neural network as the TPN. It is able to learn
complex motion patterns of the foreground objects to gen-
erate robust tubelet proposals. Hundreds of proposals in a
video can be tracked simultaneously. Such tubelet proposals
are shown to be of better quality than the ones obtained on
each frame independently, which demonstrates the impor-
tance of temporal information in videos. The visual features
extracted from the tubelet boxes are automatically aligned
into feature sequences and are suitable for learning temporal
features with the following LSTM network, which is able
to capture long-term temporal dependency for accurate pro-
posal classification.
The contribution of this paper is that we propose a new
deep learning framework that combines tubelet proposal
generation and temporal classification with visual-temporal
features. An efficient tubelet proposal generation algo-
rithm is developed to generate tubelet proposals that cap-
ture spatiotemporal locations of objects in videos. A tempo-
ral LSTM model is adopted for classifying tubelet propos-
als with both visual features and temporal features. Such
high-level temporal features are generally ignored by exist-
ing detection systems but are crucial for object detection in
videos.
2. Related work
Object detection in static images. State-of-the-art ob-
ject detection systems are all based on deep CNNs. Girshick
et al. [6] proposed the R-CNN to decompose the object de-
tection problem into multiple stages including region pro-
posal generation, CNN finetuning, and region classification.
To accelerate the training process of R-CNN, Fast R-CNN
[5] was proposed to avoid time-consumingly feeding each
image patch from bounding box proposals into CNN to ob-
tain feature representations. Features of multiple bounding
boxes within the same image are warped from the same fea-
ture map efficiently via ROI pooling operations. To accel-
erate the generation of candidate bounding box proposals,
Faster R-CNN integrates a Region Proposal Network into
the Fast R-CNN framework, and is able to generate box pro-
posals directly with neural networks.
Object detection in videos. Since the introduction of
the VID task by the ImageNet challenge, there have been
multiple object detection systems for detecting objects in
videos. These methods focused on post-processing class
scores by static-image detectors to enforce temporal consis-
tency of the scores. Han et al. [7] associated initial detec-
tion results into sequences. Weaker class scores along the
sequences within the same video were boosted to improve
the initial frame-by-frame detection results. Kang et al. [15]
generated new tubelet proposals by applying tracking algo-
rithms to static-image bounding box proposals. The class
scores along the tubelet were first evaluated by the static-
image object detector and then re-scored by a 1D CNN
model. The same group [14] also tried a different strategy
for tubelet classification and re-scoring. In addition, initial
detection boxes were propagated to nearby frames accord-
ing to optical flows between frames, and the class scores
not belonging to the top classes were suppressed to enforce
temporal consistency of class scores.
Object localization in videos. There have been works
and datasets [3, 13, 22] on object localization in videos.
However, they have a simplified problem setting, where
each video is assumed to contain only one known or un-
known class and requires annotating only one of the objects
in each frame.
3. Tubelet proposal networks
Existing methods on object detection in videos gener-
ates tubelet proposals utilizing either generic single-object
tracker starting at a few key frames [15] or data associa-
tion methods (i.e. tracking-by-detection methods) on per-
frame object detection results [7]. These methods either
are too computationally expensive to generate enough dense
ty
x
Tubelet Proposal Network
Spatial Anchors
Movement Prediction EncoderLSTM
Decoder
LSTM
Class
Label
Tubelet
Features
Tubelet Proposal Network Encoder-decoder LSTM
Tubelet
CNN
Classification
CNN
t
y
x
Figure 2. The proposed object detection system, which consists
of two main parts. The first is a tubelet proposal network to effi-
ciently generating tubelet proposals. The tubelet proposal network
extracts multi-frame features within the spatial anchors, predicts
the object motion patterns relative to the spatial anchors and gen-
erates tubelet proposals. The gray box indicates the video clip
and different colors indicate proposal process of different spatial
anchors. The second part is an encoder-decoder CNN-LSTM net-
work to extract tubelet features and classify each proposal boxes
into different classes. The tubelet features are first fed into the en-
coder LSTM by a forward pass to capture the appearance features
of the entire sequence. Then the states are copied to the decoder
LSTM for a backward pass with the tubelet features. The encoder-
decoder LSTM processes the entire clip before outputting class
probabilities for each frame.
tublets, or are likely to drift and result in tracking fail-
ures. Even for an 100-fps single-object tracker, it might take
about 56 GPU days to generate tubelets with 300 bounding
boxes per frame for the large-scale ImageNet VID dataset.
We propose a Tubelet Proposal Network (TPN) which is
able to generate tubelet proposals efficiently for videos. As
shown in Figure 2, the Tubelet Proposal Network consists
of two main components, the first sub-network extracts vi-
sual features across time based on static region proposals at
a single frame. Our key observation is that, since the re-
ceptive fields (RF) of CNNs are generally large enough, we
can perform feature map pooling simply at the same bound-
ing box locations across time to extract the visual features
of moving objects. Based on the pooled visual features,
the second component is a regression layer for estimating
bounding boxes’ temporal displacements to generate tubelet
proposals.
3.1. Preliminaries on ROI-pooling for regression
There are existing works that utilize feature map pool-
ing for object detection. The Fast R-CNN framework [5]
utilizes ROI-pooling on visual feature maps for object clas-
sification and bounding box regression. The input image is
fed into a CNN and forward propagated to generate visual
feature maps. Given different object proposals, their visual
features are directly ROI-pooled from the feature maps ac-
cording to the box coordinates. In this way, CNN only needs
to forward propagate once for each input image and saves
much computational time. Let bit = (x
i
t, y
i
t, w
i
t, h
i
t) denote
the ith static box proposal at time t, where x, y, w and h
represent the two coordinates of the box center, width and
height of the box proposal. The ROI-pooling obtains visual
features rit ∈ Rf at box bit.
The ROI-pooled features rit for each object bounding
box proposal can be used for object classification, and, more
interestingly, for bounding box regression, which indicates
that the visual features obtained by feature map pooling
contain necessary information describing objects’ locations.
Inspired by this technique, we propose to extract multi-
frame visual features via ROI-pooling, and use such fea-
tures for generating tubelet proposals via regression.
3.2. Static object proposals as spatial anchors
Static object proposals are class-free bounding boxes in-
dicating the possible locations of objects, which could be
efficiently obtained by different proposal methods such as
SelectiveSearch [30], Edge Boxes [34] and Region Proposal
Networks [24]. For object detection in videos, however,
we need both spatial and temporal locations of the objects,
which are crucial to incorporate temporal information for
accurate object proposal classification.
For general objects in videos, movements are usually
complex and difficult to predict. The static object propos-
als usually have high recall rates (e.g. >90%) at individual
frames, which is important because it is the upper bound
of object detection performance. Therefore, it is natural to
use static proposals as starting anchors for estimating their
movements at following frames to generate tubelet propos-
als. If their movements can be robustly estimated, high ob-
ject recall rate at the following times can be maintained.
Let bi1 denote a static proposal of interest at time t =
1. Particularly, to generate a tubelet proposal starting at
bi1, visual features within the w-frame temporal window
from frame 1 to w are pooled at the same location bi1 as
ri1, r
i
2, · · · , riw in order to generate the tubelet proposal. We
call bi1 a “spatial anchor”. The pooled regression features
encode visual appearances of the objects. Recovering cor-
respondences between the visual features (ri1, r
i
2, · · · , riw)
leads to accurate tubelet proposals, which is modeled by a
regression layer detailed in the next subsection.
The reason why we are able to pool multi-frame features
from the same spatial location for tubelet proposals is that
CNN feature maps at higher layers usually have large re-
ceptive fields. Even if visual features are pooled from a
small bounding box, its visual context is far greater than
the bounding box itself. Pooling at the same box locations
across time is therefore capable of capturing large possible
movements of objects. In Figure 2, we illustrate the “spa-
tial anchors” for tubelet proposal generation. The features
in the same locations are aligned to predict the movement
of the object.
We use a GoogLeNet with Batch Normalization (BN)
model [12] for the TPN. In our settings, the ROI-pooling
layer is connected to “inception 4d” of the BN model,
which has a receptive field of 363 pixels. Therefore, the
network is able to handle up to 363-pixel movement when
ROI-pooling the same box locations across time, which is
more than enough to capture short-term object movements.
Each static proposal is regarded as an anchor point for fea-
ture extraction within a temporal window w.
3.3. Supervisions for tubelet proposal generation
Our goal is to generate tubelet proposals that have high
object recall rates at each frame and can accurately track ob-
jects. Based on the pooled visual features ri1, r
i
2, · · · , riw at
box locations bit, we train a regression network R(·) that ef-
fectively estimates the relative movements w.r.t. the spatial
anchors,
mi1,m
i
2, · · · ,miw = R(ri1, ri2, · · · , riw), (1)
where the relative movementsmit =(∆x
i
t,∆y
i
t,∆w
i
t,∆h
i
t)
are calculated as
∆xit = (x
i
t − xi1)/wi1, ∆yit = (yit − yi1)/hi1, (2)
∆wit = log(w
i
t/w
i
1), ∆h
i
t = log(h
i
t/h
i
1).
Once we obtain such relative movements, the actual box lo-
cations of the tubelet could be easily inferred. We adopt a
fully-connected layer that takes the concatenated visual fea-
tures [ri1, r
i
2, · · · , riw]T as the input, and outputs 4w move-
ment values of a tubelet proposal by
[mi1, · · · ,miw]T = Ww[ri1, · · · , riw]T + bw, (3)
where Ww ∈ Rfw×4w and bw ∈ R4w are the learnable
parameters of the layer.
The remaining problem is how to design proper super-
visions for learning the relative movements. Our key as-
sumption is that the tubelet proposals should have consis-
tent movement patterns with the ground-truth objects. How-
ever, given static object proposals as the starting boxes for
tubelet generation, they usually do not have a perfect 100%
Intersection-over-Union (IoU) ratio with the ground truth
object boxes. Therefore, we require static box proposals
that are close to ground truth boxes to follow the movement
patterns of the ground truth boxes. More specifically, if a
static object proposal bit has a greater-than-0.5 IoU value
with a ground truth box bˆit, and the IoU value is greater than
those of other ground truth boxes, our regression layer tries
to generate tubelet boxes following the same movement pat-
terns mˆit of the ground truth bˆ
i
t as much as possible. The
relative movement targets mˆit = (xˆ
i
t, yˆ
i
t, wˆ
i
t, hˆ
i
t) can be de-
fined w.r.t. the ground truth boxes at time 1, bˆ1t , in the simi-
lar way as Eq. (2). It is trivial to see that mˆi1 = (0, 0, 0, 0).
Therefore, we only need to predict mˆi2 to mˆ
i
w. Note that by
learning relative movements w.r.t to the spatial anchors at
the first frame, we can avoid cumulative errors in conven-
tional tracking algorithms to some extend.
The movement targets are normalized by their mean mt
and standard deviation σt as the regression objectives,
m˜it = (mˆ
i
t −mt)/σt, for t = 1, . . . , w. (4)
To generateN tubelets that follow movement patterns of
their associated ground truth boxes, we minimize the fol-
lowing object function w.r.t. all xit, y
i
t, w
i
t, h
i
t,
L({M˜}, {M}) = 1
N
N∑
i=1
w∑
t=1
∑
k∈{x,y,w,h}
d(∆kit), (5)
where {M˜} and {M} are the sets of all normalized move-
ment targets and network outputs, and
d(x) =
{
0.5x2 if |x| < 1,
|x| − 0.5 otherwise. (6)
is the smoothed L1 loss for robust box regression in [5].
The network outputs m˙it are mapped back to the real rel-
ative movements mit by
mit = (m˙
i
t +mt) ∗ σt. (7)
By our definition, if a static object proposal covers some
area the object, it should cover the same portion of object in
the later frames (see Figure 1 (d) for examples).
3.4. Initialization for multi-frame regression layer
The size of the temporal window is also a key factor in
the TPN. The simplest model is a 2-frame model. For a
given frame, the features within the spatial anchors on cur-
rent frame and the next frames are extracted and concate-
nated, [ri1, r
i
2]
T , to estimate the movements of bi1 on the
next frames. However, since the 2-frame model only uti-
lizes minimal temporal information within a very short tem-
poral window, the generated tubelets may be non-smooth
and easy to drift. Increasing the temporal window utilizes
more temporal information so as to estimate more complex
movement patterns.
Given the temporal window size w, the dimension of the
extracted features are fw, where f is the dimension of vi-
sual features in a single frame within the spatial anchors
(e.g., 1024-dimensional “inception 5b” features from the
BN model in our settings). Therefore, the parameter size
of the regress layer is of Rfw×4w and grows quadratically
with the temporal window size w.
If the temporal window size is large, randomly initializ-
ing such a large matrix has difficulty in learning a good re-
gression layer. We propose a “block” initialization method
to use the learned features from 2-frame model to initialize
the multi-frame models.
In Figure 3, we show how to use a pre-trained 2-frame
model’s regression layer to initialize that of a 5-frame
model. Since the target mˆi1 in Equation (2) is always
(0, 0, 0, 0) we only need to estimate movements for the later
frames. The parameter matrixW2 is of sizeR2f×4 since the
input features are concatenations of two frames and the bias
term b2 is of size R4. For the 5-frame regression layer, the
parameter matrixW5 is of size R5f×(4×4) and the bias term
A
B
A A A A
B
B
B
B
4
2f
16
5f
W2
b2
W5
b5
Figure 3. Illustration of the “block” initialization method. The 2-
frame model’s regression layer has weights W2 and bias b2, the
W2 consists of two sub-matrices A and B corresponding to the
features of the first and second frames. Then a 5-frame model’s
regression layer can be initialized with the sub-matrices as shown
in the figure. The bias term b5 is a simple repetition of b2.
b5 is of R(4×4). Essentially, we utilize visual features from
frame 1 & 2 to estimate movements in frame 2, frame 1 & 3
for frame 3, and so on. The matrix W2 is therefore divided
into two sub-matrices A ∈ Rf×4 and B ∈ Rf×4 to fill the
corresponding entries of matrix W5. The bias term b5 is a
repetition of b2 for 4 times.
In our experiments, we first train a 2-frame model with
random initialization and then use the 2-frame model to ini-
tialize the multi-frame regression layer.
4. Overall detection framework with tubelet
generation and tubelet classification
Based on the Tubelet Proposal Networks, we propose a
framework that is efficient for object detection in videos.
Compared with state-of-the-art single object tracker, It only
takes our TPN 9 GPU days to generate dense tubelet pro-
posals on the ImageNet VID dataset. It is also capable of
utilizing useful temporal information from tubelet propos-
als to increase detection accuracy. As shown in Figure 2,
the framework consists of two networks, the first one is the
TPN for generating candidate object tubelets, and the sec-
ond network is a CNN-LSTM classification network that
classifies each bounding box on the tubelets into different
object categories.
4.1. Efficient tubelet proposal generation
The TPN is able to estimate movements of each static
object proposal within a temporal window w. For object
detection in videos in large-scale datasets, we need to not
only efficiently generate tubelets for hundreds of spatial an-
chors in parallel, but also generate tubelets with sufficient
lengths to incorporate enough temporal information.
To generate tubelets with length of l, (see illustration
in Figure 4 (a)), we utilize static object proposals on the
first frame as spatial anchors, and then iteratively apply
TPN with temporal window w until the tubelets cover all
l frames. The last estimated locations of the previous itera-
Iter 1
Iter 2
Iter 3
Iter 4
Iter 5
l
(a)
(b)
Figure 4. Efficiently generating tubelet proposals. (a) the TPN
generates the tubelet proposal of temporal window w and uses the
last-frame output of the proposal as static anchors for the next iter-
ation. This process iterates until the whole track length is covered.
(b) multiple static anchors in a frame are fed to the Fast R-CNN
network with a single forward pass for simultaneously generating
multiple tubelet proposals. Different colors indicate different spa-
tial anchors
tion are used as spatial anchors for the next iteration. This
process can iterate to generate tubelet proposals of arbitrary
lengths.
For N static object proposals in the same starting frame,
the bottom CNN only needs to conduct an one-time forward
propagation to obtain the visual feature maps, and thus en-
ables efficient generation of hundreds of tubelet proposals
(see Figure 4 (b)).
Compared to previous methods that adopt generic sin-
gle object trackers, our proposed methods is dramatically
faster for generating a large number of tubelets. The track-
ing method used in [15] has reported 0.5 fps running speed
for a single object. For a typical frame with 300 spatial an-
chors, it takes 150s for each frame. Our method has an av-
erage speed of 0.488s for each frame, which is about 300×
faster. Even compared to the recent 100 fps single object
tracker in [9], our method is about 6.14× faster.
4.2. Encoder-decoder LSTM (ED-LSTM) for tem-
poral classification
After generating the length-l tubelet proposal, visual fea-
tures u1t , · · · ,uit, · · · ,uil can be pooled from tubelet box
locations for object classification with temporal informa-
tion. Existing methods [15, 7, 14] mainly use temporal in-
formation in post processing, either propagating detections
to neighboring frames or temporally smoothing detection
scores. The temporal consistency of detection results is im-
portant, but to capture the complex appearance changes in
the tubelets, we need to learn discriminative spatiotemporal
features at the tubelet box locations.
As shown in Figure 2, the proposed classification sub-
network contains a CNN that processes input images to ob-
tain feature maps. Classification features ROI-pooled from
each tubelet proposal across time are then fed into a one-
layer Long Short-Term Memory (LSTM) network [11] for
tubelet classification. It is a special type of recurrent neural
network (RNN) and is widely investigated for learning spa-
tiotemporal features in recent years. Each LSTM unit has
a memory unit that conveys visual information across the
time for incorporating temporal information.
The input for each time step t of the LSTM for the ith
tubelet are the cell state cit−1, hidden state h
i
t−1 of the pre-
vious frame, and the classification features uit pooled at the
current time t. The starting state (ci0, h
i
0) of the LSTM
is set to zeros. The output is the hidden states hit, which
is connected to a fully-connected layer for predicting class
confidences and another FC layer for box regression. One
problem with the vanilla LSTM is that the initial state may
dramatically influence the classification of the first several
frames. Inspired by sequence-to-sequence LSTM in [28],
we propose an encoder-decoder LSTM model for object de-
tection in videos as shown in Figure 2. The input features
are first fed into an encoder LSTM to encode the appearance
features of the entire tubelet into the memory. The mem-
ory and hidden states are then fed into the decoder LSTM,
which then classifies the tubelet in the reverse order with the
reversed inputs from the last frame back to the first frame.
In this way, better classification accuracy can be achieved
by utilizing both past and future information. The low pre-
diction confidences caused by the all-zero initial memory
states can be avoided.
5. Experiments
5.1. Datasets and evaluation metrics
The proposed framework is evaluated on the ImageNet
object detection from video (VID) dataset introduced in the
ILSVRC 2015 challenge. There are 30 object classes in the
dataset. The dataset is split into three subsets: the training
set that contains 3862 videos, the validation set that contains
555 videos, and the test set that contains 937 videos. Ob-
jects of the 30 classes are labeled with ground truth bound-
ing boxes on all the video frames. Since the ground truth
labels for the test set are not publicly available, we report
all results on the validation set as a common practice on
the ImageNet detection tasks. The mean average precision
(Mean AP) of 30 classes is used as the evaluation metric.
In addition, we also evaluate our system on the
YouTubeObjects (YTO) [22] dataset for the object localiza-
tion task. The YTO dataset has 10 object classes, which are
a subset of the ImageNet VID dataset. The YTO dataset is
weakly annotated with only one object of one ground truth
class in the video. We only use this dataset for evaluation
and the evaluation metric is CorLoc performance measure
used in [3], i.e., the recall rate of ground-truth boxes with
IoU above 0.5.
5.2. Base CNN model training
We choose GoogLeNet with Batch Normalization (BN)
[12] as our base CNN models for both our TPN and CNN-
LSTM models without sharing weights between them. The
BN model is pre-trained with the ImageNet classification
data and fine-tuned on the ImageNet VID dataset. The static
object proposals are generated by a RPN network trained on
the ImageNet VID dataset. The recall rate of the per-frame
RPN proposals on the VID validation set is 95.92 with 300
boxes on each frame.
To integrate with Fast RCNN framework, we placed the
ROI-pooling layer after “inception 4d” rather than the last
inception module (“inception 5b”), because “inception 5b”
has 32× down-sampling with a receptive field of 715 pix-
els, which is too large for ROI-pooling to generate discrim-
inative features. The output size of ROI-pooling is 14× 14
and we keep the later inception modules and the final global
pooling after “inception 5b”. We then add one more FC
layer for different tasks including tubelet proposal, classifi-
cation or bounding box regression.
The BN model is trained on 4 Titan X GPUs for 200, 000
iterations, with 32 RoIs from 2 images on each card in every
iteration. The initial learning rate is 5×10−4 and decreases
to 1/10 of its previous value for every 60, 000 iterations.
All BN layers are frozen during the fine-tuning. After fine-
tuning on DET data, the BN model achieves 50.3% mean
AP on the ImageNet DET data. After fine-tuning the BN
model on the VID data with the same hyper-parameter set-
ting for 90, 000 iterations, it achieves 63.0% mean AP on
the VID validation set.
5.3. TPN training and evaluation
With the fine-tuned BN model, we first train a 2-frame
model on the ImageNet VID dataset. Since the TPN needs
to estimate the movement of the object proposals according
ground-truth objects’ movements, we only select static pro-
posals that have greater-than-0.5 IoU overlaps with ground-
truth annotations as spatial anchors following Section 3.3.
For those proposals that do not have greater-than-0.5 over-
laps with ground-truth boxes, they are not used for training
the TPN. During the test stage, however, all static object
proposals in every 20 frames are used as spatial anchors for
tubelet proposal generation. All tubelets are 20-frame long.
The ones starting from negative static proposals are likely
to stay in the background regions, or track the foreground
objects when they appear in their nearby regions.
We investigate different temporal window sizes w and
initialization methods described in Section 3.4. Since the
ground truth movements mˆit can be obtained from the
ground truth annotations, each positive static proposal has
an “ideal” tubelet proposal in comply with its associated
ground-truth’s movements. Three metrics are used to eval-
uate the accuracy of generated tubelets by different mod-
els (Table 1). One is the mean absolute pixel differ-
ence (MAD) of the predicted coordinates and their ground
truth. The second one is the mean relative pixel differ-
ence (MRD) with x differences normalized by widths and
y differences normalized by heights. The third metric is
the mean intersection-over-union (IOU) between predicted
Method Window MAD MRD Mean IOU
MoveTubelets Random 2 15.50 0.0730 0.7966
MoveTubelets Random 5 26.00 0.1319 0.6972
MoveTubelets RNN 5 13.87 0.0683 0.8060
MoveTubelets Block 5 12.98 0.0616 0.8244
MoveTubelets Block 11 15.20 0.0761 0.8017
MoveTubelets Block 20 18.03 0.0874 0.7731
Table 1. Evaluation of tubelet proposals obtained with varying
window sizes and different initialization methods. As the param-
eter size grows quadratically with the temporal window. The 5-
frame model with random initialization has much worse accuracy
compared to the proposed transformation initialization. As the
temporal window grows, the motion pattern becomes more com-
plex and the movement displacement may also exceed the recep-
tive field, which also causes accuracy decreases.
boxes and target boxes. From the table, we can see that
the 2-frame baseline model has a MAD of 15.50, MRD of
0.0730 and Mean IOU of 0.7966. For the 5-frame model, if
we initialize the fully-connected regression layer randomly
without using the initialization technique (other layers are
still initialized by the finetuned BN model), the perfor-
mance drops significantly compared to that of the 2-frame
model. The reason might be that the parameter size of the 5-
frame model increases by 10 times (as shown in Figure 3),
which makes it more difficult to train without a good initial
point. However, with the proposed technique, the multi-
frame regression layer with the 2-frame model, the gener-
ated tubelets have better accuracy than the 2-frame model
because of the larger temporal context.
If the temporal window continues to increase, even with
the proposed initialization techniques, the performance de-
creases. This might be because if the temporal window is
too large, the movement of the objects might be too com-
plex for the TPN to recover the visual correspondences be-
tween far-away frames. In the later experiments, we use the
5-frame TPN to generate 20-frame-long tubelet proposals.
In comparison with our proposed method, an RNN base-
line with is implemented by replacing the tubelet regression
layer with an RNN layer of 1024 hidden neurons and a re-
gression layer to predict 4 motion targets. As shown in Ta-
ble 1, the RNN baseline performs worse than our method.
5.4. LSTM Training
After generating the tubelet proposals, the proposed
CNN-LSTM models extract classification features uit at
tubelet box locations with the finetuned BN model. The
dimension of the features at each time step is 1024.
The LSTM has 1024 cell units and 1024 hidden outputs.
For each iteration, 128 tubelets from 4 videos are randomly
chosen to form a mini-batch. The CNN-LSTM is trained
using stochastic gradient descent (SGD) optimization with
momentum of 0.9 for 20000 iterations. The parameters are
initialized with standard deviation of 0.0002 and the initial
learning rate is 0.1. For every 2, 000 iteration, the learning
rate decreases by a factor of 0.5.
5.5. Results
Baseline methods. The most basic baseline method is
Fast R-CNN static detector [5] (denoted as “Static”), which
needs static proposals on every frame and does not involve
any temporal information. This baseline uses static propos-
als from the same RPN we use and the Fast R-CNN model
is the same as our base BN model. To validate the effective-
ness of the tubelet regression targets, we change them into
the precise locations of the ground truth on each frame and
also generate tubelet proposals (see Figure 1 (c)). Then we
apply a vanilla LSTM on these tubelet proposals and denote
the results as “LocTubelets+LSTM”. Our tubelet proposal
method is denoted as “MoveTubelets”. We also compare
with a state-of-the-art single-object tracking method [10]
denoted as “KCF”. As for the CNN-LSTM classification
part, the baseline methods are the vanilla LSTM (denoted
as “LSTM”), and our proposed encoder-decoder LSTM is
denoted as “ED-LSTM”.
Results on ImageNet VID dataset. The quantitative re-
sults on the ImageNet VID dataset are shown in Table 2
and 3. As a convention of detection tasks on the ImageNet
dataset, we report the results on the validation set. The per-
formance of the baseline Fast R-CNN detector finetuned on
the ImageNet VID dataset has a Mean AP of 0.630 (denoted
as “Static”). Compare to the best single model performance
in [14], which has a Mean AP of 0.615 using only the VID
data, the baseline detector has an 1.5% performance gain.
Directly applying the baseline static detector on the TPN
tubelets with temporal window of 5 results in a Mean AP of
0.623 (denoted as “MoveTubelets+Fast RCNN”). In com-
parison, a state-of-the-art tracker [10] with the baseline
static detector (“KCF+Fast RCNN”) has a Mean AP of only
0.567. In addition, although the KCF tracker runs at 50 fps
for single object tracking, it takes 6 seconds to process one
frame with 300 proposals. Our method is 12× faster.
Applying the vanilla LSTM on the tubelet propos-
als increases the Mean AP to 0.678 (denoted as “Move-
Tubelets+LSTM”), which has 5.5% performance gain over
the tubelet results and 4.8% increase over the static base-
line results. This shows that the LSTM is able to learn ap-
pearance and temporal features from the tubelet proposals
to improve the classification accuracy. Especially for class
of “whale”, the AP has over 25% improvement since whales
constantly emerge from the water and submerge. A detector
has to observe the whole process to classify them correctly.
Compared to bounding box regression tubelet proposal
baseline, our tubelet proposal model has 2.5% improve-
ment which shows that our tubelet proposals have more di-
versity to incorporate temporal information. Changing to
the encoder-decoder LSTM model has a Mean AP of 0.684
(denoted as “MoveTubelets+ED-LSTM”) with a 0.6% per-
formance gain over the vanilla LSTM model with perfor-
mance increases on over half of the classes. One thing to
notice is that our encoder-decoder LSTM model performs
better than or equal to the tubelet baseline results on all
Method ai
rp
la
ne
an
te
lo
pe
be
ar
bi
ke
bi
rd
bu
s
ca
r
ca
ttl
e
do
g
d
ca
t
el
ep
ha
nt
fo
x
g
pa
nd
a
ha
m
ste
r
ho
rs
e
lio
n
Static (Fast RCNN) 0.821 0.784 0.665 0.656 0.661 0.772 0.523 0.491 0.571 0.720 0.681 0.768 0.718 0.897 0.651 0.201
MoveTubelets+Fast RCNN 0.776 0.778 0.663 0.654 0.649 0.766 0.514 0.493 0.559 0.724 0.684 0.775 0.710 0.900 0.642 0.208
LocTubelets+LSTM 0.759 0.783 0.660 0.646 0.682 0.813 0.538 0.528 0.605 0.722 0.698 0.782 0.724 0.901 0.664 0.212
MoveTubelets+LSTM 0.839 0.794 0.715 0.652 0.683 0.794 0.533 0.615 0.608 0.765 0.705 0.839 0.769 0.916 0.661 0.158
MoveTubelets+ED-LSTM 0.846 0.781 0.720 0.672 0.680 0.801 0.547 0.612 0.616 0.789 0.716 0.832 0.781 0.915 0.668 0.216
Method liz
ar
d
m
on
ke
y
m
ot
or
ra
bb
it
r p
an
da
sh
ee
p
sn
ak
e
sq
ui
rre
l
tig
er
tra
in
tu
rtl
e
w
at
er
cr
af
t
w
ha
le
ze
br
a
m
ea
n
A
P
Static (Fast RCNN) 0.638 0.347 0.741 0.457 0.558 0.541 0.572 0.298 0.815 0.720 0.744 0.557 0.432 0.894 0.630
MoveTubelets+Fast RCNN 0.646 0.320 0.691 0.454 0.582 0.540 0.567 0.286 0.806 0.730 0.737 0.543 0.414 0.885 0.623
LocTubelets+LSTM 0.743 0.334 0.727 0.513 0.555 0.613 0.688 0.422 0.813 0.781 0.760 0.609 0.429 0.874 0.653
MoveTubelets+LSTM 0.746 0.347 0.771 0.525 0.710 0.609 0.637 0.406 0.845 0.786 0.774 0.602 0.637 0.890 0.678
MoveTubelets+ED-LSTM 0.744 0.366 0.763 0.514 0.706 0.642 0.612 0.423 0.848 0.781 0.772 0.615 0.669 0.885 0.684
Table 2. AP list on ImageNet VID validation set by the proposed method and compared methods.
Static (Fast RCNN) 0.630
TCNN [14] 0.615
Seq-NMS [7] 0.522
Closed-loop [4] 0.500
KCF Tracker [10] + Fast R-CNN 0.567
MoveTubelets + Fast R-CNN 0.623
MoveTubelets+LSTM 0.678
MoveTubelets+ED-LSTM (proposed) 0.684
Table 3. Mean AP for baseline models and proposed methods.
Figure 5. Qualitative results on the ImageNet VID dataset. The
bounding boxes are tight and stably concentrate on the objects
since the RoIs for each frame are based on the predicted locations
on the previous frame. The last 3 rows show the robustness to
handle scenes with multiple objects.)
the classes, which means that learning the temporal features
consistently improves the detection results.
The qualitative results on the ImageNet VID dataset are
shown in Figure 5. The bounding boxes are tight to the ob-
jects and we able to track and detect multiple objects during
long periods of time.
Localization on the YouTubeObjects dataset. In addition
to the object detection in video task on the ImageNet VID
dataset. We also evaluate our system on video object local-
ization task with the YouTubeObjects (YTO) dataset.
For each test video, we generate tubelet proposals and
apply the encoder-decoder LSTM model to classify the
tubelet proposals. For each test class, we select the tubelet
box with the maximum detection score on the test frames,
if the box has over 0.5 IOU overlap with one of the ground
truth boxes, this frame is accurately localized. The system
is trained on the ImageNet VID dataset and is directly ap-
plied for testing without any finetuning on the YTO dataset.
We compare with several state-of-the-art results on the YTO
Method aero bird boat car cat cow dog horse mbike train Avg.
Prest et al. [22] 51.7 17.5 34.4 34.7 22.3 17.9 13.5 26.7 41.2 25.0 28.5
Joulin et al. [13] 25.1 31.2 27.8 38.5 41.2 28.4 33.9 35.6 23.1 25.0 31.0
Kwak et al. [17] 56.5 66.4 58.0 76.8 39.9 69.3 50.4 56.3 53.0 31.0 55.7
Kang et al. [15] 94.1 69.7 88.2 79.3 76.6 18.6 89.6 89.0 87.3 75.3 76.8
MoveTubelets+ED-LSTM 91.2 99.4 93.1 94.8 94.3 99.3 90.2 87.8 89.7 84.2 92.4
Table 4. Localization results on the YouTubeObjects dataset. Our
model outperforms previous method with large margin.
dataset, and our system outperforms them with a large mar-
gin. Compared to the second best results in [15], our system
has 15.6% improvement.
6. Conclusion
In this work, we propose a system for object detection
in videos. The system consists of a novel tubelet proposal
network that efficiently generates tubelet proposals and an
encoder-decoder CNN-LSTM model to learn temporal fea-
tures from the tubelets. Our system is evaluated on the Im-
ageNet VID dataset for object detection in videos and the
YTO dataset for object localization. Experiments demon-
strate the effectiveness of our proposed framework.
Acknowledgments. This work is supported in part by
SenseTime Group Limited, in part by the General Research
Fund through the Research Grants Council of Hong
Kong under Grants CUHK14207814, CUHK14206114,
CUHK14205615, CUHK14213616, CUHK14203015,
CUHK14239816, and CUHK419412, in part by the Hong
Kong Innovation and Technology Support Programme
Grant ITS/121/15FX, in part by National Natural Science
Foundation of China under Grant 61371192, in part by
the Ph.D. Program Foundation of China under Grant
20130185120039, and in part by the China Postdoctoral
Science Foundation under Grant 2014M552339.
References
[1] S.-H. Bae and K.-J. Yoon. Robust online multi-object track-
ing based on tracklet confidence and online discriminative
appearance learning. CVPR, 2014. 1
[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. DeepLab: Semantic Image Segmentation with
Deep Convolutional Nets, Atrous Convolution, and Fully
Connected CRFs. In ICLR, 2015. 1
[3] T. Deselaers, B. Alexe, and V. Ferrari. Localizing Objects
While Learning Their Appearance. ECCV, 2010. 2, 6
[4] L. Galteri, L. Seidenari, M. Bertini, and A. Del Bimbo.
Spatio-temporal closed-loop object detection. TIP, 2017. 8
[5] R. Girshick. Fast r-cnn. ICCV, 2015. 1, 2, 3, 4, 7
[6] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. CVPR, 2014. 1, 2
[7] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran,
M. Babaeizadeh, H. Shi, J. Li, S. Yan, and T. S. Huang.
Seq-nms for video object detection. arXiv preprint
arXiv:1602.08465, 2016. 2, 5, 8
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. arXiv preprint arXiv:1512.03385,
2015. 1
[9] D. Held, S. Thrun, and S. Savarese. Learning to Track at 100
FPS with Deep Regression Networks. In ECCV, 2016. 5
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
speed tracking with kernelized correlation filters. TPAMI,
2015. 7, 8
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997. 5
[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015. 3, 6
[13] A. Joulin, K. Tang, and L. Fei-Fei. Efficient Image and Video
Co-localization with Frank-Wolfe Algorithm. ECCV, 2014.
2, 8
[14] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang,
Z. Wang, R. Wang, X. Wang, et al. T-cnn: Tubelets with con-
volutional neural networks for object detection from videos.
arXiv preprint arXiv:1604.02532, 2016. 1, 2, 5, 7, 8
[15] K. Kang, W. Ouyang, H. Li, and X. Wang. Object detection
from video tubelets with convolutional neural networks. In
CVPR, 2016. 1, 2, 5, 8
[16] K. Kang and X. Wang. Fully convolutional neural networks
for crowd segmentation. arXiv preprint arXiv:1411.4464,
2014. 1
[17] S. Kwak, M. Cho, I. Laptev, J. Ponce, and C. Schmid. Un-
supervised Object Discovery and Tracking in Video Collec-
tions. ICCV, 2015. 8
[18] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, and X. Wang. Person
search with natural language description. In CVPR, 2017. 1
[19] Y. Li, W. Ouyang, and X. Wang. Vip-cnn: A visual phrase
reasoning convolutional neural network for visual relation-
ship detection. In CVPR, 2017. 1
[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015. 1
[21] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian,
H. Li, S. Yang, Z. Wang, C.-C. Loy, et al. DeepID-net:
Deformable deep convolutional neural networks for object
detection. CVPR, 2015. 1
[22] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-
rari. Learning object class detectors from weakly annotated
video. CVPR, 2012. 2, 6, 8
[23] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Unified, real-time object detection. arXiv
preprint arXiv:1506.02640, 2015. 1
[24] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-
wards real-time object detection with region proposal net-
works. NIPS, 2015. 1, 3
[25] J. Shao, K. Kang, C. Change Loy, and X. Wang. Deeply
learned attributes for crowded scene understanding. In
CVPR, 2015. 1
[26] J. Shao, C.-C. Loy, K. Kang, and X. Wang. Slicing convo-
lutional neural network for crowd video understanding. In
CVPR, 2016. 1
[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. ICLR, 2015. 1
[28] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence
learning with neural networks. In NIPS, 2014. 6
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. CVPR, 2015. 1
[30] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.
Smeulders. Selective search for object recognition. IJCV,
2013. 3
[31] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual tracking
with fully convolutional networks. ICCV, 2015. 1
[32] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. Joint detec-
tion and identification feature learning for person search. In
CVPR, 2017. 1
[33] F. Zhu, X. Wang, and N. Yu. Crowd tracking with dynamic
evolution of group structures. In ECCV, 2014. 1
[34] C. L. Zitnick and P. Dollar. Edge Boxes: Locating Object
Proposals from Edges. ECCV, 2014. 3
"
33,"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
A Discriminatively Learned CNN Embedding for
Person Re-identification
Zhedong Zheng, Liang Zheng and Yi Yang
Abstract—In this paper, we revisit two popular convolu-
tional neural networks (CNN) in person re-identification (re-ID),
i.e.,verification and identification models. The two models have
their respective advantages and limitations due to different loss
functions. In this paper, we shed light on how to combine the
two models to learn more discriminative pedestrian descriptors.
Specifically, we propose a siamese network that simultaneously
computes the identification loss and verification loss. Given a
pair of training images, the network predicts the identities of the
two input images and whether they belong to the same identity.
Our network learns a discriminative embedding and a similarity
measurement at the same time, thus making full usage of the
re-ID annotations.
Our method can be easily applied on different pre-trained
networks. Albeit simple, the learned embedding improves the
state-of-the-art performance on two public person re-ID bench-
marks. Further, we show our architecture can also be applied in
image retrieval.
Index Terms—Large-scale Person Re-identification, Convolu-
tional Neural Networks.
I. INTRODUCTION
PErson re-identification (re-ID) is usually viewed as animage retrieval problem, which matches pedestrians from
different cameras [1]. Given a person-of-interest (query), per-
son re-ID determines whether the person has been observed
by another camera. Recent progress in this area has been due
to two factors: 1) the availability of the large-scale pedestrian
datasets. The datasets contain the general visual variance of
pedestrian and provide a comprehensive evaluation [2], [3].
2) the learned embedding of pedestrian using a convolutional
neural network (CNN).
Recently, the convolutional neural network (CNN) has
shown potential for learning state-of-the-art feature embed-
dings or deep metrics [2], [4], [5], [6], [7], [8], [9]. As
shown in Fig. 1, there are two major types of CNN structures,
i.e.,verification models and identification models. The two
models are different in terms of input, feature extraction and
loss function for training. Our motivation is to combine the
strengths of the two models and learn a more discriminative
pedestrian embedding.
Verification models take a pair of images as input and
determine whether they belong to the same person or not.
A number of previous works treat person re-ID as a binary-
class classification task or a similarity regression task [2], [4],
[5], [6]. Given a label s ∈ {0, 1}, the verification network
forces two images of the same person to be mapped to nearby
Zhedong Zheng, Liang Zheng and Yi Yang are with Faculty of Engi-
neering and IT, University of Technology Sydney, NSW, Australia. E-mail:
zdzheng12@gmail.com, liangzheng06@gmail.com, yee.i.yang@gmail.com
Fig. 1. The difference between the verification and identification models.
Green blocks represent non-linear functions by CNN. a) Identification models
treat person re-ID as a multi-class recognition task, which take one image as
input and predict its identity. b) Verification models treat person re-ID as a
two-class recognition task or a similarity regression task, which take a pair
of images as input and determine whether they belong to the same person or
not. Here we only show a two-class recognition case.
points in the feature space. If the images are of different
people, the points are far apart. However, the major problem
in the verification models is that they only use weak re-ID
labels [1], and do not take all the annotated information into
consideration. Therefore, the verification network lacks the
consideration of the relationship between the image pairs and
other images in the dataset.
In the attempt to take full advantages of the re-ID labels,
identification models which treat person re-identification as a
multi-class recognition task, are employed for feature learning
[1], [7], [8], [9]. They directly learn the non-linear functions
from an input image to the person ID and the cross-entropy
loss is used following the final layer. During testing, the
feature is extracted from a fully connected layer and then
normalized. The similarity of two images is thus computed by
the Euclidean distance between their normalized CNN embed-
dings. The major drawback of the identification model is that
the training objective is different from the testing procedure,
i.e.,it does not account for the similarity measurement between
image pairs, which can be problematic during the pedestrian
retrieval process.
The above-mentioned observations demonstrate that the
two types of models have complementary advantages and
ar
X
iv
:1
61
1.
05
66
6v
2 
 [c
s.C
V]
  3
 Fe
b 2
01
7
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
Method Strong Label Similarity Re-IDEstimation Performance
Verification Models × X fair
Identification Models X × good
Our Model X X good
TABLE I
THE ADVANTAGES AND DISADVANTAGES OF VERIFICATION AND
IDENTIFICATION MODELS ARE LISTED. WE ASSUME SUFFICIENT
TRAINING DATA IN ALL MODELS. OUR MODEL TAKES THE ADVANTAGES
OF THE TWO MODELS.
limitations as shown in Table I. Motivated by these properties,
this work proposes to combine the strengths of the two
networks and leverage their complementary nature to improve
the discriminative ability of the learned embeddings. The
proposed model is a siamese network that predicts person
identities and similarity scores at the same time. Compared to
previous networks, we take full advantages of the annotated
data in terms of pair-wise similarity and image identities.
During testing, the final convolutional activations are extracted
for Euclidepdfan distance based pedestrian retrieval. To sum-
marize, our contributions are:
• We propose a siamese network that has two losses:
identification loss and verification loss. This network
simultaneously learns a discriminative CNN embedding
and a similarity metric, thus improving pedestrian re-
trieval accuracy.
• We report competitive accuracy compared to the state-
of-art methods on two large-scale person re-ID datasets
(Market1501 [3] and CUHK03 [2]) and one instance
retrieval dataset (Oxford5k [10]).
The paper is organized as follows. We first review some
related works in Section II. In Section III, we describe how
we combine the two losses and define the CNN structure.
The implementation details are provided. In Section IV, we
present the experimental results on two large-scale person re-
identification datasets and one instance retrieval dataset. We
conclude this paper in Section V.
II. RELATED WORK
In this section we describe previous works relevant to the
approach discussed in this paper. They are mainly based on
verification models or identification models.
A. Verification Models
In 1993, Bromley et al. [11] first used verification models
to deep metric learning in signature verification. Verification
models usually take a pair of images as input and output a
similarity score by calculating the cosine distance between
low-dimensional features, which can be penalized by the
contrastive loss. Recently researchers have begun to apply
verification models to person re-identification with a focus on
data augmentation and image matching. Yi et al. [4] split a
pedestrian image into three horizontal parts and train three
part-CNNs to extract features. The similarity of two images is
computed by the cosine distance of their features. Similarly,
Cheng et al. split the convolutional map into four parts and
fuse the part features with the global features [12]. Li et al.
[2] add a patch-matching layer that multiplies the activation of
two images in different horizontal stripes. They use it to find
similar locations and treat similarity regression as binary-class
penalized by softmax loss. Later, Ahmed et al. [13] improve
the verification model by adding a different matching layer
that compares the activation of two images in neighboring
pixels. Besides, Wu et al. [5] use smaller filters and a deeper
network to extract features. Varior et al. [6] combine CNN
with some gate functions, similar to long-short-term memory
(LSTM [14]) in spirit, which aims to adaptively focus on the
similar parts of input image pairs. But it is limited by the
computational inefficiency because the query image has to
pair with every gallery image to pass through the network.
Moreover, Ding et al. [15] use triplet samples for training the
network which considers the images from the same people and
the different people at the same time.
B. Identification Models
Recent datasets such as CUHK03 [2] and Market1501 [3]
provide large-scale training sets, which make it possible to
train a deeper classification model without over-fitting. Every
identity has 9.6 training images on average in CUHK03 [2]
and has 17.2 images in Market1501 [3]. CNN can learn
discriminative embeddings by itself without part-matching.
Zheng et al. [1], [8], [16] directly use a conventional fine-
tuning approach on Market1501 [3], PRW [8] and MARS [16]
and outperform many recent results. Wu et al. [17] combine
CNN embeddings with the hand-crafted features in the FC
layer. Besides, Xiao et al. [7] jointly train a classification
model using multiple datasets and propose a new dropout
function to deal with the hundreds of classes. In [9], Xiao
et al. train a classification model similar to the faster-RCNN
[18] method and automatically predict the location of the
candidate pedestrian from the whole image, which alleviates
the pedestrian detection errors.
C. Verification-identification Models
In face recognition, the “DeepID networks” train the net-
work with the verification and identification losses [19], [20],
[21], which is similar to our network. In [19], Sun et al. jointly
train face identification and verification. Then more verifica-
tion supervision is added into the model [20] and a deeper
network is used [21].
Our method is different from their models in the following
aspects. First, in face recognition, the training dataset contains
202,599 face images of 10,177 identities [19] while the current
largest person re-id training dataset contains 12,936 images of
751 identities [3]. DeepID networks apply contrastive loss to
the verification problem, wile our model uses the cross-entropy
loss. We find that the contrastive loss leads to over-fitting when
the number of images is limited. In the experiment, we show
the proposed method learns more robust person representative
and outperforms using contrastive loss. Second, dropout [22]
cannot be applied on the embedding before the contrastive
loss, which introduces zero values at random locations. On the
contrary, we can add dropout regularization on the embedding
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
Fig. 3. The proposed model structure. Given n pairs of images of size 227 × 227, two identical CaffeNet models are used as the non-linear embedding
functions and output 4,096-dim embeddings f1, f2. Then, f1, f2 are used to predict the identity t of the two input images, respectively, and also predict the
verification label s jointly. We introduce a non-parametric layer called Square Layer to compare high level features f1, f2. Finally, the softmax loss is applied
on the three objectives.
Fig. 2. Illustration for a training batch. The number in the circle is the identity
label. Blue and red edges represent whether the image pair depicts the same
identity or not. Dotted edges represent implicit relationships and solid edges
represent explicit relationships. Our model combine the strengths of the two
models.
in the proposed model. Third, the DeepID networks are trained
from scratch, while our model benefits from the networks
pretrained on ImageNet [23]. Finally, we evaluate our method
on the tasks of person re-ID and instance retrieval, providing
more insights in the verification-classification models.
III. PROPOSED METHOD
A. Preview
Fig. 2 (a) and Fig. 2 (b) illustrate the relational graph built
by verification and identification models. In a sample batch of
size m = 10, red edges represent the positive pairs (the same
person) and blue edges represent the negative pairs (different
persons). The dotted edges denote implicit relationships built
by the identification loss and the solid edges denote explicit
relationships built by the verification loss.
In verification models, there are several operations between
the two inputs. The explicit relationship between data is built
by the pair-wise comparison, such as part matching [2], [13]
or contrastive loss [24]. For example, contrastive loss directly
calculates the Euclidean distance between two embeddings.
In identification models, the input is independent to each
other. But there is implicit relationship between the learned
embeddings built by the cross-entropy loss. The cross-entropy
loss can be formulated as loss = −log(pgt),where pgt =
Wgtfi. W is the weight of the linear function. fm, fn are the
embeddings of the two images xm, xn from the same class
k. To maximize Wkfm, Wkfn, the network converges when
fm and fn have similar vector direction with Wk. In [25],
similar observation and visualization are shown. So the learned
embeddings are eventually close for images within the same
class and far away for images in the different classes. The
relationship is implicitly built between xm, xn and bridged by
the weight Wk.
Due to the usage of the weak labels, verification models
take limited relationships into consideration. On the other
hand, classification models do not explicitly consider similarity
measurements. Fig. 2 (c) illustrates how our model works
in a batch. We benefit from simultaneously considering the
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
verification and identification losses. The proposed model thus
combines the strength of the two models (see Table I).
B. Overall Network
Our network is basically a convolutional siamese network
that combines the verification and identification losses. Fig.
3 briefly illustrates the architecture of the proposed network.
Given an input pair of images resized to 227 × 227, the
proposed network simultaneously predicts the IDs of the two
images and the similarity score. The network consists of
two ImageNet [23] pre-trained CNN models, three additional
Convolutional Layers, one Square Layer and three losses. It
is supervised by the identification label t and the verification
label s. The pre-trained CNN model can be CaffeNet [26],
VGG16 [27] or ResNet-50 [28], from which we have removed
the final fully-connected (FC) layer. The re-ID performance
of the three models is comprehensively evaluated in Section
IV. Here, we do not provide detailed descriptions of the
architecture of the CNN models and only take CaffeNet as an
example in the following subsections. The three optimization
objectives include two identification losses and one verification
loss. We use the final convolutional activations f as the
discriminative descriptor for person re-ID, which is directly
supervised by three objectives.
C. Identification Loss
There are two CaffeNets in our architecture. They share
weights and predict the two identity labels of the input image
pair simultaneously. In order to fine-tune the network on a new
dataset, we replace the final fully-connected layer (1,000-dim)
of the pre-trained CNN model with a convolutional layer. The
number of the training identities in Market-1501 is 751. So
this convolutional layer has 751 kernels of size 1× 1× 4096
connected to the output f of CaffeNet and then we add a
softmax unit to normalize the output. The size of the result
tensor is 1×1×751. The Rectified Linear Unit (ReLU) is not
added after this convolution. Similar to conventional multi-
class recognition approaches, we use the cross-entropy loss
for identity prediction, which is
pˆ = softmax(θI ◦ f), (1)
Identif(f, t, θI) =
K∑
i=1
−pi log(pˆi). (2)
Here ◦ denotes the convolutional operation. f is a 1×1×4, 096
tensor, t is the target class and θI denotes the parameters of
the added convolutional layer. pˆ is the predicted probability,
pi is the target probability. pi = 0 for all i except pt = 1.
D. Verification Loss
While some previous works contain a matching function in
the intermediate layers [2], [6], [13], our work directly com-
pares the high-level features f1, f2 for similarity estimation.
The high-level feature from the fine-tuned CNN has shown a
discriminative ability [8], [16] and it is more compact than
the activations in the intermediate layers. So in our model,
Method mAP rank-1
CaffeNet (V) 22.47 41.24
CaffeNet (I) 26.79 50.89
CaffeNet (I+V) 39.61 62.14
VGG16 (V) 24.29 42.99
VGG16 (I) 38.27 65.02
VGG16 (I+V) 47.45 70.16
ResNet-50 (V) 44.94 64.58
ResNet-50 (I) 51.48 73.69
ResNet-50 (I+V) 59.87 79.51
TABLE II
RESULTS ON MARKET1501 [3] BY IDENTIFICATION LOSS AND
VERIFICATION LOSS INDIVIDUALLY AND JOINTLY. “I” AND “V” DENOTE
THE IDENTIFICATION LOSS AND VERIFICATION LOSS, RESPECTIVELY.
the pedestrian descriptor f1, f2 in the identification model are
directly supervised by the verification loss. As shown in Fig.
3, we introduce a non-parametric layer called Square Layer to
compare the high-level features. It takes two tensors as inputs
and outputs one tensor after subtracting and squaring element-
wisely. The Square Layer is denoted as fs = (f1−f2)2, where
f1, f2 are the 4,096-dim embeddings and fs is the output
tensor of the Square Layer.
We then add a convolutional layer and the softmax output
function to embed the resulting tensor fs to a 2-dim vector
(qˆ1, qˆ2) which represents the predicted probability of the two
input images belonging to the same identity. qˆ1 + qˆ2 = 1.
The convolutional layer takes fs as input and filters it with 2
kernels of size 1 × 1 × 4096. The ReLU is not added after
this convolution. We treat pedestrian verification as a binary
classification problem and use the cross-entropy loss that is
similar to the one in the identification loss, which is
qˆ = softmax(θS ◦ fs), (3)
Verif(f1, f2, s, θS) =
2∑
i=1
−qi log(qˆi). (4)
Here f1, f2 are the two tensors of size 1× 1× 4096. s is the
target class (same/different), θS denotes the parameters of the
added convolutional layer and qˆ is the predicted probability.
If the image pair depicts the same person, q1 = 1, q2 = 0;
otherwise, q1 = 0, q2 = 1.
Departing from [19], we do not use the contrastive loss [24].
On the one hand, the contrastive loss, as a regression loss,
forces the same-class embeddings to be as close as possible.
It may make the model over-fitting because the number of
training of each identity is limited in person re-ID. On the
other hand, dropout [22], which introduces zero values at
random locations, can not be applied on the embedding before
the contrastive loss. But the cross-entropy loss in our model
can work with dropout to regularize the model. In Section
IV, we show that the result using contrastive loss is 4.39%
and 6.55% lower than the one using the cross-entropy loss on
rank-1 accuracy and mAP respectively.
E. Identification vs. Verification
The proposed network is trained to minimize the three
cross-entropy losses jointly. To figure out which objective con-
tributes more, we train the identification model and verification
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
Fig. 4. Barnes-Hut t-SNE visualization [29] of our embedding on a test split (354 identity, 6868 images) of Market1501. Best viewed when zoomed in. We
find the color is the major clue for the person re-identification and our learned embedding is robust to some viewpoint variations.
model separately. Following the learning rate setting in Section
III-F, we train the models until convergence. We also train the
network with the two losses jointly until two objectives both
converge. As the quantitative results shown in Table II, the
fine-tuned CNN model with two kinds of losses outperforms
the one trained individually. This result has been confirmed
on the three different network structures.
Further, we visualize the intermediate feature maps that are
trained using ResNet-50 [28] as the pretrained model and try to
find the differences between identification loss and verification
loss. We select three test images in the Market1501. One
image is considered to be well detected and the other two
images are not well aligned. Given one image as input, we
get its activation in the intermediate layer “res4fx”, the size
of which is 14×14. We visualize the sum of several activation
maps. As shown in Fig. 5, the identification and the verification
networks exhibit different activation patterns to the pedestrian.
We find that if we use only one kind of loss, the network tends
to find one discriminative part. The proposed model takes
advantages of both networks, so the new activation map is
mostly a union of the two individual maps. This also illustrates
the complementary nature of the two baseline networks. The
proposed model makes more neurons activated.
Moreover, as shown in Fig. 4 we visualize the embedding
by plot them to the 2-dimension map. In regard to Fig. 5, we
find the network usually has strong attention on the center part
of the human (usually clothes) and it also illustrates the color
of the clothes is the major clue for the person re-identification.
Fig. 5. Visualization of the activation maps in the ResNet-50 [28] model
trained by the two losses. The identification and the verification networks
exhibit different activation patterns to the pedestrian. The proposed model
takes advantages of both networks and the new activation map is almost a
union of the two individual maps. Our model activates more neurons.
F. Training and Optimization
Input preparation. We resize all the training images to
256×256. The mean image computed from all the training im-
ages is subtracted from all the images. During training, all the
images are randomly cropped to 227× 227 for CaffeNet [26]
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
and mirrored horizontally. For ResNet-50 [28] and VGG16
[27], we randomly crop images to 224× 224. We shuffle the
dataset and use a random order of the images. Then we sample
another image from the same/different class to compose a
positive/negative pair. The initial ratio between negative pairs
and positive pairs is 1 : 1 to alleviate the prediction bias and
we multiple it by a factor of 1.01 every epoch until it reaches
1 : 4, since the number of positive pairs is so limited that the
network risks over-fitting.
Training. We use the Matconvnet [30] package for training
and testing the embedding with CaffeNet [26], VGG16 [27]
and ResNet-50 [28], respectively. The maximum number of
training epochs is set to 75 for ResNet-50, 65 for VGG16net
and 155 for CaffeNet. The batch size (in image pairs) is set to
128 for CaffeNet, 48 for VGG16 and ResNet-50. The learning
rate is initialized as 0.001 and then set to 0.0001 for the final
5 epochs. We adopt the mini-batch stochastic gradient descent
(SGD) to update the parameters of the network. There are
three objectives in our network. Therefore, we first compute
all the gradients produced by every objectives respectively and
add the weighted gradients together to update the network.
We assign a weight of 1 to the gradient produced by the
verification loss and 0.5 for the two gradients produced by two
identification losses. Moreover, we insert the dropout function
[22] before the final convolutional layer.
Testing. We adopt an efficient method to extract features
as well as the activation in the intermediate layer. Because
two CaffeNet share weights, our model has nearly the same
memory consumption with the pretrained model. So we extract
features by only activating one fine-tuned model. Given a
227× 227 image, we feed forward the image to one CaffeNet
in our network and obtain a 4,096-dim pedestrian descriptor f .
Once the descriptors for the gallery sets are obtained, they are
stored offline. Given a query image, its descriptor is extracted
online. We sort the cosine distance between the query and all
the gallery features to obtain the final ranking result. Note that
the cosine distance is equivalent to Euclidean distance when
the feature is L2-normalized.
IV. EXPERIMENTS
We mainly verify the proposed model on two large-scale
datasets Market1501 [3] and CUHK03 [2]. We report the
results trained by three network structures. Besides, we also
report the result on Market1501+500k dataset [3]. Meanwhile,
the proposed architecture is also applied on the image retrieval
task. We modify our model and test it on a popular image
retrieval dataset, i.e.,Oxford Buildings [10]. The performance
is comparable to the state of the art.
A. Dataset
Market1501 [3] contains 32,668 annotated bounding boxes
of 1,501 identities. Images of each identity are captured by at
most six cameras. According to the dataset setting, the training
set contains 12,936 cropped images of 751 identities and
testing set contains 19,732 cropped images of 750 identities
and distractors. They are directly detected by the Deformable
Part Model (DPM) instead of using hand-drawn bboxes, which
Method Single Query Multi. Queryrank-1 mAP rank-1 mAP
BoW + KISSME [3] 44.42 20.76 - -
SL [31] 51.90 26.35 - -
Multiregion CNN [32] 45.58 26.11 56.59 32.26
DADM [33] 39.4 19.6 49.0 25.8
CAN [34] 48.24 24.43 - -
DNS [35] 55.43 29.87 71.56 46.03
Fisher Network [36] 48.15 29.94 - -
S-LSTM [37] - - 61.6 35.3
Gate Reid [6] 65.88 39.55 76.04 48.45
CaffeNet-Basel. [26] 50.89 26.79 59.80 36.50
Ours(CaffeNet) 62.14 39.61 72.21 49.62
VGG16-Basel. [27] 65.02 38.27 74.14 52.25
Ours(VGG16) 70.16 47.45 77.94 57.66
ResNet-50-Basel. [28] 73.69 51.48 81.47 63.95
Ours(ResNet-50) 79.51 59.87 85.84 70.33
TABLE III
COMPARISON WITH THE STATE-OF-THE-ART RESULTS ON THE
MARKET1501 DATASET. WE ALSO PROVIDE THE RESULTS OF THE
FINE-TUNED CNN BASELINE. THE MAP AND RANK-1 PRECISION ARE
LISTED. SQ AND MQ DENOTE SINGLE QUERY AND MULTIPLY QUERIES,
RESPECTIVELY.
is closer to the realistic setting. For each query, we aim to
retrieve the ground truth images from the 19,732 candidate
images.
The searching pool (gallery) is important to person re-
identification. In the realistic setting, the scale of the gallery is
usually large. The distractor dataset of Market1501 provides
extra 500,000 bboxes, consisting of false alarms on the back-
ground as well as the persons not belonging to any of the
original 1,501 identities [3]. When testing, we add the 500k
images to the original gallery, which makes the retrieval more
difficult.
CUHK03 dataset [2] contains 14,097 cropped images of
1,467 identities collected in the CUHK campus. Each identity
is observed by two camera views and has 4.8 images in average
for each view. The Author provides two kinds of bounding
boxes. We evaluate our model on the bounding boxes detected
by DPM, which is closer to the realistic setting. Following the
setting of the dataset, the dataset is partitioned into a training
set of 1,367 persons and a testing set of 100 persons. The
experiment is repeated with 20 random splits. Both the single-
shot and multiple-shot results will be reported.
Oxford5k buildings [10] consists of 5062 images collected
from the internet and corresponding to particular Oxford land-
marks. Some images have complex structures and may contain
other buildings. The images corresponding to 11 Oxford
landmarks are manually annotated and a set of 55 queries for
11 different landmarks are provided. This benchmark contains
many high-resolution images and the mean image size of this
dataset is 851× 921.
We use the rank-1 accuracy and mean average precision
(mAP) for performance evaluation on Market1501 (+100k)
and CUHK03, while on Oxford, we use mAP.
B. Person Re-id Evaluation
Comparison with the CNN baseline. We train the baseline
networks according the conventional fine-tuning method [1],
[8]. The baseline networks are pretrained on ImageNet [23]
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
Method rank-1 rank-5 rank-10 mAP
KISSME [38] 11.7 33.3 48.0 -
DeepReID [2] 19.9 49.3 64.7 -
BoW+HS [3] 24.3 - - -
LOMO+XQDA [39] 46.3 78.9 88.6 -
SI-CI [40] 52.2 84.3 94.8 -
DNS [35] 54.7 80.1 88.3 -
CaffeNet-Basel. 35.8 65.3 77.96 42.6
Ours (CaffeNet) 59.8 88.3 94.2 65.8
VGG16-Basel. 49.1 78.4 87.2 55.7
Ours (VGG16) 71.8 93.0 97.1 76.5
ResNet-50-Basel. 71.5 91.5 95.9 75.8
Ours (ResNet-50) 83.4 97.1 98.7 86.4
TABLE IV
COMPARISON WITH THE STATE-OF-THE-ART RESULTS REPORTED ON THE
CUHK03 DATASET USING THE SINGLE-SHOT SETTING. THE MAP AND
RANK-1 ACCURACY ARE LISTED.
and fine-tuned to predict the person identities. As shown
in Tab. III, we obtain 50.89%, 65.02% and 73.69% rank-1
accuracy by CaffeNet [26], VGG16 [27] and ResNet-50 [28],
respectively on Market1501. Note that using the baseline alone
exceeds many previous works. Our model further improves
these baselines on Market1501. The improvement can be
observed on three network architectures. To be specific, we
obtain 11.25%, 5.14% and 5.82% improvement, respectively,
using CaffeNet [26], VGG16 [27] and ResNet-50 [28] on
Market1501. Similarly, we observe 35.8%, 49.1% and 71.5%
baseline rank-1 accuracy on CUHK03 in single-shot setting.
As show in Tab. IV, these baseline results exceed some
previous works as well. We further get 14.0%, 22.7% and
11.9% improvement on the baseline by our method.
These results show that our method can work with different
networks and improve their results. It indicates that the pro-
posed model helps the network to learn more discriminative
features.
Cross-entropy vs. Contrastive loss. We replace the cross-
entropy loss with the contrastive loss as used in “DeepID
network”. However, we find a 4.39% and 6.55% drop in rank-
1 and mAP. The ResNet-50 model using the contrastive loss
has 75.12% rank-1 accuracy and 53.32% mAP. We speculate
that the contrastive loss tends to over-fit on the re-ID dataset
because no regularization is added to the verification. Cross-
entropy loss designed in our model can work with the dropout
function and avoid the over-fitting.
Comparison with the state of the art. As shown in Table
III, we compare our method with other state-of-the-art algo-
rithms in terms of mean average precision (mAP) and rank-1
accuracy on Market1501. We report the single-query as well
as multiple-query evaluation results. Our model (CaffeNet)
achieves 62.14% rank-1 accuracy and 39.61% mAP, which
is comparable to the state of the art 65.88% rank-1 accuracy
and 39.55% mAP [6]. Our model using ResNet-50 produces
the best performance 79.51% in rank-1 accuracy and 59.87%
in mAP, which outperforms other state-of-the-art algorithms.
For CUHK03, we evaluate our method in the single-shot
setting as shown in Tab. IV. There is only one right image in
the searching pool. In the evaluation, we randomly select 100
images from 100 identities under the other camera as gallery.
The proposed model yields 83.4% rank-1 and 86.4% mAP and
Method rank-1 rank-5 rank-10 mAP
S-LSTM [37] 57.3 80.1 88.3 46.3
Gate-SCNN [6] 68.1 88.1 94.6 58.8
CaffeNet-Basel. 43.3 63.5 76.8 37.2
Ours (CaffeNet) 67.2 86.2 92.3 61.5
VGG16-Basel. 58.8 80.2 87.3 51.0
Ours (VGG16) 78.8 91.8 95.4 73.9
ResNet-50-Basel. 77.1 89.6 93.9 73.1
Ours(ResNet-50) 88.3 95.7 97.8 85.0
TABLE V
COMPARISON WITH THE STATE-OF-THE-ART METHODS ON THE CUHK03
DATASET UNDER THE MULTI-SHOT SETTING. THE MULTI-SHOT SETTING
USES THE ALL IMAGES IN THE OTHER CAMERA AS GALLERY. THE MAP
AND RANK-1 ACCURACY ARE LISTED.
Fig. 6. Re-identification performance between camera pairs on Market1501:
(a) mAP and (b) rank-1 accuracy. Cameras on the vertical and horizontal axis
correspond to the probe and gallery, respectively. The cross-camera average
mAP and average rank-1 accuracy are 48.42% and 54.42%, respectively.
outperforms the state-of-the-art performance.
As shown in Tab. V, we also report the results in the multi-
shot setting, which uses all the images from the other camera
as gallery and the number of the gallery images is about 500.
We think this setting is much closer to image retrieval and
alleviate the unstable effect caused by the random searching
pool under single-shot settings. Fig. 7 presents some re-ID
samples on CUHK03 dataset. The images in the first column
are the query images. The retrieval images are sorted according
to the similarity scores from left to right. Most ground-truth
candidate images are correctly retrieved. Although the model
retrieves some incorrect candidates on the third row, we find
it is a reasonable prediction since the man with red hat and
blue coat is similar to the query. The proposed model yields
88.3% rank-1 and 85.0% mAP and also outperforms the state-
of-the-art performance in the multi-shot setting.
Results between camera pairs. CUHK03 [2] only contains
two camera views. So this experiment is evaluated on Mar-
ket1501 [3] since it contains six different cameras. We provide
the re-identification results between all camera pairs in Fig. 6.
Although camera-6 is a 720× 576 low-resolution camera and
captures distinct background with the other HD cameras, the
re-ID accuracy between camera 6 and the others is relatively
high. We also compute the cross-camera average mAP and
average rank-1 accuracy: 48.42% and 54.42% respectively.
Comparing to the previous reported results, i.e.,10.51% and
13.72% in [3], our method largely improves the performance
and observes a smaller standard deviation between cameras.
It suggests that the discriminatively learned embedding works
under different viewpoints.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
Fig. 7. Pedestrian retrieval samples on CUHK03 dataset [2] in the multi-shot
setting. The images in the first column are the query images. The retrieval
images are sorted according to the similarity scores from left to right.
Further, Fig. 4 shows the Barnes-Hut t-SNE visualization
[29] on the learned embeddings of our model. By the cluster-
ing algorithm, the persons wearing the similar-color clothes are
quit clustered together and are apart from other persons. The
learned pedestrian descriptor pay more attention to the color
and it is robust to some illusion and viewpoint variations. In
realistic setting, we think color provides the most important
information to figure out the person.
Large-scale experiments. The Market1501 dataset also
provides an additional distractor set with 500k images to
enlarge the gallery. In general, more candidate images may
confuse the image retrieval. The re-ID performance of our
model (ResNet) on the large-scale dataset is presented in Tab.
VI. As the searching pool gets larger, the accuracy drops.
With the gallery size of 500, 000 + 19, 732, we still achieve
68.26% rank1 accuracy and 45.24% mAP. A relative drop
24.4% from 59.87% to 45.24% on mAP is observed, compared
to a relative drop 37.88% from 13.94% to 8.66% in our
previous work [3]. Besides, we also compare our result with
the performance of the ResNet Baseline. As shown in Fig. 8,
it is interesting that the re-ID precision of our model decreases
more quickly comparing to the baseline model. We speculate
that the Market1501 training set is relatively small in covering
the pedestrian variations encountered in a much larger test set.
In fact, the 500k dataset was collected in a different time (the
same location) with the Market1501 dataset, so the transfer
effect is large enough that the learned embedding is inferior
to the baseline on the scale of 500 k images. In the future, we
will look into this interesting problem and design more robust
Method Gallery size 19,732 119,732 219,732 519,732
ResNet Basel. rank-1 73.69 72.15 71.55 70.67mAP 51.48 48.72 47.57 46.05
Ours (ResNet) rank-1 79.51 73.78 71.50 68.26mAP 59.87 52.28 49.11 45.24
TABLE VI
IMPACT OF DATA SIZE ON MARKET1501+500K DATASET. AS THE
DATASET GETS LARGER, THE ACCURACY DROPS.
Dataset Size(K)
100 200 300 400 500
m
AP
 (%
)
0
10
20
30
40
50
60
Ours (ResNet-50)
ResNet-Basel.
BoW-Basel.
Fig. 8. Impact of data size on Market1501+500K dataset. As the dataset gets
larger, the accuracy drops.
descriptors for the transfer dataset.
C. Instance Retrieval
We apply the identification-verification model to the generic
image retrieval task. Oxford5k [10] is a testing dataset contain-
ing buildings in the Oxford University. We train the network
on another scene dataset proposed in [41], which comprises of
a number of buildings without overlapping with the Oxford5k.
Similarly, the model is trained to not only tell which building
the image depicts but also determine whether the two input
images are from the same architecture. The training data is
high-resolution. In order to obtain more information from the
high-resolution building images, we modify the final pooling
layer of our model to a MAC layer [42], which outputs the
maximum value over the whole activation map. This layer
helps us to handle large images without resizing them to a
fixed size and output a fixed-dimension feature to retrieve the
images. During training, the input image is randomly cropped
to 320×320 from 362×362 and mirrored horizontally. During
testing, we keep the original size of the images that are not
cropped or resized and extract the feature.
In Table VII, many previous works are based on CaffeNet
or VGG16. For fair comparison, we report the baseline results
and the results of our model based on these two network
structures, respectively. Our model which uses CaffeNet as
pretrained model outperforms the state of the art. Meanwhile,
the model using VGG16 is comparable to the state-of-the-arts
methods. The proposed method show a 6.0% and 6.6% im-
provement over the baseline networks CaffeNet and VGG16,
respectively. We visualize some retrieval results in Fig. 9. The
images in the first column are the query images. The retrieval
images are sorted according to the similarity scores from left
to right. The main difficulty in the image retrieval is various
object sizes in the image. In the first row, we use the roof (part
of the building) to retrieve the images and the top five images
are correct candidate images. The other retrieval samples also
show our model is robust to the scale variations.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
Method CaffeNet mAP VGG16 mAP
mVoc/BoW [43] 48.8 -
CroW [44] - 68.2
Neural codes [45] 55.7 -
R-MAC [42] 56.1 66.9
R-MAC-Hard [41] 62.5 77.0
MAC-Hard(V) [41] 62.2 79.7
Finetuned-Baseline 60.2 69.8
Ours 66.2 76.4
TABLE VII
COMPARISON OF STATE-OF-THE-ART RESULTS ON THE OXFORD5K
DATASET. THE MAP IS LISTED. RESULTS REPORTED WITH THE USE OF
ALEXNET [26] OR VGGNET [27] ARE MARKED BY (A) OR (V)
RESPECTIVELY.
Fig. 9. Example retrieval results on Oxford5k dataset [10] using the proposed
embedding. The images in the first column are the query images. The retrieval
images are sorted according to the similarity scores from left to right. The
query images are usually from the part of the architectures.
V. CONCLUSION
In this work, we propose a siamese network that simulta-
neously considers the identification loss and the verification
loss. The proposed model learns a discriminative embedding
and a similarity measurement at the same time. It outperforms
the state of the art on two popular person re-ID benchmarks
and shows potential ability to apply on the generic instance
retrieval task.
Future work includes exploring more novel applications of
the proposed method, such as car recognition and fine-grained
classification. Besides, we will investigate how to learn a
robust descriptor to further improve the performance of the
person re-identification on large-scale testing set.
REFERENCES
[1] L. Zheng, Y. Yang, and A. G. Hauptmann, “Person re-identification:
Past, present and future,” arXiv preprint arXiv:1610.02984, 2016.
[2] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep filter pairing
neural network for person re-identification,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2014, pp. 152–
159.
[3] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable
person re-identification: A benchmark,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 1116–1124.
[4] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Deep metric learning for person re-
identification,” in Pattern Recognition (ICPR), 2014 22nd International
Conference on. IEEE, 2014, pp. 34–39.
[5] L. Wu, C. Shen, and A. v. d. Hengel, “Personnet: Person re-
identification with deep convolutional neural networks,” arXiv preprint
arXiv:1601.07255, 2016.
[6] R. R. Varior, M. Haloi, and G. Wang, “Gated siamese convolutional
neural network architecture for human re-identification,” in European
Conference on Computer Vision. Springer, 2016, pp. 791–808.
[7] T. Xiao, H. Li, W. Ouyang, and X. Wang, “Learning deep feature
representations with domain guided dropout for person re-identification,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 1249–1258.
[8] L. Zheng, Y. Yang, and A. G. Hauptmann, “Person re-identification:
Past, present and future,” arXiv preprint arXiv:1610.02984, 2016.
[9] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “End-to-end deep learning
for person search,” arXiv preprint arXiv:1604.01850, 2016.
[10] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object
retrieval with large vocabularies and fast spatial matching,” in 2007 IEEE
Conference on Computer Vision and Pattern Recognition. IEEE, 2007,
pp. 1–8.
[11] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,
E. Sa¨ckinger, and R. Shah, “Signature verification using a siamese time
delay neural network,” International Journal of Pattern Recognition and
Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.
[12] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, “Person re-
identification by multi-channel parts-based cnn with improved triplet
loss function,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 1335–1344.
[13] E. Ahmed, M. Jones, and T. K. Marks, “An improved deep learning
architecture for person re-identification,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2015, pp.
3908–3916.
[14] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[15] S. Ding, L. Lin, G. Wang, and H. Chao, “Deep feature learning
with relative distance comparison for person re-identification,” Pattern
Recognition, vol. 48, no. 10, pp. 2993–3003, 2015.
[16] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, “Mars:
A video benchmark for large-scale person re-identification,” in European
Conference on Computer Vision. Springer, 2016, pp. 868–884.
[17] S. Wu, Y.-C. Chen, X. Li, A.-C. Wu, J.-J. You, and W.-S. Zheng,
“An enhanced deep feature representation for person re-identification,”
in 2016 IEEE Winter Conference on Applications of Computer Vision
(WACV). IEEE, 2016, pp. 1–8.
[18] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1440–1448.
[19] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face rep-
resentation by joint identification-verification,” in Advances in neural
information processing systems, 2014, pp. 1988–1996.
[20] Y. Sun, X. Wang, and X. Tang, “Deeply learned face representations are
sparse, selective, and robust,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015, pp. 2892–2900.
[21] Y. Sun, D. Liang, X. Wang, and X. Tang, “Deepid3: Face recognition
with very deep neural networks,” arXiv preprint arXiv:1502.00873,
2015.
[22] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: a simple way to prevent neural networks
from overfitting.” Journal of Machine Learning Research, vol. 15, no. 1,
pp. 1929–1958, 2014.
[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International Journal of Computer
Vision, vol. 115, no. 3, pp. 211–252, 2015.
[24] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by
learning an invariant mapping,” in Computer vision and pattern recog-
nition, 2006 IEEE computer society conference on, vol. 2. IEEE, 2006,
pp. 1735–1742.
[25] W. Liu, Y. Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for
convolutional neural networks,” in Proceedings of The 33rd International
Conference on Machine Learning, 2016, pp. 507–516.
[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
[29] L. Van Der Maaten, “Accelerating t-sne using tree-based algorithms.”
Journal of machine learning research, vol. 15, no. 1, pp. 3221–3245,
2014.
[30] A. Vedaldi and K. Lenc, “Matconvnet – convolutional neural networks
for matlab,” in Proceeding of the ACM Int. Conf. on Multimedia, 2015.
[31] D. Chen, Z. Yuan, B. Chen, and N. Zheng, “Similarity learning with
spatial constraints for person re-identification,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016,
pp. 1268–1277.
[32] E. Ustinova, Y. Ganin, and V. Lempitsky, “Multiregion bilinear con-
volutional neural networks for person re-identification,” arXiv preprint
arXiv:1512.05300, 2015.
[33] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Deep attributes driven
multi-camera person re-identification,” arXiv preprint arXiv:1605.03259,
2016.
[34] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, “End-to-end compar-
ative attention networks for person re-identification,” arXiv preprint
arXiv:1606.04404, 2016.
[35] L. Zhang, T. Xiang, and S. Gong, “Learning a discriminative null space
for person re-identification,” arXiv preprint arXiv:1603.02139, 2016.
[36] L. Wu, C. Shen, and A. v. d. Hengel, “Deep linear discriminant analysis
on fisher networks: A hybrid architecture for person re-identification,”
arXiv preprint arXiv:1606.01595, 2016.
[37] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang, “A siamese
long short-term memory architecture for human re-identification,” in
European Conference on Computer Vision. Springer, 2016, pp. 135–
153.
[38] M. Ko¨stinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof, “Large
scale metric learning from equivalence constraints,” in Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE,
2012, pp. 2288–2295.
[39] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identification by local
maximal occurrence representation and metric learning,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2015, pp. 2197–2206.
[40] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, “Joint learn-
ing of single-image and cross-image representations for person re-
identification,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 1288–1296.
[41] F. Radenovic´, G. Tolias, and O. Chum, “Cnn image retrieval learns
from bow: Unsupervised fine-tuning with hard examples,” arXiv preprint
arXiv:1604.02426, 2016.
[42] G. Tolias, R. Sicre, and H. Je´gou, “Particular object retrieval with inte-
gral max-pooling of cnn activations,” arXiv preprint arXiv:1511.05879,
2015.
[43] F. Radenovic´, H. Je´gou, and O. Chum, “Multiple measurements and
joint dimensionality reduction for large scale image search with short
vectors,” in Proceedings of the 5th ACM on International Conference
on Multimedia Retrieval. ACM, 2015, pp. 587–590.
[44] Y. Kalantidis, C. Mellina, and S. Osindero, “Cross-dimensional weight-
ing for aggregated deep convolutional features,” in European Conference
on Computer Vision. Springer, 2016, pp. 685–701.
[45] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural
codes for image retrieval,” in European Conference on Computer Vision.
Springer, 2014, pp. 584–599.
"
34,"Single Shot Temporal Action Detection
Tianwei Lin1, Xu Zhao1,3,*, Zheng Shou2
1Department of Automation, Shanghai Jiao Tong University, China. 2Columbia University, USA
3Cooperative Medianet Innovation Center (CMIC), Shanghai Jiao Tong University, China
{wzmsltw,zhaoxu}@sjtu.edu.cn,zs2262@columbia.edu
ABSTRACT
Temporal action detection is a very important yet challenging prob-
lem, since videos in real applications are usually long, untrimmed
and contain multiple action instances. This problem requires not
only recognizing action categories but also detecting start time and
end time of each action instance. Many state-of-the-art methods
adopt the ""detection by classication"" framework: rst do proposal,
and then classify proposals. The main drawback of this framework
is that the boundaries of action instance proposals have been xed
during the classication step. To address this issue, we propose
a novel Single Shot Action Detector (SSAD) network based on
1D temporal convolutional layers to skip the proposal generation
step via directly detecting action instances in untrimmed video.
On pursuit of designing a particular SSAD network that can work
eectively for temporal action detection, we empirically search
for the best network architecture of SSAD due to lacking exist-
ing models that can be directly adopted. Moreover, we investigate
into input feature types and fusion strategies to further improve
detection accuracy. We conduct extensive experiments on two chal-
lenging datasets: THUMOS 2014 and MEXaction2. When setting
Intersection-over-Union threshold to 0.5 during evaluation, SSAD
signicantly outperforms other state-of-the-art systems by increas-
ing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to
11.0% on MEXaction2.
CCS CONCEPTS
• Computing methodologies → Activity recognition and un-
derstanding;
KEYWORDS
Temporal Action Detection, Untrimmed Video, SSAD network
1 INTRODUCTION
Due to the continuously booming of videos on the internet, video
content analysis has attracted wide attention from both industry
and academic eld in recently years. An important branch of video
content analysis is action recognition, which usually aims at classi-
fying the categories of manually trimmed video clips. Substantial
This research has been supported by the funding from NSFC (61673269, 61273285) and
the Cooperative Medianet Innovation Center (CMIC). * Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’17, Mountain View, CA, USA
© 2017 ACM. 978-1-4503-4906-2/17/10. . . $15.00
DOI: 10.1145/3123266.3123343
Figure 1: Overview of our system. Given an untrimmed long
video, (1) we extract Snippet-level Action Score features se-
quence with multiple action classiers; (2) SSAD network
takes feature sequence as input and directly predicts multi-
ple scales action instanceswithout proposal generation step.
progress has been reported for this task in [6, 24, 36, 38, 40]. How-
ever, most videos in real world are untrimmed and may contain
multiple action instances with irrelevant background scenes or ac-
tivities. This problem motivates the academic community to put
attention to another challenging task - temporal action detection.
This task aims to detect action instances in untrimmed video, in-
cluding temporal boundaries and categories of instances. Methods
proposed for this task can be used in many areas such as surveil-
lance video analysis and intelligent home care.
Temporal action detection can be regarded as a temporal version
of object detection in image, since both of the tasks aim to deter-
mine the boundaries and categories of multiple instances (actions
in time/ objects in space). A popular series of models in object
detection are R-CNN and its variants [8, 9, 27], which adopt the
""detect by classifying region proposals"" framework. Inspired by
R-CNN, recently many temporal action detection approaches adopt
similar framework and classify temporal action instances generated
by proposal method [3, 5, 29, 43] or simple sliding windows method
[15, 23, 39]. This framework may has some major drawbacks: (1)
proposal generation and classication procedures are separate and
have to be trained separately, but ideally we want to train them in
a joint manner to obtain an optimal model; (2) the proposal genera-
tion method or sliding windows method requires additional time
consumption; (3) the temporal boundaries of action instances gen-
erated by the sliding windows method are usually approximative
rather than precise and left to be xed during classication. Also,
since the scales of sliding windows are pre-determined, it is not
exible to predict instances with various scales.
ar
X
iv
:1
71
0.
06
23
6v
1 
 [c
s.C
V]
  1
7 O
ct 
20
17
To address these issues, we propose the Single Shot Action De-
tector (SSAD) network, which is a temporal convolutional network
conducted on feature sequence with multiple granularities. Inspired
by another set of object detection methods - single shot detection
models such as SSD [20] and YOLO [25, 26], our SSAD network
skips the proposal generation step and directly predicts temporal
boundaries and condence scores for multiple action categories,
as shown in Figure 1. SSAD network contains three sub-modules:
(1) base layers read in feature sequence and shorten its temporal
length; (2) anchor layers output temporal feature maps, which are
associated with anchor action instances; (3) prediction layers gen-
erate categories probabilities, location osets and overlap scores of
these anchor action instances.
For better encoding of both spatial and temporal information in
video, we adopt multiple action recognition models (action classi-
ers) to extract multiple granularities features. We concatenate the
output categories probabilities from all action classiers in snippet-
level and form the Snippet-level Action Score (SAS) feature. The
sequences of SAS features are used as input of SSAD network.
Note that it is non-trivial to adapt the single shot detection model
from object detection to temporal action detection. Firstly, unlike
VGGNet [31] being used in 2D ConvNet models, there is no existing
widely used pre-trained temporal convolutional network. Thus in
this work, we search multiple network architectures to nd the
best one. Secondly, we integrate key advantages in dierent single
shot detection models to make our SSAD network work the best.
On one hand, similar to YOLO9000 [26], we simultaneously predict
location osets, categories probabilities and overlap score of each
anchor action instance. On the other hand, like SSD [20], we use
anchor instances of multiple scale ratios from multiple scales feature
maps, which allow network exible to handle action instance with
various scales. Finally, to further improve performance, we fuse
the prediction categories probability with temporal pooled snippet-
level action scores during prediction.
The main contributions of our work are summarized as follows:
(1) To the best of our knowledge, our work is the rst Single Shot
Action Detector (SSAD) for video, which can eectively predict both
the boundaries and condence score of multiple action categories
in untrimmed video without the proposal generation step.
(2) In this work, we explore many congurations of SSAD net-
work such as input features type, network architectures and post-
processing strategy. Proper congurations are adopted to achieve
better performance for temporal action detection task.
(3) We conduct extensive experiments on two challenging bench-
mark datasets: THUMOS’14 [14] and MEXaction2 [1]. When setting
Intersection-over-Union threshold to 0.5 during evaluation, SSAD
signicantly outperforms other state-of-the-art systems by increas-
ing mAP from 19.0% to 24.6% on THUMOS’14 and from 7.4% to
11.0% on MEXaction2.
2 RELATEDWORK
Action recognition. Action recognition is an important research
topic for video content analysis. Just as image classication network
can be used in image object detection, action recognition models
can be used in temporal action detection for feature extraction. We
mainly review the following methods which can be used in temporal
action detection. Improved Dense Trajectory (iDT) [37, 38] feature
is consisted of MBH, HOF and HOG features extracted along dense
trajectories. iDT method uses SIFT and optical ow to eliminate
the inuence of camera motion. Two-stream network [6, 30, 40]
learns both spatial and temporal features by operating network
on single frame and stacked optical ow eld respectively using
2D Convolutional Neural Network (CNN) such as GoogleNet [35],
VGGNet [31] and ResNet [12]. C3D network [36] uses 3D convolu-
tion to capture both spatial and temporal information directly from
raw video frames volume, and is very ecient. Feature encoding
methods such as Fisher Vector [38] and VAE [24] are widely used
in action recognition task to improve performance. And there are
many widely used action recognition benchmark such as UCF101
[34], HMDB51 [18] and Sports-1M [16].
Temporal action detection. This task focuses on learning how
to detect action instances in untrimmed videos where the bound-
aries and categories of action instances have been annotated. Typi-
cal datasets such as THUMOS 2014 [14] and MEXaction2 [1] include
large amount of untrimmed videos with multiple action categories
and complex background information.
Recently, many approaches adopt ""detection by classication""
framework. For examples, many approaches [15, 23, 33, 39, 41]
use extracted feature such as iDT feature to train SVM classiers,
and then classify the categories of segment proposals or sliding
windows using SVM classiers. And there are some approaches
specially proposed for temporal action proposal [3, 5, 7, 22, 43]. Our
SSAD network diers from these methods mainly in containing no
proposal generation step.
Recurrent Neural Network (RNN) is widely used in many action
detection approaches [21, 32, 42, 44] to encode feature sequence
and make per-frame prediction of action categories. However, it is
dicult for RNNs to keep a long time period memory in practice
[32]. An alternative choice is temporal convolution. For example,
Lea et al. [19] proposes Temporal Convolutional Networks (TCN)
for temporal action segmentation. We also adopt temporal convo-
lutional layers, which makes our SSAD network can handle action
instances with a much longer time period.
Object detection.Deep learning approaches have shown salient
performance in object detection. We will review two main set of ob-
ject detection methods proposed in recent years. The representative
methods in rst set are R-CNN [9] and its variations [8, 27]. R-CNN
uses selective search to generate multiple region proposals then
apply CNN in these proposals separately to classify their categories;
Fast R-CNN [8] uses a 2D RoI pooling layer which makes feature
map be shared among proposals and reduces the time consump-
tion. Faster RCNN [27] adopts a RPN network to generate region
proposal instead of selective search.
Another set of object detection methods are single shot detection
methods, which means detecting objects directly without generat-
ing proposals. There are two well known models. YOLO [25, 26]
uses the whole topmost feature map to predict probabilities of mul-
tiple categories and corresponding condence scores and location
osets. SSD [20] makes prediction from multiple feature map with
multiple scales default boxes. In our work, we combine the charac-
teristics of these single shot detection methods and embed them
into the proposed SSAD network.
Figure 2: The framework of our approach. (a) Multiple action classiers are used to extract Snippet-level Action Scores (SAS)
feature. (b) The architecture of SSAD network: base layers are used to reduce the temporal dimension of input data; anchor
layers output multiple scale feature map associated with anchor instances and prediction layers are used for predicting cat-
egories, location and condence of anchor instances. (c) The training and prediction procedures: during training, we match
anchor instances with ground truth instances and calculate loss function for optimization. During prediction, post-processing
and NMS procedure are conducted on anchor instances to make nal prediction.
3 OUR APPROACH
In this section, we will introduce our approach in details. The frame-
work of our approach is shown in Figure 2.
3.1 Problem Denition
We denote a video as Xv = {xt }Tvt=1 where Tv is the number of
frames in Xv and xt is the t-th frame in Xv . Each untrimmed
video Xv is annotated with a set of temporal action instances
Φv =
{
ϕn =
(
φn ,φ
′
n ,kn
)}Nv
n=1, where Nv is the number of temporal
action instances inXv , andφn ,φ ′n ,kn are starting time, ending time
and category of action instance ϕn respectively. kn ∈ {1, ...,K}
where K is the number of action categories. Φv is given during
training procedure and need to be predicted during prediction pro-
cedure.
3.2 Extracting of Snippet-level Action Scores
To apply SSAD model, rst we need to make snippet-level action
classication and get Snippet-level Action Score (SAS) features.
Given a video Xv , a snippet st = (xt , Ft ,Xt ) is composed by three
parts: xt is the t-th frame in Xv , Ft = { ft ′}t+5t ′=t−4 is stacked optical
ow eld derived around xt and Xt = {xt ′}t+8t ′=t−7 is video frames
volume. So given a video Xv , we can get a sequence of snippets
Sv = {st }Tvt=1. We pad the video Xv in head and tail with rst and
last frame separately to make Sv have the same length as Xv .
Action classier. To evaluate categories probability of each
snippet, we use multiple action classiers with commendable per-
formance in action recognition task: two-stream network [30] and
C3D network [36]. Two-stream network includes spatial and tem-
poral networks which operate on single video frame xt and stacked
optical ow eld Ft respectively. We use the same two-stream net-
work architecture as described in [40], which adopts VGGNet-16
network architecture. C3D network is proposed in [36], including
multiple 3D convolution layers and 3D pooling layers. C3D network
operates on short video frames volume Xt with length l , where l is
the length of video clip and is set to 16 in C3D. So there are totally
three individual action classiers, in which spatial network mea-
sures the spatial information, temporal network measures temporal
consistency and C3D network measures both. In section 4.3, we
evaluate the eect of each action classier and their combinations.
SAS feature. As shown in Figure 2(a), given a snippet st , each
action classier can generate a score vector pt with length K ′ =
K + 1, where K ′ includes K action categories and one background
category. Then we concatenate output scores of each classiers
to form the Snippet-level Action Score (SAS) feature psas,t =(
pS ,t ,pT ,t ,pC ,t
)
, where pS ,t , pT ,t , pC ,t are output score of spa-
tial, temporal and C3D network separately. So given a snippets
sequence Sv with lengthTv , we can extract a SAS feature sequence
Pv =
{
psas,t
}Tv
t=1. Since the number of frames in video is uncertain
and may be very large, we use a large observation window with
lengthTw to truncate the feature sequence. We denote a window as
ω =
{
φω ,φ
′
ω , Pω ,Φω
}
, where φω and φ ′ω are starting and ending
time of ω, Pω and Φω are SAS feature sequence and corresponding
ground truth action instances separately.
3.3 SSAD Network
Temporal action detection is quite dierent from object detection
in 2D image. In SSAD we adopt two main characteristics from
single shot object detection models such as SSD [20] and YOLO
[25, 26]: 1) unlike ""detection by classication"" approaches, SSAD
directly predicts categories and location osets of action instances
in untrimmed video using convolutional prediction layers; 2) SSAD
combine temporal feature maps from dierent convolution layers
for prediction, making it possible to handle action instances with
various length. We rst introduce the network architecture.
Network architecture. The architecture of SSAD network is
presented in Figure 2(b), which mainly contains three sub-modules:
base layers, anchor layers and prediction layers. Base layers handle
the input SAS feature sequence, and use both convolution and
pooling layer to shorten the temporal length of feature map and
increase the size of receptive elds. Then anchor layers use temporal
convolution to continually shorten the feature map and output
anchor feature map for action instances prediction. Each cell of
anchor layers is associated with anchor instances of multiple scales.
Finally, we use prediction layers to get classication score, overlap
score and location osets of each anchor instance.
In SSAD network, we adopt 1D temporal convolution and pool-
ing to capture temporal information. We conduct Rectied Linear
Units (ReLu) activation function [11] to output temporal feature
map except for the convolutional prediction layers. And we adopt
temporal max pooling since max pooling can enhance the invari-
ance of small input change.
Base layers. Since there are no widely used pre-trained 1D
ConvNet models such as the VGGNet [31] used in 2D ConvNet
models, we search many dierent network architectures for SSAD
network. These architectures only dier in base layers while we
keep same architecture of anchor layers and prediction layers. As
shown in Figure 3, we totally design 5 architectures of base layers. In
these architectures, we mainly explore three aspects: 1) whether use
convolution or pooling layer to shorten the temporal dimension and
increase the size of receptive elds; 2) number of layers of network
and 3) size of convolution layer’s kernel. Notice that we set the
number of convolutional lter in all base layers to 256. Evaluation
results of these architectures are shown in section 4.3, and nally
we adopt architecture B which achieves the best performance.
Multi-scale anchor layers. After processing SAS feature se-
quence using base layers, we stack three anchor convolutional
layers (Conv-A1, Conv-A2 and Conv-A3) on them. These layers
have same conguration: kernel size 3, stride size 2 and 512 convo-
lutional lters. The output anchor feature maps of anchor layers
are fA1, fA2 and fA3 with size (Tw /32 × 512), (Tw /64 × 512) and
(Tw /128 × 512) separately. Multiple anchor layers decrease tempo-
ral dimension of feature map progressively and allow SSAD get
predictions from multiple resolution feature map.
Figure 3:Multiple architectures of base layers. Input and out-
put sizes are same for each architecture. Parameter of layer
is shown with the format of kernel/stride. All convolutional
layers have 512 convolutional lters. Evaluation results of
these architectures are shown in section 4.3, and we adopt
architecture B which achieves the best performance.
For each temporal feature map of anchor layers, we associate a
set of multiple scale anchor action instances with each feature map
cell as shown in Figure 4. For each anchor instance, we use con-
volutional prediction layers to predict overlap score, classication
score and location osets, which will be introduced later.
In term of the details of multi-scale anchor instances, the lower
anchor feature map has higher resolution and smaller receptive
eld than the top anchor feature map. So we let the lower anchor
layers detect short action instances and the top anchor layers detect
long action instances. For a temporal feature map f of anchor layer
with length M , we dene base scale sf = 1M and a set of scale
ratios Rf = {rd }Dfd=1, where Df is the number of scale ratios. We
use {1, 1.5, 2} for fA1 and {0.5, 0.75, 1, 1.5, 2} for fA2 and fA3. For
each ratio rd , we calculate µw = sf · rd as anchor instance’s default
width. And all anchor instances associated with them-th feature
map cell share the same default center location µc = m+0.5M . So for
an anchor feature map f with length Mf and Df scale ratios, the
number of associated anchor instances is Mf · Df .
Prediction layers. We use a set of convolutional lters to pre-
dict classication scores, overlap scores and location osets of
anchor instances associated with each feature map cell. As shown
in Figure 4, for an anchor feature map f with length Mf and Df
scale ratios, we use Df · (K ′ + 3) temporal convolutional lters
with kernel size 3, stride size 1 for prediction. The output of pre-
diction layer has size
(
Mf ×
(
Df · (K ′ + 3)
))
and can be reshaped
into
((
Mf · Df
)
× (K ′ + 3)
)
. Each anchor instance gets a prediction
score vector ppr ed =
(
pclass ,pover ,∆c,∆w
)
with length (K ′ + 3),
where pclass is classication score vector with length K ′, pover is
overlap score and ∆c , ∆w are location osets. Classication score
pclass is used to predict anchor instance’s category. Overlap score
pover is used to estimate the overlap between anchor instance and
ground truth instances and should have value between [0, 1], so it
is normalized by using sigmoid function:
p′over = siдmoid(pover ). (1)
Figure 4: Anchor instances and prediction layer in temporal
feature map. In feature map of a anchor layer, we associate
a set of multiple scale anchor instances with each feature
map cell. We use convolutional prediction layer to predict
location oset, condence and classication scores simulta-
neously for each anchor instance.
And location osets ∆c , ∆w are used for adjusting the default
location of anchor instance. The adjusted location is dened as:
φc = µc + α1 · µw · ∆c
φw = µw · exp(α2 · ∆w), (2)
where φc and φw are center location and width of anchor instance
respectively. α1 and α2 are used for controlling the eect of location
osets to make prediction stable. We set both α1 and α2 to 0.1. The
starting and ending time of action instance are φ = φc − 12 ·φw and
φ ′ = φc + 12 ·φw respectively. So for a anchor feature map f , we can
get a anchor instances set Φf =
{
ϕn =
(
φc ,φw ,pclass ,p
′
over
)}Nf
n=1,
where Nf = Mf · Df is the number of anchor instances. And the
total prediction instances set is Φp =
{
ΦfA1 ,ΦfA2 ,ΦfA3
}
.
3.4 Training of SSAD network
Training data construction. As described in Section 3.2, for an
untrimmed video Xv with lengthTv , we get SAS features sequence
Pv with same length. Then we slide window of lengthTw in feature
sequence with 75% overlap. The overlap of sliding window is aim
to handle the situation where action instances locate in boundary
of window and also used to increase the amount of training data.
During training, we only keep windows containing at least one
ground-truth instance. So given a set of untrimmed training videos,
we get a training set Ω = {ωn }Nωn=1, where Nω is the number of
windows. We randomly shue the data order in training set to
make the network converge faster, where same random seed is
used during evaluation.
Label assignment. During training, given a window ω, we can
get prediction instances setΦp via SSAD network. We need to match
them with ground truth set Φω for label assignment. For an anchor
instance ϕn in Φp , we calculate it’s IoU overlap with all ground
truth instances in Φω . If the highest IoU overlap is higher than 0.5,
we match ϕn with corresponding ground truth instance ϕд and
regard it as positive, otherwise negative. We expand ϕn with match-
ing information as ϕ ′n =
(
φc ,φw ,pclass ,p
′
over ,kд ,дiou ,дc ,дw
)
,
where kд is the category of ϕд and is set to 0 for negative instance,
дiou is the IoU overlap between ϕn and ϕд , дc and дw are center
location and width of ϕд respectively. So a ground truth instance
can match multiple anchor instances while a anchor instance can
only match one ground truth instance at most.
Hard negative mining. During label assignment, only a small
part of anchor instances match the ground truth instances, causing
an imbalanced data ratio between the positive and negative in-
stances. Thus we adopt the hard negative mining strategy to reduce
the number of negative instances. Here, the hard negative instances
are dened as negative instances with larger overlap score than
0.5. We take all hard negative instances and randomly sampled
negative instances in remaining part to make the ratio between
positive and negative instances be nearly 1:1. This ratio is chosen
by empirical validation. So after label assignment and hard negative
mining, we get Φ′p =
{
ϕ ′n
}Ntrain
n=1 as the input set during training,
where Ntrain is the number of total training instances and is the
sum of the number of positives Npos and negatives Nneд .
Objective for training. The training objective of the SSAD net-
work is to solve a multi-task optimization problem. The overall
loss function is a weighted sum of the classication loss (class), the
overlap loss (conf), the detection loss (loc) and L2 loss for regular-
ization:
L = Lclass + α · Lover + β · Lloc + λ · L2(Θ), (3)
where α , β and λ are the weight terms used for balancing each part
of loss function. Both α and β are set to 10 and λ is set to 0.0001 by
empirical validation. For the classication loss, we use conventional
softmax loss over multiple categories, which is eective for training
classication model and can be dened as:
Lclass = Lsof tmax =
1
Ntrain
Ntrain∑
i=1
(−loд(P (kд )i )), (4)
where P (kд )i =
exp(p(kд )class,i )∑
j exp(p
(kj )
class,i )
and kд is the label of this instance.
Lover is used to make a precise prediction of anchor instances’
overlap IoU score, which helps the procedure of NMS. The overlap
loss adopts the mean square error (MSE) loss and be dened as:
Lover =
1
Ntrain
Ntrain∑
i=1
(p′over,i − дiou,i ). (5)
Lloc is the Smooth L1 loss [8] for location osets. We regress the
center (ϕc ) and width (ϕw ) of predicted instance:
Lloc =
1
Npos
Npos∑
i=1
(SL1(ϕc,i − дc,i ) + SL1(ϕw,i − дw,i )), (6)
where дc,i and дw,i is the center location and width of ground truth
instance. L2(Θ) is the L2 regularization loss where Θ stands for the
parameter of the whole SSAD network.
3.5 Prediction and post-processing
During prediction, we follow the aforementioned data preparation
method during the training procedure to prepare test data, with the
following two changes: (1) the overlap ratio of window is reduced
to 25% to increase the prediction speed and reduce the redundant
predictions; (2) instead of removing windows without annotation,
we keep all windows during prediction because the removing oper-
ation is actually a leak of annotation information. If the length of
input video is shorter than Tw , we will pad SAS feature sequence
to Tw so that there is at least one window for prediction. Given a
video Xv , we can get a set of Ω = {ωn }Nωn=1. Then we use SSAD
network to get prediction anchors of each window and merge these
prediction as Φp = {ϕn }Npn=1, where Np is the number of prediction
instances. For a prediction anchor instance ϕn in Φp , we calcu-
late the mean Snippet-level Action Score p¯sas among the temporal
range of instance and multiple action classiers.
p¯sas =
1
3 · (φ ′ − φ)
φ ′∑
t=φ
(
pS,t + pT ,t + pC,t
)
, (7)
where φ and φ ′ are starting and ending time of prediction anchor
instance respectively. Then we fuse categories scores p¯sas and
pclass with multiplication factor pconf and get the pf inal :
pf inal = p
′
over · (pclass + p¯sas ) . (8)
We choose the maximum dimension kp in pf inal as the category
of ϕn and corresponding score pconf as the condence score. We
expand ϕn as ϕ ′n =
{
φc ,φw ,pconf ,kp
}
and get prediction set Φ′p ={
ϕ ′n
}Np
n=1. Then we conduct non-maximum suppress (NMS) in these
prediction results to remove redundant predictions with condence
scorepconf and get the nal prediction instances setΦ′′p =
{
ϕ ′n
}Np′
n=1,
where Np′ is the number of the nal prediction anchors. Since there
are little overlap between action instances of same category in
temporal action detection task, we take a strict threshold in NMS,
which is set to 0.1 by empirical validation.
4 EXPERIMENTS
4.1 Dataset and setup
THUMOS 2014 [14]. The temporal action detection task of THU-
MOS 2014 dataset is challenging and widely used. The training
set is the UCF-101 [34] dataset including 13320 trimmed videos of
101 categories. The validation and test set contain 1010 and 1574
untrimmed videos separately. In temporal action detection task,
only 20 action categories are involved and annotated temporally.
We only use 200 validation set videos (including 3007 action in-
stances) and 213 test set videos (including 3358 action instances)
with temporal annotation to train and evaluate SSAD network.
MEXaction2 [1]. There are two action categories in MEXac-
tion2 dataset: ""HorseRiding"" and ""BullChargeCape"". This dataset
is consisted of three subsets: YouTube clips, UCF101 Horse Rid-
ing clips and INA videos. YouTube and UCF101 Horse Riding clips
are trimmed and used for training set, whereas INA videos are
untrimmed with approximately 77 hours in total and are divided
into training, validation and testing set. Regarding to temporal an-
notated action instances, there are 1336 instances in training set,
310 instances in validation set and 329 instances in testing set.
Evaluation metrics. For both datasets, we follow the conven-
tional metrics used in THUMOS’14, which evaluate Average Preci-
sion (AP) for each action categories and calculate mean Average
Precision (mAP) for evaluation. A prediction instance is correct if
it gets same category as ground truth instance and its temporal
IoU with this ground truth instance is larger than IoU threshold θ .
Various IoU thresholds are used during evaluation. Furthermore,
redundant detections for the same ground truth are forbidden.
4.2 Implementation Details
Action classiers.To extract SAS features, action classiers should
be trained rst, including two-stream networks [40] and C3D net-
work [36]. We implement both networks based on Cae [13]. For
both MEXaction and THUMOS’14 datasets, we use trimmed videos
in training set to train action classier.
For spatial and temporal network, we follow the same training
strategy described in [40] which uses the VGGNet-16 pre-trained on
ImageNet [4] to intialize the network and ne-tunes it on training
set. And we follow [36] to train the C3D network, which is pre-
trained on Sports-1M [16] and then is ne-turned on training set.
SSAD optimization. For training of the SSAD network, we use
the adaptive moment estimation (Adam) algorithm [17] with the
aforementioned multi-task loss function. Our implementation is
based on Tensorow [2]. We adopt the Xavier method [10] to ran-
domly initialize parameters of whole SSAD network because there
are no suitable pre-trained temporal convolutional network. Even
so, the SSAD network can be easily trained with quick convergence
since it has a small amount of parameters (20 MB totally) and the
input of SSAD network - SAS features are concise high-level feature.
The training procedure takes nearly 1 hour on THUMOS’14 dataset.
Table 1: mAP results on THUMOS’14 with various IoU
threshold θ used in evaluation.
θ 0.5 0.4 0.3 0.2 0.1
Karaman et al. [15] 0.2 0.3 0.5 0.9 1.5
Wang et al. [39] 8.5 12.1 14.6 17.8 19.2
Oneata et al. [23] 15.0 21.8 28.8 36.2 39.8
Richard et al. [28] 15.2 23.2 30.0 35.7 39.7
Yeung et al. [42] 17.1 26.4 36.0 44.0 48.9
Yuan et al. [44] 18.8 26.1 33.6 42.6 51.4
Shou et al. [29] 19.0 28.7 36.3 43.5 47.7
Zhu et al. [45] 19.0 28.9 36.2 43.6 47.7
SSAD 24.6 35.0 43.0 47.8 50.1
4.3 Comparison with state-of-the-art systems
Results on THUMOS 2014. To train action classiers, we use full
UCF-101 dataset. Instead of using one background category, here
we form background categories using 81 action categories which
are un-annotated in detection task. Using two-stream and C3D
networks as action classiers, the dimension of SAS features is 303.
Figure 5: Detection AP over dierent action categories with overlap threshold 0.5 in THUMOS’14.
Table 2: Results on MEXaction2 dataset with overlap thresh-
old 0.5. Results for [1] are taken from [29].
AP(%) BullCHargeCape HorseRiding mAP(%)
DTF [1] 0.3 3.1 1.7
SCNN [29] 11.6 3.1 7.4
SSAD 16.5 5.5 11.0
For training of SSAD model, we use 200 annotated untrimmed
video in THUMOS’14 validation set as training set. The window
length Lw is set to 512, which means approximately 20 seconds of
video with 25 fps. This choice is based on the fact that 99.3% action
instances in the training set have smaller length than 20 seconds.
We train SSAD network for 30 epochs with learning rate of 0.0001.
The comparison results between our SSAD and other state-of-
the-art systems are shown in Table 1 with multiple overlap IoU
thresholds varied from 0.1 to 0.5. These results show that SSAD
signicantly outperforms the compared state-of-the-art methods.
While the IoU threshold used in evaluation is set to 0.5, our SSAD
network improves the state-of-the-art mAP result from 19.0% to
24.6%. The Average Precision (AP) results of all categories with
overlap threshold 0.5 are shown in Figure 5, the SSAD network
outperforms other state-of-the-art methods for 7 out of 20 action
categories. Qualitative results are shown in Figure 6.
Results on MEXaction2. For training of action classiers, we
use all 1336 trimmed video clips in training set. And we randomly
sample 1300 background video clips in untrimmed training videos.
The prediction categories of action classiers are ""HorseRiding"",
""BullChargeCape"" and ""Background"". So the dimension of SAS fea-
tures equals to 9 in MEXaction2.
For SSAD model, we use all 38 untrimmed video in MEXaction2
training set training set. Since the distribution of action instances’
length in MEXaction2 is similar with THUMOS’14, we also set the
interval of snippets to zero and the window length Tw to 512. We
train all layers of SSAD for 10 epochs with learning rate of 0.0001.
We compare SSAD with SCNN [29] and typical dense trajec-
tory features (DTF) based method [1]. Both results are provided
by [29]. Comparison results are shown in Table 2, our SSAD net-
work achieve signicant performance gain in all action categories
Table 3: Comparisons between dierent action classiers
used in SSAD on THUMOS’14, where two-stream network
includes both spatial and temporal networks.
Action Classier used for SAS Feature mAP (θ = 0.5)
C3D Network 20.9
Two-Stream Network 21.9
Two-Stream Network+C3D Network 24.6
Table 4: Comparisons amongmultiple base layers congura-
tions on THUMOS’14. A, B, C, D, E are base layers congura-
tions which presented in Figure 3.
Network Conguration A B C D E
mAP(θ = 0.5) 23.7 24.6 24.1 23.9 23.4
of MEXaction2 and the mAP is increased from 7.4% to 11.0% with
overlap threshold 0.5. Figure 6 shows the visualization of prediction
results for two action categories respectively.
4.4 Model Analysis
We evaluate SSAD network with dierent variants in THUMOS’14
to study their eects, including action classiers, architectures of
SSAD network and post-processing strategy.
Action classiers. Action classiers are used to extract SAS
feature. To study the contribution of dierent action classiers, we
evaluate them individually and coherently with IoU threshold 0.5.
As shown in Table 3, two-stream networks show better performance
than C3D network and the combination of two-stream and C3D
network lead to the best performance. In action recognition task
such as UCF101, two-stream network [40] achieve 91.4%, which is
better than 85.2% of C3D [36] network (without combining with
other method such as iDT [38]). So two-stream network can predict
action categories more precisely than C3D in snippet-level, which
leads to a better performance of the SSAD network. Furthermore,
the SAS feature extracted by two-stream network and C3D network
are complementary and can achieve better result if used together.
Figure 6: Visualization of prediction action instances by SSAD network. Figure (a) shows prediction results for two action
categories in THUMOS’14 dataset. Figure (b) shows prediction results for two action categories in MEXaction2 dataset.
Table 5: Evaluation on dierent post-processing strategy on
THUMOS’14.
pclass ! ! ! !
psas ! ! ! !
pover ! ! !
mAP (θ = 0.5) 22.8 13.4 24.3 19.8 23.3 24.6
Architectures of SSAD network. In section 3.3, we discuss
several architectures used for base network of SSAD. These archi-
tectures have same input and output size. So we can evaluate them
fairly without other changes of SSAD. The comparison results are
shown in Table 4. Architecture B achieves best performance among
these congurations and is adopted for SSAD network. We can
draw two conclusions from these results: (1) it is better to use max
pooling layer instead of temporal convolutional layer to shorten
the length of feature map; (2) convolutional layers with kernel size
9 have better performance than other sizes.
Post-processing strategy.We evaluate multiple post-processing
strategies. These strategies dier in the way of late fusion to gener-
ate pf inal and are shown in Table 5. For example, pclass is used
for generate pf inal if it is ticked in table. Evaluation results are
shown in Table 5. For the categories score, we can nd that pclass
has better performance than p¯sas . And using the multiplication
factor pover can further improve the performance. SSAD network
achieves the best performance with the complete post-processing
strategy.
5 CONCLUSION
In this paper, we propose the Single Shot Action Detector (SSAD)
network for temporal action detection task. Our SSAD network
drops the proposal generation step and can directly predict action
instances in untrimmed video. Also, we have explored many cong-
urations of SSAD network to make SSAD network work better for
temporal action detection. When setting Intersection-over-Union
threshold to 0.5 during evaluation, SSAD signicantly outperforms
other state-of-the-art systems by increasing mAP from 19.0% to
24.6% on THUMOS’14 and from 7.4% to 11.0% on MEXaction2.
In our approach, we conduct feature extraction and action detec-
tion separately, which makes SSAD network can handle concise
high-level features and be easily trained. A promising future direc-
tion is to combine feature extraction procedure and SSAD network
together to form an end-to-end framework, so that the whole frame-
work can be trained from raw video directly.
REFERENCES
[1] 2015. MEXaction2. http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+
action+dataset. (2015).
[2] M. Abadi, A. Agarwal, P. Barham, and others. 2016. Tensorow: Large-
scale machine learning on heterogeneous distributed systems. arXiv preprint
arXiv:1603.04467 (2016).
[3] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. 2016. Fast temporal activity
proposals for ecient detection of human actions in untrimmed videos. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
1914–1923.
[4] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Feifei. 2009. ImageNet: A large-
scale hierarchical image database. (2009), 248–255.
[5] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem. 2016. Daps: Deep action
proposals for action understanding. In European Conference on Computer Vision.
Springer, 768–784.
[6] C. Feichtenhofer, A. Pinz, and A. Zisserman. 2016. Convolutional two-stream
network fusion for video action recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 1933–1941.
[7] J. Gemert, M. Jain, E. Gati, C. G. Snoek, and others. 2015. Apt: Action localization
proposals from dense trajectories. BMVA Press.
[8] R. Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE International Conference
on Computer Vision. 1440–1448.
[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. 2014. Rich feature hierarchies
for accurate object detection and semantic segmentation. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 580–587.
[10] X. Glorot and Y. Bengio. 2010. Understanding the diculty of training deep
feedforward neural networks.. In Aistats, Vol. 9. 249–256.
[11] X. Glorot, A. Bordes, and Y. Bengio. 2011. Deep Sparse Rectier Neural Networks..
In Aistats, Vol. 15. 275.
[12] K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 770–778.
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,
and T. Darrell. 2014. Cae: Convolutional architecture for fast feature embedding.
In Proceedings of the 22nd ACM international conference on Multimedia. ACM,
675–678.
[14] Y. G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar.
2014. THUMOS challenge: Action recognition with a large number of classes. In
ECCV Workshop.
[15] S. Karaman, L. Seidenari, and A. Del Bimbo. 2014. Fast saliency based pooling of
sher encoded dense trajectories. In ECCV THUMOS Workshop, Vol. 1.
[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei.
2014. Large-scale video classication with convolutional neural networks. In
Proceedings of the IEEE conference on Computer Vision and Pattern Recognition.
1725–1732.
[17] D. Kingma and J. Ba. 2014. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014).
[18] H. Kuehne, H. Jhuang, R. Stiefelhagen, and T. Serre. 2013. HMDB51: A large
video database for human motion recognition. In High Performance Computing
in Science and Engineering ’12. Springer, 571–582.
[19] C. Lea, R. Vidal, A. Reiter, and G. D. Hager. 2016. Temporal Convolutional
Networks: A Unied Approach to Action Segmentation. In Computer Vision–
ECCV 2016 Workshops. Springer, 47–54.
[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. C. Berg. 2016.
SSD: Single shot multibox detector. In European Conference on Computer Vision.
Springer, 21–37.
[21] S. Ma, L. Sigal, and S. Sclaro. 2016. Learning activity progression in LSTMs for
activity detection and early detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 1942–1950.
[22] P. Mettes, J. C. van Gemert, and C. G. Snoek. 2016. Spot on: Action localization
from pointly-supervised proposals. In European Conference on Computer Vision.
Springer, 437–453.
[23] D. Oneata, J. Verbeek, and C. Schmid. 2014. The LEAR submission at Thumos
2014. ECCV THUMOS Workshop (2014).
[24] Z. Qiu, T. Yao, and T. Mei. 2016. Deep Quantization: Encoding Convolutional
Activations with Deep Generative Model. arXiv preprint arXiv:1611.09502 (2016).
[25] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. 2016. You only look once:
Unied, real-time object detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 779–788.
[26] J. Redmon and A. Farhadi. 2016. YOLO9000: Better, Faster, Stronger. arXiv
preprint arXiv:1612.08242 (2016).
[27] S. Ren, K. He, R. Girshick, and J. Sun. 2015. Faster r-cnn: Towards real-time
object detection with region proposal networks. InAdvances in neural information
processing systems. 91–99.
[28] A. Richard and J. Gall. 2016. Temporal action detection using a statistical language
model. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 3131–3140.
[29] Z. Shou, D. Wang, and S.-F. Chang. 2016. Temporal action localization in
untrimmed videos via multi-stage cnns. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 1049–1058.
[30] K. Simonyan and A. Zisserman. 2014. Two-stream convolutional networks
for action recognition in videos. In Advances in Neural Information Processing
Systems. 568–576.
[31] K. Simonyan and A. Zisserman. 2015. Very Deep Convolutional Networks for
Large-Scale Image Recognition. In International Conference on Learning Repre-
sentations.
[32] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. 2016. A multi-stream
bi-directional recurrent neural network for ne-grained action detection. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
1961–1970.
[33] G. Singh and F. Cuzzolin. 2016. Untrimmed Video Classication for Activity
Detection: submission to ActivityNet Challenge. arXiv preprint arXiv:1607.01979
(2016).
[34] K. Soomro, A. R. Zamir, and M. Shah. 2012. UCF101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012).
[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-
houcke, and A. Rabinovich. 2015. Going deeper with convolutions. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 1–9.
[36] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. 2015. Learning
spatiotemporal features with 3d convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision. 4489–4497.
[37] H. Wang, A. Kläser, C. Schmid, and C.-L. Liu. 2011. Action recognition by
dense trajectories. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
Conference on. IEEE, 3169–3176.
[38] H. Wang and C. Schmid. 2013. Action recognition with improved trajectories. In
Proceedings of the IEEE International Conference on Computer Vision. 3551–3558.
[39] L. Wang, Y. Qiao, and X. Tang. 2014. Action recognition and detection by
combining motion and appearance features. THUMOS14 Action Recognition
Challenge 1 (2014), 2.
[40] L. Wang, Y. Xiong, Z. Wang, and Y. Qiao. 2015. Towards good practices for very
deep two-stream convnets. arXiv preprint arXiv:1507.02159 (2015).
[41] R. Wang and D. Tao. 2016. UTS at activitynet 2016. AcitivityNet Large Scale
Activity Recognition Challenge 2016 (2016), 8.
[42] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. 2016. End-to-end learning
of action detection from frame glimpses in videos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2678–2687.
[43] G. Yu and J. Yuan. 2015. Fast action proposals for human action detection and
search. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 1302–1311.
[44] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. 2016. Temporal Action Localization
with Pyramid of Score Distribution Features. In IEEE Conference on Computer
Vision and Pattern Recognition. 3093–3102.
[45] Y. Zhu and S. Newsam. 2016. Ecient Action Detection in Untrimmed Videos
via Multi-Task Learning. arXiv preprint arXiv:1612.07403 (2016).
"
35,"Temporal Cross-Media Retrieval with Soft-Smoothing *
David Semedo
NOVA LINCS
Universidade NOVA de Lisboa
Caparica, Portugal
df.semedo@campus.fct.unl.pt
Joao Magalhaes
NOVA LINCS
Universidade NOVA de Lisboa
Caparica, Portugal
jm.magalhaes@fct.unl.pt
ABSTRACT
Multimedia information have strong temporal correlations that
shape the way modalities co-occur over time. In this paper we
study the dynamic nature of multimedia and social-media infor-
mation, where the temporal dimension emerges as a strong source
of evidence for learning the temporal correlations across visual and
textual modalities. So far, cross-media retrieval models, explored the
correlations between different modalities (e.g. text and image) to
learn a common subspace, in which semantically similar instances
lie in the same neighbourhood. Building on such knowledge, we
propose a novel temporal cross-media neural architecture, that de-
parts from standard cross-media methods, by explicitly accounting
for the temporal dimension through temporal subspace learning.
The model is softly-constrained with temporal and inter-modality
constraints that guide the new subspace learning task by favouring
temporal correlations between semantically similar and temporally
close instances. Experiments on three distinct datasets show that
accounting for time turns out to be important for cross-media re-
trieval. Namely, the proposed method outperforms a set of baselines
on the task of temporal cross-media retrieval, demonstrating its
effectiveness for performing temporal subspace learning.
CCS CONCEPTS
• Information systems → Multimedia and multimodal re-
trieval;
KEYWORDS
Cross-media; temporal cross-media; temporal smoothing; multime-
dia retrieval
ACM Reference Format:
David Semedo and Joao Magalhaes. 2018. Temporal Cross-Media Retrieval
with Soft-Smoothing *. In 2018 ACM Multimedia Conference (MM ’18), Octo-
ber 22–26, 2018, Seoul, Republic of Korea. ACM, New York, NY, USA, 9 pages.
https://doi.org/10.1145/3240508.3240665
* Please cite the ACM MM 2018 version of this paper.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MM ’18, October 22–26, 2018, Seoul, Republic of Korea
© 2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5665-7/18/10. . . $15.00
https://doi.org/10.1145/3240508.3240665
time
Crash category 
density
Crash BCrash A
Crash category
Temporal 
cross-media 
subspace
Time-sensitive  
cross-media 
projections
Figure 1: Temporal Dynamics of semantic category Crash
(TDF2016), and temporal pairwise variations with corre-
sponding visual elements.
1 INTRODUCTION
Solid advances have been proposed to annotate media with complex
keywords [24] and retrieve information across different modalities
[22, 29, 31]. Cross-media models enable the retrieval of content
from a target modality (e.g. text) given another modality (e.g. im-
age). The field has been extensively researched and attracted many
contributions [8, 22, 32], with the most widely used approach being
common space learning [20]. Among the pioneering works, is the
work of Rasiwasia et al. [22] which leveraged on the Canonical
Correlation Analysis [12] (CCA) algorithm to learn a multimodal
linearly correlated subspace. The field then evolved to the adop-
tion of neural-network methods, which allow the learning of com-
plex non-linear projections [8, 19, 32], through the composition of
several non-linear functions. Recently, Wang et al. [29] elegantly
formulated the cross-media subspace learning problem under an
adversarial learning framework. A common assumption of previous
works is the static corpora assumption, thus, overlooking tempo-
ral correlations between visual-textual pairs. Looking at Figure 1
one can see that images and texts are not static across the corpus
timespan, with the later being reflected on visual-textual pairs in
the form of temporal correlations. These lead to the existence of
cross-modal pairwise correlations that change over time.
Modelling such dynamic cross-media relations over time raises
many challenges for current state-of-the-art cross-media analysis.
However, it is possible to see how numerous works have lever-
aged on the dynamics of social media for diverse tasks, such as
Emerging Events Tracking [15], Dynamic Sentiment Analysis [27],
Natural Disasters Prediction [23], among others. In all these works,
content temporal relevance insights proved to be crucial. Namely,
they exploit the fact that user contributed content, from certain
topics, follows some temporal pattern. For instance, as illustrated
in figure 1, the semantic category Crash, has two modes along the
ar
X
iv
:1
81
0.
04
54
7v
1 
 [c
s.M
M
]  
10
 O
ct 
20
18
temporal dimension with different cross-modal correlations, cor-
responding to a multimodal variation of the category crash across
the corpus timespan.
In this paper, we hypothesise that, for dynamic corpora, both
visual and textual modalities change over time, creating the need
for time-sensitive cross-media retrieval. To assess this hypothesis,
new time-sensitive cross-media subspace learning approaches will
be investigated. The goal is to learn effective projections where
cross-media patterns are decomposed over the temporal dimension,
thus, proving the hypothesis. We take a step further by seeking a
space where semantically similar and mutually temporally related
instances, lie in the same neighbourhood.
We propose TempXNet, a novel Temporal Cross-Media Neural
Architecture, to learn projections for both textual and visual data,
while modelling temporal correlations between modalities. A tem-
poral subspace learning approach is devised, by enforcing temporal
constraints between semantically similar instances, in order to learn
time-sensitivemodality projections. This novel strategywill enable ef-
fective retrieval in a temporally-aware cross-media subspace. Many
cross-media works have already incorporated external information
(e.g. semantic categories information), when performing subspace
learning, breaking instances’ pairwise coupling, in order to better
capture visual-textual correlations [9, 21, 35]. Accordingly, in the
proposed method the underlying dynamics of instances are cap-
tured by considering two distinct temporal correlation modelling
approaches. These model the intra-category temporal correlations
at two levels of granularity: at the document’s timestamp level
and at individual word’s level. The key aspects of TempXNet are
summarised as follows:
(1) The proposed model is flexible enough to support cross-
media temporal correlations following parametric, non- para-
metric and latent-variable distributions.
(2) The formulation of the subspace learning objective function,
that captures the temporal correlation, is a differentiable
function which can be conveniently optimised by gradient-
based algorithms.
Experiments on three datasets illustrate the importance of consider-
ing temporal correlations in cross-media retrieval tasks. TempXNet
outperformed recent cross-media retrieval baselines. We further
evaluated the behaviour of different temporal distributions to model
the cross-media dynamics of social media content.
2 RELATEDWORK
The literature on modelling and incorporating temporal aspects for
multimodal retrieval is very scarce. A pioneer approach was devised
by Kim and Xing [14], where temporal clues are used to improve
search relevance at query time, modelling content streams using
a multi-task regression on multivariate point processes. Visual-
textual pairs are treated as random events in time and space. Image
ranking, under this framework is improved by using a multi-task
learning approach that considers multiple image descriptors when
capturing temporal correlations. Uricchio et al. [28] evaluated the
value of temporal information, such as tag frequency, for the task
of image annotation and retrieval. The authors confirm that some
tags reveal a dynamic behaviour that was found to be aligned with
Google search trends, thus supporting our hypothesis regarding the
behaviour of visual-textual pairs, on (social) dynamic corpus. On
the other hand, for orthogonal tasks but directly dealing with social
corpora, time, or more specifically, temporal relevance of elements,
has been exploited [15, 23, 27]. Our hypothesis is directly inspired
and supported by the findings of such works, which successfully
exploited temporal insights, encoded on social corpora.
For static collections, the task of cross-media retrieval, between
visual and textual modalities, has been extensively researched [7, 8,
17, 21, 22, 26, 29, 32]. Namely, neural network methods have proved
to be highly effective at learning non-linear projections. Feng et al.
[8] proposed the Correlation Autoencoder (Corr-AE), which is com-
prised by two Autoencoders (one for each modality), whose in-
termediate layers are tied by a similarity measure. The loss func-
tion is then defined by the autoencoder reconstruction error and
an additional cost term, measuring the correlation error. Yan and
Mikolajczyk [32] leveraged on Deep Canonical Correlation Analy-
sis (DCCA) to match images and text. DCCA exploits the fact that
the CCA objective function can be formulated based on a matrix
trace norm. Fan et al. [7], combine image global (CNN features) and
descriptive (e.g. caption) representations using a network fusion
approach to obtain a richer semantic embedding space. Very re-
cently, Wang et al. [29] proposed to learn a common space using an
adversarial learning approach. Inspired by state-of-the-art subspace
learning works [8, 29, 32, 32] we consider neural networks, which
have proved to be very effective. Namely, we adopt the two-network
(one for each modality) base architecture for projection learning,
predominant across multiple state-of-the-art works.
Joint representation learning for video sequences, with well
aligned visual and audiomodalities, has been actively researched [13,
18, 25, 33]. These are commonly based on temporal methods, such
as Recurrent Neural Networks, that are able to capture dependen-
cies on sequences, over time steps. However, these sequences are
fundamentally different from temporal correlations of visual-textual
pairs on social media, as they are assumed to show coherence over
time and be perfectly aligned.
Hence, there is considerable literature [15, 23, 27] outside cross-
media retrieval to support this paper hypothesis. Therefore, we
explicitly address the problem and propose a temporal cross-media
subspace learning approach for dynamic corpora, in which latent
temporal correlations are intrinsically accounted.
3 TEMPORAL CROSS-MEDIA SUBSPACE
The main hypothesis we wish to investigate (and quantify) is that
both visual and textual modality correlations change over time.
This is supported by the existence of dynamic visual-textual pairs
(Figure 1) originating multi-dimensional correlations among the
temporal, visual and textual dimensions of the problem’s data. Con-
sequently, we argue that temporal correlations between instances
of a same semantic category should lead to the investigation of new
subspaces that capture such data interactions.
3.1 Subspace Definition
Let C be a corpus of timestamped documents, with a timespan
TS = [ts , tf ], where ts and tf are the first and last span instants
respectively, where each instance di ∈ C is defined as
di = (x iI ,x iT , t i ), (1)
R
G
B 
Im
ag
e
...
CNN Representation 
Beautiful 
Edinburgh 
Castle 
#edfringe
Te
xt
BoW Representation 
...
...
Time
Image Space
Text Space
Time
Beautiful 
Edinburgh Castle 
#edfringe
Fireworks!
Temporal Cross-Media Sub-space
Visual Projection NN
Textual Projection NN
D
Two Component Loss:
Modality + Temporal 
Alignment
Time
Temporal Density of Category: 
Castle
D
Semantic Category: Castle
LTXM
Figure 2: Temporal Cross-Media subspace learning overview. Visual (green) and textual (red) instances are mapped to a D
dimensional cross-media space. The space is perturbed to approximate temporally correlated instances, of a same category,
and to separate uncorrelated ones. Best viewed in color.
where x iI ∈ RDI and x iT ∈ RDT are the instance’s image dI and text
dT feature representations, respectively, and t i the instance’s times-
tamp. Accordingly, DI and DT correspond to the image and text
features dimensionality, respectively. We defineV as the vocabulary
of the text data and L as the set of semantic categories of the corpus
C. Each instance di is associated with a set of semantic categories
l i = {l i1, . . . , l ij , . . . , l i|l i |}, such that each l ij ∈ L. It follows that each
instance di can be associated with one or more categories. This al-
lows us to introduce the formal definition of temporal cross-media
subspace:
Definition 1. A temporal cross-media subspace refers to a subspace
that is learned from timestamped multimedia data to organize data
according to their semantic category and temporal correlations
across different modalities.
Formally, the temporal cross-media subspace will have the fol-
lowing properties:
Property 1. Two elements will be maximally correlated in the
new subspace if they share at least one semantic category
and if they are strongly correlated in time;
Property 2. Considering the same semantic category, tempo-
rally correlated instances will lie in the same neighbourhood,
while temporally uncorrelated instances are expected to lie
far apart;
Property 3. Complex temporal behaviours will be captured
by a function simtemp , grounded on a temporal distribution
θtemp , which can follow a parametric, non-parametric or
latent-variable model.
3.2 Time-sensitive Cross-Media Projections
Given the aforementioned definitions, it follows that both xI and
xT original spaces are dissimilar and obtained without accounting
for time. Namely each space may have different dimensionality,
semantics and distributions, making them incompatible. This leads
us to the projections:
PI (·;θI ) : RDI 7→ RD PT (·;θT ) : RDT 7→ RD (2)
mapping imagesdI and textsdT to a common temporal cross-media
subspace, with dimensionality D, according to the to image and
text projections models θI and θT , correspond, respectively.
To learn the time-sensitive cross-media projections PI (·;θI ) and
PT (·;θT ), it is essential to maximise the correlation in the new
subspace between the two modalities, both at the semantic and
temporal dimensions. Thus, the projections into the temporal cross-
media subspace need to capture the temporal traits of semantic
categories, which are grounded on temporal correlations across
visual and textual modalities. In practice, we argue for projections
that are learned with novel temporally constrained objective func-
tions of the form
argmin
θI ,θT
LTXM (θI ,θT )
s.t. Ltemp (θI ,θT ;θtemp ) = 0,
(3)
where LTXM corresponds to a cross-media loss that minimises the
distance over semantically similar representations and maximises
the distance between semantically dissimilar instance’s represen-
tations. The cross-media loss is subject to temporal smoothing
constraints, imposed by a temporal factor Ltemp , grounded on a
temporal model θtemp , that enforces the aforementioned properties.
As will be detailed later, temporal constraints Ltemp are relaxed
as an additive smoothing term, added to LTXM .
3.3 Temporal Cross-Media Retrieval
Many tasks can be solved in the temporal cross-media subspace,
which are not supported by current cross-media models. Given a
cross-media query q, defined as
q = (⟨xI ⟩, ⟨xT ⟩), (4)
where one or both modalities may be specified, the goal is to re-
trieve the set of temporally and semantically correlated instances
(from the remaining modality when only one is specified). Our
model intrisically encodes temporal correlations on a cross-media
space such that it does not require a timestamp as input to project
modalities close to temporally correlated instances. Also, it allows
marginalising the model along of the visual and textual variables,
and obtain temporally grouped data.
In the following sections we will detail how the TempXNet archi-
tecture is materialised. Namely the loss function from equation 3,
the temporal factor Ltemp and the neural cross-media architecture.
4 TEMPORAL CROSS-MEDIA LEARNING
The goal of temporal cross-media subspace learning is to create
a new subspace where semantic and temporal latent correlations,
for instances of the same category, are represented at essentially
two granularity levels: 1) inter-modality pairwise correlation, and
2) inter-instance correlation.
On the obtained temporal subspace, the encoding of the temporal
dimension is achieved by smoothing the aligned representations
subspacewith a set of temporal constraints imposed on the loss func-
tion from equation 3. The temporal factor term Ltemp is backed
up by a temporal model θtemp , estimated from the corpus, that
provides two temporal insights: 1) instance’s temporal signatures
over the corpus C timespan, and 2) a smoothed temporal correla-
tion functions, based on the aforementioned temporal signatures.
It is important to see how this cost function leads to cross-media
projections fundamentally different from previous works, where
images and text are grouped in a temporally agnostic manner.
4.1 Projection Learning
Apart from the temporal insights, it is crucial to learn effective
modality projections, that map original modality vectors to a new
space where pairwise (visual and textual modalities) and instance’s
semantic correlations are represented. Inspired by state-of-the-art
cross-media retrieval approaches [29, 31, 34], we formulate LTXM
as a pairwise ranking-loss [11], as it has been shown that minimisa-
tion of this loss is directly related with the maximisation of nDCG
and MAP [5]. Thus, LTXM is defined as follows:
LTXM (θI ,θT ) =∑
i,n
max(0,m − PI (x iI ) · PT (x iT ) + PI (x iI ) · PT (xnT )) +∑
i,n
max(0,m − PT (x iT ) · PI (x iI ) + PT (x iT ) · PI (xnI )),
(5)
where xnI and x
n
T are images and texts representations from negative
instances, w.r.t. an instance di . Similarity between projections is
computed by a dot product over two unit-norm, ℓ2 normalised
vectors, making it equivalent to cosine similarity.
We devise a neural network architecture to perform subspace
learning and learn projections PI (·) and PT (·). Figure 2 depicts
the neural architecture. Following [7, 17, 32], we consider two
neural networks to learn non-linear mappings, with θI and θT ,
denoting each sub-network’s learnable parameters for image and
textual modalities, respectively. Through the composition of sev-
eral non-linearities, neural networks are able to model complex
latent correlations. Thus, for each modality, a feedforward network,
comprising 2 fully connected layers is used. The first layer has 1024
dimensions and the second one has D dimensions. For semantically
rich image feature representation, a convolutional neural network
is prefixed to the input of the visual modality projection network.
Each modality network takes as input the corresponding modal-
ity of an instance di . Namely a visual projection sub-network takes
as input the RGB image diI , and a textual projection sub-network a
bag-of-words representation of the text diT . Both original modality
representations are embedded onto a new D-dimensional subspace.
Both sub-networks are then jointly optimised by minimising the
loss function L(θI ,θT ), from eq. 3. Apart from the training phase,
both sub-networks are decoupled and thus can be used indepen-
dently to map a single modality.
4.2 Temporal Cross-Media Soft-Constraints
Temporal subspace learning properties are enforced over semanti-
cally similar instances only, through a set of soft-constraints. Thus,
the temporal factor Ltemp is defined as:
Ltemp (θI ,θT ) = λ
∑
i
Ltemp (di ;θI ,θT ), (6)
where λ is an hyper-parameter used to control the influence of the
temporal factor.Ltemp (θI ,θT ) is then added to eq. 3 as a smoothing
term. The rationale of equation 6 is to smooth the model by con-
straining the learned projections for every instance di , with tempo-
ral soft-constraints. For a single instance di , let J = {j : l j ∩ l i , ∅}
be the set of positive examples di of category of l j . The constraints
are:
• Temporally correlated instances, with distant cross-modality
projections, should have similar projections. Violations to
this constraint are captured as follows:
C1(di ) = 1|J |
∑
j ∈J
simtemp (t i , t j ;θtemp )
· (1 − simcmod (di ,d j ));
(7)
• Temporally uncorrelated instances, with close cross-modality
projections, should lie far apart, thus having distant projec-
tions. Violations to this constraint are captured as follows:
C2(di ) = 1|J |
∑
j ∈J
(1 − simtemp (t i , t j ;θtemp ))
· simcmod (di ,d j ),
(8)
where simtemp (t i , t j ;θtemp ), detailed in section 4.3, is a temporal
correlation assessment function that evaluates how correlated in
time two instances di and d j are. Finally, simcmod (di ,d j ) is a cross-
modality similarity function that evaluates how close each modality
projection is, w.r.t. to the other modality, on the cross-media sub-
space. We average pairwise violations to deal with unbalanced
positive sets.
The two soft-constraints are then combined as:
Ltemp (di ;θI ,θT ) = (C1(di ) +C2(di )). (9)
Essentially, for a given instance di , Ltemp iterates through all the
positive instances d j (sharing at least one semantic category with
di ), and computes the two products between temporal and cross-
modality distances.
4.2.1 Cross-modality similarity. Cross-modality similarity
simcmod , computed over semantically similar instances of di , is
defined based on the harmonic mean between the cross-modality
projections’ similarities:
simcmod (di ,d j ) = 2·
PI (x iI )PT (x
j
T ) · PT (x iT )PI (x
j
I )
PI (x iI ) · PT (x
j
T ) + PT (x iT ) · PI (x
j
I ) + ϵ
(10)
where again, similarity is computed by a dot product ℓ2 normalised
vectors. A small constant ϵ is added to the denominator to avoid
zero division. Essentially, simcmod assesses the alignment between
the representations obtained by projections PI (·) and PT (·), over
two instances, by equally weighting both modalities’ projections.
4.3 Temporal Soft-Smoothing Functions
For semantic categories and words, correlation strength within
different instances, is expected to vary over time. We posit that
the later is reflected on the dynamic behaviour of each element (a
semantic category or a word). On a corpus C, such behaviour is
accounted by Ltemp , through a temporal correlation assessment
function simtemp . We materialise the later based on two fundamen-
tally different levels: category and word temporal behaviour.
4.3.1 Category-based Correlations. We propose to assess tempo-
ral correlations by directly comparing temporal density distribution
®ϕl of categories. Given ®ϕl , we define the temporal density of l , at
time t , as a probability function p(t | ®ϕl ). Then, Category-based cor-
relations are then defined as:
simtemp (t i , t j ) = p(t i | ®ϕl ) · p(t j | ®ϕl ), (11)
such that p(t | ®ϕl ) corresponds to the relevance of label l , at time t .
When two instances share more than one label, we consider the
value of the label that maximises simtemp . Kernel Density Estima-
tion (KDE), with a Gaussian Kernel, is used to obtain a smoothed
estimation of ®ϕl . The bandwidth h hyper-parameter is used to con-
trol the smoothness of the estimated density.
4.3.2 Topic-based Correlations. Individual word’s dynamic be-
haviour provides a richer insight regarding visual-textual temporal
pairs correlations. Namely, it is expected that some domain-specific
words will have a rich dynamic behaviour, depicting temporal cor-
relations, which should be accounted. Such correlations are also
much more fine-grained, when compared to individual semantic
categories.
We model temporal density distributions ®ϕw of each wordw ∈
x iT of a instance d , through a dynamic topic modelling approach.
Topic-based correlations are then defined as:
simtemp (t i , t j ) = p(t j |x iT ) =
∏
w ∈x iT
p(t j | ®ϕw ), (12)
such that p(t | ®ϕw ) corresponds to the density of word w , at time
t . Essentially, equation 12 measures temporal correlation by com-
paring the temporal density of words in diT , at timestamp t
j of
document d j .
To estimate ®ϕw , we resort to Dynamic Topic Modelling (DTM),
that intrinsically accounts for the time evolution of latent top-
ics. From the DTMs methods family, we consider Dynamic Latent
Dirichlet Allocation [3] (D-LDA) to study temporal behaviours of
individual words. The LDA [4] method represents documents as
a finite mixture over a set of estimated latent topics, where each
latent topic is characterised by a distribution over words, from
which documents are assumed to be generated from. It consists
of an exchangeable model, as joint probabilities over words are
invariant to permutations. D-LDA takes a step further by explicitly
addressing topic evolution and dropping the exchangeable property.
Documents are arranged into a set of time slices and for each time
slice, documents are modelled using a P-component topic model
(LDA), where its latent topics at time slice t evolve from latent
topics of slice t − 1. D-LDA is applied to the corpus C with time
slices referring to individual days.
For each word and latent-topic p, a temporal density curve ®ϕwp
is estimated. The element-wise mean over all latent-topics is com-
puted as ®ϕw = ∑Pp=0 ®ϕwp , and normalised. Then, for a given word
w :
p(t | ®ϕw ) = fdlda (t ,w) = ®ϕw (t), (13)
where ®ϕw (t) denotes the estimated averaged temporal density, at
time instant t , across all topics. Given that we average each ®ϕwp
over the P latent-topics and that each word w reveals different
behaviours on each latent-topic, we obtain a model that captures
word variations w.r.t. word correlations with groups of words, over
time.
5 EVALUATION
5.1 Datasets
5.1.1 NUS-WIDE [6]. Comprised of a total of 269,648 images
from the Flickr network, annotated with a total of 81 semantic
categories. We crawl images’ metadata and stored the datetaken
field to be used as timestamp. Each image has multiple tags and
may belong to multiple semantic categories. We consider the 1000
more frequent tags for text representation. Images that are missing,
do not have associated tags, or without timestamp are excluded.
We only keep images from year 1999 to 20091, resulting in a total
of 169,283 images. We use the NUS-WIDE dataset for temporal
cross-media as some tags have been shown to reveal a dynamic
behaviour [28]. Train, validation and test splits comprise 129,500,
22,854 and 17,112 instances, respectively.
1The dataset was released on 2009, with its distribution having a mean of 2006.69 ±
1.175.
5.1.2 SocialStories Dataset. 2 This dataset consists of a collec-
tion of social media documents covering a large number of sub-
events about two distinct major events of interest for the general
public. In particular, we considered Twitter as a source of social
media content. We specifically considered events that span over
multiple days and that contain considerable amounts of diverse
visual material. These are expected to have strong temporal corre-
lation across modalities with respect to its semantics. Taking the
aforementioned aspects into account, we selected the following
events:
Edinburgh Festival 2016 (EdFest 2016) 3 - Consists of a cel-
ebration of the performing arts, gathering dance, opera, mu-
sic and theatre performers from all over the world. The
event takes place in Edinburgh, Scotland and has a dura-
tion of 3 weeks in August. The dataset contains 82,348 doc-
uments where 1,186 were annotated with 13 semantic cate-
gories (Audience/ Crowd, Castle, Selfies/Group Photos/Posing,
Fireworks, Music, Streets of Edinburgh, Food, Dance/Dancing,
Show/Performance, Building(s)/Monuments, Sky/Clouds, Per-
son, Water).
Le Tour de France 2016 (TDF 2016) 4 - Consists of one of
the main road cycling race competitions. The event takes
place in France (day 1-8, 11-17, 20-23 ), Spain (day 9), An-
dorra (day 9-11), Switzerland (day 17-19), and has a duration
of 23 days in July. The dataset contains 325,074 documents
where 747 were annotated with 13 semantic categories (Spec-
tators, Bicycle/Pedalling, Road, Yellow-Jersey, Car/Truck, Pelo-
ton, Crash, Field(s)/ Mountain(s) Buildings/Monument(s) Food
Sky/Clouds, Water and Person).
After crawling content with event specific hashtags and seeds,
we applied a set of content filtering techniques [2, 16] to discard
SPAM and annotated documents event-specific semantic categories.
Annotators were asked to annotate media documents (image and
text) with one or more categories. An additional None category
is shown, when none of the categories apply to the instance. We
obtained a total of 1186 and 747 annotated pairs, with an average
of 3.0 ± 1.47 and 2.4 ± 1.26 categories per instance, for EdFest2016
and TDF2016, respectively.
5.2 Methodology
We evaluate the retrieval performance using mean Average Preci-
sion (mAP@K), which is the standard evaluation metric for cross-
media retrieval [8, 22, 29, 30, 32] and normalized Discounted Cu-
mulative Gain (nDCG@K). We follow [8, 29] and set K = 50. For
mAP@K, an instance is relevant if it shares at least one category.
For nDCG@K , relevance is defined as the number of common cate-
gories. Cross-media retrieval methods are evaluated in two tasks:
1) Image-to-Text retrieval (I-T) and 2) Text-to-Image (T-I) retrieval.
We complement our evaluation with a qualitative analysis.
5.3 Implementation Details
Networks are jointly trained using SGD, with 0.9 momentum, and
a learning rate of η = 5 × 10−3, with a decay of 1 × 10−6, and each
2http://datasets.novasearch.org/
3https://www.eif.co.uk/
4http://www.letour.com/
gradient update step being θI = θI − η 1s ∇θI (LTXM + Ltemp )
and θT = θT − η 1s ∇θT (LTXM + Ltemp ). Early stopping is used
to avoid overfitting. Mini-batch size is set to 10, 000, and 64, for
NUS-WIDE and SocialStories, respectively, and the total number of
epochs is set to 25. For each neuron, we use tanh non-linearities. Pre-
trained ResNet-50 [10], with the last fully connected layer removed
(softmax), is used for image representation. In SocialStories, DLDA
was trained on the full un-annotated dataset. The number of latent
topics P is set to 10 through cross-validation. We set D = 100,
λ = 1.0, pairwise-ranking loss marginm = 1.0, and KDE bandwidth
h = 1. We adopt the TF-IDF bag-of-words representation for texts.
For baselines, image representations are obtained from the same
pre-trained ResNet-50 CNN.
5.4 Experiments
Cross-Modal Retrieval.We start by evaluating the proposed sub-
space learning model, TempXNet, with each of the three distinct
temporal correlations on the task of cross-media retrieval. Namely,
we evaluate semantic category-based temporal correlations,
TempXNet-Cat (eq. 11); and latent-topic word-based temporal cor-
relations, TempXNet-Lat (eq. 12). We also evaluate an additional
straightforward temporal correlation, TempXNet-Rec, in which
the correlation corresponds to how close in time two instances are
simtemp (t i , t j ) = e−|t i−t j |/h , where h = 0.3. We adopt as baseline
the CCA [22] method, a linear subspace learning approach, and
non-linear neural-based methods, Bi-AE [17], Bi-DBN [26], Corr-
AE [8], Corr-Cross-AE [8], Corr-Full-AE [8], and DCCA [1, 32].
All baselines are atemporal. We use the TempXNet with the three
temporal smoothing functions. For all datasets, we use 90% of the
data for development and the remaining for testing. We further
split the development data using 15% for validation. We consider
days as the temporal granularity for social stories and years for
NUS-WIDE.
All methods are evaluated on the three datasets, of varying di-
mensions, representing corpora with different topic broadness, and
thus distinct temporal dynamics. Figure 1, Figure 2 and Figure 3,
show themAP@50 and nDCG@50 results for the NUS-WIDE, Ed-
Fest2016 and TDF2016 datasets, respectively.
The first observation we draw from the results is that TempXNet
is highly effective across the three datasets, outperforming all the
baselines, on both tasks, on all metrics. Specifically, TempXNet
is able to rank at the top (nDCG) highly relevant instances (i.e.
instances that share more semantic categories). This confirms our
hypothesis regarding modelling temporal correlations, through
temporal subspace learning.
Regarding the different temporal smoothing functions, apart
from NUS-WIDE, where distinct temporal correlations achieved
identical performance, for EdFest2016 and TDF2016 performance os-
cillate. This reflects the existence of distinct temporal distributions,
underlying each dataset. TempXNet-Lat outperforms the other cor-
relations on EdFest2016. As TempXNet-Lat exploits temporal corre-
lations of words, it is able to capture correlations between instances
based on word’s temporal behaviour. Additionally, on EdFest2016,
TempXNet-Rec outperforms TempXNet-Cat. This indicates that for
EdFest2016, latent-based and recency-based temporal correlations
1 10 20 30 40 50 60 70 80 90 1000.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
I↦ T
EdFest2016
TempXNet-Rec
TempXNet-Cat
TempXNet-Lat
1 10 20 30 40 50 60 70 80 90 1000.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
T↦ I
EdFest2016
TempXNet-Rec
TempXNet-Cat
TempXNet-Lat
1 10 20 30 40 50 60 700.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
I↦ T
TDF2016
TempXNet-Rec
TempXNet-Cat
TempXNet-Lat
1 10 20 30 40 50 60 700.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
T↦ I
TDF2016
TempXNet-Rec
TempXNet-Cat
TempXNet-Lat
Figure 3: Precision-Scope curves for EdFest2016 and TDF2016.
Table 1: Comparison on Cross-Media retrieval (mAP@50 and
nDCG@50) on NUS-WIDE.
Method % I 7→ T T 7→ I Avg.
mAP nDCG mAP nDCG mAP nDCG
CCA [22] 74.2 84.4 68.7 80.7 71.5 82.6
Bi-AE [17] 74.1 84.9 69.1 80.0 71.6 82.4
Bi-DBN [26] 69.5 81.7 53.7 67.8 61.6 74.7
Corr-AE [8] 76.2 86.3 74.3 83.9 75.2 85.1
Corr-Cross-AE [8] 72.8 84.4 74.8 84.4 73.8 84.4
Corr-Full-AE [8] 75.4 86.0 75.5 84.6 75.5 85.3
DCCA [1, 32] 73.9 85.1 76.1 85.0 75.0 85.1
TempXNet-Rec 78.7 86.6 79.9 87.6 79.3 87.1
TempXNet-Cat 78.8 86.6 80.0 87.7 79.4 87.2
TempXNet-Lat 79.1 86.9 79.5 87.4 79.3 87.2
Table 2: Comparison on Cross-Media retrieval (mAP@50 and
nDCG@50) on EdFest2016.
Method % I 7→ T T 7→ I Avg.
mAP nDCG mAP nDCG mAP nDCG
CCA [22] 58.6 75.5 53.3 73.7 56.0 74.6
Bi-AE [17] 64.9 83.8 66.4 83.0 65.7 83.4
Bi-DBN [26] 56,7 78.3 46.7 67.1 51.7 72.7
Corr-AE [8] 67.8 85.8 67.8 83.0 67.8 84.4
Corr-Cross-AE [8] 60.0 80.6 64.3 81.4 62.2 81.0
Corr-Full-AE [8] 68.0 85.4 68.7 83.2 68.3 84.3
DCCA [1, 32] 89.7 96.2 72.4 85.5 81.1 90.9
TempXNet-Rec 94.5 97.4 95.5 97.7 95.0 97.6
TempXNet-Cat 94.0 96.9 93.6 97.3 93.8 97.1
TempXNet-Lat 96.4 98.6 95.5 98.1 96.0 98.4
are more preferred, instead of category-based correlations. This
means that: words temporal behaviour, for this particular dataset,
helps discriminating instances, and instances that occur close to
the query timestamp are more relevant. Such behaviour is expected
when there are sporadic sub-events, provoking shifts on word’s
usage.
On TDF2016 dataset, TempXNet-Cat outperforms all the other
baselines and correlations by a considerable margin. This result indi-
cates that for this dataset, focusing on semantic categories temporal
density distributions helps retrieving more relevant content. This
may be due to the existence of distributions with multiple modes
(e.g. periodic dynamic behaviour). In fact, TDF2016 topics are to
some extent periodic, e.g. stages, mountain races, news regarding
winners, etc.
Figure 3 shows the precision-scope curves for both EdFest2016
and TDF2016 datasets, on the Image-to-Text and Text-to-Image
Table 3: Comparison on Cross-Media retrieval (mAP@50 and
nDCG@50) on TDF2016.
Method %
I 7→ T T 7→ I Avg.
mAP nDCG mAP nDCG mAP nDCG
CCA [22] 58.0 76.9 57.7 75.4 57.8 76.2
Bi-AE [17] 72.5 88.6 67.0 82.2 69.7 85.5
Bi-DBN [26] 64.5 82.9 56.1 74.2 60.3 78.6
Corr-AE [8] 73.5 89.1 71.4 86.1 72.4 87.6
Corr-Cross-AE [8] 70.5 85.9 72.2 86.3 71.4 86.0
Corr-Full-AE [8] 74.1 89.4 71.8 86.5 73.0 88.0
DCCA [1, 32] 88.4 95.5 73.8 86.2 81.1 90.9
TempXNet-Rec 87.2 93.9 89.1 94.6 88.2 94.3
TempXNet-Cat 92.6 96.8 91.5 95.9 92.1 96.4
TempXNet-Lat 88.1 94.7 90.3 95.8 89.2 95.3
tasks. On the x axis we vary the value of k , and the y axis shows
the corresponding mAP@k . On EdFest2016, it can be observed
that TempXNet-Lat always outperforms the remaining correlations.
Similarly, on TDF2016 TempXNet-Cat also outperforms the remain-
ing correlations, which is consistent with the previously discussed
results.
In the presence of datasets with different intrinsic temporal dis-
tributions, our temporal cross-media subspace learning model is
able to effectively model such distributions, provided that a suit-
able temporal correlation is used. Apart from the three temporal
correlations evaluated, TempXNet can accommodate any other
temporal distributions, as long as they can be expressed through a
differentiable function.
Media temporal correlations. In this section we perform a
qualitative analysis of the different temporal correlations. The goal
is to assess how well temporal correlations are captured by each
temporal model. To this end, we query each model and compare
its relevant instances distribution with the true ground-truth tem-
poral distribution. Specifically, we perform two queries, one for
EdFest2016 in which the target are instances of the semantic cate-
gory Castle and one for TDF2016 in which the target are instances
of Crash, respectively. Each query comprises only the textual modal-
ity, corresponding to the T 7→ I setting. The two top performing
temporal correlations (TempXNet-Cat and TempXNet-Lat) and the
DCCA baseline are considered. For each query we show four sam-
ple images. Figure 4 depicts the result of this experiment. Each plot
depicts the temporal distribution of ground-truth (GT) and relevant
instances retrieved by each model, with the x-axis corresponding
to time. We normalise each distribution to the [0, 1] range.
EdFest2016: Castle
TDF2016: Crash
DCCA
TempXNet-Cat
TempXNet-Lat
DCCA
TempXNet-Cat
TempXNet-Lat
Temporal Cross-media Retrieved Images
Figure 4: Quantitative analysis of the different temporal correlations on the EdFest2016 and TDF2016 dataset. Each plot depicts
the temporal distribution of ground-truth instances, from the categories Castle and Crash. We use days as time granularity.
On the EdFest2016 plot, one can observe that the temporal dis-
tribution of the semantic category Castle has multiple peaks, with
the larger ones being on the borders. These correspond to the beg-
ginning and ending of the festival, in which at the end a fireworks
show takes place. Although the temporal correlations are different,
it can be seen that both TempXNet-Cat and TempXNet-Lat are
able to cover both larger peaks, by retrieving relevant instances at
the corresponding moments in time. Even though TempXNet-Cat
achieved a better fit to GT, TempXNet-Lat achieved better retrieval
results. This is may be due to the fact it covers the most salient
peaks. On TDF2016 there were several crashes during the event,
and this is reflected in the peaks of the ground-truth. Given the
somewhat periodic nature of these peaks, TempXNet-Cat reveals
a better fit to the GT curve. The fit of the TempXNet-Lat correla-
tion is slightly worse, as it is based on individual word dynamics,
and despite the periodic shape of the category Crash, words that
occur in Crash instances may not reveal this pattern (e.g. usually it
refers to racers names, etc.). The DCCA baseline completely fails
to capture the temporal distribution of relevant documents. Given
this observations, we verify that our model can effectively grasp
temporal correlations of data.
6 CONCLUSIONS
In this paper we looked into the important problem of modelling
semantically similar media that vary over time. Current state-of-
the-art cross-media methods assume that collections are static, over-
looking visual and textual correlations (and cross-correlations) that
change over time. TempXNet was thoroughly evaluated, exposing
four fundamental concluding points.
Temporal cross-media subspace. A novel approach to cross-
media retrieval was successfully proposed. It derives from the idea
that multimedia data should be organized according to their seman-
tic category and temporal correlations across different modalities.
Several key components make the creation of this subspace possible.
Principled temporal soft-constraints. The creation of the
subspace is temporally constrained by estimating temporal correla-
tions of semantic categories and words, encoding the underlying
dynamics of modalities. The investigated forms of soft-constraints
stem from well-grounded statistical principles leading to a solid
and rigorous optimisation framework. Hence, modality projections
are learned through two coupled neural networks that are jointly
optimised, subject to the aforementioned temporal constraints.
Models of temporal cross-media correlations.We observed
that temporal correlations are seldomly simple as the recencymodel
of temporal correlations was never the best model. In fact, we could
contrast the results in the EdFest2016 and the TDF2016 datasets
and conclude that both datasets follow different distributions: the
EdFest2016 has several one time shows and events, and the TDF2016
contains several repeated events.
Improved retrieval precision in dynamic domains.Account-
ing for temporal cross-media correlations improved cross-modality
retrieval across all datasets. The proposed TempXNet models out-
performed past cross-media models. Moreover, the best retrieval
precision was obtained by the TempXNet-Cat and TempXNet-Lat
that model temporal correlations with different levels of granularity.
ACKNOWLEDGMENTS
This work has been partially funded by the H2020 ICT project COG-
NITUS with the grant agreement number 687605 and by the FCT
project NOVA LINCS Ref. UID/CEC/04516/2013. We also gratefully
acknowledge the support of NVIDIA Corporation with the donation
of the GPUs used for this research.
REFERENCES
[1] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. 2013. Deep Canon-
ical Correlation Analysis. In Proceedings of the 30th International Conference on
International Conference on Machine Learning - Volume 28 (ICML’13). JMLR.org,
III–1247–III–1255. http://dl.acm.org/citation.cfm?id=3042817.3043076
[2] Fabricio Benevenuto, Gabriel Magno, Tiago Rodrigues, and Virgilio Almeida.
2010. Detecting spammers on twitter. In Collaboration, electronic messaging,
anti-abuse and spam conference (CEAS), Vol. 6. 12.
[3] David M. Blei and John D. Lafferty. 2006. Dynamic Topic Models. In Proceedings
of the 23rd International Conference on Machine Learning (ICML ’06). ACM, New
York, NY, USA, 113–120. https://doi.org/10.1145/1143844.1143859
[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet
Allocation. J. Mach. Learn. Res. 3 (March 2003), 993–1022. http://dl.acm.org/
citation.cfm?id=944919.944937
[5] Wei Chen, Tie yan Liu, Yanyan Lan, Zhi ming Ma, and Hang Li.
2009. Ranking Measures and Loss Functions in Learning to Rank.
In Advances in Neural Information Processing Systems 22, Y. Ben-
gio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta
(Eds.). Curran Associates, Inc., 315–323. http://papers.nips.cc/paper/
3708-ranking-measures-and-loss-functions-in-learning-to-rank.pdf
[6] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao
Zheng. July 8-10, 2009. NUS-WIDE: A Real-World Web Image Database from
National University of Singapore. In Proc. of ACM Conf. on Image and Video
Retrieval (CIVR’09). Santorini, Greece.
[7] Mengdi Fan, Wenmin Wang, Peilei Dong, Liang Han, Ronggang Wang, and
Ge Li. 2017. Cross-media Retrieval by Learning Rich Semantic Embeddings of
Multimedia. In Proceedings of the 2017 ACM on Multimedia Conference (MM ’17).
ACM, New York, NY, USA, 1698–1706. https://doi.org/10.1145/3123266.3123369
[8] Fangxiang Feng, Xiaojie Wang, and Ruifan Li. 2014. Cross-modal Retrieval with
Correspondence Autoencoder. In Proceedings of the 22Nd ACM International
Conference on Multimedia (MM ’14). ACM, New York, NY, USA, 7–16. https:
//doi.org/10.1145/2647868.2654902
[9] Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. 2014. A Multi-
View Embedding Space for Modeling Internet Images, Tags, and Their Seman-
tics. Int. J. Comput. Vision 106, 2 (Jan. 2014), 210–233. https://doi.org/10.1007/
s11263-013-0658-4
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).
[11] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 2000. Large Margin Rank
Boundaries for Ordinal Regression. In Advances in Large Margin Classifiers, P. J.
Bartlett, B. Schölkopf, D. Schuurmans, and A. J. Smola (Eds.). MIT Press, 115–132.
[12] Harold Hotelling. 1936. Relations Between Two Sets of Variates. Biometrika 28,
3/4 (Dec. 1936), 321–377. https://doi.org/10.2307/2333955
[13] D. Hu, X. Li, and X. Lu. 2016. Temporal Multimodal Learning in Audiovisual
Speech Recognition. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). 3574–3582. https://doi.org/10.1109/CVPR.2016.389
[14] Gunhee Kim and Eric P. Xing. 2013. Time-sensitive Web Image Ranking and
Retrieval via Dynamic Multi-task Regression. In Proceedings of the Sixth ACM
International Conference on Web Search and Data Mining (WSDM ’13). ACM, New
York, NY, USA, 163–172. https://doi.org/10.1145/2433396.2433417
[15] Jey Han Lau, Nigel Collier, and Timothy Baldwin. 2012. On-line Trend Analysis
with Topic Models: #twitter Trends Detection Topic Model Online. In COLING.
[16] Andrew J. McMinn, Yashar Moshfeghi, and Joemon M. Jose. 2013. Building a
Large-scale Corpus for Evaluating Event Detection on Twitter. In Proceedings of
the 22Nd ACM International Conference on Information & Knowledge Management
(CIKM ’13). ACM, New York, NY, USA, 409–418. https://doi.org/10.1145/2505515.
2505695
[17] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and An-
drew Y. Ng. 2011. Multimodal Deep Learning. In Proceedings of the 28th Inter-
national Conference on International Conference on Machine Learning (ICML’11).
Omnipress, USA, 689–696. http://dl.acm.org/citation.cfm?id=3104482.3104569
[18] Yingwei Pan, Yehao Li, Ting Yao, Tao Mei, Houqiang Li, and Yong Rui. 2016.
Learning Deep Intrinsic Video Representation by Exploring Temporal Coher-
ence and Graph Structure. In Proceedings of the Twenty-Fifth International
Joint Conference on Artificial Intelligence (IJCAI’16). AAAI Press, 3832–3838.
http://dl.acm.org/citation.cfm?id=3061053.3061155
[19] Yuxin Peng, Xin Huang, and Jinwei Qi. 2016. Cross-media Shared Representation
by Hierarchical Learning with Multiple Deep Networks. In Proceedings of the
Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI’16).
AAAI Press, 3846–3853. http://dl.acm.org/citation.cfm?id=3061053.3061157
[20] Y. Peng, X. Huang, and Y. Zhao. 2017. An Overview of Cross-media Retrieval:
Concepts, Methodologies, Benchmarks and Challenges. IEEE Transactions on
Circuits and Systems for Video Technology PP, 99 (2017), 1–1. https://doi.org/10.
1109/TCSVT.2017.2705068
[21] Y. Peng, J. Qi, X. Huang, and Y. Yuan. 2018. CCL: Cross-modal Correlation
Learning With Multigrained Fusion by Hierarchical Network. IEEE Transactions
on Multimedia 20, 2 (Feb 2018), 405–420. https://doi.org/10.1109/TMM.2017.
2742704
[22] Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G.
Lanckriet, Roger Levy, and Nuno Vasconcelos. 2010. A New Approach to
Cross-modal Multimedia Retrieval. In Proceedings of the 18th ACM Interna-
tional Conference on Multimedia (MM ’10). ACM, New York, NY, USA, 251–260.
https://doi.org/10.1145/1873951.1873987
[23] Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake Shakes
Twitter Users: Real-time Event Detection by Social Sensors. In Proceedings of the
19th International Conference on World Wide Web (WWW ’10). ACM, New York,
NY, USA, 851–860. https://doi.org/10.1145/1772690.1772777
[24] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional
Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014).
arXiv:1409.1556 http://arxiv.org/abs/1409.1556
[25] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. 2015. Unsuper-
vised Learning of Video Representations using LSTMs. In Proceedings of the 32nd
International Conference on Machine Learning (Proceedings of Machine Learn-
ing Research), Francis Bach and David Blei (Eds.), Vol. 37. PMLR, Lille, France,
843–852. http://proceedings.mlr.press/v37/srivastava15.html
[26] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In In International Conference on Machine
Learning Workshop.
[27] Mikalai Tsytsarau, Themis Palpanas, and Malu Castellanos. 2014. Dynamics of
News Events and Social Media Reaction. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’14).
ACM, New York, NY, USA, 901–910. https://doi.org/10.1145/2623330.2623670
[28] Tiberio Uricchio, Lamberto Ballan, Marco Bertini, and Alberto Del Bimbo. 2013.
Evaluating Temporal Information for Social Image Annotation and Retrieval. In
Image Analysis and Processing – ICIAP 2013, Alfredo Petrosino (Ed.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 722–732.
[29] Bokun Wang, Yang Yang, Xing Xu, Alan Hanjalic, and Heng Tao Shen. 2017.
Adversarial Cross-Modal Retrieval. In Proceedings of the 2017 ACM on Multimedia
Conference (MM ’17). ACM, New York, NY, USA, 154–162. https://doi.org/10.
1145/3123266.3123326
[30] Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. 2016. A Compre-
hensive Survey on Cross-modal Retrieval. CoRR abs/1607.06215 (2016).
[31] L. Wang, Y. Li, and S. Lazebnik. 2016. Learning Deep Structure-Preserving
Image-Text Embeddings. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). 5005–5013. https://doi.org/10.1109/CVPR.2016.541
[32] F. Yan and K. Mikolajczyk. 2015. Deep correlation for matching images and
text. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
3441–3450. https://doi.org/10.1109/CVPR.2015.7298966
[33] Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A.
Bernal, and Jiebo Luo. 2017. Deep Multimodal Representation Learning from
Temporal Data. In CVPR. IEEE Computer Society, 5066–5074.
[34] T. Yao, T. Mei, and C. W. Ngo. 2015. Learning Query and Image Similarities with
Ranking Canonical Correlation Analysis. In 2015 IEEE International Conference
on Computer Vision (ICCV). 28–36. https://doi.org/10.1109/ICCV.2015.12
[35] M. Zhan, L. Li, Q. Huang, and Y. Liu. 2017. Cross-media retrieval with semantics
clustering and enhancement. In 2017 IEEE International Conference on Multimedia
and Expo (ICME). 1398–1403. https://doi.org/10.1109/ICME.2017.8019310
"
36," Procedia Computer Science  9 ( 2012 )  1334 – 1337 
1877-0509 © 2012 Published by Elsevier Ltd. 
doi: 10.1016/j.procs.2012.04.146 
International Conference on Computational Science, ICCS 2012 
Initial pattern library algorithm for human action recognition 
H.S. Lia, B.H. Xub*
a School of Information and Communication, Guilin University of Electronic Technology,Guilin,China 
bSchool of information science and technology, Beijing Normal University,Beijing,China 
Abstract 
Human action recognition is currently one of the most active research topics in society management, including human moving 
detection, human moving classification, human moving tracking, and activity recognition and description. In this paper, we have
proposed a new classifying and sorting initial pattern library algorithm for human action recognition. First, we classify the 
training vector set to two subsets by vector variance. Secondly, sort the subsets to put the similar pattern vectors together. Last, 
select some number of pattern vectors from the sorted subsets to form the initial pattern library. This new initial pattern library is 
tested by self-organizing maps (SOM) algorithm. Experimental results in image recognition show that this new initial pattern 
library algorithm is better than the common random sampling initial pattern library. 
Keywords: Human action recognition; Self-organizing maps; Initial pattern libary; Image recognition 
1. Introduction 
Human action recognition is currently one of the most active research topics in society management and physical 
education, including human moving detection, human moving classification, human moving tracking, and activity 
recognition and description [1].  
The self-organizing map (SOM) is an unsupervised machine learning algorithm [2], and is widely applied in 
pattern recognition [3, 4] and human action analysis [5, 12]. As it can reveal the complex nonlinear relationship of 
high dimensional data and figure it in low dimensional space, SOM algorithm has been paid lots of attention.  
In human action analysis and recognition, the initial pattern library plays an important part which reflects the start 
of training. The random algorithm is the simplest initial pattern library algorithm while it has several disadvantages, 
such as limited adaptability to the source, poor learning efficiency and convergence performance. Splitting 
algorithm is another common initial pattern library algorithm which is more efficient than random algorithm, but it 
is more complex and needs a great deal of computation. 
In order to improve the performance of the pattern library, a new classifying and sorting initial pattern library 
algorithm is proposed in this paper. First, we classify the training vector set to two subsets based on vector variance. 
Secondly, sort the subsets to put the similar pattern vectors together. Last, select some number of pattern vectors 
E-mail address: hongsongli@yahoo.cn (H.S. Li). 
Available online at www.sciencedirect.com
Open access under CC BY-NC-ND license.
1335 H.S. Li and B.H. Xu /  Procedia Computer Science  9 ( 2012 )  1334 – 1337 
from the sorted subsets to form the initial pattern library. 
2. SOM Algorithm 
Step-1: Given a neural network of size  and a training vector set , where  MN , }1,,1,0),({ −= LttX N  
is the size of pattern library, M  is the size of each pattern vector and  is the number of training vectors. Initialize 
the neighborhood  and the initial pattern library .  
L
}1−N,,1,00({NEj ), =j }1,,1,0)0({ −= NjjW
Step-2: Input a new training vector . )(tX
Step-3: Compute the distortion  between  and each pattern vector . Select the minimum 
distortion pattern vector  given by 
)(td j )(tX )(tjW
)(* tjW
)(min)(
10
* tdtd jNjj −≤≤=  
Step-4:  Modify  and its neighboring pattern vectors by )(* tjW
else
tNEjj
t
tttt
t j
j
jj
j
)(,
)(
)]()()[()(
)1( *
*∈−+
=+
W
WXW
W
α
 
Where, 
j
 is Euclidean distance neighborhood around the minimum distortion pattern vector which is 
decreased with  given by 
)(* tNE
t
1
*
/
10)(
Tt
j eAAtNE
−+=
 
Where,  are constants determining the neighborhood range. T  is a constant, determining the decreasing rate. 0 1 1
The learning rate 
A A
)(tα  ( 1)(0 ≤≤ tα ) determines the modification value of pattern vector. Theoretically, if 
)(tα  is small enough, the average error function of the system will reach the minimum after a long time training. In 
practice, learning rate is usually determined by 
2
2 /
2)(
TteAt −=α  
Where,  is a constant determining the maximum of learning rate, and  is a constant determining the decreasing 
rate. 
2A 2T
Step-5: Go to Step-2. 
3. Initial Pattern Library Algorithm 
3.1. Random  Algorithm 
Random algorithm can be classified to random data setting and random sampling. In random data setting 
algorithm, the initial pattern vector are set to random data. Because the initial pattern library is independent with the 
training set, and some invalid pattern vector often exist in the pattern library after training, thus random data setting 
algorithm is not usually used. 
In random sampling algorithm, the initial pattern vectors are selected randomly from the training set. For example, 
if the training set is denoted by , the initial pattern library can be obtained by selecting 
vectors , where . This algorithm has two advantages: it doesn’t need any 
}1,,1,0),({ −= LttX
)) p NLp /=1((,),(),0( Np −XXX
1336   H.S. Li and B.H. Xu /  Procedia Computer Science  9 ( 2012 )  1334 – 1337 
calculation; there will be no invalid pattern vectors in the pattern library. However, sometimes many similar vectors 
or unrepresentative vectors are selected, and the pattern library performance will degrade. 
3.2. Splitting Algorithm 
The splitting algorithm [9] proposed by Linde, Buzo and Gray can be implemented as follows: 
Step-1: Compute the centroid (represented by ) of all training vectors, split  to two close vector 
 and , where e  is a fix perturbation vector. 
)0(W )0(W
eWW += )0()0(1 eWW −=
)0()0(
2
Step-2: Use  as the initial pattern library, design the pattern library with two pattern vectors by LBG 
algorithm. The resulting pattern library is represented by .
},{ )0(2
)0(
1 WW
},{ )1(2
)1(
1 WW
Step-3: Split  to four pattern vectors as Step-1. },{ )1(2
)1(
1 WW
Step-4: Design the pattern library with four pattern vectors as Step-2, and the resulting pattern vectors are split to 
eight pattern vectors. Repeat this procedure until  initial pattern vectors are created. N
The defect of splitting algorithm is too complex, and it is not suitable for SOM algorithm. 
3.3. Classifying and Sorting Algorithm 
As variance reflects the frequency feature of the image block (i.e. pattern vector), and different frequencies have 
different impacts on human vision, we consider dividing the vectors in the training set into two categories by 
variance. Then training is performed on each category respectively which can save a lot of training time and make 
the training more pertinent. Last, pattern libraries are combined to one pattern library after training. The classifying 
and sorting algorithm is described as follows: 
Step-1: Compute the variance of each vector in the training set, then classify the vectors to low-frequency or high-
frequency by a variance threshold, the results are represented by , .  )}({ tLX )}({ tHX
Step-2: Sort and  as follows (taking as example):  )}({ tLX )}({ tHX )}({ tLX
Set  to be the first initial pattern vector , calculate the distance between  and the rest vectors in 
, take the vector with the shortest distance ( i.e. the nearest one to ) to be the second initial pattern vector 
, then take the vector  from the residual vectors who is the nearest to  to be the third initial pattern vector. 
Repeat this procedure to the end. Finally, we get a sorted low-frequency initial pattern library represented by .
)0(LX
)}t
0W 0W
({ LX
1W
0W
W2W 1
)}(~{ tLX
Step-3: Select pattern vectors with an interval from  and  to get the low-frequency and high-
frequency initial pattern libraries. 
)}(~{ tLX )}(
~{ tHX
In our experiments, the sizes of low-frequency and high-frequency pattern library are 410 and 1638. Then the 
size of the whole pattern library is 2048. 
4. Experimental Results 
We have simulated three initial pattern library design algorithms using three standard testing images (‘Lena’, 
‘Pepper’ and ‘Fishing boat’). The size of image block is . Distortion measure is calculated 
by
88×=M
2)()()( tttd j WX −= .
The reconstructed image quality is measured by the PSNR, , and MSE is the mean 
square error between original image and reconstructed image. 
)/255(log10 210 MSEPSNR =
1337 H.S. Li and B.H. Xu /  Procedia Computer Science  9 ( 2012 )  1334 – 1337 
The experimental results are shown in Table 1for PSNR of the reconstruction images. We can see that SOM 
algorithm is better than LBG algorithm. Our classifying and sorting algorithm is better than random sampling 
algorithm, and the average PSNR improvement is about 1.5 dB. 
Table1.  PSNR of reconstructed images 
Training algorithm Initial pattern algorithm Lena (dB) Pepper (dB) Boat (dB) Average (dB) 
Random Sampling 30.05 30.49 28.06 29.39 LBG
Splitting 30.98 31.97 29.20 30.56 
Random Sampling 30.73 31.51 29.88 30.71 SOM
Classifying and Sorting 32.36 32.97 31.53 32.55 
References 
1. S. Sempena, N.U Maulidevi. P.R. Aryan, Human action recognition using dynamic time warping. 2011 International Conference on 
Electrical Engineering and Informatics (ICEEI), 2011, pp. 1-5. 
2. T. Kohonen, An introduction to neural computing, Neural Networks, Jan. 1988, pp.3. 
3. L.R. Ezequied, Invariant pattern identification by self-organizing network, Pattern Recognition Letters, 22(2001), pp. 983. 
4. V.V. Gafiychuk, B.Y. Datsko, Analysis of data clusters obtained by self-organizing methods, Physical Statistical Mechanics and its 
Applications, 341(2004), pp. 547. 
5. J.K. Aggarwal, Human activity recognition - a grand challenge Signals, Circuits and Systems, International Symposium,  2(2007), pp. 1-5. 
6. W. Huang,, Human action recognition based on Self Organizing Maps,  2010 IEEE International Conference on Acoustics Speech and 
Signal Processing (ICASSP2010), pp. 2130-2133. 
7. L.Szerdiova, D. Simsik, Z. Dolna, Standing long jump and study of movement dynamics using human motion analysis, 2010 IEEE 8th 
International Symposium on Applied Machine Intelligence and Informatics (SAMI2010),  pp. 263 – 266.
8. H. Liu, S. Lin, Y.D. Zhang, K. Tao, Automatic video-based analysis of athlete action, 14th International Conference on Image Analysis 
and Processing(ICIAP 200) , pp. 205 – 210, 2007. 
9. K.F. Sim, K. Sundaraj, Human motion tracking on broadcast golf swing video using optical flow and template matching, 2010 International 
Conference on Computer Applications and Industrial Electronics (ICCAIE2010),  pp.169-173.  
10. G.Y. Zhu, Q.M. Huang, Human behavior analysis for highlight ranking in broadcast racket sports video, IEEE Transactions on 
Multimedia,vol. 9 ,  6( 2007) , pp. 1167 – 1182. 
11. P.U. Maheswari, M. Rajaram, A novel approach for mining association rules on sports data using principal component analysis: for 
cricket match perspective, 2009 IEEE International Conference on Advance Computing(IACC 2009), pp.1047-1080. 
12. J.H. Ma, P.C.Yuen, W.W. Zou; J.H. Lai, Supervised neighborhood topology learning for human action recognition , 2009 IEEE 12th 
International Conference on Computer Vision (ICCV2009), pp. 476-481. 
"
37,"Learning Visual Features from Snapshots for Web Search
Yixing Fan†,‡, Jiafeng Guo‡, Yanyan Lan‡, Jun Xu‡, Liang Pang†,‡ and Xueqi Cheng‡
†University of Chinese Academy of Sciences
‡CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology
{fanyixing,pangliang}@soware.ict.ac.cn,{guojiafeng,lanyanyan,junxu,xqc}@ict.ac.cn
ABSTRACT
When applying learning to rank algorithms to Web search, a large
number of features are usually designed to capture the relevance
signals. Most of these features are computed based on the extracted
textual elements, link analysis, and user logs. However, Web pages
are not solely linked texts, but have structured layout organizing a
large variety of elements in dierent styles. Such layout itself can
convey useful visual information, indicating the relevance of a Web
page. For example, the query-independent layout (i.e., raw page lay-
out) can help identify the page quality, while the query-dependent
layout (i.e., page rendered with matched query words) can further
tell rich structural information (e.g., size, position and proximity) of
the matching signals. However, such visual information of layout
has been seldom utilized in Web search in the past. In this work, we
propose to learn rich visual features automatically from the layout
of Web pages (i.e., Web page snapshots) for relevance ranking. Both
query-independent and query-dependent snapshots are considered
as the new inputs. We then propose a novel visual perception model
inspired by human’s visual search behaviors on page viewing to
extract the visual features. is model can be learned end-to-end to-
gether with traditional human-craed features. We also show that
such visual features can be eciently acquired in the online seing
with an extended inverted indexing scheme. Experiments on bench-
mark collections demonstrate that learning visual features from
Web page snapshots can signicantly improve the performance of
relevance ranking in ad-hoc Web retrieval tasks.
CCS CONCEPTS
•Information systems →Learning to rank;
KEYWORDS
Web Search; Visual Feature; Snapshot
1 INTRODUCTION
Modern search engines have widely adopted learning to rank (LTR)
methods for Web page ranking. A fundamental step of LTR meth-
ods is to design a large number of features which are capable of
characterizing the relevance between a document and a query. As
revealed in literature, most of these features are computed based on
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permied. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
CIKM’17, November 6–10, 2017, Singapore.
© 2017 ACM. ISBN 978-1-4503-4918-5/17/11. . .$15.00
DOI: hps://doi.org/10.1145/3132847.3132943
the extracted textual elements (e.g., title, main content, and anchor
texts), link analysis (e.g., PageRank and HITS) and user logs (e.g.,
clickthrough ratio). For instance, in the well-known LETOR 4.0
collection [26], there are 46 human-craed features in total, among
which 42 features are constructed based on the textual elements
(e.g., term frequencies, BM25 and language model scores based on
title, body, anchor texts, and URL), and 4 features are based on
link analysis (e.g., PageRank, inlink number, outlink number, and
number of child page).
However, Web pages are not solely linked texts, but have struc-
tured layout organizing a large variety of elements in dierent
styles. Such layout itself can convey useful visual information,
indicating the relevance of a Web page. In the rst place, the query-
independent layout, i.e., the raw Web page layout, can help identify
the page quality. For example, a high-quality Web page of news,
blog or review is oen well structured with head bars, side bars, and
main body containing rich textual content, as shown in Figure 1 (A).
Users can perceive the authority of a page from its formal layout.
On the contrary, a low-quality Web page may contain many oating
images and advertisement with useless information, as shown in
Figure 1 (B). Secondly, the query-dependent layout, i.e., the Web
page rendered with matched query words, can further tell rich
structural information of the matching signals. For example, on a
relevant Wikipedia page of the query “national park” as shown in
Figure 1 (C), we can observe that there are many matching signals
distributed in the main content, large matching signals in the title,
and high spacial proximity between these matching signals. On the
contrary, an irrelevant Web page of “national park” could also con-
tain a number of query words but in which the matching signals may
distribute in side areas (e.g., advertisement) as shown in Figure 1
(D), or even be invisible in some spamming pages due to keyword
stung. In summary, search users may perceive many useful visual
information from the Web page layout for relevance judgment,
while such information have not been eectively modeled in Web
search in literature.
ere have been a few studies aempting to take into account the
layout information in Web search but from a non-visual way. For
example, Zhou et al. [37] observed that pages like tables and lists are
unlikely to be relevant for ad hoc queries, and assumed such pages
have unusual word distributions. Based on this assumption, they
constructed two content features to estimate the Web page quality.
Bendersky et al. [3] designed page quality features relate to the
readability, layout, ease-of-navigation and so on from HTML tags
and textual content. ese features are used to promote high-quality
pages and penalize low-quality pages in Web search. Obviously,
all the above methods utilized the layout information indirectly
(i.e., dened from textual content or HTML tags) with manually
ar
X
iv
:1
71
0.
06
99
7v
1 
 [c
s.I
R]
  1
9 O
ct 
20
17
Figure 1: Snapshots of dierent kinds ofWeb page: (A) High qualityWeb pages with formal layout; (B) Low qualityWeb pages
with images and advertisement; (C) A relevant page of the query “national park” with keyword matching highlighted; and (D)
A non-relevant page of the query “national park” with keyword matching highlighted.
designed features. is may largely restrict the exploitation of the
visual information in Web page for relevance modeling.
In this paper, we propose to learn rich visual features automatic-
ally from the layout of Web pages for relevance ranking. Specically,
we take the snapshot of a Web page (i.e., a rendered image of the
Web page) as a new type of input in the learning to rank framework.
Both query-independent and query-dependent snapshots have been
introduced in our work. We then propose a visual perception (ViP)
model inspired by human’s visual searching behaviors on page
viewing (i.e., F-biased viewing paern [9]) to extract visual features
from the snapshots for relevance ranking. Specically, the ViP
model is a neural model which contains four stacked layers, namely
snapshot segmentation layer, local perception layer, sequential ag-
gregation layer, and relevance decision layer. e proposed ViP
model can be learned in an end-to-end way together with traditional
human-craed features. Besides, for practical implementation of
the ViP model, we also introduce an ecient indexing scheme for
Web page snapshots.
We evaluate the eectiveness of the proposed model based on
two representative ad-hoc retrieval benchmark datasets from the
LETOR collection [26]. For comparison, we take into account some
well-known traditional retrieval models as well as several state-of-
the-art learning to rank models. e empirical results show that our
model outperform all the baselines in terms of all the evaluation
metrics. We also provide detailed analysis on the proposed model,
and conduct case studies to provide beer understanding on the
learned visual signals.
e main contributions of this paper include:
1. We argue that visual information from Web page layout
is valuable for relevance modeling in Web search, and
introduce Web snapshots as an additional input for learning
visual features.
2. We propose a novel visual perception model over the Web
snapshots, which can automatically learn visual features
for relevance modeling in an end-to-end way.
3. We conduct rigorous comparisons over existing repres-
entative retrieval models, and demonstrate that learning
visual features from Web snapshots can signicantly im-
prove the performance of relevance ranking in ad-hoc Web
retrieval tasks.
2 RELATEDWORK
In this section, we briey review three research areas related to our
work, including visual search, Web page quality based on layout
and Web search using layout information.
2.1 Visual Search
Visual search, which studies how visual elements aect users’ in-
formation seeking experience and how users view Web pages, has
been extensively studied in the past decades [18, 28, 34].
Although Web pages mainly rely on textual content to convey in-
formation, page layout has been recognized as of great importance
in information seeking experience1. It is imperative that Web pages
are constructed to enable a high level of usability for all users [30],
but poorly designed layouts can quickly lead to fatigue, with a
resultant lowering of speed and accuracy on task performance [32].
In [20], Larson et al. studied the principles for the design of multiple
hyperlinks on a Web page for information retrieval tasks. ey
1hps://www.nngroup.com/articles/let-users-control-font-size/
showed a medium structure in terms of depth and breadth outper-
formed the broadest but shallow structure overall. In [21], Ling et
al. studied the eect of the combination of text and background
color on visual search performance and subjective preference. ey
found that higher contrasts between text and background color
led to faster searching and were rated more favourable. Ling [22]
explored the inuence of the font type and line length on two tasks,
i.e., visual search and information retrieval. ey found the eect
of line length was signicant, while font type has lile impact on
task performance. Pearson et al. [24] studied the eect of spatial
layout and link color in Web pages on performance of visual search
and interactive search tasks. ey found that both link color and
presentation position of menus have signicant eect on user’s
information seeking experience.
Besides these above analysis, there have also been a line of
studies on how users view Web pages. It has been widely accepted
that users’ Web-viewing behavior is signicantly dierent from that
on natural images. Specically, several distinct paerns have been
revealed in the past work, such as F-biased viewing paern which
scans a page in “F” shape [9, 28], and banner blindness paern
which avoids banner-like advertisement [14].
ese previous studies have shown that the layout of a Web page
has signicant impact on users’ information seeking experience
and also inspired us on model designing for learning visual features
from Web page layouts.
2.2 Web Pageality based on Layout
In the eld of Human-Computer Interaction (CHI), it has been
widely studied how dierent layouts of Web pages aect users’
quality decisions on Web pages.
In [10], Fogg et al. conducted an online study that investigated
how dierent elements of Web sites aect people’s perception of
credibility. ey found seven types of elements, where ve types
increase credibility perceptions and two hurt credibility. Fogg et
al. [11] gathered 2684 people’s comments about the evaluation on
credibility of two live Web sites. ey found the “design look” of
the Web page the most prominent issue when people evaluated
Web page credibility.
Besides the above questionnaire methods, there have been a line
of studies on the eect of the layout based on automatic analysis.
One of the rst automatic assessment systems originated in Web
engineering. Syntax checkers were employed over HTML codes
to analyze the quality of Web pages [4]. Chakrabarti et al. [6]
introduced six features of Web pages, such as the dominant color,
the presence of advertisement, logos, animations, frames, and the
frequency of links and graphics, to analyze the quality of Web
pages. ey found that pages which follow popular Web design
guidelines might aract more viewers than other pages. Mandl [23]
extracted about 100 features of Web pages by a page proler as
indicators of the quality. ese features are mainly derived from
the HTML code and try to capture design aspects, for example,
number of list, number of colors, number of DOM elements and
so on. Song et al. [31] utilized a vision-based page segmentation
algorithm to partition a Web page into semantic blocks based on
the visual layout information. Spatial features (e.g., position and
size) and content features (e.g., the number of images and links)
were extracted to estimate the importance of each blocks.
All these studies demonstrated that the Web page layout has
strong impact on users’ perception of the Web page quality.
2.3 Web Search using Layout Information
ere have been a few studies aempting to take into account the
layout information in Web search. For example, Zhou et al. [37]
observed that Web pages like tables and lists are unlikely to be
relevant for ad hoc queries, and assumed such pages have unusual
word distributions. Based on this assumption, they constructed
two content features to estimate the Web page quality. ey incor-
porated the page quality into language model and demonstrated
that it can signicantly outperform the query likelihood model.
Bendersky et al. [3] presented a quality-biased ranking method that
promotes Web pages containing high-quality content, and penal-
izes low-quality Web pages. In their model, the quality of the page
content is determined by the readability, layout, ease-of-navigation
and so on. eir results showed that by taking into account the
quality of the Web page, consistent retrieval performance improve-
ment could be obtained as compared with the methods relying on
text-based and link-based features. Pirlo et al. [25] presented a
layout-based retrieval system to search commercial forms. ey
constructed layout-based features from the extracted grid-based
structural components. ey demonstrated the eectiveness of the
layout information in retrieval of commercial forms.
Although the Web page layout has been considered in retrieval
in previous work, the existing models usually utilized the layout
information indirectly (dened from textual content or HTML tags)
with manually designed features. is may largely restrict the ex-
ploitation of the visual information in Web page layout for relevance
modeling.
3 OUR APPROACH
In this section, we describe our model on learning visual features
from Web page layout for relevance ranking in detail. Specically,
we take the snapshot of a Web page as the input, and propose a
visual perception (ViP) model to extract visual features from the
snapshots. is model is learned end-to-end together with tradi-
tional human-craed features. In the following, we rst introduce
the snapshot construction process. We then talk about the ViP
model and model training in detail. Finally, we discuss a new in-
dexing scheme for the implementation of our ViP model.
3.1 Snapshot Construction
Typically, a Web page is a document wrien in HTML or comparable
markup language, organizing various Web resource elements in
a structured way. Web browsers coordinate the various elements
for the wrien page to present the Web page to users. In order
to leverage the layout information of Web pages, we propose to
render the source Web page into a snapshot as is shown in Web
browsers perceived by search users. is render process could be
eciently conducted using an simple render tool.
In our work, we consider two types of snapshots for a Web
page, namely query-independent snapshot and query-dependent
snapshot. e query-independent snapshot captures the raw Web
Figure 2: (A)ery-independent snapshot. (B)ery-dependent snapshot.
page layout information, which can be directly generated using the
render tool over the raw Web page source code, as illustrated in
Figure 2 (A) . e query-dependent snapshot, on the other hand,
aims to capture the Web page layout information as well as match-
ing signals given a specic query. is is to simulate how users
perceive a Web page given the information need. We achieve this
by highlight the matched query words on a Web page using some
background color, as shown in Figure 2 (B) . In this work, all the
query words are rendered with the same background color for sim-
plicity. In fact, one may use dierent colors for dierent words to
convey richer information (e.g., query word importance) and we
will leave this as our future work.
3.2 e Visual Perception Model
Given the snapshot of a Web page, here we aim to design a model
that can learn visual features automatically for relevance ranking.
As the snapshot is an image, a simple idea is to directly employ
an existing neural model, e.g., the convolutional neural network
(CNN), for this purpose. However, users’ viewing paerns on Web
pages may not be the same as that on the general image [29]. A
model that can beer t users’ viewing paerns on Web pages may
lead to beer feature learning performance on the snapshots.
In fact, there have been extensive studies on how users view Web
pages in the eld of visual search [9, 18, 28]. It has been widely ac-
cepted that users are accustomed to reading row by row from top to
boom, which forms the well-known F-biased viewing paern [9].
Inspired by these observations, we propose a deep neural model
that can simulate the F-biased viewing paern of search users on
Web pages to extract visual features from snapshots. We refer to
our model as a visual perception (ViP) model. e architecture of
the ViP model is depicted in Figure 3, which contains four stacked
layers, namely snapshot segmentation layer, local perception layer,
sequential aggregation layer, and relevance decision layer. In the
following, we will introduce these layers in detail.
3.2.1 Snapshot Segmentation Layer. e snapshot segmentation
layer focuses on producing a set of region proposals. is is a
typical step for image processing and dierent region segmentation
methods have been proposed in dierent tasks, such as selective
search [33], objective detection [7, 13], multi-scale combinatorial
grouping [1] and so on. In this work, we propose to generate a set
of horizontal region proposals to simulate the row by row scanning
behaviors in F-biased viewing paern. Specically, we segment a
snapshot into several horizontal regions with equal height as shown
in Figure 3. Dierent heights actually capture dierent granularity
in row scanning, and we have studied the height eect in Section
4.4. Formally, given an input snapshot image I , a set of region
proposals P = {p1,p2, ...,pN } are generated, where N denotes the
number of region proposals.
3.2.2 Local Perception Layer. Based on the above region propos-
als, we employ a convolutional neural network, which is good at
producing abstract image features, to generate row features from
each local region. Specically, for each region proposal pi , the
k−th kernel W(1,k ) scans over the proposal Z(0) = pi to generate a
feature map Z(1,k ):
Z(1,k )i, j = σ (
rk−1∑
s=0
rk−1∑
t=0
W(1,k )s,t · Z(0)i+s, j+t + b(1,k )) , (1)
where rk denotes the size of the k−th kernel. In this paper, we use
square kernel, and adopt ReLU [8] as the activation function σ . W
and b are parameters to be learned. We take a max-pooling aer
each convolution:
W(2,k )i, j = max0≤s≤dk
max
0≤t ≤dk
Z(1,k )i ·dk+s, j ·dk+t , (2)
where dk denotes the width of the pooling kernel.
Aer the rst convolution and max pooling layer, we continue
to obtain higher abstract features Z(l ), l ≥ 2 by further convolution
and max pooling, with general formulations:
Z(l+1,k
′ )
i, j = σ
( cl−1∑
k=0
rk−1∑
s=0
rk−1∑
t=0
W(k+1,k
′ )
s,t ·Z(l,k)i+s, j+t+b(l+1,k )
)
, l = 2, 4, 6...
(3)
Z(l+2,k
′ )
i, j = max0≤s≤dk
max
0≤t ≤dk
Z(l+1,k
′ )
i ·dk+s, j ·dk+t , (4)
where cl denotes the number of feature maps in the l−th layer.
Finally, the output of the last max pooling layer Z(l ) is aened
as the row feature qi of the local region proposal pi .
3.2.3 Sequential Aggregation Layer. Based on the row features
Q = {q1, q2, q3, ..., qN } generated in the local perception layer,
we aempt to generate the overall visual features by aggregating
these row features. We adopt the recurrent neural network which
naturally ts the sequentially scanning behaviors (i.e., from top
to boom) in F-biased viewing paern. Here, we use long-short
Figure 3: e Architecture of the ViP Model
term memory network (LSTM) [15], a powerful model for variable-
length sequential data, to accumulate the features qi from each local
region. Specically, as shown in Figure 3, we feed the row features
into LSTM sequentially to generate the accumulated features at
dierent positions as follows.
it = σ (Wiqt + Uiht−1 + bi ) , (5)
ft = σ (Wf qt + Uf ht−1 + bf ) , (6)
ct = ft ct−1 + it tanh(Wcqt + Ucht−1 + bc ) , (7)
ot = σ (Woqt + Uoht−1 + bo ) , (8)
ht = ot tanh(ct ) , (9)
where qt denotes the t-th proposal features, it ,ft ,ot denote the
input, forget, and output gates respectively, ct denotes the inform-
ation stored in memory cell and ht denotes the t-th accumulated
evidence, Wi ,Wf ,Wo , Wc , Ui ,Uf ,Uo ,Uc ,bi ,bf ,bo and bc are para-
meters to be learned, σ denotes the sigmoid function.
We take the last output of the LSTM model as the visual features
hN of the snapshot.
3.2.4 Relevance Decision Layer. To generate the nal relevance
score of a Web page, we leverage both the extracted visual fea-
tures from the snapshot and traditional human-craed features.
Specically, we concatenate the visual feature vector hN with the
traditional feature vector u to form the nal relevance features v.
We then feed the relevance feature v into a 2-layer feedforward
neural network to get the nal relevance score s.
s = W1 · σ (W0 · v + b0) + b1 , (10)
where W0, W1, b0, and b1 are parameters to be learned, σ denotes
the ReLU function. In this way, the ViP model can be learned end-
to-end to automatically extract visual features from Web snapshots
as well as to produce a relevance model.
3.3 Model Training
Since the ad-hoc Web retrieval task is fundamentally a ranking
problem, we utilize the pairwise ranking loss such as hinge loss to
train our model. Specically, given a triple (q,d+,d−), where d+
is ranked higher than d− with respect to query q, the hinge loss
function is dened as:
J(q,d+,d−;θ ) = max(0, 1 − s(q,d+) + s(q,d−)) ,
where s(q,d) denotes the relevance score for (q,d), and θ includes
the parameters in the local perception layer, sequential aggregation
layer, and relevance decision layer. It is worth noting that the overall
model is a combination of traditional human-craed features and
learned visual features, where the traditional features are static
without learning in this model. In this way, the model can easily be
biased to the visual features. us, we introduced `2 regularization
terms into the loss function,
L(q,d+,d−;θ ) = J(q,d+,d−;θ ) + λ1 ‖Φ1‖22 + λ2 ‖Φ2‖22 ,
where Φ1 denotes parameters in the local perception layer and
sequential aggregation layer, Φ2 denotes parameters in the relev-
ance decision layer, λ1 and λ2 denote the corresponding regularizer
co-ecient, respectively. In this way, we can introduce stronger
regularization onW1 to alleviate the model bias. e optimization is
relatively straightforward with standard backpropagation. We ap-
ply stochastic gradient decent method Adam [17] with mini-batches
(100 in size), which can be easily parallelized on single machine
with multi-cores.
3.4 e Indexing Scheme of Snapshots
In order to implement the ViP model for practical Web search, we
need an ecient indexing scheme for the Web page snapshots.
For query-independent snapshots, it is simple to implement since
we only need to associate each Web page snapshot to its page
ID. For query-dependent snapshots, we propose an ecient index-
ing scheme that can be well incorporated into the widely adopted
keyword based inverted indexing for implementation. Specically,
during the traditional inverted indexing construction process, for
each keyword in a Web page, we generate a keyword-dependent
snapshot and recognize all the highlighted positions of this keyword
in the snapshot. Each highlighted position is actually a rectangle, so
we use the position of the top-le and boom-right of the rectangle
Figure 4: e indexing scheme to incorporate snapshot information into the inverted les.
Table 1: Statistics of the datasets used in this study.
#queries #pages #q rel #rel per q
MQ2007 1692 65,323 1455 10.3
MQ2008 784 14,384 564 3.7
to record it. Specically, we record the relative osets of these
two positions, denoted by (Px , Py ) and (Qx ,Qy ) in Figure 4, with
respect to the top-le corner of the snapshot. We append these
osets to the posting les aer the original position of the keyword
in the page, denoted by Oset. In this way, the keyword-dependent
snapshot can be discarded since all the highlighted positions have
been recorded.
At the testing time, given a query and a candidate Web page, we
can obtain all the highlighted positions in the snapshot of all the
query keywords during the posting list merging process. We can
also obtain the query-independent snapshot of the Web page by the
page ID. In this way, we can easily generate the query-dependent
snapshot by simply modifying the corresponding pixels at all the
highlighted positions to some predened color based on the query-
independent snapshot.
4 EXPERIMENT
In this section, we conduct experiments to demonstrate the eect-
iveness of our proposed model on benchmark collections.
4.1 Experimental Settings
We rst introduce our experimental seings, including datasets,
baseline methods/implementations, and evaluation methodology.
4.1.1 Data Sets. To evaluate the performance of our model, we
conducted experiments using two LETOR benchmark datasets [26]:
Million ery Track 2007 (MQ2007) and Million ery Track 2008
(MQ2008). Both datasets use the GOV2 collection which includes 25
million Web pages in 426 gigabytes. We choose these two datasets
according to three criteria: 1) the dataset is public; 2) there are a
large number of queries compared with other public datasets; 3)
the source Web page is available. e details of the two datasets are
given in Table 1. As we can see, there are 1692 queries on MQ2007
and 784 queries on MQ2008. However, the number of queries with
at least one relevant page is 1455 and 564, respectively. e average
number of relevant page per query is about 10.3 and 3.7 on MQ2007
and MQ2008, respectively. In LETOR, there are 46 human-craed
features for each query and document pair, as described in the
Introduction section.
4.1.2 Snapshot Pre-Processing. For each Web page, we gener-
ated both query-independent and query-dependent snapshots us-
ing a render tool. During rendering, we found 180 pages in total
failed to generate the snapshots due to the missing link objects (e.g.,
stylesheet) over the two collections. For these pages, we employed a
fake snapshot by averaging all other snapshots. It is worthy noting
the length of the snapshots may vary signicantly over dierent
Web pages. To make it simple and ecient, we only kept the rst
screen size of the snapshot and down-sampled it to xed resolution
(i.e., 64×64), which corresponds to the rst impression of the page
for search users. We have also studied the performance of snap-
shots at dierent resolutions in Section 4.5. Besides, a simple image
normalization was conducted by removing the average pixel values
per data point, and then re-scaling linearly the range to [-1, 1].
4.1.3 Baseline Methods. We adopt two types of baselines for
comparison, including traditional retrieve models and the state-
of-the-art learning to rank models. Traditional retrieval models
include
QL: ery likelihood model is one of the best performing
language models based on Dirichlet smoothing [36].
BM25: e BM25 formula [27] is another highly eective
retrieval model that represents the classical probabilistic
retrieval model.
Learning to rank models include
RankSVM: RankSVM [16] is a representative pairwise learn-
ing to rank model based on SVMstruct.
RankBoost:RankBoost [12] formalizes learning to rank as
a problem of binary classication, and combines a set of
weak rankers as nal ranking function based on boosting
approach.
AdaRank: AdaRank [35] is another boosting approach which
aims to directly optimize the performance measure. Here
we utilize NDCG as the performance measure function.
LambdaMart: LambdaMart [5] is a state-of-the-art learning
to rank algorithm that uses gradient boosting to produce
an ensemble of retrieval models.
Table 2: Analysis of the ViP model over the MQ2007 and MQ2008 datasets. Signicant improvement or degradation with
respect to ViPBaseline is indicated (+/-) (p-value ≤ 0.05).
MQ2007
Model Name P@1 P@5 P@10 NDCG@1 NDCG@5 NDCG@10 MAP
without snapshot ViPBaseline 0.478 0.416 0.386 0.410 0.415 0.445 0.467
query independent
snapshot
ViPCNN 0.482 0.428+ 0.390 0.418+ 0.427+ 0.451 0.472
ViP 0.494+ 0.435+ 0.397 + 0.425+ 0.436+ 0.461+ 0.476
query dependent
snapshot
ViPCNN 0.486 0.430+ 0.393 0.421+ 0.430+ 0.453 0.475
ViP 0.505+ 0.439+ 0.398+ 0.434+ 0.441+ 0.464+ 0.481+
MQ2008
Model Name P@1 P@5 P@10 NDCG@1 NDCG@5 NDCG@10 MAP
without snapshot ViPBaseline 0.437 0.340 0.248 0.365 0.472 0.228 0.473
query independent
snapshot
ViPCNN 0.449+ 0.343 0.249 0.372 0.473 0.229 0.477
ViP 0.458+ 0.346 0.250 0.382+ 0.475 0.230 0.480
query dependent
snapshot
ViPCNN 0.454+ 0.346 0.249 0.375+ 0.474 0.229 0.479
ViP 0.466+ 0.356+ 0.252+ 0.396+ 0.494+ 0.235+ 0.494+
For RankSVM, we directly use the implementation in SVMrank [16].
RankBoost, AdaRank, and LambdaMart are implemented using
RankLib2, which is a widely used learning to rank tool.
We refer to our proposed model as ViP. For network cong-
urations (e.g., numbers of layers and hidden nodes), we tune the
hyper-parameters on a validation set. Specically, in the local per-
ception layer, we set the size of region proposal to 4 × 64. We have
also studied the performance of dierent proposal sizes in Section
4.4. For each proposal, there are 2 convolution layer each followed
by a max pooling layer. In the rst convolution layer, there are
8 kernels whose sizes are all set to 2 × 2, and the following max
pooling size is 2× 2 with strides set to 2. In the Second convolution
layer, there are 16 kernels whose sizes are all set to 2 × 2, and the
following max pooling size is the same as the previous max pooling
layer. us, we get a 1 × 16 vector feature for each region proposal.
In the sequential aggregation layer, the dimension of LSTM is set to
10. In the nal relevance decision layer, the multi-layer perceptron
is a 2-layer feed forward neural network with one hidden layer
whose dimension size is set to 10. e regularization parameters λ1
and λ2 are set to 0.0005 and 0.0001, respectively. All the other train-
able parameters are initialized randomly by uniform distribution
within [−0.1, 0.1].
4.1.4 Evaluation Methodology. Given the limited number of
queries for each collection, we conduct 5-fold cross-validation to
minimize over-ing without reducing the number of learning
instances. eries for each dataset are divided into 5 folds as de-
scribed in LETOR4.0 [26]. e parameters for each model are tuned
on 4-of-5 folds. e last fold in each case is used for evaluation.
is process is repeated 5 times once for each fold. e results
reported were the average over the 5 folds. As for evaluation meas-
ures, precision (P), mean average precision (MAP), and normalized
discounted cumulative gain (NDCG) at position 1, 5, and 10 were
used in our experiments. We performed signicant tests using the
paired t-test. Dierences are considered statistically signicant
when the p−value is lower than 0.05.
2hps://sourceforge.net/p/lemur/wiki/RankLib/
4.2 Analysis of the ViP model
In this section, we conduct experiments to analyze the ViP model
in learning visual features from snapshots. For this purpose, we
introduce two variants of the ViP model. In the rst variant, we
disable the visual feature learning part and only keep the human-
craed features, referred to as ViPBaseline . In the second variant, we
replace the CNN+RNN layer with a widely adopted CNN model [2,
19] for image recognition, referred to as ViPCNN . We also test all
these models on both query-independent and query-dependent
snapshots. e results are shown in Table 2.
From the results we observe that, the ViPBaseline model, which
only leverages the human-craed features, can already obtain reas-
onably good retrieval performance. Furthermore, when snapshots
are included, not maer query-independent or query-dependent,
the retrieval performance can be signicantly improved on both
datasets. It indicates that learning visual features from Web page
snapshot is of great importance in relevance ranking. Meanwhile,
we nd that the improvement of query-dependent snapshot is con-
sistently larger than that of the query-independent snapshot in
terms of all the evaluation metrics. is is not surprising since
the query-dependent snapshots provide richer visual information,
i.e., the matching signals between a Web page and a query, than
query-independent snapshots. We also try to combine both query-
independent and query-dependent snapshots together to learn the
visual features, but no obvious improvement can be observed. It in-
dicates that the information in query-independent snapshots might
have already been included in query-dependent snapshots.
Moreover, when comparing the ViP model with the ViPCNN
model, we can see that ViP can outperform ViPCNN consistently in
terms of all the evaluation metrics on both query-independent and
query-dependent snapshots. For example, the relative improvement
of ViP over ViPCNN on query-dependent snapshot is about 3.9%
and 3.1% in terms of P@1 and NDCG@1 on MQ2007, respectively.
e results indicate that the proposed ViP model, which is inspired
by the F-biased viewing paern, can learn the visual features from
snapshots more eectively than the existing CNN model which is
proposed for general image recognition.
Table 3: Comparison of dierent retrieval models over the MQ2007 and MQ2008 datasets. Signicant improvement or degrad-
ation with respect to our model(ViP with query dependent snapshot) is indicated (+/-) (p-value ≤ 0.05).
MQ2007
Model Name P@1 P@5 P@10 NDCG@1 NDCG@5 NDCG@10 MAP
BM25 0.427− 0.388− 0.366− 0.358− 0.384− 0.414− 0.450−
QL 0.401− 0.372− 0.359− 0.347− 0.366− 0.398− 0.430−
RankSVM 0.472− 0.413− 0.381− 0.408− 0.414− 0.442− 0.464−
RankBoost 0.462− 0.405− 0.374− 0.401− 0.410− 0.436− 0.457−
AdaRank 0.461− 0.408− 0.373− 0.400− 0.415− 0.439− 0.460−
LambdaMart 0.481− 0.418− 0.384− 0.412− 0.421− 0.446− 0.468−
ViP 0.505 0.439 0.398 0.434 0.441 0.464 0.481
MQ2008
Model Name P@1 P@5 P@10 NDCG@1 NDCG@5 NDCG@10 MAP
BM25 0.408− 0.337− 0.245− 0.344− 0.461− 0.220− 0.465−
QL 0.380− 0.323− 0.236− 0.315− 0.441− 0.206− 0.453−
RankSVM 0.421− 0.350− 0.247− 0.357− 0.475− 0.228− 0.471−
RankBoost 0.441− 0.347− 0.248− 0.368− 0.475− 0.228− 0.478−
AdaRank 0.434− 0.342− 0.243− 0.368− 0.468− 0.221− 0.476−
LambdaMart 0.449− 0.346− 0.249− 0.376− 0.471− 0.230− 0.478−
ViP 0.466 0.356 0.252 0.396 0.494 0.235 0.494
4.3 Comparison of Retrieval Models
In this section, we compare our model (ViP model based on query-
dependent snapshots) against existing retrieval models over the
two benchmark datasets. e main results are shown in Table 3.
From the results, we have the following observations: (1) For the
two traditional models, we can see that BM25 is a strong baseline
which performs beer than QL. (2) All the learning to rank models
perform signicantly beer than the traditional retrieval models.
It is not surprising since learning to rank models combine various
features including the two baseline traditional retrieval models.
Among all the learning to rank models, LambdaMart performs
best. (3) We observe that our proposed ViP model can outperform
all the existing models in terms of all the evaluation measures
on both datasets, and all the improvements are statistically sig-
nicant (p−value≤ 0.05). For example, On MQ2008 dataset, the
relative improvement of our ViP model against the best-performing
baseline (i.e. LambdMart) is 3.8%, 5.3%, and 3.3% with respect to
P@1, NDCG@1, and MAP, respectively. e improvement of our
model over traditional learning to rank model demontrates the
eectiveness of the learned visual features from Web snapshot.
4.4 Impact of Proposal Size
Since we utilize the xed-height horizontal region proposals of
Web snapshot in learning, we would like to study the eect of
dierent heights on the ranking performance. In fact, the proposal
height determines not only the granularity of the local information
perceived, but also the number of proposals to be aggregated. With
a small proposal height, the model can obtain ne-granularity local
information, but at the cost of aggregating longer sequence of local
features which may require large memories. With a large proposal
height, there would be less number of local features to be processed,
but it may lose valuable detailed local information. We conduct
experiments to compare dierent proposal sizes, varying in the
P@10 NDCG@10 MAP
36
38
40
42
44
46
48
50
38.9
45.6
47.4
39.8
46.4
48.1
39.2
46.0
47.8
38.2
45.0
47.1
% P2x64 P4x64 P8x64 P16x64
Figure 5: Performance comparison of the ViP model over
dierent proposal sizes on MQ2007.
range of 2×64, 4×64, 8×64, and 16×64. From the results shown in
Figure 5, we can see that the performance rst increases and then
decreases, with the increase of proposal size. e best performance
is obtained when the proposal size is set to be 4× 64 (w.r.t. dierent
evaluation measures). is height approximately corresponding to
two lines of text in the original Web page in the normal font size.
4.5 Impact of Image Resolution
As is described in Section 4.1, we down-sample a Web snapshot
to a lower spatial resolution with 64 × 64 pixels for eciency. As
we know, dierent snapshot resolutions preserve dierent amount
of information, which may aect the visual features learned by
the ViP model. Here we analyze the eect of dierent snapshot
resolutions, varying in the range of 16×16, 32×32, 64×64, 128×128,
and 256 × 256 pixels. Note that in the previous section, we nd
Figure 6: (A) e query-dependent snapshot of a non-relevant page for query “rusty black bird”; (B) e query-dependent
snapshot of a highly relevant page for query “rusty black bird”. (C) e feature importance vector learned by the ViPBaseline
model. (D) e feature importance vector learned by the ViP model.
that the performance could also be aected by the size of region
proposal. us, for snapshots with dierent resolution, we also
tune the best proposal size. We nd the best proposal size under
dierent resolutions is 2 × 16, 2 × 32, 4 × 64, 8 × 128, and 16 ×
256, respectively. erefore, we compare the performance over
dierent snapshot resolutions under the best-performing region
proposal size, and the results are depicted in Figure 7. Moreover,
we also ploed the performance results of the ViPBaseline model for
additional comparison.
From the results we can see that, the performance increases
rapidly with the snapshot resolution, and then keeps stable aer
a certain point. When the resolution is low, the performance of
the ViP model may even decrease as compared with the ViPBaseline
model which only uses human-craed features. e possible reason
might be that the snapshot with too small resolution may lose
useful information of the Web page, leading to undesired noise
in learning. We observe that the medium resolution with size
64 × 64 can already obtain signicant performance improvement.
is is quite important since we only need to store relatively small
snapshots for eective usage in real search application.
4.6 Case Study
To beer understand what can be learned by the ViP model, here
we conduct some case studies. Figure 6 shows two candidate Web
pages of the query “rusty black bird” in MQ2008 dataset, which have
totally dierent layouts. e page (A) (DocId: GX059-61-15727287
in GOV2 corpus) shows a list of vertebrate animal species list,
and is labeled as Non-Relevant to the query. e page (B) (DocId:
GX095-93-12495293 in GOV2 corpus) describes the rusty black bird
in detail, and is labeled as Highly Relevant to the query. When
we apply the ViPBaseline model over the query, we nd page (A) at
the second position in the ranking list. e possible reason is that
there are more than 300 hits of the query keyword ”bird” in this
page, and the ViPBaseline model thus may produce a high relevance
16 × 16 32 × 32 64 × 64 128 × 128 256 × 25637
39
41
43
45
47
49
37.7
39.0
39.8 39.9 39.6
44.2
45.1
46.4 46.4 46.346.5
47.3
48.1 47.9 48.2
image resolution
% P@10 NDCG@10 MAP
Figure 7: Performance comparison over snapshots under dif-
ferent resolutions on MQ2007.
score by mainly relying on human-craed textual features. On the
other hand, the page (B), which has fewer query term matches, is
ranked at a lower position (i.e., the seventh position). However, if
we take into account the layout of the Web page, we can clearly
see that the matching signals in page (A) are distributed vertically
along with many blank areas, indicating a table or list of elements
in a Web page. While in page (B), there are many matching signals
embedded in passages closely with some large matching singles in
the top position (i.e., title), indicating a descriptive article in the
Web page. By taking these visual features into account, the ViP can
beer detect the relevant Web page, and promote page (B) to the
second position and penalize page (A) to the nineteenth position in
the ranking list.
Moreover, we also depict the learned weights in the feedforward
layer to analyze the feature importance. For beer visualization and
analysis, we simplify the decision layer in our model by using only
one-layer feedforward neural network. In this way, the weights
form a vector with the size of the total features. As shown in
Figure 6, the Figure 6(C) is the learned weights of the ViPBaseline
model, and Figure 6(D) is the learned weight of the ViP model.
In both Figures, the color is corresponding to the signal strength,
where red represents the highest value. From Figure 6(C) we nd
that the most important features are BM25 score of the body and
the number of child page. However, when the visual features are
included, the weights change signicantly. From Figure 6(D) we
observe that many visual features become very important, while
the importance of many human-craed features, especially those
link analysis features, decreases signicantly. For example, the
weight of PageRank, inlink number, and outlink number decreased
5.5%, 39%, and 54.5%, respectively (i.e. from 41 to 43). is is
reasonable since many link analysis features also convey page
quality information, which can now be well captured by visual
features from Web snapshots.
5 CONCLUSIONS
In this paper, we propose to learn visual features from Web page
snapshots to improve the performance of ad-hoc Web retrieval.
Both query-independent and query-dependent snapshots have been
introduced as new inputs. We then propose a novel visual percep-
tion model over the snapshots, which can automatically learn visual
features in an end-to-end way. Experimental results on two bench-
mark datasets have demonstrated that visual features from Web
snapshots can signicantly improve the performance of ad-hoc
Web retrieval. We have also shown that this method can be e-
ciently implemented in practical search systems with an ecient
indexing scheme. For future work, it would be interesting to apply
our visual perception model to other Web page related applications,
e.g., spamming detection, homepage identication or mobile Web
search.
6 ACKNOWLEDGMENTS
is work was funded by the 973 Program of China under Grant No.
2014CB340401, the National Natural Science Foundation of China
(NSFC) under Grants No. 61232010, 61433014, 61425016, 61472401,
and 61203298, the Youth Innovation Promotion Association CAS
under Grants No. 20144310 and 2016102, and the National Key R&D
Program of China under Grants No. 2016QY02D0405. We would
like to thank Zhicheng Dou for the Web page rendering tool.
REFERENCES
[1] Pablo Arbela´ez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and
Jitendra Malik. 2014. Multiscale combinatorial grouping. In CVPR. 328–335.
[2] Ting B., Hong-Jian D., Wayne Xin Z., Ding-Yi Y., and Ji-Rong W. 2017. An
Experimental Study of Text Representation Methods for Cross-Site Purchase
Preference Prediction Using the Social Text Data. JCST 32, 4 (2017), 828–842.
[3] Michael Bendersky, W Bruce Cro, and Yanlei Diao. 2011. ality-biased ranking
of web documents. In Proceedings of the fourth ACM international conference on
Web search and data mining. ACM, 95–104.
[4] Neil Bowers. 1996. Weblint: quality assurance for the World Wide Web. Computer
Networks and ISDN Systems 28, 7 (1996), 1283–1290.
[5] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An
overview. Learning 11 (2010), 23–581.
[6] Soumen Chakrabarti, Mukul M Joshi, Kunal Punera, and David M Pennock. 2002.
e structure of broad topics on the web. In WWW. ACM, 251–262.
[7] Dan C Cires¸an, Alessandro Giusti, Luca M Gambardella, and Ju¨rgen Schmidhuber.
2013. Mitosis detection in breast cancer histology images with deep neural
networks. In MICCAI. Springer, 411–418.
[8] George E. Dahl, Tara N. Sainath, and Georey E. Hinton. 2013. Improving deep
neural networks for LVCSR using rectied linear units and dropout. In IEEE
International Conference on Acoustics, Speech and Signal Processing. 8609–8613.
[9] Pete Faraday. 2000. Visually Critiquing Web Pages. Springer Vienna, Vienna,
155–166. DOI:hp://dx.doi.org/10.1007/978-3-7091-6771-7 17
[10] BJ Fogg, Jonathan Marshall, Othman Laraki, Alex Osipovich, Chris Varma, Nich-
olas Fang, Jyoti Paul, Akshay Rangnekar, John Shon, Preeti Swani, and others.
2001. What makes Web sites credible?: a report on a large quantitative study. In
SIGCHI. ACM, 61–68.
[11] BJ Fogg, Cathy Soohoo, David R Danielson, Leslie Marable, Julianne Stanford,
and Ellen R Tauber. 2003. How do users evaluate the credibility of Web sites?:
a study with over 2,500 participants. In Proceedings of the 2003 conference on
Designing for user experiences. ACM, 1–15.
[12] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An ecient
boosting algorithm for combining preferences. JMLR 4, Nov (2003), 933–969.
[13] Ross Girshick, Je Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich
feature hierarchies for accurate object detection and semantic segmentation. In
CVPR. 580–587.
[14] Rebecca Grier, Philip Kortum, and James Miller. 2007. How users view web pages:
An exploration of cognitive and perceptual mechanisms. In Human computer
interaction research in Web design and evaluation. IGI Global, 22–41.
[15] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[16] orsten Joachims. 2006. Training linear SVMs in linear time. In SIGKDD. ACM,
217–226.
[17] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-
tion. arXiv preprint arXiv:1412.6980 (2014).
[18] Muneo Kitajima, Marilyn H Blackmon, and Peter G Polson. 2000. A
comprehension-based model of web navigation and its application to web usab-
ility analysis. In People and computers XIVUsability or else! Springer, 357–373.
[19] Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. 2012. Imagenet classi-
cation with deep convolutional neural networks. In NIPS. 1097–1105.
[20] Kevin Larson and Mary Czerwinski. 1998. Web page design: Implications
of memory, structure and scent for information retrieval. In SIGCHI. ACM
Press/Addison-Wesley Publishing Co., 25–32.
[21] Jonathan Ling and Paul Van Schaik. 2002. e eect of text and background
colour on visual search of Web pages. Displays 23, 5 (2002), 223–230.
[22] Jonathan Ling and Paul Van Schaik. 2006. e inuence of font type and line
length on visual search and information retrieval in web pages. International
Journal of Human-Computer Studies 64, 5 (2006), 395–404.
[23] omas Mandl. 2006. Implementation and evaluation of a quality-based search
engine. In Proceedings of the seventeenth conference on Hypertext and hypermedia.
ACM, 73–84.
[24] Robert Pearson and Paul van Schaik. 2003. e eect of spatial layout of and link
colour in web pages on performance in a visual search task and an interactive
search task. IJHCS 59, 3 (2003), 327–353.
[25] Giuseppe Pirlo, Michela Chimienti, Michele Dassisti, Donato Impedovo, and
Angelo Galiano. 2013. Layout-Based Document-Retrieval System by Radon
Transform Using Dynamic Time Warping. In International Conference on Image
Analysis and Processing. Springer, 61–70.
[26] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection
for research on learning to rank for information retrieval. Information Retrieval
13, 4 (2010), 346–374.
[27] Stephen E Robertson and Steve Walker. 1994. Some simple eective approx-
imations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR.
Springer-Verlag New York, Inc., 232–241.
[28] Derek Sco. 1993. Visual search in modern human-computer interfaces. Beha-
viour & Information Technology 12, 3 (1993), 174–189.
[29] Chengyao Shen and Qi Zhao. 2014. Webpage saliency. In ECCV. Springer, 33–46.
[30] B Shneiderman. 2000. Universal design. Communication of ACM 43, 5 (2000),
84–91.
[31] Ruihua Song, Haifeng Liu, Ji-Rong Wen, and Wei-Ying Ma. 2004. Learning
block importance models for web pages. In Proceedings of the 13th international
conference on World Wide Web. ACM, 203–211.
[32] Dennis J Streveler and Anthony I Wasserman. 1984. antitative measures
of the spatial properties of screen designs. In Proceedings of the IFIP TC13 First
International Conference on Human-Computer Interaction. 81–89.
[33] Jasper RR Uijlings, Koen EA Van De Sande, eo Gevers, and Arnold WM
Smeulders. 2013. Selective search for object recognition. IJCV 104, 2 (2013),
154–171.
[34] Jeremy M Wolfe. 1994. Guided search 2.0 a revised model of visual search.
Psychonomic bulletin & review 1, 2 (1994), 202–238.
[35] Jun Xu and Hang Li. 2007. Adarank: a boosting algorithm for information
retrieval. In SIGIR. ACM, 391–398.
[36] Chengxiang Zhai and John Laerty. 2001. A study of smoothing methods for
language models applied to ad hoc information retrieval. In SIGIR. ACM, 334–342.
[37] Yun Zhou and W Bruce Cro. 2005. Document quality models for web ad hoc
retrieval. In CIKM. ACM, 331–332.
"
38,"An overview of view-based 2D-3D indexing methods
Raluca Diana Petre, Titus Zaharia, Francoise Preteux
To cite this version:
Raluca Diana Petre, Titus Zaharia, Francoise Preteux. An overview of view-based 2D-3D
indexing methods. SPIE - Mathematics of Data/Image Coding, Compression, and Encryp-
tion with Applications XII, Aug 2010, San Diego, United States. 7799, pp.779904, 2010,
<10.1117/12.861542>. <hal-00738207>
HAL Id: hal-00738207
https://hal.archives-ouvertes.fr/hal-00738207
Submitted on 3 Oct 2012
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destine´e au de´poˆt et a` la diffusion de documents
scientifiques de niveau recherche, publie´s ou non,
e´manant des e´tablissements d’enseignement et de
recherche franc¸ais ou e´trangers, des laboratoires
publics ou prive´s.
An overview of view-based 2D/3D indexing methods 
 
Raluca-Diana Petre
(1)
, Titus Zaharia
(1)
, Françoise Prêteux
(2) 
(1)
Institut TELECOM; TELECOM SudParis, ARTEMIS Department; UMR CNRS 8145 MAP5 
9 rue Charles Fourier, 91011 Evry Cedex, France 
(2)
 Mines ParisTech; 60 Boulevard Saint-Michel, 75272 Paris, France 
{Raluca-Diana.Petre, Titus.Zaharia}@it-sudparis.eu, Francoise.Preteux@mines-paristech.fr 
ABSTRACT   
This paper proposes a comprehensive overview of state of the art 2D/3D, view-based indexing methods. The principle of 
2D/3D indexing methods consists of describing 3D models by means of a set of 2D shape descriptors, associated with a 
set of corresponding 2D views (under the assumption of a given projection model). Notably, such an approach makes it 
possible to identify 3D objects of interest from 2D images/videos. An experimental evaluation is also proposed, in order 
to examine the influence of the number of views and of the associated viewing angle selection strategies on the retrieval 
results. Experiments concern both 3D model retrieval and image recognition from a single view. Results obtained show 
promising performances, with recognition rates from a single view higher then 66%, which opens interesting 
perspectives in terms of semantic metadata extraction from still images/videos.  
Keywords: indexing and retrieval, 2D and 3D shape descriptors, multiview matching, similarity measures, MPEG-7 
standard, 3D meshes.  
1. INTRODUCTION  
The domain of 3D graphics has known a spectacular expansion during the last decade, due to the development of both 
hardware and software resources. Within this context, more and more industrial applications involving 3D objects have 
been entered our daily life. Computer aided design (CAD), gaming, special effects and film production, biology, 
medicine, chemistry, archaeology or geography are some examples of application domains that use intensively 3D object 
representations. In addition, the amount of available 3D models is continuously increasing, due to the availability of 
advanced 3D content creation platforms (e.g. Maya, 3DSMax, Blender) as well as of low-cost 3D scanners.  
The availability of large 3D model repositories throws to the scientific community new challenges in terms of 3D 
content processing, analysis and representation. One fundamental issue concerns the content re-use within audio-visual 
production chains. The goal in this case is to retrieve from large 3D databases similar 3D models that can be exploited 
within the framework of new production projects. Content-based similarity retrieval of 3D mesh models offers a 
promising solution to this issue. The principle consists of describing in a salient manner the object’s visual features, such 
as shape, colour, texture, motion and then to use them for comparison purposes and automatic 3D object retrieval. Let us 
note that in most of the cases the key feature exploited for retrieval purposes is the 3D shape, which is the sole available 
in all cases: 3D models are not necessarily textured, coloured or animated, but they always define a 3D shape.  
As a consequence, numerous 3D shape retrieval approaches have been proposed during the last decade. For some 
comprehensive overviews, the reader is invited to refer to
 11, 18, 14, 21
.  
Among the various families of 3D shape descriptions, an interesting approach is proposed by the so-called view-based, 
or 2D/3D methods. In this case, instead of directly describing the 3D shape information, the 3D object is represented as a 
set of 2D images associated to the 3D object, corresponding to 3D/2D projections from several viewing angles. The 2D 
projection images are finally described by 2D descriptors. Here again, we distinguish two distinct approaches. A first 
one uses exclusively 2D information derived from the projections (e.g. resulting support regions, silhouette images…). 
The second one integrates some 3D information by considering the depth maps derived during the 3D/2D projection 
process 
35, 36, 34
. 
A first and major advantage of such 2D/3D shape representations is essentially related to topological aspects: describing 
2D images represented on a fixed topology (i.e. corresponding to a regular 2D lattice of pixels) is more tractable in 
practice than analysing 3D meshes with arbitrary and often non-regular connectivity. In addition, such methods offer 
high retrieval rates, quite competitive with most promising purely 3D approaches
 21
. Finally, 2D/3D approaches that 
  
 
 
exploit uniquely 2D features (i.e. no depth maps) open new perspectives in terms of applications. Notably, the main 
interest of such methods is related to the possibility of matching 3D models with 2D objects identified (e.g., with the 
help of some segmentation techniques) in still images or videos. In particular, such an approach might bring interesting 
and original solutions to the well-known problem of semantic gap
 23, 24
, which can be synthesized as follows. Given an 
object or a set of objects detected with the help of computer vision techniques from still images or videos, how can we 
interpret its meaning?  
Existing approaches make intensively use of machine learning techniques
 24
, in order to bridge the gap between rough 
pixels and semantically meaningful interpretations of the image content. In this context, 3D modelling offers an 
interesting and complementary axis of research. Large and semantically categorized 3D repositories are today available. 
Thus, reliable 2D/3D matching techniques can lead to a semantic labelling of the content.  
Let us also note that the advantage of using 3D models when searching for 2D images comes from the completeness of 
such representations. Thus, 2D representations of the same 3D object can present extremely different appearances in 
terms of shape. In this case, solely the 3D knowledge can relate effectively such different views (Figure 1).  
 
Figure 1: Different views of a 3D object representing a bicycle. The first (profile) and the last (front) views are completely 
different in terms of 2D shape, but can be related if the 3D model is available. 
 
In this paper we notably tackle the issue of shape-based 2D/3D indexing and propose a state of the art in the field. Solely 
2D/3D techniques employing exclusively 2D shape information will be presented, since our objective is to investigate if 
such approaches can provide useful solutions for semantic identification of objects detected in 2D images/videos.  
The rest of the paper is organized as follows. Section 2 proposes a state of the art of 2D/3D shape retrieval approaches. 
Generic principle and main families of methods are here described and discussed. The analysis of the literature shows 
that a key issue concerns the viewing angle selection procedure used to generate 2D projections. For this reason, in 
Section 3, we propose an experimental protocol and evaluate, under the same experimental conditions (i.e. test set, 2D 
shape descriptor and similarity metrics), the impact of the considered viewing angles on the performances of both 3D to 
3D model retrieval and image recongnition from a single view applications. Finally, Section 4 concludes the paper and 
opens perspectives of future work.  
2. STATE OF THE ART  
Let us first briefly recall the principle of shape-based 2D/3D indexing techniques.   
 
2.1. Shape-based 2D/3D indexing: principle 
The underlying assumption of all 2D/3D methods can be stated as follows: two 3D objects are similar iff the associated 
2D projections (with respect to a set of viewing angles) are similar.  
Each 3D model M is projected and rendered in 2D from N different viewing direction  in . For simplicity, we will 
assume that the 3D object is centered in the origin of a 3D Cartesians system (Oxyz) and that the viewing angles ni are 
defined as samples on the unit sphere. A set of silhouette images (i.e. binary projections of the object), denoted by i(M) 
is thus obtained (Figure 2). Each projection i(M) is further described by a 2D shape descriptor di(M).  
When considering such an approach, some fundamental questions are coming out: how many projections should we 
choose? Which are the viewing angles that optimally represent the shape? Which are the appropriate 
descriptors/representations that can effectively describe the 2D shape of the obtained projections? How to minimize the 
computational complexity of the matching algorithms?  
In the same time, when considering the issue of 2D/3D retrieval, invariance aspects with respect to similarity transforms 
(i.e. rotation, translation, isotropic scaling and combinations of them) should be taken into account. This issue is 
  
 
 
particularly critical in the case of 2D/3D approaches: due to self-occlusions of object’s subpart and global perspective 
deformations, the 2D shape of the corresponding silhouette images can change drastically from one view to another. The 
selection of an appropriate set of viewing angles is crucial for achieving a 3D pose invariance behavior.  
 
Figure 2: Projecting a model (model from NIST 3D model database): a. Viewing directions; b. Model projection according 
to the n1 direction; c. the resulting silhouette images.  
Since this issue is fundamental for successful 2D/3D similarity retrieval, we have categorized the various families of 
approaches with respect to the viewing angle selection procedure involved. A first and largely popular family of 
approaches considers a principal component analysis (PCA) of the 3D geometry, in order to obtain an object-dependent, 
canonical coordinate system invariant under 3D rotation. Such methods are presented in section 2.2. A second solution, 
further presented in section 2.3, consists of evenly distributing the camera viewing angles around the object. 
 
2.2. Methods using PCA-based projection 
Principal component analysis (PCA) is a well-known statistical procedure which optimally decorrelates a set of multi-
valued variables. In our case, the variables to be considered represent the vertex positions of the 3D object expressed as 
vectors in a 3D coordinate system. 
The object M is first placed with the gravity centre in the origin of the considered coordinate system. Then, the (33) 
covariance matrix CM is computed, as described by the following equation:  

i
t
iiM vv
V
C
1
   ,     (1) 
where  txixixii vvvv ,, is the 3D coordinate vector of the vi vertex, V is the total number of vertices and subscript t 
denotes the transpose operator.   
As defined, the covariance matrix CM is symmetric and positive defined. Thus, it can be diagonalized by an orthogonal 
transform whose columns are its normalized, unit-length eigenvectors. The eigenvectors are also called principal axes or 
axes of inertia and the corresponding image planes (i.e. projection planes orthogonal to the principal axes) and referred 
to as principal planes.  
In the case of PCA-based methods, the principal axes are used as viewing angles. They offer an object-dependent, 
orthogonal coordinate system. The orthogonality of the eigenvectors ensures the minimization of the redundancy 
between views. However, in order to obtain a more complete representation, additional viewing angles can be defined 
starting from the three eigenvectors. For example, the MPEG-7 Multiview description scheme (DS)
 38
 recommends the 
use of the diagonal directions of the four octants of the semi-space defined by the principal axes (Figure 3).  
Let us also note that the corresponding eigenvalues provide a measure of the object’s extent along each principal axis, 
which can be used for scale normalization
 28
.  
As representative of the 2D/3D shape-based retrieval approaches, let us first mention the Multiview DS proposed by 
MPEG-7 standard. Within the MPEG-7 framework, two different MPEG-7 2D shape descriptors
 31, 32, 37
, the ART 
(Angular Radial Transform)
 33
 and CSS (Contour Scale Space)
 6
, can be considered. The MPEG-7 approach is described 
in details and experimentally evaluated in
 21
. A maximum number of seven projection images can be used. Three of them 
  
 
 
correspond to the principal directions and the other four to the diagonal, secondary views. The pre-processing stage 
involves translation and scaling, the 3D object being transformed such that its gravity centre coincides with the 
coordinate system origin and fits the unit sphere.  
 
Figure 3: Selection of the principal and secondary axes.  
In the case of the 2D-ART descriptor, the object’s support function is represented as a weighted sum of 34 ART radial 
basis functions. In order to achieve rotation invariance, solely the absolute values of the coefficients are used. The 
similarity measure simply consists of L1 distances between ART coefficients. The 2D-ART is thus invariant under 
similarity transforms, and is suitable for meshes of arbitrary topologies, which can present holes or multiple connected 
components under the projection operator. More restrictive, the MPEG-7 CSS descriptor assumes that the shape of the 
object can be described by a unique closed contour. The CSS descriptor is obtained by successively convolving an arc-
length parametric representation of the curve with a Gaussian kernel. The curvature peaks are thus robustly determined 
in a multi-scale analysis process and serve to characterize the contour shape, with their value and corresponding position 
(expressed as curvilinear abscise).The associated similarity measure between two contour in CSS representation is based 
on a matching procedure which takes into account the cost of fitted and unfitted curvature peaks.  
Whatever the 2D shape descriptor considered, when comparing two 3D models, a distance value eij is obtained for each 
pair of i and j views. An error matrix E=(eij) is thus computed. The global similarity measure between the two models is 
then defined as:  
 ])([min),( 21 EpTraceMMD
p 
     (2) 
where   represents the set of all possible permutations between the columns of matrix E, p is a permutation in  , and 
p(E) represents the permuted version of matrix E.  
Let us note that such a similarity measure is highly expensive since the number of possible permutations is non-
polynomial with the number of views. In practice, such a similarity measure can be applied only for a reduced number of 
views and becomes computationally un-tractable when the number of views exceeds 7.  
In
 5
, authors re-consider the MPEG-7 CSS representation. Here, the contour of each projection image is represented in 
CSS and decomposed into a set of segments called tokens, i.e. sets of 2D points delimited by two inflexion points. The 
tokens are then clustered and hierarchically organized in a M-Tree structure
 25
. To compare two tokens, a sum of 
geodesic distances between points is computed. The obtained descriptor is intrinsically invariant to similarity transforms.  
In
 8
 authors propose a 2D/3D approach based on the MCC (Multi-scale Convexity/Concavity) representation introduced 
in
 26
. The 3D object is scaled with respect to its bounding sphere and PCA is applied in order to normalize the pose of the 
model. A number of three to nine silhouettes are then computed. As in the case of the MPEG-7 CSS descriptor, a scale-
space analysis is here performed. Each silhouette contour is successively convolved with K = 10 Gaussian functions, 
with increasing kernel bandwidths. The variations (in 2D position) of 100 sample points, measured between successive 
scale spaces are then exploited as a shape descriptor. The similarity measure proposed involves a point to point matching 
procedure, which is quadratic with the number of sample points. In the same paper, a second method, so-called 
Silhouette Intersection (SI), is proposed. Only the three views, corresponding to the PCA principal directions are here 
exploited. The signature of a model is simply constituted by the three obtained binary silhouettes. The distance between 
two silhouette images is defined as the number of pixels belonging to the symmetric difference
 27
 of the two silhouettes 
(Figure 4). The global distance between two 3D objects is defined as the sum of silhouette distances between pairs of 
images corresponding to the same axes. However, the SI method is not robust against small variations of the shape. In 
addition, results strongly depend on the PCA alignment.  
  
 
 
 
Figure 4: Example of two superposed silhouettes (here, a duck and an eagle). Even if the two models are similar, there 
is a large number of pixels belonging to the symmetric difference. 
A different, single view approach is proposed recently in 
4
. Authors propose to exploit an unique projection onto the 
principal plane of maximal eigenvalue. The projection is described using two descriptors: region-based Zernike 
moments
 7
 and contour-based Fourier descriptors 
22
. The algorithm is very intuitive and fast, since a single projection 
image is used.  
Finally, let us mention the approach proposed in 
13
. A voxelized, volumetric representation is determined prior to 
performing the PCA. The viewing angle directions used to project the model correspond to the three principal axes. Each 
of the obtained images is decomposed into L=60 concentric circles defined around the object’s gravity center (Figure 5). 
The number of pixels located within each circle is computed, normalized to the total obects’ and forms the feature vector 
associated to each projection. The so-called principal plane descriptor (PPD) proposed is defined as the set of all three 
feature vectors. Let us note that, because of the concentric circular regions involved, the PPD is intrinsically invariant 
under 2D rotation.  
 
  
a. b. c. 
Figure 5: Principle of the PPD approach: a. the 3D model; b. the object’s projections onto the principal planes;  
c. concentric circles used to determine the descriptor.  
However, the method implicitly assumes an ordering of the three principal directions (i.e. by decreasing values of 
corresponding eigenvalues). Such an approach may lead, in the general case, to miss-alignments, as shown in
 20, 14
. This 
problem is illustrated in Figure 6.  
 
Figure 6: PCA miss-alignment. The choice of the principal axes is different for the two models with respect to their 
components which creates wrong matches (view 1 and view 2).  
PCA-based methods offer the advantage of obtaining a representation associated with a canonical, object-dependent 
coordinate system that can partially solve the 3D transform invariance issues. However, the principal axes may present 
strong variations when dealing with similar models 
20, 14
. Furthermore, a second limitation is related to the eventual miss-
alignments that might occur. The reliability of the PCA is a key factoring this process that should be taken into account 
appropriately.  
  
 
 
In order to overcome such limitations, a second family of methods, described in the next section, proposes to perform the 
3D/2D projection according to a set of dense and evenly distributed viewing angles.  
 
2.3. Methods using evenly distributed viewing angles  
Instead of computing preferential projection planes, this second family of approaches uses a set of dense and evenly 
distributed viewing angles.  
In 
15
 an extension of the SI algorithm 
8
, so-called Enhanced Silhouette Intersection (ESI), is introduced. Instead of 
exploiting the PCA directions, the vertices of a dodecahedron are here used to generate ten views, acquired after object 
normalization in translation, scale and rotation 
30
. As in the case of the SI method, the distance between two images is 
given by the number of pixels included in the symmetric differences between the corresponding support regions. 
However, for the global distance between two 3D models, instead of summing up the distances between similar views, a 
weighted sum is proposed. This choice starts from the simple assumption that the relevance of a projection is 
proportional to the root square of its area. Both the descriptor extraction procedure and the dissimilarity measure are fast 
to compute. Compared with the SI algorithm, the robustness of the alignment is increased with the help of the approach 
described in
 30
. However, articulated object matching is not supported and even small variations of the shape can greatly 
influence the retrieval results. 
In 
1
, authors introduce the LightField Descriptor (LFD) which encodes ten silhouettes of the 3D object obtained by 
projection from the vertices of the dodecahedron. Translation and scaling invariance of the image are extrinsically 
achieved by normalizing the size of the projection images. Furthermore, the silhouettes are encoded by both Zernike 
moments
 7
 and Fourier descriptor 
22
. A number of 35 coefficients Zernike moments are used and of 10 coefficients for 
the Fourier descriptor. Thus, the resulting descriptor includes 45 coefficients for each projection image, and 450 for each 
LFD associated with a 3D model. To compare two LFDs, the dissimilarity measured used is the L1 distance between the 
descriptor’s coefficients. The minimum sum of the distances between all possible permutations of views provides the 
dissimilarity between the 3D models. Let us note that in the case of LFD there are 60 possible permutations and for each 
of them 10 individual distances between pairs of images need to be computed. In addition, as one LFD is not totally 
invariant under rotation, a set of 10 LFDs per model is used to improve the robustness. This leads to a total number of 
5460 comparisons to be computed. The need for multiple matches for each two objects makes LFD very time 
consuming. In order to reduce the computational cost, a multi-step fitting approach is adopted. In the first stage a 
reduced number of images per model and of coefficients is used in order to filter the results and retain a reduced number 
of candidate models. This procedure allows the early rejection of non-relevant models. The results obtained show that 
this algorithm outperforms most 3D shape descriptors at the cost of a significantly increased computational complexity. 
An modified version of the LFD method is proposed in
 17
. Authors start from the observations that two different objects 
can have similar projections (Figure 7), under the assumption that the scaling normalization is performed upon the 
silhouette images.  
The model View 1 View 2 
 
 
 
 
 
 
 
 
 
 
   
Figure 7: Dissimilar objects presenting similar views after scale normalization.  
The Modified LFD (MLFD) approach proposed skips the resizing step of the original LFD algorithm. Tested on the 
Princeton Shape Database, the MLFD slightly outperformed the LFD performance (the Nearest Neighbor measure 
increases by 5.3%, the First  and Second Tier measures by 4.1%, respectively 3.6% and the Discounted Cumulative Gain 
increases by 2.3%). This shows that the normalization issues, needed for achieving invariance, should be considered in 
the 3D space rather than in the domain of 2D projections and/or descriptors.  
  
 
 
Recently, a Compact Multi-View Descriptor (CMVD) was proposed by 
3
. The authors tested the CMVD on both binary 
and depth images. The descriptor extraction starts with the normalization stage, which includes translation, rotation and 
scaling. In order to compute the principal axes, both PCA and VCA 
10
 are performed. A number of 18 projections are 
obtained by placing the camera on the vertices of a 32-hedron and each of them is described by three sets of coefficients. 
First, 78 coefficients of the 2D Polar-Fourier Transform are computed. The usage of polar coordinates ensures rotation 
invariance. Secondly, 2D Zernike moments up to the 12
th
 order are obtained, resulting in 56 coefficients. Finally, 78 
additional coefficients, corresponding to 2D Krawtchouk moments 
19
 are considered. When comparing two projections 
images, the L1 norm is used to compute the distance between two descriptor vectors. Authors also take into account the 
fact that in some cases the first principal axis may not be successfully selected among the three principal axes. In order 
to deal with such an issue, 3  8 = 24 different alignments are considered. The total dissimilarity between two sets of 
images is obtained by summing up the dissimilarities between corresponding images. The distance between two models 
is the minimum distance that results when comparing the 18 projections of the first model with each of the 24 sets of 
images of the second model. In terms of computational complexity, the view generation process is the most time 
consuming. The 2D rotation invariance is ensured by the image descriptors considered. However, 3D rotation is 
provided only by the PCA&VCA alignment. Experiments were conducted on several 3D model databases and have 
shown that CMVD performs similar to LFD while offering a reduced computational complexity.  
 
Some approaches further perform a selection of the resulting silhouettes in order to reduce their number. The selection 
process supposes to cluster the silhouettes according to their 2D similarity, resulting in a reduced number of 
representative projections that can be used to appropriately describe the 3D object. 
In 
2
, authors present a method based on a similarity aspect graph. First introduced in 
29
, the aspect graph represents a 
subset of object projections obtained from a uniform viewing angle distribution. Here, the initial viewing angles are 
uniformly sampling, with a 5° step, the 3D object’s principal plane in the (0, 180°) interval. A number of 36 silhouette 
images are thus obtained. A similarity metric is computed between each two successive projections. The aspects are 
defined as group of similar silhouettes with respect to the considered similarity measure. They are obtained with the help 
of a clustering algorithm that minimizes intra-class similarity while maximizing inter-class similarity. A stable, or 
prototype view is determined for each aspect. Finally, the prototypes views are represented as a graph structure where 
each node corresponds to a stable view (and each two adjacent stable views are connected by an edge of the graph.  
Concerning the similarity matching process, only one projection image is used as query for each model, and compared to 
all the prototypes in the database. The number of comparisons is thus proportionally with the number of prototypes per 
model. The object is matched against the one in the database having the most similar prototypes to the query. The same 
similarity measure is used to compare prototypes as the one exploited for their selection. The results presented in the 
paper are obtained using the shock graph descriptor 
12
 which is time consuming in the matching stage.  
The main drawback of this method is the computational complexity of both prototype selection and similarity measure 
used for retrieval purposes. Also, since the viewing angles lie in a unique plane, the selection of this plane has to 
generate a robust response in order to ensure a 3D pose invariant behaviour.  
In 
16
 a similar method is proposed. The approach aims at selecting the viewing angles based on the degree of 
representativity of the corresponding projection images. Here, 162 silhouette images are rendered according to a uniform 
viewing angle distribution, defined over the unit sphere as a spherical triangular mesh. The obtained images are 
described using Zernike moments
 7
 up to order 15. The similarity between each two adjacent projections is computed 
using the L2 norm distance. Then, a spherical weighted graph is constructed using the viewing angles as vertices. The 
weight of each edge is equal to the similarity between the views connected by the considered edge. Stable view regions 
represent sub-parts of the graph with similar corresponding projections (i.e. sets of edges with low weights). Two stable 
view regions are separated by so-called heavy edges. Thus, based on the edge weights, the graph is partitioned into eight 
sub-graphs representing the stable view regions. Furthermore, a representative viewpoint needs to be found for each 
stable region. In order to achieve this goal, a pertinence value is assigned to each viewpoint. This value is based on the 
mesh saliency, a measure that evaluates the mesh curvature evolution when smoothed at different filter scales. The 
saliency measure is also used to sort the views according to the amount of information they carry. This algorithm is 
complex because of the weighted graph construction which is the most time consuming stage (162 vertices of 6 adjacent 
edges each generate 486 edges and as many weights to be computed). 
  
 
 
The proposed algorithm proves to be effective for determining representative viewpoints. However, it does not address 
the issue of 3D model matching. Notably, it would be interesting to establish in what measure the viewing angles thus 
determined could be effectively exploited within the framework of 2D/3D shape-based retrieval applications. 
In general, the methods that use evenly distributed viewing angles generate a higher number of projections (100 views 
for LFD, 18 views for CMVD) than those based on the PCA analysis (between 3 and 9 views) Obviously, a larger 
number of images will carry more information which results in a more complete description. In the case of aspect graph 
and stable views algorithms, the redundancy is reduced by selecting a sub-set of representative images. However, the 
main drawback of such approaches remains their high computational complexity.  
Table 1 synthesizes the various descriptors presented in this section. For each method, the extraction and the matching 
complexities, respectively denoted by CE and CM, are qualitatively estimated (i.e. + for low complexity and +++ for 
high). The numbers of views per model as well as the viewing angle selection procedure are also indicated. The last 
column recalls the 2D descriptor used to describe the projection images. 
Table 1. Overview of 2D/3D approaches (CE – Descriptor’s extraction complexity, CM – Matching complexity). 
Method CE CM 
No of 
views 
Viewing angle 
selection 
2D descriptor 
PPA 4 + + 1 PCA 
Zernike moments & contour-based Fourier 
descriptor 
PPD 13 + + 3 PCA Sums of pixels into concentric circles 
MPEG-7 2D/3D CSS 
37
  + ++ 7 PCA Curvature scale space Descriptor 
MPEG-7 2D/3D ART 
37
  ++ ++ 7 PCA Angular Radial Transform 
MCC 8  ++ +++ 3/9 PCA Curve evolution when filtered with Gaussians 
SI 8  + + 3 PCA Binary images  
Aspect graph 2 +++ +++ 5 – 10 Aspect graph prototypes Shock graph 
Stable views 16 +++ N/A 8 
Spherical graph stable 
views 
Zernike moments coefficients (up to order 15) 
ESI 15 + + 10 Even distribution Binary images 
LFD 1  +++ +++ 100 Even distribution Zernike moments & Fourier descriptor 
CMVD 3  ++ ++ 18 Even distribution 
Zernike moments, Polar Fourier and Krawtchouk 
moments coefficients. 
The literature shows a wide palette of useful approaches. However, evaluating the importance of the viewing angle 
selection process remains difficult, since methods use different descriptors and normalization procedures. In order to 
perform a fair comparison, we have established an experimental evaluation protocol, described in the next section.  
3. EXPERIMENTAL EVALUATION 
Experiments have been carried out on the MPEG-7 test dataset 
21
, which includes 363 models semantically categorized 
in 23 classes. Categories include humanoids, airplanes, cars, trees (with or without leafs), five synthetic letter models 
(from A to E), rifles, missiles, pistols, helicopters, motorcycles… Some sample models are illustrated in Figure 8. The 
categories exhibit a relatively important intra-class variability 
20
 in terms of 3D shape. This ground truth databases 
allows us to perform an objective comparison of the different approaches.  
Concerning the viewing angles considered, we have considered both PCA and uniform distributions.  
For the PCA approach, a number of 3 and 7 directions, corresponding to the three principal and four secondary axes, 
have been retained (cf. Section 2.2).  
 
 
 
  
 
 
 
Figure 8: Sample models from the MPEG-7 3D dataset. 
Concerning the even distributions, we have first adopted the dodecahedron-based strategy proposed by the LFD 
approach, which leads to 10 viewing angles (Figure 9.a). In addition, we have considered a second strategy, based on 
successive subdivisions of a regular octahedron 
20, 21
 (Figure 9.b). The initial octahedron gives 3 distinct axes, 
corresponding to 3 out of 6 vertices. At one level of subdivision, 9 viewing angles are obtained. Finally, at two levels of 
subdivision, 33 directions can be used. 
 
a. Dodecahedron b. Recursive subdivisions of a regular octahedron. 
Figure 9: Uniform viewing angle selection with dodecahedron (LFD) and octahedron approaches.  
Whatever the viewing angle selection strategy, a same descriptor is associated to each projection image. For the sake of 
generality, we have adopted the MPEG-7 2D ART descriptor with 35 coefficients, which is able to deal with silhouettes 
of arbitrary topologies.  
As objective evaluation measures, we have retained the First Tear (FT) and Second Tear (ST) Scores
 9
, previously used 
in
 21
, which respectively represent the percentage of correct retrieved models within the Q and 2Q first retrieved results 
(where Q denotes the total number of models of the query’s category).  
In a first time, we have considered the issue of 3D to 3D model retrieval.  
3.1. 3D to 3D model retrieval  
Two similarity metrics have been adopted. The first one, so-called diagonal, assumes that the PCA alignment is correct 
for all models in the database. Thus the similarity between two 3D models is computed as the sum of distances between 
2D ART descriptors associated to the corresponding 2D silhouettes. The second one, so-called minimum, exploits a 
greedy strategy for fitting the various 2D views. When comparing two 3D objects, the best match, corresponding to the 
minimal distance between all combinations of views is first determined. The corresponding views are considered as 
aligned and the process is successively applied upon the remaining sets of views, until all the views are matched.  
Concerning the normalization issues, each object has been first centered in the origin of the 3D coordinated system. A 
PCA analysis has been applied in order to align the object’s principal axes with the systems axes. The PCA 
eiegenvalues, which provide a measure of the object’s size have been exploited for scale normalization, as described 
in 
20
. In order to evaluate the impact of the PCA alignment, we have also considered the following experiment, applied in 
the case of the LFD strategy. After centering and scaling, the object has been rotated with a random 3D rotation matrix. 
In this way, we are able to investigate the retrieval performances when the object has an arbitrary pose.  
Table 2 presents the FT and ST results obtained with different viewing angles and similarity metrics. The LFD column 
states for applying the LFD projection on the randomly rotated object, while LFDPCA concerns a preliminary alignment 
of the object with the PCA axes.  
 
  
 
 
Table 2: Retrieval FT and ST scores (%) obtained on the MPEG-7 dataset (in bold, the maximum scores obtained). 
Matching strategy Score PCA3 PCA7 LFDPCA LFD OCTA9 OCTA33 
Minimum 
FT 
 
63.65 64.10 63.72 59.87 65.44 65.21 
Diagonal 65.57 66.11 65.37 45.19 66.57 67.47 
Minimal 
BE 
 
71.86 73.15 71.61 70.66 73.73 73.84 
Diagonal 73.76 73.54 73.50 58.77 75.56 74.71 
 
When considering the Minimum matching metric, the maximal retrieval scores are achieved for the octahedron-based 
viewing angle selection strategy with 9 (OCT9) and 33 (OCT33 views, with FT and ST scores up to 65.44% and 
73,84%). However, such scores are very lightly superior to those obtained with PCA3 (FT = 63,65%, ST = 71.86%) and 
PCA7 (FT = 64,10%, ST = 73.15%). This shows that increasing the number of views does not necessarily leads to a 
spectacular gain in term of retrieval efficiency. This can be explained by the fact that in this case, the number of false 
positives responses provided by the 2D ART also increases with the number of views. In addition, in the cases of objects 
presenting symmetries, several of the views considered can be similar, which can further bias the results.  
In the case of the Diagonal matching strategy, the scores are slightly superior, with an average gain of about 2%.  
Finally, when comparing LFD and LFDPCA approaches, we can observe that the scores are improved by the PCA 
alignement. This is natural when considering the Diagonal matching, where we obtain a significant gain (20%), but 
holds also in the case of the Minimal strategy (with a gain of 4%), and thus shows the relevance of the PCA alignment.  
A second experiment concerns the aspects of image recognition from a single view.  
3.2. Image recognition from a single view  
A test set has been first constituted. It includes 46 3D objects, randomly selected from the MPEG-7 dataset, such that 
each category is represented by two objects. For each test object, different test images have been generated. A first one 
gives a frontal view and corresponds to the object’s projection onto the principal plane (defined by the principal axes of 
first and second largest eigenvalues). A second image corresponds to the projection from a randomly generated angle of 
view. Finally, a third one, is also randomly generated but in this case the angle of view is restricted to a 45° interval 
around the normal to the principal plane. This is motivated by the fact that in videos, objects pose variation can be 
considered as limited.  
Two performance metrics have been retained in this case. The first one, so-call First Answer (FA) provides the 
percentage of queries where an image from the correct category has been found on the first retrieved position. The 
second one is the category recognition rate (CRR), defined as follows. For each query, the categories obtained in the 
first 10 retrieved images are determined. We decide then that the query object belongs to the most represented category 
among them (i.e. category with the greatest number of objects retrieved in the top 10 results). The CRR is finally defined 
as the percentage of correct decisions over the whole test set of 46 models. 
Table 3 presents the obtained results.  
In the case of the randomly generated query images, a set of 10 images has been generated for each type of random trial 
(i.e. RAND and RAND45). The results reported in Table 3 represent the average scores obtained.  
The best FA and CRR scores are obtained when using the PP image (FA = 82.6% and CRR = 69.56, for OCTA33 
representations). This shows the high relevance of using the frontal image in the retrieval process. However, in the case 
of real objects detected from images or videos, we cannot ensure that the PP image can always be detected.  
When using the completely random images (RAND), the recognition scores are drastically lower. In this case, the best 
FA score is achieved by OCTA33 (67,39%). Concerning the CRR, the maximum value is obtained by the LFD 
representation (56,62%). The recognition rates are increasing when limiting the 3D object’s pose variation in the case of 
RAND45. Here again, the best FA is provided by OCTA33 (67,39%), while the maximum CRR is given by the LFD 
approach (67,78%). Let us note that this score approaches the optimal obtained for the PP image with OCTA33 
(69,56%).  
  
 
 
Table 3: Image recognition from a single view with FA and CRR scores in function of the different viewing angle selection strategies 
and of the type of query image (PP – frontal view on the principal plane, RAND – completely random pose, RAND45 – random pose 
restricted to a 45° around the normal to the principal plane). The best scores are represented in bold.  
Query image type 
 
Score PCA3 PCA7 LFD LFDPCA OCTA9 OCTA33 
PP 
 
FA 
71.73 76.08 78.26 54.34 80.43 82.60 
RAND45 45.65 52.17 69.56 60.86 56.52 73.90 
RAND 21.73 47.82 65.21 56.52 47.82 67.39 
PP 
 
CRR 
63.04 63.04 63.04 47.82 69.56 69.56 
RAND45 39.32 52.56 67.78 58.10 58.89 66.79 
RAND 23.91 45.65 56.52 52.17 43.47 50.00 
The obtained results clearly show the superiority of strategies based on evenly distributed viewing angles with respect to 
PCA-based approaches (e.g. with gain of 10% to 15% for LFD w.r.t. to PCA7), for image recognition purposes. In 
addition, the perfectly even distribution associated to the LFD approach offers highly interesting performances. 
4. CONCLUSIONS AND FUTURE WORK 
In this paper, we have presented an overview of the state of the art 3D/3D shape-based retrieval approaches. Two great 
families of approaches have been identified and described. The first one concerns methods based on a PCA alignment, 
while the second is based on dense and evenly distributed viewing angles. Uniquely approaches that are using 
exclusively 2D features have been considered. 
In order to study the influence of viewing angles selection, we set up an experimental evaluation protocol, related to 
applications of both 3D to 3D model retrieval and image recognition from a single view. As 2D shape descriptors, we 
have considered the MPEG-7 2D ART. Experimental results, obtained on the MPEG-7 ground truth dataset show that:  
 for 3D to 3D model retrieval, increasing the number of views do not significantly enhance the retrieval performances, 
due to false positives and miss-alignments,  
 in the case of image recognition the evenly distributed viewing angles strategies lead to superior performances.  
Our future work concerns the extension of this study to other, more discriminant descriptors, such as MPEG-7 CSS, 
Hough 2D, Fourier descriptors, or combination which can provide complementary 2D shape information for a more 
complete characterization of the 2D shape that can further increase the recognition rates. At a longer term, our work will 
concern with real-life objects, detected from natural videos and still images.  
5. ACNOWLEDGMENT 
This work has been partially supported by the UBIMEDAI Research Lab, between Institut TELECOM and Alcatel-
Lucent Bell-Labs.  
REFERENCES 
1. Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen and Ming Ouhyoung, ""On visual similarity based 3D model retrieval"", Computer 
Graphics Forum, vol. 22, no. 3, pp. 223-232, 2003. 
2. C. Cyr and B. Kimia, ""3D object recognition using shape similarity-based aspect graph"", Proc. 8th IEEE Int. Conf. Comput. 
Vision, Vancouver, BC, Canada, pp. 254–261, 2001. 
3. Petros Daras, Apostolos Axenopoulos, ""A Compact Multi-View descriptor for 3D Object Retrieval"", International Workshop on 
Content-Based Multimedia Indexing, June 2009. 
4. YuJie Liu, Xiao-Dong Zhang, ZongMin Li; Hua Li, ""3D model feature extraction method based on the projection of principle 
plane"", Computer-Aided Design and Computer Graphics, 2009 (CAD/Graphics '09), Page(s):463 – 469, August 2009. 
5. S. Mahmoudi and M. Daoudi, ""3D models retrieval by using characteristic views"", Proceedings of the 16th International 
Conference on Pattern Recognition, pp. 457-460, Quebec, Canada, 2002. 
6. F. Mokhtarian, A.K. Mackworth, ""A Theory of Multiscale, Curvature-Based Shape Representation for Planar Curves"", IEEE 
Transaction on Pattern Analysis and Machine Intelligence, pp. 789-805, August 1992 
7. R. Mukundan and K. R. Ramakrishnan, ""Moment Functions in Image Analysis: Theory and Applications"", World Scientific 
Publishing Co Pte Ltd., September 1998. 
  
 
 
8. T. Napoléon, T. Adamek, F. Schmitt,N.E. O'Connor, ""Multi-view 3D retrieval using silhouette intersection and multi-scale 
contour representation"", SHREC 2007 - Shape Retrieval Contest, Lyon, France, June 2007. 
9. R. Osada, T. Funkhouser, B. Chazelle, and D. Dobkin, ""Shape distributions"", ACM Transactions on Graphics (TOG), vol. 21, pp. 
807-832, Octomber 2002. 
10. J. Pu, and K. Ramani, “An Approach to Drawing-Like View Generation From 3D Models”, In Proc. Of International Design 
Engineering Technical Conferences, September 2005.  
11. Zheng Qin, Ji Jia, Jun Qin, ""Content Based 3D Model Retrieval: A survey"", Content Based Multimedia Indexing 2008, 
June 2008. 
12. T. B. Sebastian, P. N. Klein, and B. B. Kimia, ""Alignment based recognition of shape outlines"", Proceedings of the 4th 
International Workshop on Visual Form, May 2001. 
13. J.L. Shih, W.C. Wang, “A 3D Model Retrieval Approach based on The Principal Plane Descriptor”, Proceedings of The Second 
Internat. Conf. on Innovative Computing, Information and Control (ICICIC), pp. 59-62, 2007. 
14. Johan W.H. Tangelder and Remco C. Veltkamp, ""A Survey of Content Based 3D Shape Retrieval Methods"", Proceedings of the 
Shape Modeling International 2004 (SMI'04), pp. 145-156, Genova, Italy, 2004 
15. Liuying Wen, Guoxin Tan, ""Enhanced 3D Shape Retrieval Using View-Based Silhouette Representation"", International 
Conference on Audio, Language and Image Processing, August 2008.  
16. H. Yamauchi, W. Saleem, S. Yoshizawa, Z. Karni, A. Belyaev, H.-P. Seidel, ""Towards Stable and Salient Multi-View 
Representation of 3D Shapes"", IEEE Int. Conf. on Shape Modeling and Applications, 2006, pp.40-40, 14-16, June 2006.  
17. Tao Yang, Bo Liu, Hongbin Zhang, ""3D model retrieval based on exact visual similarity"", 9th International Conference on Signal 
Processing, December 2008. 
18. Y. Yang, H. Lin, Y. Zhang, ""Content-based 3D Model Retrieval: A Survey"", IEEE Trans/ on Systems, Man, and Cybernetics – 
Part C: Applications and Reviews, Vol. 37, No6, p. 1081-1098, November 2007.  
19. P.T.Yap, R.Paramesran and S.H.Ong, ""Image Analysis by Krawtchouk Moments"", IEEE Transactions on Image Processing, Vol. 
12, No. 11, pp. 1367-1377, November 2003.  
20. T. Zaharia, F. Prêteux, ""3D shape-based retrieval within the MPEG-7 framework"", Proc. SPIE Conf. on Nonlinear Image 
Processing and Pattern Analysis XII, Vol. 4304, pp.133-145, San Jose, CA, USA, January 2001 
21. T. Zaharia, F. Prêteux, ""3D versus 2D/3D Shape Descriptors: A Comparative study"", In SPIE Conf. on Image Processing: 
Algorithms and Systems, Vol. 2004 , Toulouse, France, January 2004. 
22. D. S. Zhang and G. Lu. ""An Integrated Approach to Shape Based Image Retrieval"", Proc. of 5th Asian Conferenceon Computer 
Vision (ACCV), pp. 652-657, Melbourne, Australia, January 2002. 
23. Tianyang Lv, Guobao Liu, Shao-bin Huang, Zheng-xuan Wang, ""Semantic 3D Model Retrieval Based on Semantic Tree and 
Shape Feature"",  Sixth International Conference on Fuzzy Systems and Knowledge Discovery, pp. 452-457, December 2009.  
24. Boyong Gao, Herong Zheng, Sanyuan Zhang, ""An Overview of Semantics Processing in Content-Based 3D Model Retrieval"", 
Artificial Intelligencw and computational Intelligence, pp. 54-59, January 2010.  
25. Ciaccia Patella Rabitti ,  P. Ciaccia ,  M. Patella ,  F. Rabitti ,  P. Zezula, ""Indexing Metric Spaces with M-tree"", in SEBD 1997, 
pp. 67-86, 1997.  
26. T. Adamek and N. E. O’Connor, ""A multiscale representation method for nonrigid shapes with a single closed contour"", IEEE 
Trans. Circuits Syst. Video Techn, Volume 14, Issue 5, pp. 742–753, May 2004. 
27. Helmut Alt, Ulrich Fuchs, Günter Rote, Gerald Weber, ""Matching Convex Shapes with Respect to the Symmetric Difference"", 
Lecture Notes in Computer Science, Volume 1136/1996, pp. 320-333, May 1998.   
28. T.Zaharia, F. Preteux, ""Shape-based retrieval of 3D mesh models"", Multimedia and Expo, 2002. ICME '02. Proceedings. 2002 
IEEE International Conference on Volume 1,  pp. 437-440, August 2002. 
29. J. J. Koenderink, A. J. van Doorn, ""The singularilarities of the visual mapping"". Biol. Cyber., Volume 24, pp. 51–59, 1976. 
30. Chen Ding-Yun, Ouhyoung Ming, ""A 3D model alignment and retrieval system"", Proceedings of International Computer 
Symposium, Workshop on Multimedia Technologies, Hualien,Taiwan, pp. 1436-1443, December 2002 
31. M. Bober, ""MPEG-7 Visual Shape Descriptors"", IEEE Transaction on Circuits and Systems for Video Technology, Volume 11, 
Issue 6, pp. 716-719, August 2002  
32. B.S. Manjunath, Phillipe Salembier, Thomas Sikora, ""Introduction to MPEG-7: Multimedia Content Description Interface"", John 
Wiley & Sons, Inc., New York, NY, 2002.  
33. W.-Y. Kim, Y.-S. Kim, ""A New Region-Based Shape Descriptor"", ISO/IEC MPEG99/M5472, Maui, Hawaii, December 1999. 
34. J.L. Shih, C.H. Lee, and J.T. Wang, ""A 3D Model Retrieval System Using the Derivative Elevation and 3D-ART"", Proceedings 
of the 2008 IEEE Asia-Pacific Services Computing Conference, p.p. 739-744, 2008.  
35. R. Ohbuchi, K. Osada, T. Furuya, T. Banno, ""Salient local visual features for shape-based 3D model retrieval"", Shape Modeling 
and Applications, 2008. SMI 2008, pp. 93-102, June 2008. 
36. P. Papadakis, I. Pratikakis, T. Theoharis, G. Passalis, and S. Perantonis, ""3D object retrieval using an efficient and compact 
hybrid shape descriptor"",  Eurographics Workshop on 3D Object Retrieval, Crete, Greece, April 15, 2008 
37. ISO/IEC 15938-3: 2002, MPEG-7-Visual, Information Technology – Multimedia content description interface – Part 3: Visual, 
2002. 
38. ISO/ IEC 15938-5, Information technology - MultimediaContent Description. Interface - Part 5: Multimedia Description 
Schemes. 2003. 
"
39,"ar
X
iv
:1
70
8.
05
96
3v
1 
 [s
tat
.M
L]
  2
0 A
ug
 20
17
Neural Networks Compression for Language
Modeling
Artem M. Grachev1,2, Dmitry I. Ignatov2, and Andrey V. Savchenko3
1 Samsung R&D Institute Rus, Moscow, Russia
2 National Research University Higher School of Economics, Moscow, Russia
3 National Research University Higher School of Economics, Laboratory of
Algorithms and Technologies for Network Analysis, Nizhny Novgorod, Russia
grachev.art@gmail.com
Abstract. In this paper, we consider several compression techniques
for the language modeling problem based on recurrent neural networks
(RNNs). It is known that conventional RNNs, e.g, LSTM-based networks
in language modeling, are characterized with either high space complex-
ity or substantial inference time. This problem is especially crucial for
mobile applications, in which the constant interaction with the remote
server is inappropriate. By using the Penn Treebank (PTB) dataset we
compare pruning, quantization, low-rank factorization, tensor train de-
composition for LSTM networks in terms of model size and suitability
for fast inference.
Keywords: LSTM, RNN, language modeling, low-rank factorization,
pruning, quantization
1 Introduction
Neural network models can require a lot of space on disk and in memory. They
can also need a substantial amount of time for inference. This is especially im-
portant for models that we put on devices like mobile phones. There are several
approaches to solve these problems. Some of them are based on sparse compu-
tations. They also include pruning or more advanced methods. In general, such
approaches are able to provide a large reduction in the size of a trained net-
work, when the model is stored on a disk. However, there are some problems
when we use such models for inference. They are caused by high computation
time of sparse computing. Another branch of methods uses different matrix-
based approaches in neural networks. Thus, there are methods based on the
usage of Toeplitz-like structured matrices in [1] or different matrix decomposi-
tion techniques: low-rank decomposition [1], TT-decomposition (Tensor Train
decomposition) [2,3]. Also [4] proposes a new type of RNN, called uRNN (Uni-
tary Evolution Recurrent Neural Networks).
In this paper, we analyze some of the aforementioned approaches. The mate-
rial is organized as follows. In Section 2, we give an overview of language mod-
eling methods and then focus on respective neural networks approaches. Next
we describe different types of compression. In Section 3.1, we consider the sim-
plest methods for neural networks compression like pruning or quantization. In
Section 3.2, we consider approaches to compression of neural networks based on
different matrix factorization methods. Section 3.3 deals with TT-decomposition.
Section 4 describes our results and some implementation details. Finally, in Sec-
tion 5, we summarize the results of our work.
2 Language modeling with neural networks
Consider the language modeling problem. We need to compute the probability
of a sentence or sequence of words (w1, . . . , wT ) in a language L.
P(w1, . . . , wT ) = P(w1, . . . , wT−1)P(wT |w1, . . . , wT−1) =
=
T∏
t=1
P(wt|w1, . . . , wt−1) (1)
The use of such a model directly would require calculation P(wt|w1, . . . , wt−1)
and in general it is too difficult due to a lot of computation steps. That is
why a common approach features computations with a fixed value of N and
approximate (1) with P(wt|wt−N , . . . , wt−1). This leads us to the widely known
N -gram models [5,6]. It was very popular approach until the middle of the 2000s.
A new milestone in language modeling had become the use of recurrent neural
networks [7]. A lot of work in this area was done by Thomas Mikolov [8].
Consider a recurrent neural network, RNN, whereN is the number of timesteps,
L is the number of recurrent layers, xt−1ℓ is the input of the layer ℓ at the moment
t. Here t ∈ {1, . . . , N}, ℓ ∈ {1, . . . , L}, and xt0 is the embedding vector. We can
describe each layer as follows:
ztℓ =Wℓx
t
ℓ−1 + Vℓx
t−1
ℓ + bl (2)
xtℓ =σ(z
t
ℓ), (3)
where Wℓ and Vℓ are matrices of weights and σ is an activation function. The
output of the network is given by
yt = softmax
[
WL+1x
t
L + bL+1
]
. (4)
Then, we define
P(wt|wt−N , . . . , wt−1) = y
t. (5)
While N -gram models even with not very big N require a lot of space due
to the combinatorial explosion, neural networks can learn some representations
of words and their sequences without memorizing directly all options.
Now the mainly used variations of RNN are designed to solve the problem of
decaying gradients [9]. The most popular variation is Long Short-Term Memory
(LSTM) [7] and Gated Recurrent Unit (GRU) [10]. Let us describe one layer of
LSTM:
itℓ = σ
[
W il x
t
l−1 + V
i
l x
t−1
l + b
i
l
]
input gate (6)
f tℓ = σ
[
W
f
l x
t
l−1 + V
f
l x
t−1
l + b
f
l
]
forget gate (7)
ctℓ = f
t
l · c
t−1
l + i
t
l tanh
[
W cl x
t
l−1 + U
c
l x
t−1
l + b
c
l
]
cell state (8)
otℓ = σ
[
W ol x
t
ℓ−1 + V
o
l x
t−1
l + b
o
l
]
output gate (9)
xtℓ = o
t
ℓ · tanh[c
t
l ], (10)
where again t ∈ {1, . . . , N}, ℓ ∈ {1, . . . , L}, ctℓ is the memory vector at the layer
ℓ and time step t. The output of the network is given the same formula 4 as
above.
Approaches to the language modeling problem based on neural networks are
efficient and widely adopted, but still require a lot of space. In each LSTM layer
of size k × k we have 8 matrices of size k × k. Moreover, usually the first (or
zero) layer of such a network is an embedding layer that maps word’s vocabulary
number to some vector. And we need to store this embedding matrix too. Its size
is nvocab×k, where nvocab is the vocabulary size. Also we have an output softmax
layer with the same number of parameters as in the embedding, i.e. k×nvocab. In
our experiments, we try to reduce the embedding size and to decompose softmax
layer as well as hidden layers.
We produce our experiments with compression on standard PTB models.
There are three main benchmarks: Small, Medium and Large LSTM models
[11]. But we mostly work with Small and Medium ones.
3 Compression methods
3.1 Pruning and quantization
In this subsection, we consider maybe not very effective but still useful tech-
niques. Some of them were described in application to audio processing [12]
or image-processing [13,14], but for language modeling this field is not yet well
described.
Pruning is a method for reducing the number of parameters of NN. In
Fig 1. (left), we can see that usually the majority of weight values are con-
centrated near zero. It means that such weights do not provide a valuable con-
tribution in the final output. We can set some threshold and then remove all
connections with the weights below it from the network. After that we retrain
the network to learn the final weights for the remaining sparse connections.
Quantization is a method for reducing the size of a compressed neural net-
work in memory. We are compressing each float value to an eight-bit integer
representing the closest real number in one of 256 equally-sized intervals within
the range.
Fig. 1. Weights distribution before and after pruning
−3 −2 −1 0 1 2 3
Value
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
Fr
e
q
u
e
n
cy
−3 −2 −1 0 1 2 3
Value
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Fr
e
q
u
e
n
cy
Pruning and quantization have common disadvantages since training from
scratch is impossible and their usage is quite laborious. In pruning the reason
is mostly lies in the inefficiency of sparse computing. When we do quantization,
we store our model in an 8-bit representation, but we still need to do 32-bits
computations. It means that we have not advantages using RAM. At least until
we do not use the tensor processing unit (TPU) that is adopted for effective 8-
and 16-bits computations.
3.2 Low-rank factorization
Low-rank factorization represents more powerful methods. For example, in [1],
the authors applied it to a voice recognition task. A simple factorization can be
done as follows:
xtl = σ
[
W aℓ W
b
ℓ x
t
ℓ−1 + U
a
l U
b
l x
t−1
ℓ + bl
]
(11)
Following [1] require W bl = U
b
ℓ−1. After this we can rewrite our equation for
RNN:
xtl = σ
[
W al m
t
l−1 + U
a
l m
t−1
l + bl
]
(12)
mtl = U
b
l x
t
l (13)
yt = softmax
[
WL+1m
t
L + bL+1
]
(14)
For LSTM it is mostly the same with more complicated formulas. The main
advantage we get here from the sizes of matrices W al , U
b
l , U
a
l . They have the
sizes r × n and n × r, respectively, where the original Wl and Vl matrices have
size n × n. With small r we have the advantage in size and in multiplication
speed. We discuss some implementation details in Section 4.
3.3 The Tensor Train decomposition
In the light of recent advances of tensor train approach [2,3], we have also decided
to apply this technique to LSTM compression in language modeling.
The tensor train decomposition was originally proposed as an alternative
and more efficient form of tensor’s representation [15]. The TT-decomposition
(or TT-representation) of a tensor A ∈ Rn1×...×nd is the set of matrices Gk[jk] ∈
R
rk−1×rk , where jk = 1, . . . , nk, k = 1, . . . , d, and r0 = rd = 1 such that each of
the tensor elements can be represented asA(j1, j2, . . . , jd) = G1[j1]G2[j2] . . .Gd[jd].
In the same paper, the author proposed to consider the input matrix as a multi-
dimensional tensor and apply the same decomposition to it. If we have matrix A
of size N ×M , we can fix d and such n1, . . . , nd, m1, . . . ,md that the following
conditions are fulfilled:
∏d
j=1 nj = N ,
∏d
i=1mi = M . Then we reshape our ma-
trix A to the tensor A with d dimensions and size n1m1 × n2m2 × . . .× ndmd.
Finally, we can perform tensor train decomposition with this tensor. This ap-
proach was successfully applied to compress fully connected neural networks [2]
and for developing convolution TT layer [3].
In its turn, we have applied this approach to LSTM. Similarly, as we describe
it above for usual matrix decomposition, here we also describe only RNN layer.
We apply TT-decomposition to each of the matrices W and V in equation 2 and
get:
ztℓ = TT(Wi)x
t
ℓ−1 +TT(Vl)x
t−1
ℓ + bℓ. (15)
Here TT(W ) means that we apply TT-decomposition for matrix W . It is nec-
essary to note that even with the fixed number of tensors in TT-decomposition
and their sizes we still have plenty of variants because we can choose the rank
of each tensor.
4 Results
For testing pruning and quantization we choose Small PTB Benchmark. The
results can be found in Table 1. We can see that we have a reduction of the size
with a small loss of quality.
For matrix decomposition we perform experiments with Medium and Large
PTB benchmarks. When we talk about language modeling, we must say that the
embedding and the output layer each occupy one third of the total network size.
It follows us to the necessity of reducing their sizes too. We reduce the output
layer by applying matrix decomposition. We describe sizes of LR LSTM 650-
650 since it is the most useful model for the practical application. We start with
basic sizes for W and V , 650× 650, and 10000× 650 for embedding. We reduce
each W and V down to 650× 128 and reduce embedding down to 10000× 128.
The value 128 is chosen as the most suitable degree of 2 for efficient device
implementation. We have performed several experiments, but this configuration
is near the best. Our compressed model, LR LSTM 650-650, is even smaller
than LSTM 200-200 with better perplexity. The results of experiments can be
found in Table 2.
In TT decomposition we have some freedom in way of choosing internal ranks
and number of tensors. We fix the basic configuration of an LSTM-network with
Table 1. Pruning and quantization results on PTB dataset
Model Size No. of params Test PP
LSTM 200-200 (Small benchmark) 18.6 Mb 4.64 M 117.659
Pruning output layer 90%
w/o additional training 5.5 Mb 0.5 M 149.310
Pruning output layer 90%
with additional training 5.5 Mb 0.5 M 121.123
Quantization (1 byte per number) 4.7 Mb 4.64 M 118.232
two 600-600 layers and four tensors for each matrix in a layer. And we perform
a grid search through different number of dimensions and various ranks.
We have trained about 100 models with using the Adam optimizer [16]. The
average training time for each is about 5-6 hours on GeForce GTX TITAN X
(Maxwell architecture), but unfortunately none of them has achieved acceptable
quality. The best obtained result (TT LSTM 600-600) is even worse than
LSTM-200-200 both in terms of size and perplexity.
Table 2. Matrix decomposition results on PTB dataset
Model Size No. of params Test PP
PTB LSTM 200-200 18.6 Mb 4.64 M 117.659
Benchmarks LSTM 650-650 79.1 Mb 19.7 M 82.07
LSTM 1500-1500 264.1 Mb 66.02 M 78.29
Ours LR LSTM 650-650 16.8 Mb 4.2 M 92.885
TT LSTM 600-600 50.4 Mb 12.6 M 168.639
LR LSTM 1500-1500 94.9 Mb 23.72 M 89.462
5 Conclusion
In this article, we have considered several methods of neural networks compres-
sion for the language modeling problem. The first part is about pruning and
quantization. We have shown that for language modeling there is no difference
in applying of these two techniques. The second part is about matrix decompo-
sition methods. We have shown some advantages when we implement models on
devices since usually in such tasks there are tight restrictions on the model size
and its structure. From this point of view, the model LR LSTM 650-650 has
nice characteristics. It is even smaller than the smallest benchmark on PTB and
demonstrates quality comparable with the medium-sized benchmarks on PTB.
Acknowledgements. This study is supported by Russian Federation President
grant MD-306.2017.9. A.V. Savchenko is supported by the Laboratory of Algo-
rithms and Technologies for Network Analysis, National Research University
Higher School of Economics.
References
1. Lu, Z., Sindhwan, V., Sainath, T.N.: Learning compact recurrent neural networks.
Acoustics, Speech and Signal Processing (ICASSP) (2016)
2. Novikov, A., Podoprikhin, D., Osokin, A., Vetrov, D.P.: Tensorizing neural net-
works. In: Advances in Neural Information Processing Systems 28: Annual Con-
ference on Neural Information Processing Systems 2015. (2015) 442–450
3. Garipov, T., Podoprikhin, D., Novikov, A., Vetrov, D.P.: Ultimate tensorization:
compressing convolutional and FC layers alike. CoRR/NIPS 2016 workshop: Learn-
ing with Tensors: Why Now and How? abs/1611.03214 (2016)
4. Arjovsky, M., Shah, A., Bengio, Y.: Unitary evolution recurrent neural networks.
In: Proceedings of the 33nd International Conference on Machine Learning, ICML
2016. (2016) 1120–1128
5. Jelinek, F.: Statistical Methods for Speech Recognition. MIT Press (1997)
6. Kneser, R., Ney, H.: Improved backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference on Acoustics, Speech and Signal
Processing 1 (1995) 181–184.
7. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
(9(8)) (1997) 1735–1780
8. Mikolov, T.: Statistical Language Models Based on Neural Networks. PhD thesis,
Brno University of Technology (2012)
9. Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J.: Gradient flow in recurrent
nets: the difficulty of learning long-term dependencies. S. C. Kremer and J. F.
Kolen, eds. A Field Guide to Dynamical Recurrent Neural Networks (2001)
10. Cho, K., van Merrienboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259, 2014f (2014)
11. Zaremba, W., Sutskever, I., Vinyals, O.: Recurrent neural network regularization.
Arxiv preprint (2014)
12. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. Acoustics, Speech
and Signal Processing (ICASSP) (2016)
13. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kaut, J.: Pruning convolu-
tional neural networks for resource efficient transfer learning. arXiv preprint
arXiv:1611.06440 (2016)
14. Rassadin, A.G., Savchenko, A.V.: Deep neural networks performance optimiza-
tion in image recognition. Proceedings of the 3rd International Conference on
Information Technologies and Nanotechnologies (ITNT) (2017)
15. Oseledets, I.V.: Tensor-train decomposition. SIAM J. Scientific Computing 33(5)
(2011) 2295–2317
16. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. The Interna-
tional Conference on Learning Representations (ICLR) (2015)
"
40,"International Journal of Computer Applications (0975 – 8887)  
Volume 62– No.7, January 2013 
1 
An ANN-based Method for Detecting Vocal 
Fold Pathology 
 
Vahid Majidnezhad  
Department of Computer Engineering,      
Shabestar Branch, Islamic Azad University, 
Shabestar, Iran. 
 
Igor Kheidorov  
Department of Computer Engineering,       
Shabestar Branch, Islamic Azad University, 
Shabestar, Iran. 
ABSTRACT 
There are different algorithms for vocal fold pathology 
diagnosis. These algorithms usually have three stages which 
are Feature Extraction, Feature Reduction and Classification. 
While the third stage implies a choice of a variety of machine 
learning methods, the first and second stages play a critical 
role in performance and accuracy of the classification system. 
In this paper we present initial study of feature extraction and 
feature reduction in the task of vocal fold pathology diagnosis. 
A new type of feature vector, based on wavelet packet 
decomposition and Mel-Frequency-Cepstral-Coefficients 
(MFCCs), is proposed. Also Principal Component Analysis 
(PCA) is used for feature reduction. An Artificial Neural 
Network is used as a classifier for evaluating the performance 
of our proposed method. 
Keywords 
Wavelet Packet Decomposition, Mel-Frequency-Cepstral-
Coefficient (MFCC), Principal Component Analysis (PCA), 
Artificial Neural Network (ANN). 
1. INTRODUCTION 
Speech signal information often plays an important role for 
specialists to understand the process of vocal fold pathology 
formation. In some cases vocal signal analysis can be the only 
way to analyze the state of vocal folds. Nowadays diverse 
medical techniques exist for direct examination and detection 
of pathologies. Laryngoscopy, electromyography, 
videokimography are most frequently used by medical 
specialists. But these methods possess a number of 
disadvantages. Human vocal tract is hardly-accessible for 
visual examination during phonation process and that makes it 
more problematic to identify a pathology. Moreover, these 
diagnostic means may cause the patients feel much discomfort 
and distort the actual signal so that it may lead to incorrect 
diagnosis as well [1-4]. 
Acoustic analysis as a diagnostic method has no drawbacks, 
peculiar to the above mentioned methods. It possesses a 
number of advantages. First of all, acoustic analysis is a non-
invasive diagnostic technique that allows pathologists to 
examine many people in short time period with minimal 
discomfort. It also allows pathologists to reveal the 
pathologies on early stages of their origin. This method can be 
of great interest for medical institutions. In recent years a 
number of methods were developed for segmentation and 
classification of speech signals with pathology. The general 
scheme of vocal fold pathology diagnostic methods consists 
of three stages which are feature extraction, feature reduction 
and classification.  
 
 
Different parameters for feature extraction are used. 
Traditionally, one deals with such parameters like pitch, jitter, 
shimmer, amplitude perturbation, pitch perturbation, signal to 
noise ratio, normalized noise energy [5] and others [6-9]. 
Feature extraction, using the above mentioned parameters, has 
shown its efficiency for a number of practical tasks. In the 
proposed method, we have used the Mel-Frequency-Cepstral-
Coefficients (MFCCs), Energy and Shannon Entropy 
parameters for creating the features vector. Also different 
approaches for feature reduction are used such as Principal 
Component Analysis (PCA) [10-13] and Linear Discriminant 
Analysis (LDA) [14]. In the proposed method we have used 
PCA for feature reduction. Finally, the reduced features are 
used for speech classification into the healthy and 
pathological class. Different machine learning methods such 
as Support Vector Machines [9], Hidden Markov Model [15], 
etc can be used as a classifier. In the proposed method we 
have used the ANN for the classification purpose. 
2. METHODOLOGY 
The wavelet transform, as was shown in [5], is a flexible tool 
for time-frequency analysis of speech signals. This led us to 
supposition that feature vectors based on wavelets can show 
good results. The idea to build feature vector on wavelets for 
audio classification was previously reported by Li et al [16] 
and Tzanetakis et al in [17]. These authors used the discrete 
wavelet transform (DWT) coefficients for their method of 
feature extraction for content-based audio classification. 
Kukharchik et al in [18] used continues wavelet transform 
(CWT) coefficients for their method of feature extraction. 
Cavalcanti et al in [19] used Wavelet Packet Decomposition 
(WPD) nodes coefficients for their method for feature 
extraction. In this paper we have also used the wavelet packet 
decomposition to create the wavelet packet tree and to extract 
the features. 
The block diagram of our proposed method is illustrated in 
Fig. 1. In the first stage, by the use of MFCC and Wavelet 
Packet Decomposition, feature vector containing 139 features 
is made. In the second stage, by the use of PCA method, the 
dimension of feature vector from 139 is decreased. In the last 
stage, by the use of Artificial Neural Network (ANN), the 
speech signal classified into two classes: pathological or 
healthy. 
 
 
 
 
 
 
International Journal of Computer Applications (0975 – 8887)  
Volume 62– No.7, January 2013 
2 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig  1: The scheme of the proposed method for detection 
of vocal fold pathology. 
 
2.1 Feature extraction 
As it is shown in Fig. 1, first, by the use of cepstral 
representation of input signal, 13 Mel-Frequency-Cepstral-
Coefficients (MFCC) are extracted. Then the wavelet packet 
decomposition in 5 levels is applied on the input signal to 
make the wavelet packet tree. Then, from the nodes of 
resulting wavelet packet tree, 63 energy features along with 
63 shannon entropy features are extracted. Finally, by the 
combination of these features, the initial feature vector with 
the length of 139 features is created. 
2.1.1 Mel-Frequency-Cepstral-Coefficients 
MFCCs are widely used features to characterize a voice signal 
and can be estimated by using a parametric approach derived 
from linear prediction coefficients (LPC), or by the non-
parametric discrete fast Fourier transform (FFT), which 
typically encodes more information than the LPC method. 
The signal is windowed with a hamming window in the time 
domain and converted into the frequency domain by FFT, 
which gives the magnitude of the FFT. Then the FFT data is 
converted into filter bank outputs and the cosine transform is 
found to reduce dimensionality. The filter bank is constructed 
using 13 linearly-spaced filters (133.33Hz between center 
frequencies,) followed by 27 log-spaced filters (separated by a 
factor of 1.0711703 in frequency.) Each filter is constructed 
by combining the amplitude of FFT bin. The Matlab code to 
calculate the MFCC features was adapted from the Auditory 
Toolbox (Malcolm Slaney). The MFCCs are used as features 
in [14] to classify the speech into pathology and healthy class. 
We have used reduction of MFCC information by averaging 
the sample’s value of each coefficient. 
2.1.2 Wavelet Packet Decomposition 
Recently, wavelet packets (WPs) have been widely used by 
many researchers to analyze voice and speech signals. There 
are many out-standing properties of wavelet packets which 
encourage researchers to employ them in widespread fields. 
The most important, multi resolution property of WPs is 
helpful in voice signal synthesis [20-21]. 
The hierarchical WP transform uses a family of wavelet 
functions and their associated scaling functions to decompose 
the original signal into subsequent sub-bands. The 
decomposition process is recursively applied to both the low 
and high frequency sub-bands to generate the next level of the 
hierarchy. In this study, mother wavelet function of the tenth 
order Daubechies has been chosen and the signals have been 
decomposed to five levels. The mother wavelet used in this 
study is reported to be effective in voice signal analysis [22-
23] and is being widely used in many pathological voice 
analyses [21]. Due to the noise-like effect of irregularities in 
the vibration pattern of damaged vocal folds, the distribution 
manner of such variations within the whole frequency range 
of pathological speech signals is not clearly known. 
Therefore, it seems reasonable to use WP rather than DWT or 
CWT to have more detail sub-bands.  
2.2 Feature Reduction 
Using every feature for classification process is not good idea 
and it may be causes to the increasing the rate of 
misclassification. Therefore, it is better to choose the proper 
features from the whole features. This process is called as 
“Feature Reduction”. One way for feature reduction is 
Principal Component Analysis (PCA) which is used 
frequently in pervious works such as [10-13]. 
PCA method searches a mapping to find the best 
representation for distribution of data. Therefore, it uses a 
signal-representation criterion to perform dimension reduction 
while preserving much of the randomness or variance in the 
high-dimensional space as possible. The first principal 
component accounts for as much of the variability in the data 
as possible, and each succeeding component accounts for as 
much of the remaining variability as possible. PCA involves 
the calculation of the eigenvalues decomposition of a data 
covariance matrix or singular value decomposition of a data 
matrix, usually after mean centering the data for each 
attribute. PCA is mathematically defined as an orthogonal 
linear transformation that transforms the data to a new 
coordinate system such that the greatest variance by any 
projection of the data comes to lie on the first coordinate, 
called the first principal component, the second greatest 
variance on the second coordinate, and so on. 
2.3  Artificial Neural Network 
An artificial neural network (ANN) as a computing system is 
made up of a number of simple, and highly interconnected 
processing elements, which processes information by its 
dynamic state response to external inputs. In recent times the 
study of the ANN models is gaining rapid and increasing 
importance because of their potential to offer solutions to 
some of the problems which have hitherto been intractable by 
standard serial computers in the areas of computer science and 
artificial intelligence. Neural networks are better suited for 
achieving human-like performance in the fields such as 
speech processing, image recognition, machine vision, robotic 
control, etc. 
Processing elements in an ANN are also known as neurons. 
These neurons are interconnected by means of information 
channels called interconnections. Each neuron can have 
multiple inputs; while there can be only one output. Inputs to 
a neuron could be from external stimuli or could be from 
output of the other neurons. Copies of the single output that 
comes from a neuron could be input to many other neurons in 
the network. When the weighted sum of the inputs to the 
neuron exceeds a certain threshold, the neuron is fired and an 
output signal is produced. The network can recognize input 
Speech Signal 
Healthy or Pathological Speech 
MFCC Coefficients 
Extraction 
Creating the Feature 
Vector (139 Features) 
Artificial Neural 
Network classifier 
Applying the Wavelet 
Packet Decomposition 
Energy and Shannon 
Entropy Extraction 
PCA-Based Feature 
Reduction 
International Journal of Computer Applications (0975 – 8887)  
Volume 62– No.7, January 2013 
3 
patterns once the weights are adjusted or tuned via some kind 
of learning process [24]. 
3. EXPERIMENTS AND RESULTS 
The database was created by specialists from the Belarusian 
Republican Center of Speech, Voice and Hearing Pathologies. 
We have selected 75 pathological speeches and 55 healthy 
speeches randomly which are related to sustained vowel “a”. 
All the records are wave files in PCM format. The whole 
scheme of our proposed method is illustrated in Fig. 1. We 
have adopted a 10 fold cross-validation scheme to assess the 
generalization capabilities of the system in our experiments.  
It is necessary to know how many neurons in the hidden layer 
are required to achieve the optimal results. So, in the first 
experiment the numbers of neurons in the hidden layer are 
investigated and the results are shown in Fig. 2. As it is 
obvious, using 5 neurons in the hidden layer lead to the 
optimal case which is 84.62% of accuracy.  
 
Fig 2: The Classification Results based on the numbers of 
Neurons. 
 
 
Fig 3: The Classification Results based on the numbers of 
Features. 
 
Finally, according to the result of the first experiment, an 
ANN with 5 neurons is created for using in the second 
experiment. The goal of the second experiment is to reduce 
the initial feature vector’s length so that the accuracy of the 
classification phase increases. For this purpose, the 
conventional PCA method is used and the results are shown in 
Fig. 3. As it is obvious, using the 36 features lead to the best 
result which is 91.54% of accuracy. The 36 selected features, 
by the means of PCA method, are shown in the table 1. 
Table 1. The selected features for the construction of 
feature vector. 
Feature Reduction 
Method 
The selected features Accuracy 
PCA 
(feature vector 
length=36) 
The 1-8 and 10-13 coefficients of 
MFCC. 
Energy at the 9-10, 17, 19, 22-23, 
32, 35-38 and 46th nodes of WP 
Tree. 
Entropy at the 1, 4, 9, 16-17, 19, 
22, 33-34, 37, 39 and 45th nodes 
of WP Tree. 
91.54% 
None 
(feature vector 
length=139) 
All the 63 entropy & 63 energy & 
13 MFCC coefficients. 
84.62% 
 
4. CONCLUSION AND FUTURE 
WORKS 
Acoustic analysis is a proper method in vocal fold pathology 
diagnosis so that it can complement and in some cases replace 
the other invasive, based on direct vocal fold observation, 
methods. In this article, an ANN-Based method for vocal fold 
pathology diagnosis is proposed so that in the proposed 
scheme, Mel-Frequency-Cepstral-Coefficients along with the 
wavelet packet decomposition are used for feature extraction 
phase. Also PCA method for the feature reduction phase is 
used. And finally the ANN is used for the classification phase. 
Two experiments are designed to investigate the optimal case 
for the numbers of neurons in the hidden layer of the ANN 
and also the optimal case for the feature vector length as the 
input of ANN.  
Also it may be possible to try to build a complete multiclass 
classification system so that detection of different type of 
pathological speech will be possible. For this propose, we 
suppose that further research for more sophisticated feature 
extraction phase. 
5. ACKNOWLEDGMENTS 
This work was supported by the speech laboratory of the 
United Institute of Informatics Problems of NASB. The 
authors wish to thank the Belarusian Republican Center of 
Speech, Voice and Hearing Pathologies by its support in the 
speech database. 
6. REFERENCES 
[1] Alonso, J.B., Leon, J.D., Alonso, I. and Ferrer, M.A. 
2001. Automatic Detection of Pathologies in the Voice 
by HOS Based Parameters. EURASIP Journal on 
Applied Signal Processing, 2001:4, 275-284. 
[2] Ceballos, L.G., Hansen, J. and Kaiser, J. 2005. A Non-
Linear Based Speech Feature Analysis Method with 
Application to Vocal Fold Pathology Assessment. IEEE 
Trans. Biomedical Engineering, 45(3): 300-313. 
[3] Ceballos, L.G., Hansen, J. and Kaiser, J. 1996. Vocal 
Fold Pathology Assessment Using AM Autocorrelation 
Analysis of the Teager Energy Operator. ICSLP-1996 
Proc., 757-760. 
International Journal of Computer Applications (0975 – 8887)  
Volume 62– No.7, January 2013 
4 
[4] Adnene, C. and Lamia, B. 2003. Analysis of Pathological 
Voices by Speech Processing. Signal Processing and Its 
Applications. 2003 Proc., 1(1): 365-367. 
[5] Manfredi, C. 2000. Adaptive Noise Energy Estimation in 
Pathological Speech Signals. IEEE Trans. Biomedical 
Engineering, 47(11):1538-1543. 
[6] Llorente, J.I.G. and Vilda, P.G. 2004. Automatic 
Detection of Voice Impairments by Means of Short-
Term Cepstral Parameters and Neural Network Based 
Detectors. IEEE Trans. Biomedical Engineering, 
51(2):380-384. 
[7] Rosa, M.D.O, Pereira, J.C. and Grellet M. 2000. 
Adaptive Estimation of Residue Signal for Voice 
Pathology Diagnosis. IEEE Trans. Biomedical 
Engineering, 47(1): 96-104. 
[8] Mallat, S.G. 1989. A Theory for Multi-resolution Signal 
Decomposition: the Wavelet Representation. IEEE Trans 
Pattern Analysis and Machine Intelligence, 11(7):674-
693. 
[9] Majidnezhad, V. and Kheidorov, I. A Novel Method for 
Feature Extraction in Vocal Fold Pathology Diagnosis. In 
the Proceeding of the 3rd International Conference on 
Wireless Mobile Communication and Healthcare. 2012, 
in press. 
[10] Wallen, E.J. and Hansen, J.H. 1996. A Screening Test for 
Speech Pathology Assessment Using Objective Quality 
Measures. ICSLP 96. Proc., 2: 776-779. 
[11] Chen, W., Peng, C., Zhu, X., Wan, B. and Wei, D. 2007. 
SVM-based identification of pathological voices. 
Proceedings of the 29th Annual International Conference 
of the IEEE EMBS. 
[12] Go´mez, P., Dı´az, F., A´lvarez, A., Murphy, K., Lazaro, 
C., Martinez, R. and Rodellar, V. 2005. Principal 
component analysis of spectral perturbation parameters 
for voice pathology detection. Proceedings of the 18th 
IEEE Symposium on Computer-Based Medical Systems, 
pp.41–46.  
[13] Michaelis, D., Frohlich, M. and Strube, H.W. 1998. 
Selection and combination of acoustic features for the 
description of pathologic voices. Journal of the 
Acoustical Society of America, 103, 1628–1639. 
[14] Marinaki, M., Kotropoulos, C., Pitas, I. and Maglaveras, 
N. 2004. Automatic detection of vocal fold paralysis and 
edema. Proceedings of Eighth International Conference 
on Spoken Language Processing—ICSLP. 
[15] Majidnezhad, V. and Kheidorov, I. 2012. A HMM-Based 
Method for Vocal Fold Pathology Diagnosis. IJCSI 
International Journal of Computer Science Issues, Vol. 9, 
Issue 6, No 2, 135-138.  
[16] Li, T., Oginara, M. and Li, Q. 2003. A comparative study 
on content based music genre classification. In the Proc. 
Of the 26th annual int.ACM SIGIR conf. on Research 
and development in information retrieval, pp. 282–289. 
[17] Tzanetakis, G. and Cook, P. 2002. Musical genre 
classification of audio signals. IEEE Trans. on Speech 
and Audio Processing, vol. 10, no. 5, pp. 293-302. 
[18] Kukharchik, P., Martynov, D., Kheidorov, I. and Kotov, 
O. 2007. Vocal fold pathology detection using modified 
wavelet-like features and support vector machines. 15th 
European Signal Processing Conference (EUSIPCO 
2007), 2214-2218. 
[19] Cavalcanti, N., Silva, S., Bresolin, A., Bezerra, H. and 
Guerreiro, A. 2010. Comparative Analysis between 
Wavelets for the Identification of Pathological Voices. 
Proceedings of the 15th Iberoamerican congress 
conference on Progress in pattern recognition, image 
analysis, computer vision, and applications. 
[20] Herisa, H.K., Aghazadeh, B.S. and Bahrami, M.N. 2009. 
Optimal feature selection for the assessment of vocal fold 
disorders. Computers in Biology and Medicine, 39, 860-
868. 
[21] Fonseca, E., Guido, R.C., Pereira, J.C., Scalassarsa, P.R., 
Maciel, C.D. and Pereira, J.C.  2007. Wavelet time 
frequency analysis and least squares support vector 
machines for identification of voice disorders. 
Computers in Biology and Medicine, 37, 571–578. 
[22] Guido, R.C., Pereira, J.C., Fonseca, E., Sanchez, F.L. and 
Vierira, L.S. 2005. Trying different wavelets on the 
search for voice disorders sorting. Proceedings of the 
37th IEEE International Southeastern Symposium on 
System Theory, pp. 495–499. 
[23] Umapathy, K. and Krishnan, S. 2005. Feature analysis of 
pathological speech signals using local discriminant 
bases technique. Medical and Biological Engineering and 
Computing, 43, 457–464. 
[24] Lee, K.Y., Cha, Y.T. and Park, J.H. 1992. SHORT-
TERM LOAD FORECASTING USING AN 
ARTIFICIAL NEURAL NETWORK. Transactions on 
Power Systems, Vol. 7, No. 1, pp. 124-132. 
 
"
41,"Data Augmentation for Skin Lesion Analysis
Fa´bio Perez1, Cristina Vasconcelos2, Sandra Avila3, and Eduardo Valle1
1RECOD Lab, DCA, FEEC, University of Campinas (Unicamp), Brazil
2Computer Science Department, IC, Federal Fluminense University (UFF), Brazil
3RECOD Lab, IC, University of Campinas (Unicamp), Brazil
Abstract. Deep learning models show remarkable results in automated
skin lesion analysis. However, these models demand considerable amounts
of data, while the availability of annotated skin lesion images is of-
ten limited. Data augmentation can expand the training dataset by
transforming input images. In this work, we investigate the impact of
13 data augmentation scenarios for melanoma classification trained on
three CNNs (Inception-v4, ResNet, and DenseNet). Scenarios include
traditional color and geometric transforms, and more unusual augmen-
tations such as elastic transforms, random erasing and a novel augmen-
tation that mixes different lesions. We also explore the use of data aug-
mentation at test-time and the impact of data augmentation on various
dataset sizes. Our results confirm the importance of data augmentation
in both training and testing and show that it can lead to more perfor-
mance gains than obtaining new images. The best scenario results in an
AUC of 0.882 for melanoma classification without using external data,
outperforming the top-ranked submission (0.874) for the ISIC Challenge
2017, which was trained with additional data.
Keywords: Skin lesion analysis · Data augmentation · Deep learning
1 Introduction
Deep learning has achieved impressive results in computer vision tasks, including
skin lesion analysis [4]. However, deep learning models are data-hungry, and
collecting and annotating skin lesion images can be challenging.
In image classification tasks, knowledge transfer and data augmentation are
regularly employed for small datasets. Knowledge transfer usually takes place
by initially training a Convolutional Neural Network (CNN) in a large source
dataset (e.g., ImageNet) and using its weights as a starting point for training
in the smaller target dataset [10]. Data augmentation goal is to add new data
points to the input space by modifying training images while preserving semantic
information and target labels. Thus, it is used to reduce overfitting.
In this work, we: (i) investigate the impact of applying diverse data augmen-
tation techniques to three different CNN architectures (namely Inception-v4 [13],
ResNet [5], and DenseNet [6]); (ii) investigate the impact of data augmentation
on different dataset sizes; and (iii) evaluate the use of different data augmenta-
tion methods during test-time, aiming to reduce generalization error. We con-
ar
X
iv
:1
80
9.
01
44
2v
1 
 [c
s.C
V]
  5
 Se
p 2
01
8
2 F. Perez et al.
ducted the experiments on the ISIC Challenge 2017 dataset [3] for melanoma
classification task.
2 Related Work
Data augmentation is broadly used in CNN architectures, such as AlexNet [8],
Inception [7,13,14], ResNet [5], and DenseNet [6]. These architectures are trained
on the ImageNet dataset , which contains millions of annotated images. Some
examples of data augmentation techniques are color modifications and geometric
transforms (rotation, scaling, random cropping).
Models can also benefit from data augmentation on test-time. Krizhevsky et
al. [8] average the predictions on 10 patches (cropped from the center plus the
four corners and then flipped) extracted from each test image. Szegedy et al. [14]
report gains with a method that generates 144 patches by cropping images at
different resolutions, when compared with the 10-crop method. These methods
are commonly used in competitions to increase final performance but can be
expensive for production.
Data augmentation is also extensively employed in skin lesion classification, a
task that has much less available training data. Data augmentation is ubiquitous
among top-ranked submissions in the ISIC Challenge 2017 [1, 9, 11].
Some works specifically explore data augmentation for skin lesion analy-
sis [12, 15, 16]. Vasconcelos and Vasconcelos [16] report gains in performance by
using data augmentation with geometric transforms (rotations by multiples of
90 degrees; flips; lesion-preserving crops), PCA-based color augmentation, and
specialist warping that preserves lesions symmetries and anti-symmetries. Valle
et al. [15] highlight the importance of using data augmentation for both train-
ing and testing. They averaged the predictions for 50 augmented test samples.
Pham et al. [12] compare the effects of data augmentation on classifiers (SVM,
neural networks, and random forest) trained with features extracted with a pre-
trained Inception-v4. Their results indicate that using more samples in test data
augmentation (100 vs. 50) increases the model’s performance.
In this work, we further investigate the use of data augmentation for skin
lesion analysis, by comparing: test techniques (testing on a single image; test data
augmentation; and test cropping, commonly employed in CNN architectures for
image classification); 13 different data augmentation scenarios, including a novel
augmentation; and the effects of data augmentation on different dataset sizes.
3 Methodology
3.1 CNN Architectures
We evaluated every experiment on three very deep CNNs that are widely used
in computer vision problems: Inception-v4 [13], ResNet-152 [5], and DenseNet-
161 [6]. We chose these networks as they achieve increased depth with different
design choices and represent the state of the art in image classification.
Data Augmentation for Skin Lesion Analysis 3
The Inception-v4 [13] architecture has modules that concatenate feature
maps from parallel convolutional blocks, leading to increased width and depth.
Residual Networks (ResNets) [5] use shortcut connections between layers, al-
lowing even deeper networks. Densely Connected Networks (DenseNets) [6] con-
catenate the output of each layer to all subsequent layers inside a dense block,
increasing the parameter efficiency and reducing overfitting.
Since we used the same optimization hyperparameters for the three networks,
we do not intend to compare the numeric values alone, but rather compare big-
picture results and trends.
3.2 Data Augmentation Techniques
We evaluated 13 data augmentation scenarios, comprising different image pro-
cessing techniques, and some combinations of them. Table 1 describes the im-
plementation details for each scenario. Fig. 1 shows examples of all scenarios.
Fig. 1: Examples of augmentation scenarios, described in Table 1.
3.3 Training and Evaluation
We trained each network with Stochastic Gradient Descent (SGD) with a mo-
mentum factor 0.9, batch size of 32, starting learning rate 1e-3, reduced to 1e-4
after the 10th epoch. The training data was shuffled before each epoch. The
networks were initialized with weights trained on the ImageNet dataset, and
fine-tuned with the ISIC Challenge 2017 train dataset (2000 images) [3]. The
experiments were implemented with PyTorch (pytorch.org). Augmentations
were implemented with torchvision and imgaug (github.com/aleju/imgaug).
All images were resized offline to a maximum width or height of 1024 pixels
to avoid expensive resizing during training. On training, images were resized to
the default input sizes for each network (224 × 224 for DenseNet and ResNet;
299 × 299 for Inception-v4), although larger sizes were possible due to global
average pooling. Images were normalized (subtract from the mean and divide by
the standard deviation) based on the ImageNet dataset, in which the networks
were pretrained. Augmentations were randomly applied online during training.
4 F. Perez et al.
Table 1: Augmentation scenarios. Scenarios J to M represent augmentations
compositions applied in the presented order.
ID Name Description
A No Augmentation No data augmentation. Only preprocess images, as de-
scribed in Section 3.3.
B Saturation, Contrast,
and Brightness
Modify saturation, contrast, and brightness by random fac-
tors sampled from an uniform distribution of [0.7, 1.3], sim-
ulating changes in color due to camera settings and lesion
characteristics.
C Saturation, Contrast,
Brightness, and Hue
As described in B, but also shift the hue by a value sampled
from an uniform distribution of [−0.1, 0.1].
D Affine Rotate the image by up to 90◦, shear by up to 20◦, and scale
the area by [0.8, 1.2]. New pixels are filled symmetrically at
edges. This can reproduce camera distortions and create new
lesion shapes.
E Flips Randomly flip the images horizontally and/or vertically.
F Random Crops Randomly crop the original image. The crop has 0.4−1.0 of
the original area, and 3/4− 4/3 of the original aspect ratio.
G Random Erasing Fill part of the image (area up to 30% of the original image)
with random noise. The transformation is applied with a
probability of 0.5. Implemented as described in [17]. The
network may benefit from occlusion by learning to look for
different lesion attributes.
H Elastic Warp images with Thin Plate Splines (TPS). The warp is
generated by defining the origins as an evenly-spaced 4× 4
grid of points, and destinations as random points around
the origins (by up to 10% of the image width on each direc-
tion). This can produce new lesion shapes while maintaining
medical attributes.
I Lesion Mix Mix two lesions, by inserting part of a foreground lesion
(cut by its segmentation mask) into a background lesion.
We apply Gaussian blur to the foreground lesion to avoid
sharp edges, and equalize its color histogram with respect
to the segmented background lesion. The resulting image is
labeled as melanoma only if one of the two original lesions
was labeled as melanoma. This can simulate clinical condi-
tions with two lesions occur at the same location. We did
not apply this transform at test-time.
J Basic Set F → D → E → C.
K Basic Set + Erasing F → G → D → E → C.
L Basic Set + Elastic F → D → H → E → C.
M Basic Set + Mix I → F → D → E → C.
We applied early stopping to interrupt the training, monitoring the AUC
value for the ISIC Challenge 2017 official validation dataset (150 images) for
each epoch. The AUC value was calculated by averaging the predictions for
16 randomly augmented copies of each validation image, by applying the same
Data Augmentation for Skin Lesion Analysis 5
transforms used during training. The early stopping monitor interrupted the
training when the validation AUC did not improve after 8 epochs. The final
test AUC was calculated on the ISIC Challenge 2017 official test dataset (600
images) in three different ways: i) inputting the original test images to the net-
work; ii) averaging the predictions for 64 randomly augmented copies of each
test image; iii) averaging the predictions for 144 patches produced by cropping
each test image as described in [14]. The weights used for testing were selected
from the best AUC in the validation dataset. The validation-time and test-time
augmentations followed the same transforms as the training.
For every setup, we run 6 separate trainings to reduce the effects of random-
ness. We used Sacred (github.com/IDSIA/sacred) to organize all experiments.
To guarantee reproducibility, we provide the documented source code used
in the experiments (github.com/fabioperez/skin-data-augmentation).
4 Results and Discussion
4.1 Augmentation on Training and Testing
In this section, we discuss the results of train and test data augmentation for
the proposed scenarios. Fig. 2 summarizes the results.
0.70
0.75
0.80
0.85
0.90
In
ce
pt
io
n­
v4
0.70
0.75
0.80
0.85
0.90
R
es
N
et
­1
52
A
No Aug.
B
Color
C
Color
(w/ Hue)
D
Affine
E
Flips
F
Rand.
Crops
G
Rand.
Erasing
H
Elastic
I
Lesion
Mix
J
Basic
Set
K
Basic
+
Erasing
L
Basic
+
Elastic
M
Basic
+
Mixed
0.70
0.75
0.80
0.85
0.90
D
en
se
N
et
­1
61
Test on Original Image Test w/ Data Augmentation Test w/ 144­crops
A
U
C
Fig. 2: Mean AUC values for augmentation scenarios. Each color and marker
represent a prediction method: • original image; N test-time data augmentation
(64 images);  144 crops. Error bars represent the standard deviation for 6 runs.
Values reported on ISIC Challenge 2017 test set.
6 F. Perez et al.
Scenario C (saturation, contrast, brightness, and hue) resulted in better AUC
than scenario B (saturation, contrast, brightness) for all three networks. How-
ever, both color transforms performed worse than scenario A (no augmentation)
with 144 crops on ResNet. Geometric transforms — affine (B), random crops (F),
and elastic transformations (H) — had more consistent improvements among all
three networks.
Random erasing (G) shows little improvements for Inception and DenseNet,
but produce worse results than scenario A (no augmentation) with ResNet. Us-
ing 144 crops was better than test data augmentation, probably due to the
destructive behavior of the method. When combined with other transforma-
tions (scenario K), random erasing reduced the test AUC in comparison with
scenario J (basic set combining traditional augmentations).
Scenario H (elastic) shows promising results, but when applied with other
common augmentation techniques (L) also performed worse than scenario J.
This may occur due to deformations produced by the combined augmentation.
Lesion mix (I and M) had worse performances when compared to other aug-
mentations, indicating that the generated images were not useful. We presume
that the produced images were not able to preserve relevant features from both
source lesions.
Scenario J (basic set) yields the best AUC values for all three networks: 0.854
for Inception-v4, 0.882 for ResNet, and 0.879 for DenseNet. The top-ranked
submissions for melanoma classification scored 0.874 [11], 0.870 [1], 0.868 [9].
They used, respectively, 9640, 9600, and 3444 images for training. Our method
achieved a higher AUC with ResNet and DenseNet without additional data.
Scenario J also has the highest AUC for the validation set in all three networks.
For every scenario, averaging augmented samples or 144 crops resulted in bet-
ter performance than predicting on the original image alone. Even when no data
augmentation was employed during training, 144 crops significantly increased
the AUC, indicating that the model can benefit from different representations
of the input image.
For ResNet and DenseNet, 144 crops has similar results to using data aug-
mentation on test-time. Considering that we used 64 augmented samples vs 144
crops, test data augmentation can lead to faster inference.
Particularly, Inception-v4 has a worse performance with 144 crops than with
test data augmentation in most scenarios. This may indicate that Inception-v4
suffers from overfitting, considering that data augmentation produced similar
patterns on both training and testing.
4.2 Impact of Data Augmentation on Different Dataset Sizes
We trained each network on random subsets of 1500, 1000, 500, 250, and 125
images of the original data to analyze the effects of having limited training data.
We generated a random subset for each one of the 6 runs. Fig. 3 summarizes
the results.
Applying data augmentation (scenario J) during both training and testing
noticeably improved performance for datasets with 500 or more images. Data
Data Augmentation for Skin Lesion Analysis 7
125 500 1000 1500 2000
Inception-v4
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
AU
C
125 500 1000 1500 2000
ResNet-152
125 500 1000 1500 2000
DenseNet-161
No augmentation
Aug. (train)
Aug. (train + test)
Dataset Length
Fig. 3: Mean AUC values for different training dataset sizes, randomly sampled
from the ISIC Challenge 2017 training dataset. Colors and markers represent the
use of data augmentation:  no data augmentation; N train data augmentation
(scenario J); • train and test data augmentation (scenario J, averaging each test
image on 64 augmented samples). Bands represent the standard deviation for
6 runs. Values reported on ISIC Challenge 2017 test set.
augmentation for training only worsened the results for very small data sizes
(< 500 images) and led to little or no improvement for other sizes, showing the
importance of applying data augmentation during test-time.
The impact of data augmentation on Inception-v4 was more perceptible than
on other networks, which may be caused by the regularizing properties of ResNet
and DenseNet architectures. Training Inception-v4 with 500 images and data
augmentation resulted in better performance than training with 1000, 1500 or
2000 images without augmentation. ResNet and DenseNet achieved a higher
AUC with 1000 images and data augmentation than with 1500 and 2000 images
without augmentation. This indicates that, in some cases, using data augmen-
tation can be more effective than adding new training data. Nevertheless, em-
ploying data augmentation does not reduce the importance of adding new data,
giving that the network can benefit from both.
5 Conclusion
The results highlight the positive impact of using data augmentation for training
melanoma classification models. Moreover, models can also benefit from test data
augmentation.
The best augmentation scenario (J), which combines geometric and color
transformations, surpasses the top-ranked AUC values for the ISIC Challenge
2017 without any additional data. Fine-tuning hyperparameters and model en-
sembling may result in additional performance gains.
Lesion mix augmentation (scenarios I and M) have inferior results when com-
pared with other scenarios. We implemented this augmentation through hand-
crafted image processing techniques, which may not be appropriate for produc-
ing reliable images. More advanced approaches, such as Generative Adversarial
Networks or other generative architectures [2], might lead to better results.
8 F. Perez et al.
Acknowledgments
We gratefully acknowledge NVIDIA Corporation for the donation of GPUs and
Microsoft Azure for the GPU-powered cloud platform used in this work. C. Vas-
concelos and E. Valle are partially funded by Google Research LATAM 2017. E.
Valle is also partially funded by CNPq PQ-2 grant (311905/2017-0) and Uni-
versal grant (424958/2016-3). RECOD Lab. is partially supported by diverse
projects and grants from FAPESP, CNPq, and CAPES.
References
1. Bi, L., Kim, J., Ahn, E., Feng, D.: Automatic skin lesion analysis using large-scale
dermoscopy images and deep residual networks. arXiv: 1703.04197 (2017)
2. Bissoto, A., Perez, F., Valle, E., Avila, S.: Skin lesion synthesis with generative
adversarial networks. In: ISIC Skin Image Analysis Workshop (2018)
3. Codella, N.C.F., Gutman, D., Celebi, M.E., Helba, B., Marchetti, M.A., Dusza,
S.W., et al.: Skin Lesion Analysis Toward Melanoma Detection: A Challenge at
the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the
International Skin Imaging Collaboration (ISIC). arXiv: 1710.05006 (2017)
4. Fornaciali, M., Carvalho, M., Bittencourt, F.V., Avila, S., Valle, E.: Towards
automated melanoma screening: Proper computer vision & reliable results.
arXiv:1604.04024 (2016)
5. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: IEEE CVPR. pp. 770–778 (2016)
6. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected
convolutional networks. In: IEEE CVPR (2017)
7. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In: ICML. pp. 448–456 (2015)
8. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-
volutional neural networks. In: NIPS. pp. 1106–1114 (2012)
9. Matsunaga, K., Hamada, A., Minagawa, A., Koga, H.: Image classification of
melanoma, nevus and seborrheic keratosis by deep neural network ensemble. arXiv:
1703.03108 (2017)
10. Menegola, A., Fornaciali, M., Pires, R., Bittencourt, F.V., Avila, S., Valle, E.:
Knowledge transfer for melanoma screening with deep learning. In: ISBI (2017)
11. Menegola, A., Tavares, J., Fornaciali, M., Li, L.T., Avila, S., Valle, E.: RECOD
titans at ISIC challenge 2017. arXiv: 1703.04819 (2017)
12. Pham, T.C., Luong, C.M., Visani, M., Hoang, V.D.: Deep CNN and data augmen-
tation for skin lesion classification. In: ACIIDS. pp. 573–582 (2018)
13. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet
and the impact of residual connections on learning. In: AAAI. vol. 4, p. 12 (2017)
14. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan, D.,
et al.: Going deeper with convolutions. In: IEEE CVPR. pp. 1–9 (2015)
15. Valle, E., Fornaciali, M., Menegola, A., Tavares, J., Bittencourt, F.V., Li, L.T.,
Avila, S.: Data, depth, and design: Learning reliable models for melanoma screen-
ing. arXiv: 1711.00441 (2017)
16. Vasconcelos, C.N., Vasconcelos, B.N.: Experiments using deep learning for der-
moscopy image analysis. Pattern Recognition Letters (2017)
17. Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmenta-
tion. arXiv: 1708.04896 (2017)
"
42,"Parameterized Synthetic Image Data Set for Fisheye Lens  
 
Zhen Chen 
Institute of Product and Process Innovation  
Leuphana University Lüneburg  
Lüneburg, Germany  
zhen.chen@leuphana.de 
Anthimos Georgiadis  
Institute of Product and Process Innovation  
Leuphana University Lüneburg 
 Lüneburg, Germany 
georgiadis@uni.leuphana.de
 
 
Abstract—Based on different projection geometry, a fisheye 
image can be presented as a parameterized non-rectilinear 
image. Deep neural networks(DNN) is one of the solutions to 
extract parameters for fisheye image feature description. 
However, a large number of images are required for training a 
reasonable prediction model for DNN. In this paper, we propose 
to extend the scale of the training dataset using parameterized 
synthetic images. It effectively boosts the diversity of images and 
avoids the data scale limitation. To simulate different viewing 
angles and distances, we adopt controllable parameterized 
projection processes on transformation. The reliability of the 
proposed method is proved by testing images captured by our 
fisheye camera. The synthetic dataset is the first dataset that is 
able to extend to a big scale labeled fisheye image dataset. It is 
accessible via: http://www2.leuphana.de/misl/fisheye-data-set/. 
Keywords-Fisheye lens; Synthetic data set; Neural network; 
Image processing;  
I.  INTRODUCTION 
Machine vision technology become a powerful tool on the 
field of surveillance, automated inspection, robot control and 
navigation[1]. In the task of robot navigation, a navigation 
system builds up a map and localizes the position by 
continually updating the robust landmarks from the vision 
system. According to comparing landmarks stored in the 
memory, the navigation system recalls scenarios that passed 
by and then determines the location of the robot on an excited 
map. The mentioned map building and position searching 
procedure are called simultaneous localization and mapping 
(SLAM) in the machine vision community[2]. 
The fisheye camera, benefited from the very large field of 
view (FOV), provides rich vision information in SLAM. The 
lens from the camera is capable of achieving over 180 degrees’ 
visual angles instead of the general 40 to 60 degrees. 
Therefore, the fisheye lens achieves more and more attention 
in the application of visual navigation systems. D. Caruso, et 
al. apply a fisheye cameras to a large-scale visual SLAM and 
develop the monocular SLAM algorithm[3]. M. Bertozzi, et 
al. develop a system for pedestrian and vehicle detection and 
tracking by means of fisheye images[4].  
Deep convolution neural networks (DCNN) is another 
popular research field in the machine vision community. 
Benefits from enormous computational capabilities of GPU, 
the DCNN is trained deeper and deeper. The recent research 
shows that deeper convolution neural networks are able to 
describe more complex features hidden behind the image[5]. 
Some research results on the application of the DCNN are 
available. D. Zhe, et al. use the DCNN to evaluate the quality 
of images with the view of the human on the classes of good 
and bad[6]. Y. Kawano, et al. improve food recognition 
accuracy by using the feature abstracted from the DCNN[7].   
The DCNN is a potential method to explore the hidden 
meaning behind the fisheye image. The main challenge on 
machine learning engineering is how to train a generalize 
neural network model without overfitting. Some skills are 
helpful in preventing overfitting, such as parameter 
regularization and dropout. However, the most effective way 
for solving such problem is to enlarge the size of the training 
dataset. Unfortunately, collecting and labeling fisheye images 
are extremely time-consuming. So it is not feasible to train a 
neural network detector on novel categories. In this paper, we 
propose to bypass the expensive collecting procedure and 
build a synthetic training image dataset with an existing 
perspective image dataset. For the parameterized synthetic 
image dataset, the optical calibration is not necessary, since 
the camera’s parameters are pre-defined on the process of the 
synthesis. With the synthesis fisheye dataset, verification and 
evaluation of a new algorithm can be completed with a virtual 
optical environment. Under the following sections, we 
analyze the geometrical model of the fisheye lens, and 
introduce the algorithm and the procedures for generating 
synthetic fisheye images. The validation the proposed method 
is tested by new neural network detector. The image dataset is 
accessible via: http://www2.leuphana.de/misl/fisheye-data-
set/ 
II. THE STRUCTURE OF THE FISHEYE LENS 
The fisheye camera system is a typical non-linear system, 
in which the spatial resolution decreases from the image’s 
center to the edge. Considering a series of factors, like size, 
focus, iris and geometry, different camera manufacturers have 
different fisheye system designs, especially fisheye lenses [8]. 
A typical fisheye lens is consisting of several layer of highly-
integrated optical glasses, such as convex lens, concave lens 
and filter lens.  
When complex glasses combination is applied for a unique 
light route design, it is difficult to describe a fisheye geometry 
with a single model. Thus, several standard imaging models 
are raised based on the typical fisheye projection model. As 
shown in Fig. 1, the ray’s incidence angle φ  and the 
corresponded reflection angle β are different because of the 
light refraction. With different expression between the image 
radius r  and the incidence angle φ , the fisheye projection 
models are classified as[9]: 
Figure 1.  Fisheye projection geometry. 
Equidistant projection: 
r = h ∙  φ 
Stereographic projection: 
r = 2h ∙  tan
φ
2
 
Equisolid-angle projection: 
r = 2h ∙  sin
φ
2
 
Orthographic projection: 
r = c ∙  sinφ 
To project a real object point into a fisheye image, the 
camera coordinate system (x, y, z) and the image coordinate 
system (𝑥′ , 𝑦′) is defined. The projection model for the above 
projection could be expressed as follows[10]: 
Equidistant projection: 
𝑥′ = 𝑐 ∙
𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
√(
𝑦
𝑥
)
2
+ 1
+ 𝑥0
′ + Δ𝑥′ 
𝑦′ = 𝑐 ∙
𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
√(
𝑦
𝑥
)
2
+ 1
+ 𝑦0
′ + Δ𝑦′ 
Stereographic projection: 
𝑥′ = 𝑐 ∙
tan (
1
2 ∙ 𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑥0
′ + Δ𝑥′ 
𝑦′ = 𝑐 ∙
tan (
1
2 ∙ 𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑦0
′ + Δ𝑦′ 
Equisolid-angle projection: 
𝑥′ = 𝑐 ∙
sin (
1
2 ∙ 𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑥0
′ + Δ𝑥′ 
𝑦′ = 𝑐 ∙
sin (
1
2 ∙ 𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑦0
′ + Δ𝑦′ 
Orthographic projection: 
𝑥′ = 𝑐 ∙
sin (𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑥0
′ + Δ𝑥′ 
𝑦′ = 𝑐 ∙
sin (𝑎𝑟𝑐𝑡𝑎𝑛
√𝑥2 + 𝑦2
𝑧
)
√(
𝑦
𝑥
)
2
+ 1
+ 𝑦0
′ + Δ𝑦′ 
In which (𝑥0
′ , 𝑦0
′ ) are principle point. The correction 
parameters ∆𝑥, ∆𝑦 are defined as: 
∆𝑥 = 𝑥′ ∙ (𝐴1𝑐 + 𝐴2𝑟
′4 + 𝐴3𝑟
′6) + 𝐵1
∙ (𝑟′2 + 2𝑥′2) + 2𝐵1𝑥
′𝑦′
+ 𝐶1 ∙ 𝑥
′ + 𝐶2 ∙ 𝑦
′

∆𝑦 = 𝑦 ′ ∙ (𝐴1𝑟
′2 + 𝐴2𝑟
′4 + 𝐴3𝑟
′6) + 2𝐵1𝑥
′𝑦′
+ 𝐵2(𝑟
′2 + 2𝑦′2)

In which (𝐴1, 𝐴2, 𝐴3)  are radial distortion parameters, 
(𝐵1, 𝐵2)  are decentering distortion parameters and (𝐶1, 𝐶2)  
are the horizontal scale factor and the shear factor. 
III. SYNTHETIC FISHEYE IMAGE 
Perspective images captured by the traditional camera are 
comparatively easier to be collected. Even though, it takes 
much effort to manually label each image, especially when the 
number of images is large. For a new algorithm verification, 
labeling images occupies a large amount of time. In this paper, 
we propose to synthesize a fisheye image dataset by 
transforming a well-known labeled perspective image dataset. 
By using the proposed method, we are able to prepare a useful 
training dataset quickly. The developed synthetic fisheye 
image dataset is accessible online, to my knowledge, which is 
the first public synthetic fisheye image dataset by 
transforming a known perspective image dataset.    
There are several large-scale labeled image datasets on the 
natural scene. The popular datasets in the machine vision 
community are MNIST, CIFAR10 / CIFAR100, Imagenet and 
so on, which are often used for training and testing a neural 
network (NN). The MNIST dataset is a size-normalized hand 
written digits, which is often used by beginners for 
practicing[11]. CIFAR10 and CIFAR100 dataset, collected by 
K. Alex, N. Vinod and H. Geoffrey, have maximum 80 
million color images in 100 classes. The resolution of the 
images is unified to 32 × 32 for each class[12]. In the work 
from this paper, we choose Imagenet Large Scale Visual 
Recognition Challenge (ILSVRC) 2012[13] training dataset 
as the raw image dataset, which provides 1.2 million labeled 
 
images in 1000 categories. One reason we choose this dataset 
is that high-resolution raw images maintain an amount of 
detail information during the synthesis. Another reason is that 
the scale of the synthetic dataset is flexible because it can be 
extended to 1000 classes. To confirm the reliability of the 
proposed method, we select 12 classes’ images from indoor 
objects and transform them into fisheye images. These 12 
classes consist of ballpoint pen, cellular telephone, desktop 
computer, espresso maker, printer, projector, shopping cart, 
stone wall, television, wall clock, wardrobe and water bottle. 
  For the synthesis, we expect to create the effects like 
observing from different distances and angles. Based on this 
idea, an original image from the ILSVRC can be transformed 
into dozens of fisheye images, especially by adding center bias 
into the transformation. Besides, we hope to keep the same 
visible regions between the raw image and the synthetic 
fisheye image. To realize that, we unify the image’s size 
before the transformation: an images’ longer side is first 
scaled down to 512 pixels and then extend the new images’ 
shorter side to 512 pixels by filling black pixels.  
Figure 2.  Representative samples of the synthetic fisheye images 
Equidistant projection is applied on the transformation in 
this paper, as shown on equation (5) and (6). To obtain the 
synthetic effects for different viewing angles and distances, 
the center offset for the synthetic images is set in which the 
distance ranges from 0 to 1/6 of the image’s width and the 
degree ranges from 26 to 35 degrees separately. 
Representative samples of the synthetic fisheye images can be 
seen in the Fig. 2. 
IV. EXPERIMENTAL TEST 
A. The structure of the neural network 
Convolutional Neural Networks (CNN) is composed of a 
succession of convolutional layers and fully connected layers. 
Each layer receives feedback only from the previously 
connected layer. Therefore, it is time-consuming to train an 
entire convolutional network from scratch on a large-scale 
dataset. Fine-tuning a pre-trained NN is a way to accelerate 
the parameter’s update for a special task[14]. On the fine-
tuning, the preserved parameters from a pre-trained NN would 
be transferred to a new NN.  
The neural network architecture we use is Alexnet, which 
wins the ImageNet Large Scale Visual Recognition Challenge 
(ILSVRC) in 2012[15]. The Alexnet consists of five 
convolutional layers, three fully-connected layers and a 1000 
classes softmax layer. The acceleration convergence on the 
convolutional and fully connected layer is applied by the 
ReLU non-linearity, instead of the sigmoid and the tahn 
function. The pooling layers are inserted at the first, the 
second and the fifth layer. In the pooling layers, an 
overlapping pooling method is utilized in order to eliminate 
overfitting problem. The architecture of Alexnet is shown in 
Fig. 3. 
Figure 3.  The neural network architecture for Alexnet 
In this paper, we test the synthetic images in 12 classes, 
for which the original softmax layer is replaced by a new 12 
classes softmax classifier. Then, the parameters of the pre-
trained NN are transferred to a new model. Considering the 
great difference between the dataset of the pre-trained NN and 
the new synthetic fisheye dataset, we also fine-tune the lower 
layer of the neural network to ensure the adjustment of the low 
level feature. 
Convolution
Batch Norm
ReLU
Convolution
Batch Norm
ReLU
Input
Addition
Output
 
 
B. DCNN training  
In this section, we practice the knowledge transferring 
from a pre-trained Alexnet model to a new fisheye model in 
order to confirm the reliability of the proposed method.  
Besides, we test the new model’s accuracy by images captured 
by a fisheye camera. The pre-trained model is trained from 
ILSVRC 2012 image dataset.  
The tests run on Dell Precision T5500 workstation that 
configures dual Intel Xeon Processor E5645 (6 cores, 2.4 
GHz) and MSI Geforce GTX 1080 Graphic card (2560 CUDA 
cores, 8 GB RAM). The implementation is based on the Deep 
Learning Library Caffe. The synthetic images are divided into 
three groups: 100349 sample images to train the neural 
network, 33452 sample images to validate the neural and 
33449 sample images to test the accuracy of the neural 
network. We give the softmax layer a larger learning rate to 
accelerate the learning process. The softmax layer is fine-
tuned with the setting: Stochastic gradient descent (SGD) sets 
as 0.01 base learning rates and 0.5 times step down for every 
20 percentage step size. The accuracy of the fine-tuned model 
on the test dataset reaches to 82.5%.  
Figure 4.  The visualization of the detector on the first layer: a. perspective 
image neural network model, b. fisheye image neural network model 
The fine-tuned model has a 12 class’s softmax classifier 
which is supported by 12 neurons for every class. We also 
fine-tune the pre-trained model with 12 classes perspective 
image on the same object, in order to reveal the changes 
between perspective image classification model and fisheye 
image classification model. Fig. 4 illustrates the filters weight 
on the first CONV layer for perspective NN model and fisheye 
NN model respectively. From the visualization of the weight, 
fisheye NN model has similar filters weights as perspective 
NN model. But the weight image of the fisheye NN model has 
lower image sharpness, which weakens the expression of 
sharp edges and corners in the image. The changes on the first 
layer demonstrate the improvement of the new model for the 
curved images. 
C. Image classification test  
In the experiments, fisheye images are collected using 
Fujifilm Fujinon F-FE185C057HA-1 lens (0.1 m to infinity 
focusing range, 185-degree viewing angle on 2/3 inch sensor) 
and Point Grey Grasshopper 3 camera (GS3-PGE-50S5C-C, 
2448 x 2048 resolution, 5.0 MP megapixels, 2/3 inch sensor). 
Six objects (two ballpoint pens, three computers and a plastic 
bottle) are placed on the desk. Images are shot from three 
distance ranges separately: short distance (0.3-0.8 m), middle 
distance (0.8 - 2 m) and long distance (2 - 5 m).  
With the increasing of the distance between the object and 
the camera, the number of pixels occupied by each object 
decreases. The object that close to the edge of the image 
occupies fewer pixels because of the high compression ratio 
at the edge of the lens. We collect three images for each 
distance ranges. Considering external affection on the 
experiments, we parallel place two ballpoint pens in the 
distance of 5 cm and also set three computers closely. The 
resolution ratio of a raw fisheye image is 1800 x 1800 pixels.  
Five groups of objects’ images are collected from different 
distances and angles, in which two pens’ images are collected 
in same images and three computers are together. Interest 
regions are artificially selected and cropped. To fit the size of 
the input image for neural network, we scale down the images 
the longer side to 350 pixels, as shown in Fig. 5.  
Figure 5.  The images captured by fisheye camera. 
We test the performance of the new DCNN mode on the 
acquired photos. The assessment of the DCNN classification 
performance is by its top-1 and top-3 prediction reliability. As 
shown in the Fig. 6, the confidences levels of the top-1 
 
 
 
classification for each image are in the range of 75% and 95%, 
except one test for a computer image attends to 48.1%.The 
overall accuracy for the top-1 prediction on the collected 
images attends to 97.6%. 
Figure 6.  Top-1 classification prediction for the captured fisheye images 
V. CONCLUTION 
DCNN is widely used in the intelligent system, especially 
on the field of machine vision. For a large scale of DCNN 
training, the absence of the labeled images become the biggest 
disadvantage. In this paper, we propose to synthesize the 
fisheye image by transferring the labeled perspective image in 
order to reuse the existing resources. The transformation 
applies the equidistant projection. The proposed method is 
approved by the trained fisheye NN model and is tested on 
fisheye images captured by real fisheye camera. The results 
show that the synthetic fisheye image could be used on the big 
scale DCNN training. The synthetic fisheye dataset is 
available online, which is the first labeled fisheye dataset 
created by using existing dataset. 
 
 
 
 
 
REFERENCES 
 
[1] E. R. Davies, Machine vision: theory, algorithms, practicalities. 
Elsevier, 2004. 
[2] H. Durrant-Whyte and T. Bailey, ""Simultaneous localization and 
mapping: part I,"" IEEE robotics & automation magazine, vol. 13, no. 
2, pp. 99-110, 2006. 
[3] D. Caruso, J. Engel, and D. Cremers, ""Large-scale direct slam for 
omnidirectional cameras,"" in Intelligent Robots and Systems (IROS),  
2015 IEEE/RSJ International Conference on, 2015, pp. 141-148: IEEE. 
[4] M. Bertozzi, L. Castangia, S. Cattani, A. Prioletti, and P. Versari, ""360° 
detection and tracking algorithm of both pedestrian and vehicle using 
fisheye images,"" in Intelligent Vehicles Symposium (IV), 2015 IEEE, 
2015, pp. 132-137: IEEE. 
[5] C. Szegedy et al., ""Going deeper with convolutions,"" 2015: Cvpr. 
[6] Z. Dong, X. Shen, H. Li, and X. Tian, ""Photo quality assessment with 
DCNN that understands image well,"" in International Conference on 
Multimedia Modeling, 2015, pp. 524-535: Springer. 
[7] Y. Kawano and K. Yanai, ""Food image recognition with deep 
convolutional features,"" in Proceedings of the 2014 ACM International 
Joint Conference on Pervasive and Ubiquitous Computing: Adjunct 
Publication, 2014, pp. 589-593: ACM. 
[8] J. J. Kumler and M. L. Bauer, ""Fish-eye lens designs and their relative 
performance,"" in Current Developments in Lens Design and Optical 
Systems Engineering, 2000, vol. 4093, pp. 360-370: International 
Society for Optics and Photonics. 
[9] E. Schwalbe, ""Geometric modelling and calibration of fisheye lens 
camera systems,"" in Proc. 2nd Panoramic Photogrammetry Workshop, 
Int. Archives of Photogrammetry and Remote Sensing, 2005, vol. 36, 
no. Part 5, p. W8. 
[10] D. Schneider, E. Schwalbe, and H.-G. Maas, ""Validation of geometric 
models for fisheye lenses,"" ISPRS Journal of Photogrammetry and 
Remote Sensing, vol. 64, no. 3, pp. 259-266, 2009. 
[11] Y. LeCun, ""The MNIST database of handwritten digits,"" http://yann. 
lecun. com/exdb/mnist/, 1998. 
[12] A. Krizhevsky, V. Nair, and G. Hinton, ""Cifar-10 and cifar-100 
datasets,"" URl: https://www. cs. toronto. edu/kriz/cifar. html, vol. 6, 
2009. 
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ""Imagenet: 
A large-scale hierarchical image database,"" in Computer Vision and 
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2009, 
pp. 248-255: IEEE. 
[14] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, ""How transferable are 
features in deep neural networks?,"" in Advances in neural information 
processing systems, 2014, pp. 3320-3328. 
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ""Imagenet classification 
with deep convolutional neural networks,"" in Advances in neural 
information processing systems, 2012, pp. 1097-1105. 
   
 
 
 
 
 
"
43,"1Enabling Self-aware Smart Buildings by
Augmented Reality
Muhammad Aftab, Sid Chi-Kin Chau, and Majid Khonji
Abstract—Conventional HVAC control systems are usually
incognizant of the physical structures and materials of buildings.
These systems merely follow pre-set HVAC control logic based
on abstract building thermal response models, which are rough
approximations to true physical models, ignoring dynamic spatial
variations in built environments. To enable more accurate and
responsive HVAC control, this paper introduces the notion of self-
aware smart buildings, such that buildings are able to explicitly
construct physical models of themselves (e.g., incorporating build-
ing structures and materials, and thermal flow dynamics). The
question is how to enable self-aware buildings that automatically
acquire dynamic knowledge of themselves. This paper presents
a novel approach using augmented reality. The extensive user-
environment interactions in augmented reality not only can
provide intuitive user interfaces for building systems, but also
can capture the physical structures and possibly materials of
buildings accurately to enable real-time building simulation
and control. This paper presents a building system prototype
incorporating augmented reality, and discusses its applications.
Index Terms—Augmented Reality, Smart Buildings
I. INTRODUCTION
The rise of augmented reality provides an exciting venue
for extensive human-machine interactions. In essence, aug-
mented reality overlays computer-generated information with
the physical world in a composite view of a user through
motion tracking sensors and computer vision. Entertainment,
design and navigation are key applications of augmented
reality [1]. Nonetheless, the potential of augmented reality
can also be harnessed by other innovative applications. In this
paper, we explore a novel methodology utilizing augmented
reality as an empowering technological tool for more effective
building automation, and particularly, HVAC control.
Traditional building management systems usually follow
simple pre-set control logic programmed in the hardware con-
trollers. Often, a reductionistic approach is employed, wherein
the thermal response behavior of buildings is approximated
by first-principle models, and linear time-invariant dynamic
processes with 1D parameters by ordinary differential equa-
tions (also known as lumped element resistance-capacitance
(RC) models) [2], [3], [4]. HVAC control strategies are then
devised based on such abstract models, which can only approx-
imate simple building geometry in near-future time horizon.
These first-principle models are often difficult to calibrate
in a dynamic environment because the physical observable
M. Aftab is with Aalborg University. S. C.-K. Chau is with Aus-
tralian National University. M. Khonji is with Masdar Institute, Khal-
ifa University. E-mail: muhaftab@cs.aau.dk, chi-kin.chau@cl.cam.ac.uk,
mkhonji@masdar.ac.ae
This paper appears in ACM International Conference on Future Energy
Systems (e-Energy), 2018
parameters from buildings (e.g., building structures, locations
of doors and windows) are not explicitly incorporated in
the models. Also, the error accumulates considerably when
a longer time horizon of the control strategy is considered.
While non-linear models are complicated and impractical,
other alternatives based on true physical models of buildings
are more viable.
A. Need for Self-aware Buildings
To accurately and responsively control buildings, more
precise representation of building thermal response behav-
ior ought to be considered, particularly the detailed spatial-
temporal dynamics of thermal flow. The physical thermal
response model of a building can be represented by partial
differential equations, which often are employed in sophis-
ticated building simulators [5]. The conventional wisdom is
that such building simulators need considerable processing
power, and hence are only feasibly utilized in the design
stage of buildings. Nonetheless, there have been considerable
advances in embedded system technologies, which provide
low-cost platforms with powerful processors and sizeable
memory storage in a small footprint. There are opportunities
to harness such embedded system technologies for real-time
building automation systems. Particularly, accurate building
thermal response simulations based on physical thermal re-
sponse models can be executed efficiently in real-time on these
embedded systems. Therefore, this paper introduces the notion
of self-aware buildings, such that buildings can explicitly
construct physical models of themselves (e.g., incorporating
building structures, materials, and thermal flow dynamics) to
enable accurate real-time simulation-guided control in building
automation systems.
B. Augmented Reality as an Enabling Tool
Despite the promising benefits of self-aware buildings,
the question is how to enable self-aware buildings that can
automatically acquire dynamic knowledge of themselves. This
paper presents a novel approach using augmented reality. The
extensive user-environment interactions in augmented reality
not only can provide intuitive user interfaces for building
management systems, but also allow the systems to capture the
physical structures and possibly materials of buildings accu-
rately. Augmented reality is also useful for eliciting interactive
user feedback for personalization of building management sys-
tems. This paper presents a prototype of building management
system incorporating augmented reality (based on Google’s
Project Tango platform) and discusses its applications. Our
ar
X
iv
:1
70
8.
05
17
4v
2 
 [c
s.H
C]
  2
1 M
ay
 20
18
2main contribution is to demonstrate that augmented reality
can help optimize building control by empowering building
management systems to automatically recognize the building
structures, and then infer the building material properties and
HVAC system parameters. The advantages of incorporating
augmented reality also include providing appealing motivation
to users for upgrading their building management systems that
can integrate with home infotainment systems.
Researchers are beginning to explore the use of augmented
reality for building navigation, management, and control. The
authors in [6] present opportunities and challenges involved
in the use of 3D information of the indoor building en-
vironment for the purpose of indoor navigation. A method
for 3D reconstruction of indoor and outdoor environments
is proposed in [7]. They perform several filtering steps on
monochrome fisheye images to reconstruct scenes directly on
a mobile device. A history of augmented reality technology,
its challenges, and applications can be found in [8].
II. SELF-AWARE BUILDINGS
Before defining the notion of self-aware buildings, we dis-
cuss several problems of traditional approaches [4], [3] that are
based on abstract thermal response models for HVAC control.
First, the true physical state of a building is approximated by a
canonical state, which normally represents the average state in
the space, without considering the spatial variations of thermal
response behavior in different regions of the building. There is
a considerable loss of state information in the abstract models,
which are unable to incorporate the physical observable pa-
rameters from buildings (e.g., building structures, locations of
doors and windows). Second, the measurements from the real
world do not always match consistently with the outcomes of
abstract models. The lack of physical structural information
impedes the calibration of such models. Third, the dynamic
process in an abstract model does not correspond to the true
behavior of the physical world. As a result, the discrepancies
of abstract models can grow considerably over time.
On the contrary, self-aware building systems aim to acquire
dynamic faithful knowledge of buildings and learn to control
themselves adaptively. Specifically, a self-aware building sys-
tem can perform the following processes in an autonomous
manner [9]:
1) Sensing: Gathering environmental data, such as occu-
pants’ behavioral information through various sensors,
as well as temperature, humidity and external weather
information. Its purpose is to acquire the knowledge of
the current system state of a building.
2) Recognition: Capturing the physical model of a building
by measuring the structures and recognizing the materials.
Furthermore, the configurations of HVAC system may
be inferred. Its purpose is to create an accurate physical
building model.
3) Simulation: Simulating the dynamics of system behavior
(e.g., thermal flow behavior) under various parameters.
The accuracy of simulation can be improved by a better
physical building model and extensive knowledge of the
system state. The simulation should be performed in real-
time.
4) Prediction: Forecasting future environmental data, such
as occupants’ behavior and external weather.
5) Optimization: Deciding the system actions to maximize
certain desired objectives (e.g., energy efficiency, com-
fort).
6) Diagnosis: Identifying abnormal behavior and adaptively
correcting system states.
7) Control: Applying the actuations of systems in real time.
These processes interact with each other in a way as visualized
in Fig. 1. A fully self-aware building system is expected to
provide autonomy to these processes, with minimal human
assistance. Although automating some of the processes (e.g.,
control and optimization) have been studied extensively, other
processes (e.g., recognition) pose new challenges. This paper
aims to shed light particularly on automated recognition for
self-aware buildings.
Fig. 1: A self-aware building system consists of several
autonomous processes.
A. Recognition for Self-aware Buildings
To enable self-aware buildings, it is desirable to acquire the
following aspects of information of a building:
1) Structures: Including the dimension and geometry of the
building interior.
2) Materials: Including the types of walls (e.g., cement,
wood) and windows (e.g., shaded or not), as well as
insulation and conductance properties of built materials.
3) HVAC Systems: Including the capacity of heating/cooling
power and locations of HVAC ducts.
4) Geography: Including orientation, environmental shading
of the building.
While the aforementioned information can be provided by
the explicit assistance of users (e.g., constructing 3D model
from the BIM models or AutoCAD diagrams of the building,
etc.), we aim to develop systems that implicitly gather the
information with minimal human assistance.
III. AUGMENTED REALITY
This paper explores the possibilities of using augmented
reality, an interactive technology with the physical world by
overlaying digital information, as an enabling tool for self-
aware buildings. The extensive user-environment interactions
3Fig. 2: Augmented reality enabled building management system: (a) building geometry recognition, (b) building simulation
visualization, and (c) incorporation of user feedback on occupancy behavior.
in augmented reality allow us to implicitly capture the struc-
tures of physical buildings accurately, which can be used for
real-time building simulations.
To illustrate the concept, we present several user interfaces
of an augmented reality enabled building management system
in Fig. 2. For example, the system will automatically recognize
the dimension and geometry of building interior, as visualized
in Fig. 2(a). It will identify the locations and dimensions of
windows and doors. Also, it will detect the orientation of a
building by magnetic sensing. Hence, a faithful representation
of building will be acquired for the construction of physical
building model. On the other hand, to increase users’ aware-
ness of the energy efficiency of HVAC units, the system will
overlay simulated thermal flow information with the physical
view of the building, as visualized in Fig. 2(b). Furthermore,
users are able to input and modify the information used by
building management system through an intuitive interactive
user interface. For example, users can provide feedback on
occupancy behavior, as visualized in Fig. 2(c). User input
information can improve the system knowledge, and enable
a personalized system to address the needs of specific users.
While there are other options to capture physical building
models (e.g., LIDAR), augmented reality is more convenient
and cost-effective, providing rich user-building interaction
opportunities.
A. Project Tango Platform
Augmented reality has been a mature technology, with
an increasing number of available platforms and commercial
products, and emerging new developments in software systems
and hardware sensors. Among the state-of-the-art augmented
reality platforms, Project Tango is a popular option developed
by Google, which will be deployed extensively on enhanced
Android tablets or smartphones with depth sensors that are
able to acquire 3D information of a physical world in indoor
built environments [10], [11], [12].
Project Tango platform integrates several technologies of
augmented reality, such as depth-sensing based on infra-red
light, motion tracking camera, and environmental landmark
learning [6]. We tested on the first commercial Tango phone
(Lenovo Phab pro 2 shown in Fig. 3), which shows good accu-
racy of capturing 3D structures in indoor built environments. A
Tango phone provides a collection of pre-installed mobile apps
Fig. 3: Tango phone (Lenovo Phab pro 2), and capturing 3D
scene using Tango phone.
for typical augmented reality experiences, such as Measure app
(for remote measurement of object dimensions), Constructor
app (for capturing 3D scenes) and several augmented reality
games. Google also provides specified Tango SDK for ad-
vanced augmented reality app development.
IV. SYSTEM PROTOTYPE
In this section, we present a system prototype incorporating
augmented reality into building management system. In par-
ticular, we demonstrate our system for creating an accurate
3D building model by recognizing the building structures,
and then inferring the building material properties and certain
HVAC system parameters.
A. Recognizing Building Structures
We first capture the 3D scene of a room by Tango Construc-
tor app, which is able to capture approximate building spatial
structures and texture information (depicted in Fig. 4(a)).
Then, we convert the 3D scene to a 3D point cloud dataset
(depicted in Fig. 4(b)) for advanced processing in MATLAB,
which has extensive Computer Vision System Toolbox. To
obtain structural information (i.e., planes corresponding to
walls and floor) in the 3D point cloud dataset, we employ
the M-estimator Sample Consensus (MSAC) algorithm [13].
MSAC is a modified version of the Ransom Sample Consensus
(RANSAC) [14] algorithm for improved surface detection.
MSAC is available in MATLAB as function pcfitplane().
The result of plane detection by MSAC on point cloud
dataset is depicted in Fig. 4(c). Initially, MSAC extracts the
floor dimensions by detecting the horizontal plane in the point
cloud. Then, MSAC is applied iteratively to the point cloud
4Fig. 4: Recognizing building structures from a Tango phone. (a) Captured 3D scene of a room. (b) Converted 3D point cloud.
(c) Detected room geometry (e.g., floor and walls). (d) Converted 3D model for building simulation in EnergyPlus.
dataset until all vertical planes are detected. Each vertical plane
whose height is within a certain threshold of the maximum
detected height is labeled as a wall. The x-axis, y-axis, and
z-axis respectively represent the length, width, and height of
the room that were detected by MSAC. Finally, the extracted
geometry will be used to construct a 3D model for building
simulation in EnergyPlus (depicted in Fig. 4(d)).
B. Inferring Building Materials & HVAC
Unlike the physical structures of a building that can be
visually perceptible, the building materials are more difficult
to be discerned. Although one may use advanced techniques
like machine learning and image recognition, we employ a
simpler method in this paper. With the accurate 3D models of
buildings constructed based on augmented reality technology,
it is possible to infer the building material properties and
certain HVAC system parameters based on iterative compar-
isons between simulations (with appropriate calibrations of
simulation model parameters) and empirical observations of
the real-world building behavior [15].
The basic idea is that we first identify a collection of key
parameters in the building simulator (see Table I), and then
calibrate these parameters to match as close as possible with
the observed thermal response measurements under various
external weather conditions and HVAC operations. We em-
ploy EnergyPlus [16], a popular building simulation program.
EnergyPlus provides detailed building models, incorporating
building structures (e.g., locations of doors and windows,
etc.) and material properties, which makes them easier to
calibrate than first-principle or non-linear models. By iterative
calibration of the parameters of building simulation model,
we seek to minimize the discrepancy between the actual
and simulated indoor temperature values. The calibration of
simulation model is performed as follows:
1) The calibration algorithm sets the initial material proper-
ties and HVAC system parameters.
2) EnergyPlus runs a simulation with adjusted parameters
and returns simulated indoor temperature.
3) The algorithm tracks the changes in the discrepancy
between actual and simulated temperature and applies
gradient descent to adjust the respective parameters. The
process repeats iteratively until the discrepancy is within
a certain threshold.
A final calibrated building model will provide an estimation
of the building material properties and certain HVAC system
parameters. This calibration process is carried out using co-
simulation, where the EnergyPlus building model is coupled
with a calibration algorithm implemented in Python using
BCVTB (Building Controls Virtual Test Bed) as software
interface [17]. It is worth noting that gradient descent may
arrive at parameter values that agree with the current set
of observations but these might not be the true values of
the parameters. For this reason, the calibration algorithm
continuously checks the model in the background, and re-
calibrates it whenever the discrepancy between the actual and
simulated temperature exceeds the threshold. This way, the
model always provides a good estimation of the parameter
values. A demonstration using the 3D model from Fig. 4(d) is
20
22
24
26
28
30
32
T
em
pe
ra
tu
re
 (
°C
)
actual temperature
simulated temperature before calibration (CVRMSE=5.30%)
simulated temperature after calibration (CVRMSE=1.76%)
00:
00
03:
00
06:
00
09:
00
12:
00
15:
00
18:
00
21:
00
24:
00
Off
On
A
C
Fig. 5: Inferring building material properties and HVAC sys-
tem parameters based on calibration of building simulations.
illustrated in Fig. 5, showing the thermal response of the initial
uncalibrated model and of the iteratively calibrated model.
After calibration, the simulated temperature of the calibrated
EnergyPlus model can closely match the actual temperature.
Fig. 5 also depicts the observed status of HVAC system during
different times of the day, as in the bottom strips. Finally,
Table I presents the inferred parameter values of building
simulation model, including building material properties and
HVAC system.
V. CONCLUSION
This paper first presented a notion of self-aware building
that is capable of recognizing of building structures and
properties to construct accurate simulation models. Then, we
5Parameter Field Calibrated Value
Walls Thickness 30.0cmConductivity 0.311W/(m-K)
Windows Thickness 0.31cmConductivity 0.85W/(m-K)
Door Thickness 2.54cmConductivity 0.15W/(m-K)
Roof Thickness 10.16cmConductivity 0.53W/(m-K)
HVAC Cooling Capacity 8943W
Air Flow Rate 0.384m3/sec
TABLE I: Key building simulation model parameters.
demonstrated a system prototype using augmented reality to
capture the build structures and then to infer building materials
and HVAC system parameters. In future work, we will imple-
ment a full system that incorporates simulation visualization,
user feedback elicitation, and automatic real-time simulation-
guided control for HVAC operations.
REFERENCES
[1] D. Van Krevelen and R. Poelman, “A survey of augmented reality
technologies, applications and limitations,” International journal of
virtual reality, vol. 9, no. 2, p. 1, 2010.
[2] M. Aftab, S. C.-K. Chau, and P. Armstrong, “Smart air-conditioning
control by wireless sensors: An online optimization approach,” in
Proceedings of the Fourth International Conference on Future Energy
Systems, ser. e-Energy ’13. New York, NY, USA: ACM, 2013, pp. 225–
236. [Online]. Available: http://doi.acm.org/10.1145/2487166.2487192
[3] J. Hu and P. Karava, “A state-space modeling approach and multi-level
optimization algorithm for predictive control of multi-zone buildings
with mixed-mode cooling,” Building and Environment, vol. 80, pp. 259
– 273, 2014.
[4] R. Balan, J. Cooper, K.-M. Chao, S. Stan, and R. Donca, “Parameter
identification and model based predictive control of temperature inside
a house,” Energy and Buildings, vol. 43, no. 2 - 3, pp. 748 – 758, 2011.
[5] D. B. Crawley, J. W. Hand, M. Kummert, and B. T. Griffith, “Contrasting
the capabilities of building energy performance simulation programs,”
Building and Environment, vol. 43, no. 4, pp. 661 – 673, 2008.
[6] A. A. Diakite´ and S. Zlatanova, “First experiments with the tango tablet
for indoor scanning,” ISPRS Annals of Photogrammetry, Remote Sensing
and Spatial Information Sciences, pp. 67–72, 2016.
[7] T. Scho¨ps, T. Sattler, C. Ha¨ne, and M. Pollefeys, “3d modeling on the go:
Interactive 3d reconstruction of large-scale scenes on mobile devices,”
in 3D Vision (3DV), 2015 International Conference on. IEEE, 2015,
pp. 291–299.
[8] D. Van Krevelen and R. Poelman, “Augmented reality: Technologies,
applications, and limitations,” 2007.
[9] A. Mahdavi, R. Brahme, and S. Gupta, “Self-aware buildings: a
simulation-based approach,” 2001.
[10] X. Blanco, “Google’s project tango tablet is ripe for
development,” 2015. [Online]. Available: https://www.cnet.com/
products/google-project-tango/preview/
[11] Lenovo, “Lenovo phab 2 pro,” 2018. [On-
line]. Available: https://www3.lenovo.com/in/en/tablets/android-tablets/
tablet-phab-series/Lenovo-Phab-2-Pro/p/WMD00000220
[12] Asus, “Zenfone ar,” 2018. [Online]. Available: https://www.asus.com/
in/Phone/ZenFone-AR-ZS571KL/
[13] P. H. Torr and A. Zisserman, “Mlesac: A new robust estimator with
application to estimating image geometry,” Computer Vision and Image
Understanding, vol. 78, no. 1, pp. 138–156, 2000.
[14] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm
for model fitting with applications to image analysis and automated
cartography,” Communications of the ACM, vol. 24, no. 6, pp. 381–395,
1981.
[15] M. Aftab, C. Chen, C.-K. Chau, and T. Rahwan, “Automatic hvac
control with real-time occupancy recognition and simulation-guided
model predictive control in low-cost embedded system,” Energy and
Buildings, vol. 154, pp. 141 – 156, 2017. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0378778817305091
[16] D. B. Crawley, L. K. Lawrie, F. C. Winkelmann, W. Buhl, Y. Huang,
C. O. Pedersen, R. K. Strand, R. J. Liesen, D. E. Fisher, M. J. Witte,
and J. Glazer, “Energyplus: a new-generation building energy simulation
program,” Energy and Buildings, vol. 33, no. 4, pp. 319 – 331, 2001.
[17] M. Wetter, “Co-simulation of building energy and control systems with
the building controls virtual test bed,” Journal of Building Performance
Simulation, vol. 4, no. 3, pp. 185–203, 2011.
"
44,"Adversarial Sparse-View CBCT
Artifact Reduction
Haofu Liao1 ( ), Zhimin Huo2, William J. Sehnert2, Shaohua Kevin Zhou3,
and Jiebo Luo1
1 Department of Computer Science, University of Rochester, Rochester, USA
hliao6@cs.rochester.edu
2 Carestream Health Inc., Rochester, USA
3 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
Abstract. We present an effective post-processing method to reduce
the artifacts from sparsely reconstructed cone-beam CT (CBCT) images.
The proposed method is based on the state-of-the-art, image-to-image
generative models with a perceptual loss as regulation. Unlike the tra-
ditional CT artifact-reduction approaches, our method is trained in an
adversarial fashion that yields more perceptually realistic outputs while
preserving the anatomical structures. To address the streak artifacts that
are inherently local and appear across various scales, we further propose a
novel discriminator architecture based on feature pyramid networks and
a differentially modulated focus map to induce the adversarial training.
Our experimental results show that the proposed method can greatly cor-
rect the cone-beam artifacts from clinical CBCT images reconstructed
using 1/3 projections, and outperforms strong baseline methods both
quantitatively and qualitatively.
1 Introduction
Cone-beam computed tomography (CBCT) is a variant type of computed to-
mography (CT). Compared with conventional CT, CBCT usually has shorter
examination time, resulting in fewer motion artifacts and better X-ray tube effi-
ciency. One way to further shorten the acquisition time and enhance the health-
care experience is to take fewer X-ray measurements during each CBCT scan.
However, due to the “cone-beam” projection geometry, CBCT images typically
contain more pronounced streak artifacts than CT images and this is even worse
when fewer X-ray projections are used during the CBCT reconstruction [1].
A number of approaches have been proposed to address the artifacts [13,8]
that are commonly encountered in CBCT images. However, to our best knowl-
edge, no scheme has been proposed to correct the cone-beam artifacts introduced
by sparse-view CBCT reconstruction in a post-processing step. Instead of reduc-
ing artifacts from the CBCT images directly, many other systems [11,14] propose
to introduce better sparse-view reconstruction methods that yield less artifacts.
Although encouraging improvements have been made, the image quality from the
current solutions are still not satisfactory when only a small number of views are
ar
X
iv
:1
81
2.
03
50
3v
1 
 [c
s.C
V]
  9
 D
ec
 20
18
2 H. Liao et al
Fig. 1. The overall architecture of the proposed method.
used. This work attempts to fill this gap by refining the sparsely reconstructed
CBCT images through a novel cone-beam artifact reduction method.
In relation to this study, there are many works that leverage deep neural net-
works (DNNs) for low-dose CT (LDCT) denoising. [2] used a residual encoder-
decoder architecture to reduce the noise from LDCT images, and achieved su-
perior performance over traditional approaches. More recently, [10] introduced
generative adversarial networks (GANs) [3] into their architecture to obtain more
realistic outputs, and this work was further improved by [12] where a combina-
tion of perceptual loss [5] and adversarial loss was used.
Similarly, this work also proposes to use DNNs for sparse-view CBCT artifact
reduction. We train an image-to-image generative model with perceptual loss to
obtain outputs that are perceptually close to the dense-view CBCT images. To
address the artifacts at various levels, we further contribute to the literature with
a novel discriminator architecture based on feature pyramid networks (FPN) [6]
and a differentially modulated focus map so that the adversarial training is
biased to the artifacts at multiple scales. The proposed approach is evaluated
on clinical CBCT images. Experimental results demonstrate that our method
outperforms strong baseline methods both qualitatively and quantitatively.
2 Methods
Let xs be a sparse-view CBCT image, which is reconstructed from a sparse set
or low number of projections (or views), and xd be its dense-view counterpart,
which is reconstructed from a dense set or a high number of projections (or
views). The proposed method is formulated under an image-to-image generative
model as illustrated in Fig. 1 where we train a generator that transforms xs to
an ideally artifact-free image that looks like xd. The discriminator is used for the
adversarial training, and the perceptual network is included for additional per-
ceptual and structural regularization. We use LSGAN [7] against a regular GAN
to achieve more stable adversarial learning. The adversarial objective functions
for the proposed model can be written as
min
D
LA(D;G,Λ) = EXd [‖Λ (D(xd)− 1)‖2] + EXs [‖ΛD(G(xs))‖2], (1)
min
G
LA(G;D,Λ) = EXs [‖Λ (D(G(xs))− 1)‖2], (2)
Adversarial Sparse-View CBCT Artifact Reduction 3
Fig. 2. Detailed network structure of the generator and discriminator.
where Λ is a focus map detailed in Sec. 2.2. Here, we apply a PatchGAN-like [4]
design to the discriminator so that the realness is patch based and the output is
a score map. The generator G and discriminator D are trained in an adversarial
fashion. D distinguishes between xd and the generated CBCT image G(xs) (Eq.
1), while G generates CBCT image samples as “real” as possible so that D
cannot tell if they are dense-view CBCT images or generated by G (Eq. 2).
Training with the adversarial loss, alone, usually introduces additional arti-
facts, and previous works often use MSE loss to induce the learning [10]. However,
as shown by [12], MSE loss does not handle streak artifacts very well. Therefore,
we adopt the choice of [12] by using a perceptual loss to induce the learning and
give more realistic outputs. Let φ(i)(·) denote the feature maps extracted by the
i-th layer of the perceptual network φ and Ni denote the number of elements in
φ(i)(·), the perceptual loss can be computed by
LP = 1
Ni
‖φ(i)(xd)− φ(i)(G(xs))‖1. (3)
In this work, the perceptual network φ is a pretrained VGG16 net [9] and we
empirically find that i = 8 works well.
2.1 Network Structure
The generator is based on an encoder-decoder architecture [4]. As shown in Fig.
2, the generator has four encoding blocks (in gray) and four decoding blocks (in
blue). Each encoding block contains a convolutional layer followed by a batch
normalization layer and a leaky ReLU layer. Similarly, each decoding block con-
tains a deconvolutional layer followed by a batch normalization layer and a ReLU
layer. Both the convolutional and deconvolutional layers have a 4×4 kernel with a
stride of 2 so that they can downsample and upsample the outputs, respectively.
Outputs from the encoding blocks are shuttled to the corresponding decoding
blocks using skip connections. This design allows the low-level context informa-
tion from the encoding blocks to be used directly together with the decoded
high-level information during generation.
A typical discriminator (Fig. 2 Discriminator A) usually contains a set of
encoding blocks followed by a classifier to determine the input’s realness. In this
case, the discrimination is performed at a fixed granularity that is fine when the
4 H. Liao et al
(a) xd (b) G(xs) (c) D(G(xs)) (d) Λ
Fig. 3. Saturated (c) score map D(G(xs)) and (d) focus map Λ computed between
(a) dense-view CBCT image xd and (b) generated CBCT image G(xs).
task is a generative task such as style transfer or image translation, or there
is a systematic error to be corrected such as JPEG decompression or super-
resolution. For sparse-view CBCT images, the artifacts appear randomly with
different scales. To capture such a variation of artifacts, we propose a discrimi-
nator that handles the adversarial training at different granularities.
The core idea is to create a feature pyramid and perform discrimination
at multiple scales. As illustrated in Fig. 2 Discriminator B, the network uses
two outputs and makes decisions based on different levels of semantic feature
maps. We adapt the design from FPN [6] so that the feature pyramid has strong
semantics at all scales. Specifically, we first use three encoding blocks to ex-
tract features at different levels. Next, we use an upsample block (in purple)
to incorporate the stronger semantic features from the top layer into the out-
puts of the middle layer. The upsample block consists of a unsampling layer
and a 3 × 3 convolutional layer (to smooth the outputs). Because the feature
maps from the encoding blocks have different channel sizes, we place a lateral
block (in green, essentially a 1×1 convolutional layer) after each encoding block
to match this channel difference. In the end, there are two classifiers to make
joint decisions on the semantics at different scales. Each classifier contains two
blocks. The first block (in yellow) has the same layers as an encoding block,
except that the convolutional layer has a 3 × 3 kernel with a stride of 1. The
second block (in green) is simply a 1 × 1 convolutional layer with stride 1. Let
D1(x) and D2(x) denote the outputs from the two classifiers, then the new
adversarial loss can be given by minD LA(D;G,Λ1, Λ2) =
∑2
i=1 LA(Di;G,Λi)
and minG LA(G;D,Λ1, Λ2) =
∑2
i=1 LA(G;Di, Λi). We also experimented with
deeper discriminators with more classifiers for richer feature semantics, but found
that they contribute only minor improvements over the current setting.
2.2 Focus Map
When an image from the generator looks mostly “real” (Fig 3 (b)), the score
map (Fig 3 (c)) output by the discriminator will be overwhelmed by borderline
scores (those values close to 0.5). This saturates the adversarial training as bor-
derline scores make little contribution to the weight update of the discriminator.
Adversarial Sparse-View CBCT Artifact Reduction 5
To address this problem, we propose to introduce a modulation factor to the
adversarial loss so that the borderline scores are down-weighted during training.
Observing that when a generated region is visually close to the corresponding
region of a dense-view image (Fig 3 (a)), it is more likely to be “real” and causes
the discriminator to give a borderline score. Therefore, we use a feature difference
map (Fig 3 (d)) to perform this modulation.
Let φ
(j)
m,n(·) denote the (m,n)-th feature vector of φ(j)(·), then the (m,n)-th
element of the feature difference map Λ between xd and G(xs) is defined as
λm,n =
1
Zj
‖φ(j)m,n(xd)− φ(j)m,n(G(xs))‖, (4)
where Zj is a normalization term given by
Zj =
1
Nj
∑
m,n
‖φ(j)m,n(xd)− φ(j)m,n(G(xs))‖. (5)
We use the same perceptual network φ as the one used for computing the per-
ceptual loss, and j is chosen to match the resolution of D1(x) and D2(x). For
the VGG16 net, we use j = 16 for Λ1 and j = 9 for Λ2.
3 Experiments
Datasets The CBCT images were obtained by a multi-source CBCT scanner
dedicated for lower extremities. In total, knee images from 27 subjects are un-
der investigation. Each subject is associated with a sparse-view image and a
dense-view image that are reconstructed using 67 and 200 projection views, re-
spectively. Each image is processed, slice by slice, along the sagittal direction
where the streak artifacts are most pronounced. During the training, the inputs
to the models are 256× 256 patches that randomly cropped from the slices.
Models Three variants of the proposed methods as well as two other baseline
methods are compared: (i) Baseline-MSE: a similar approach to [10] by combin-
ing MSE loss with GAN. 3D UNet1 and LSGAN is used for fair comparison;
(ii) Baseline-Perceptual: a similar approach to [12] by combining perceptual loss
with GAN. It is also based on our UNet and LSGAN infrastructure for fair com-
parison; (iii) Ours-FPN: our method using FPN as the discriminator and setting
Λ1 = Λ2 = 1; (iv) Ours-Focus: our method using focus map and conventional
discriminator (Fig. 2 Discriminator A); (v) Ours-Focus+FPN: our method using
focus map as well as the FPN discriminator. We train all the models using Adam
optimization with the learning rate lr = 10−4 and β1 = 0.5. We use λa = 1.0,
λm = 100, and λp = 10 to control the weights between the adversarial loss, the
MSE loss, and the perceptual loss. The values are chosen empirically and are the
same for all models (if applicable). All the models are trained for 50 epochs with
1 Identical to the 2D UNet used in this work with all the 2D convolutional and de-
convolutional layers replaced by their 3D counterparts.
6 H. Liao et al
Fig. 4. Qualitative sparse-view CBCT artifact reduction results by different models.
The same brightness and contrast enhancement are applied to the images for better
and uniform visualization. (a) xd (b) xs (c) Baseline-MSE (d) Baseline-Perceptual (e)
Ours-Focus (f) Ours-FPN (g) Ours-Focus+FPN
5-fold cross-validation. We perform all the experiments on an Nvidia GeForce
GTX 1070 GPU. During testing, the average processing time on 384××384×417
CBCT volumes for the 2D UNet (generator of model (ii)-(v)) is 16.05 seconds,
and for the 3D UNet (generator of model (i)) is 22.70 seconds.
Experimental Results Fig. 4 shows the qualitative results of the models. Al-
though the baseline methods overall have some improvements over the sparse-
view image, they still cannot handle the streak artifacts very well. “Baseline-
Perceptual” produces less pronounced artifacts than “Baseline-MSE”, which
demonstrates that using perceptual loss and processing the images slice by slice
in 2D give better results than MSE loss with 3D generator. Our models (Fig. 4
(e-f)) in general produce less artifacts than the baseline models. We can barely
see the streak artifacts. They generally produce similar outputs and the result
from “Ours-Focus+FPN” is slightly better than “Ours-FPN” and “Ours-Focus”.
This means that using FPN as the discriminator or applying a modulation factor
to the adversarial loss can indeed induce the training to artifacts reduction.
We further investigate the image characteristics of each model in a region of
interest (ROI). A good model should have similar image characteristics to the
dense-view images in ROIs. When looking at the pixel-wise difference between
the dense-view ROI and the model ROI, no structure information should be
observed, resulting a random noise map. Fig. 5(a) shows the ROI differences
of the models. We can see a clear bone structure from the ROI difference map
between xs and xd (Fig. 5(a) third row), which demonstrates a significant dif-
ference in image characteristics between these two images. For “Baseline-MSE”,
the bone structure is less recognizable, showing more similar image characteris-
tics. For “Baseline-Perceptual” and our models, we can hardly see the structural
information and mostly observe random noises. This indicates that these models
have very similar image characteristics to a dense-view image. We also measure
Adversarial Sparse-View CBCT Artifact Reduction 7
(a) Difference Maps (b) Mean and Standard Deviation
Fig. 5. ROI characteristics. (a) Patches are obtained by subtracting the corresponding
ROI from xd (third row). First row from left to right: xs, baseline-MSE, baseline-
perceptual. Second row from left to right: Ours-Focus, Ours-FPN, Ours-Focus+FPN.
(b) Each bar indicates the mean value of the ROI. The numbers on the top of each bar
indicate the standard deviations. The vertical lines indicates the changes of the mean
value when ± standard deviations is applied. Pixel values are normalized to [0, 1].
Table 1. Quantitative sparse-view CBCT artifact reduction results of different models.
xs
Baseline Ours
MSE Perc. Focus FPN FPN+Focus
SSIM 0.839 0.849 0.858 0.879 0.871 0.884
PSNR (dB) 34.07 34.24 35.39 36.26 36.38 36.14
RMSE (10−2) 1.98 1.96 1.70 1.54 1.52 1.56
the mean and standard deviation of the pixel values within the ROI. We can
see that our models have very close statistics with xd, especially the pixel value
statistics of “Ours-Focus” and “Ours-Focus+FPN” are almost identical to xd,
demonstrating better image characteristics.
We then evaluate the models quantitatively by comparing their outputs with
the corresponding dense-view CBCT image. Three evaluation metrics are used:
structural similarity (SSIM), peak signal-to-noise ratio (PSNR), and root mean
square error (RMSE). Higher values for SSIM and PSNR and lower values for
RMSE indicate better performance. We can see from Table 1 that the base-
line methods give better scores than xs. Similar to the case in the qualitative
evaluation, “Baseline-Perceptual” performs better than “Baseline-MSE”. Our
methods consistently outperform the baseline methods by a significant margin.
“Ours-FPN” gives best performance in PSNR and RMSE. However, PSNR and
RMSE only measure the pixel level difference between two images. To measure
the performance in perceived similarity, SSIM is usually a better choice, and we
find “Ours-FPN+Focus” has a slightly better performance on this metric. This
confirms our observation in qualitative evaluation.
8 H. Liao et al
4 Conclusion
We have presented a novel approach to reducing artifacts from sparsely-reconstructed
CBCT images. To our best knowledge, this is the first work that addresses arti-
facts introduced by sparse-view CBCT reconstruction in a post-processing step.
We target this problem using an image-to-image generative model with a per-
ceptual loss as regulation. The model generates perceptually realistic outputs
while making the artifacts less pronounced. To further suppress the streak arti-
facts, we have also proposed a novel FPN based discriminator and a focus map
to induce the adversarial training. Experimental results show that the proposed
mechanism addresses the streak artifacts much better, and the proposed models
outperform strong baseline methods both qualitatively and quantitatively.
Acknowledgement. The work presented here was supported in part by New
York State through the Goergen Institute for Data Science at the University of
Rochester and the corporate sponsor Carestream Health Inc.
References
1. Bian, J., Siewerdsen, J.H., Han, X., Sidky, E.Y., Prince, J.L., Pelizzari, C.A., Pan,
X.: Evaluation of sparse-view reconstruction from flat-panel-detector cone-beam
CT. Physics in Medicine & Biology 55(22), 6575 (2010)
2. Chen, H., Zhang, Y., Kalra, M.K., Lin, F., Chen, Y., Liao, P., Zhou, J., Wang, G.:
Low-dose CT with a residual encoder-decoder convolutional neural network. IEEE
transactions on medical imaging 36(12), 2524–2535 (2017)
3. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural
information processing systems. pp. 2672–2680 (2014)
4. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-
tional adversarial networks. arXiv preprint arXiv:1611.07004 (2016)
5. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and
super-resolution. arXiv preprint arXiv:1603.08155 (2016)
6. Lin, T.Y., Dolla´r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. arXiv preprint arXiv:1612.03144 (2016)
7. Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Smolley, S.P.: Least squares gener-
ative adversarial networks. arXiv preprint arXiv:1611.04076 (2016)
8. Ning, R., Tang, X., Conover, D.: X-ray scatter correction algorithm for cone beam
CT imaging. Medical physics 31(5), 1195–1202 (2004)
9. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
10. Wolterink, J.M., Leiner, T., Viergever, M.A., Isˇgum, I.: Generative adversarial
networks for noise reduction in low-dose CT. IEEE TMI 36(12), 2536–2545 (2017)
11. Xia, D., Langan, D.A., Solomon, S.B., Zhang, Z., Chen, B., Lai, H., Sidky, E.Y.,
Pan, X.: Optimization-based image reconstruction with artifact reduction in c-arm
CBCT. Physics in Medicine & Biology 61(20), 7300 (2016)
12. Yang, Q., Yan, P., Zhang, Y., Yu, H., Shi, Y., Mou, X., Kalra, M.K., Wang, G.: Low
dose CT image denoising using a generative adversarial network with wasserstein
distance and perceptual loss. arXiv preprint arXiv:1708.00961 (2017)
Adversarial Sparse-View CBCT Artifact Reduction 9
13. Zhang, Y., Zhang, L., Zhu, X.R., Lee, A.K., Chambers, M., Dong, L.: Reducing
metal artifacts in cone-beam CT images by preprocessing projection data. Inter-
national Journal of Radiation Oncology Biology Physics 67(3), 924–932 (2007)
14. Zhang, Z., Han, X., Pearson, E., Pelizzari, C., Sidky, E.Y., Pan, X.: Artifact re-
duction in short-scan CBCT by use of optimization-based reconstruction. Physics
in Medicine & Biology 61(9), 3387 (2016)
"
45,"ar
X
iv
:1
31
0.
11
37
v1
  [
cs
.C
R]
  4
 O
ct 
20
13
GOTCHA Password Hackers!∗
Jeremiah Blocki
Carnegie Mellon University
jblocki@cs.cmu.edu
Manuel Blum
Carnegie Mellon University
mblum@cs.cmu.edu
Anupam Datta
Carnegie Mellon University
danupam@cmu.edu
October 7, 2013
Abstract
We introduce GOTCHAs (Generating panOptic Turing Tests to Tell
Computers and Humans Apart) as a way of preventing automated offline
dictionary attacks against user selected passwords. A GOTCHA is a ran-
domized puzzle generation protocol, which involves interaction between a
computer and a human. Informally, a GOTCHA should satisfy two key
properties: (1) The puzzles are easy for the human to solve. (2) The puz-
zles are hard for a computer to solve even if it has the random bits used
by the computer to generate the final puzzle — unlike a CAPTCHA [43].
Our main theorem demonstrates that GOTCHAs can be used to mitigate
the threat of offline dictionary attacks against passwords by ensuring that
a password cracker must receive constant feedback from a human being
while mounting an attack. Finally, we provide a candidate construction
of GOTCHAs based on Inkblot images. Our construction relies on the
usability assumption that users can recognize the phrases that they orig-
inally used to describe each Inkblot image — a much weaker usability
assumption than previous password systems based on Inkblots which re-
quired users to recall their phrase exactly. We conduct a user study to
evaluate the usability of our GOTCHA construction. We also generate
a GOTCHA challenge where we encourage artificial intelligence and se-
curity researchers to try to crack several passwords protected with our
scheme.
1 Introduction
Any adversary who has obtained the cryptographic hash of a user’s password
can mount an automated brute-force attack to crack the password by com-
∗This work was partially supported by the NSF Science and Technology TRUST and the
AFOSR MURI on Science of Cybersecurity. The first author was also partially supported by
an NSF Graduate Fellowship.
1
paring the cryptographic hash of the user’s password with the cryptographic
hashes of likely password guesses. This attack is called an offline dictionary
attack, and there are many password crackers that an adversary could use [17].
Offline dictionary attacks against passwords are — unfortunately — powerful
and commonplace [25]. Adversaries have been able to compromise servers at
large companies (e.g., Zappos, LinkedIn, Sony, Gawker [5, 2, 9, 4, 1, 3]) result-
ing in the release of millions of cryptographic password hashes 1. It has been
repeatedly demonstrated that users tend to select easily guessable passwords
[27, 18, 11], and password crackers are able to quickly break many of these
passwords[39]. Offline attacks are becoming increasingly dangerous as comput-
ing hardware improves — a modern GPU can evaluate a cryptographic hash
function like SHA2 about 250 million times per second [49] — and as more
and more training data — leaked passwords from prior breaches — becomes
available [25]. Symantec reported that compromised passwords have significant
economic value to an adversary (e.g., compromised passwords are sold on black
market for between $4 and $30 ) [22].
HOSPs (Human-Only Solvable Puzzles) were suggested by Canetti, Halevi
and Steiner as a way of defending against offline dictionary attacks [14]. The
basic idea is to change the authentication protocol so that human interaction
is required to verify a password guess. The authentication protocol begins with
the user entering his password. In response the server randomly generates a
challenge — using the password as a source of randomness — for the user to
solve. Finally, the server appends the user’s response to the user’s password,
and verifies that the hash matches the record on the server. To crack the user’s
password offline the adversary must simultaneously guess the user’s password
and the answer to the corresponding puzzle. The challenge should be easy for
a human to solve consistently so that a legitimate user can authenticate. To
mitigate the threat of an offline dictionary attack the HOSP should be difficult
for a computer to solve — even if it has all of the random bits used to generate
the challenge.
The basic HOSP construction proposed by Canetti et al. [14] was to to fill a
hard drive with regular CAPTCHAs (e.g., distorted text) by storing the puzzles
without the answers. This solution only provides limited protection against
an adversary because the number of unique puzzles that can be generated is
bounded by the size of the hard drive (e.g., the adversary could pay people to
solve all of the puzzles on the hard drive). See appendix B for more discussion.
Finding a usable HOSP construction which does not rely on a very large dataset
of pregenerated CAPTCHAs is an open problem. Several candidate HOSPs were
experimentally tested [15] (they are called POSHs in the second paper), but the
usability results were underwhelming.
Contributions We introduce a simple modification of HOSPs that we call
GOTCHAs (Generating panOptic Turing Tests to Tell Computers and Humans
Apart). We use the adjective Panoptic to refer to a world without privacy —
1In a few of these cases [3, 1] the passwords were stored in the clear.
2
there are no hidden random inputs to the puzzle generation protocol. The basic
goal of GOTCHAs is similar to the goal of HOSPs — defending against offline
dictionary attacks. GOTCHAs differ from HOSPs in two ways (1) Unlike a
HOSP a GOTCHA may require human interaction during the generation of the
challenge. (2) We relax the requirement that a user needs to be able to answer
all challenges easily and consistently. If the user can remember his password
during the authentication protocol then he will only ever see one challenge. We
only require that the user must be able to answer this challenge consistently.
If the user enters the wrong password during authentication then he may see
new challenges. We do not require that the user must be able to solve these
challenges consistently because authentication will fail in either case. We do
require that it is difficult for a computer to distinguish between the “correct”
challenge and an “incorrect” challenge. Our main theorem demonstrates that
GOTCHAs like HOSPs can be used to defend against offline dictionary attacks.
The goal of these relaxations is to enable the design of usable GOTCHAs.
We introduce a candidate GOTCHA construction based on Inkblot images.
While the images are generated randomly by a computer, the human mind can
easily imagine semantically meaningful objects in each image. To generate a
challenge the computer first generates ten inkblot images (e.g., figure 1). The
user then provides labels for each image (e.g., evil clown, big frog). During au-
thentication the challenge is to match each inkblot image with the corresponding
label. We empirically evaluate the usability of our inkblot matching GOTCHA
construction by conducting a user study on Amazon’s Mechanical Turk. Finally,
we challenge the AI community to break our GOTCHA construction.
Organization The rest of the paper is organized as follows: We next discuss
related work in section 1.1. We formally define GOTCHAs in section 2 and for-
malize the properties that a GOTCHA should satisfy. We present our candidate
GOTCHA construction in section 3, and in section 3.1 we demonstrate how our
GOTCHA could be integrated into an authentication protocol. We present the
results from our user study in section 3.2, and in section 3.3 we challenge the
AI and security communities to break our GOTCHA construction. In section
4 we prove that GOTCHAs like HOSPs can also be used to design a password
storage system which mitigates the threat of offline attacks. We conclude by
discussing future directions and challenges in section 5.
1.1 Related Work
Inkblots [42] have been proposed as an alternative way to generate and remem-
ber passwords. Stubblefield and Simon proposed showing the user ten randomly
generated inkblot images, and having the user make up a word or a phrase to
describe each image. These phrases were then used to build a 20 character pass-
word (e.g., users were instructed to take the first and last letter of each phrase).
Usability results were moderately good, but users sometimes had trouble re-
membering their association. Because the Inkblots are publicly available there
3
Figure 1: Randomly Generated Inkblot Image—An evil clown?
is also a security concern that Inkblot passwords could be guessable if different
users consistently picked similar phrases to describe the same Inkblot.
We stress that our use of Inkblot images is different in two ways: (1) Usabil-
ity: We do not require users to recall the word or phrase associated with each
Inkblot. Instead we require user’s to recognize the word or phrase associated
with each Inkblot so that they can match each phrase with the appropriate
Inkblot image. Recognition is widely accepted to be easier than the task of
recall [7, 45]. (2) Security: We do not need to assume that it would be difficult
for other humans to match the phrases with each Inkblot. We only assume that
it is difficult for a computer to perform this matching automatically.
CAPTCHAs — formally introduced by Von Ahn et al. [43] — have gained
widespread adoption on the internet to prevent bots from automatically reg-
istering for accounts. A CAPTCHA is a program that generates a puzzle —
which should be easy for a human to solve and difficult for a computer to solve
— as well as a solution. Many popular forms of CAPTCHAs (e.g., reCAPTCHA
[44]) generate garbled text, which is easy 2 for a human to read, but difficult
for a computer to decipher. Other versions of CAPTCHAs rely on the natural
human capacity for audio [37] or image recognition [19].
CAPTCHAs have been used to defend against online password guessing
attacks — users are sometimes required to solve a CAPTCHA before signing
2Admitedly some people would dispute the use of the label ‘easy.’
4
into their account. An alternative approach is to lock out a user after several
incorrect guesses, but this can lead to denial of service attacks [16]. However,
if the adversary has access to the cryptographic hash of the user’s password,
then he can circumvent all of these requirements and execute an automatic
dictionary attack to crack the password offline. By contrast HOSPs — proposed
by Canetti et al.[14] — were proposed to defend against offline attacks. HOSPs
are in some ways similar to CAPTCHAs (Completely Automated Turing Tests
to Tell Computers and Humans Apart) [43]. CAPTCHAs are widely used on
the internet to fight spam by preventing bots from automatically registering for
accounts. In this setting a CAPTCHA is sent to the user as a challenge, while
the secret solution is used to grade the user’s answer. The implicit assumption is
that the answer and the random bits used to generate the puzzle remain hidden
— otherwise a spam bot could simply regenerate the puzzle and the answer.
While this assumption may be reasonable in the spam bot setting, it does not
hold in our offline password attack setting in which the server has already been
breached. A HOSP is different from a CAPTCHA in several key ways: (1)
The challenge must remain difficult for a computer to solve even if the random
bits used to generate the puzzle are made public. (2) There is no single correct
answer to a HOSP. It is okay if different people give different responses to a
challenge as long as people can respond to the challenges easily, and each user
can consistently answer the challenges.
The only HOSP construction proposed in [14] involved stuffing a hard drive
with unsolved CAPTCHAs. The problem of finding a HOSP construction that
does not rely on a dataset of unsolved CAPTCHAs was left as an open problem
[14]. Several other candidate HOSP constructions have been experimentally
evaluated in subsequent work [15] (they are called POSHs in the second paper),
but the usability results for every scheme that did not rely on a large dataset
on unsolved CAPTCHAs were underwhelming.
GOTCHAs are very similar to HOSPs. The basic application — defending
against offline dictionary attacks — is the same as are the key tools: exploit-
ing the power of interaction during authentication, exploiting hard artificial
intelligence problems. While the authentication with HOSPs is interactive, the
initial generation of the puzzle is not. By contrast, our GOTCHA construction
requires human interaction during the initial generation of the puzzle. This
simple relaxation allows for the construction of new solutions. In the HOSP pa-
per humans are simply modeled as a puzzle solving oracle, and the adversary is
assumed to have a limited number of queries to a human oracle. We introduce a
more intricate model of the human agent with the goal of designing more usable
constructions.
Password Storage Password storage is an incredibly challenging problem.
Adversaries have been able to compromise servers at many large companies (e.g.,
Zappos, LinkedIn, Sony, Gawker [5, 2, 9, 4, 1, 3]). For example, hackers were
able to obtain 32 million plaintext passwords from RockYou using a simple SQL
injection attack [1]. While it is considered an extremely poor security practice
5
to store passwords in the clear [41], the practice is still fairly common [12, 3, 1].
Many other companies [4, 12] have used cryptographic hashes to store their
passwords, but failed to adopt the practice of salting (e.g., instead of storing
the cryptographic hash of the password h(pw) the server stores (h (pw, r) , r)
for a random string r [6]) to defend against rainbow table attacks. Rainbow
tables, which consist of precomputed hashes, are often used by an adversary to
significantly speed up a password cracking attack because the same table can
be reused to attack each user when the passwords are unsalted [33].
Cryptographic hash functions like SHA1, SHA2 and MD5 — designed for
fast hardware computation — are popular choices for password hashing. Unfor-
tunately, this allows an adversary to try up to 250 million guesses per second on
a modern GPU [49]. The BCRYPT [35] hash function was designed specifically
with passwords in mind — BCRYPT was intentionally designed to be slow to
compute (e.g., to limit the power of an adversary’s offline attack). The BCRYPT
hash function takes a parameter which allows the programmer to specify how
costly the hash computation should be. The downside to this approach is that
it also increases costs for the company that stores the passwords (e.g., if we
want it to cost the adversary $1,000 for every million guesses then it will also
cost the company at least $1,000 for every million login attempts).
Users are often advised (or required) to follow strict guidelines when selecting
their password (e.g., use a mix of upper/lower case letters, include numbers and
change the password frequently) [38]. However, empirical studies show that
user’s are are often frustrated by restricting policies and commonly forget their
passwords [28, 29, 20] 3. Furthermore, the cost of these restrictive policies can
be quite high. For example, a Gartner case study [47] estimated that it cost
over $17 per password-reset call. Florencio and Herley [21] studied the economic
factors that institutions consider before adopting password policies and found
that they often value usability over security.
2 Definitions
In this section we seek to establish a theoretical basis for GOTCHAs. Several
of the ideas behind our definitions are borrowed from theoretical definitions of
CAPTCHAs [43] and HOSPs [14]. Like CAPTCHAs and HOSPs, GOTCHAs
are based on the assumption that some AI problem is hard for a computer
to solve, but easy for a person to solve. Ultimately, these assumptions are
almost certainly false (e.g., because the human brain can solve a GOTCHA
it is reasonable to believe that there exists a computer program to solve the
problems). However, it may still be reasonable to assume that these problems
cannot be solved by applying known ideas. By providing a formal definition of
GOTCHAs we can determine whether or not a new idea can be used to break
a candidate GOTCHA construction.
We use c ∈ C to denote the space of challenges that might be generated. We
use H to denote the set of human users and H (c, σt) to denote the response
3In fact the resulting passwords are sometimes more vulnerable to an offline attack! [28, 29]
6
that a human H ∈ H gives to the challenge c ∈ C at time t. Here, σt denotes
the state of the human’s brain at time t. σt is supposed to encode our user’s
existing knowledge (e.g., vocabulary, experiences) as well as the user’s mental
state at time t (e.g., what is the user thinking about at time t). Because σt
changes over time (e.g., new experiences) we use H (c) = {H (c, σt) t ∈ N} to
denote the set of all answers a human might give to a challenge c. We use A
to denote the range of possible responses (answers) that a human might give to
the challenges.
Definition 1 Given a metric d : A × A → R, we say that a human H can
consistently solve a challenge c ∈ C with accuracy α if ∀t ∈ N
d (H (c, σ0) , H (c, σt)) ≤ α ,
where σ0 denotes the state of the human’s brain when he initially answers the
challenge. If |H (c)| = 1 then we simply say that the human can consistently
solve the challenge.
Notation: When we have a group of challenges 〈c1, . . . , ck〉 we will sometimes
write H (〈c1, . . . , ck〉, σt) =
〈H (c1, σt) , . . . , H (ck, σt)〉 for notational convenience. We use y ∼ D to denote
a random sample from the distribution D, and we use r
$
← {0, 1}n to denote a
element drawn from the set {0, 1}n uniformly at random.
One of the requirements of a HOSP puzzle system [14] is that the human H
must be able to consistently answer any challenge that is generated (e.g., ∀c ∈ C,
H can consistently solve c). These requirements seem to rule out promising ideas
for HOSP constructions like Inkblots[15]. In this construction the challenge
is a randomly generated inkblot image I, and the response H (I, σ0) is word
or phrase describing what the user initially sees in the inkblot image (e.g.,
evil clown, soldier, big lady with a ponytail). User studies have shown that
H (I, σ0) does not always match H (I, σt) — the phrase describing what the
user sees at time t [15]. In a few cases the errors may be correctable (e.g.,
capitalization, plural/singular form of a word), but oftentimes the phrase was
completely different — especially if a long time passed in between trials 4. By
contrast, our GOTCHA construction does not require the user to remember
the phrases associated with each Inkblot. Instead we rely on a much weaker
assumption — the user can consistently recognize his solutions. We say that a
human can recognize his solutions to a set of challenges if he can consistently
solve a matching challenge (definition 2) in which he is asked to match each of
his solutions with the corresponding challenge.
Definition 2 Given an integer k, and a permutation π : [k]→ [k], a matching
challenge cˆpi = (~c,~a) ∈ C of size k is given by a k-tuple of challenges ~c =
4We would add the requirement that the human must be able to consistently answer the
challenges without spending time memorizing and rehearsing his response to the challenge.
Otherwise we could just as easily force the user to remember a random string to append on
to his password.
7
〈cpi(1), . . . , cpi(k)〉 ∈ C
k and solutions ~a = H (〈c1, . . . , ck〉, σ0). The response to a
matching challenge is a permutation π′ = H (~cpi, σt).
For permutations π : [k]→ [k] we use the distance metric
dk (π1, π2) = |{i π1(i) 6= π2(i) ∧ 1 ≤ i ≤ k}| .
dk (π1, π2) simply counts the number of entries where the permutations don’t
match. We say that a human can consistently recognize his solution to a
matching challenge cˆpi with accuracy α if ∀t.dk (H (cˆpi, σt) , π) ≤ α. We use
{π′ dk (π, π
′) ≤ α} to denote the set of permutations π′ that are α-close to π.
The puzzle generation process for a GOTCHA involves interaction between
the human and a computer: (1) The computer generates a set of k challenges.
(2) The human solves these challenges. (3) The computer uses the solutions to
produce a final challenge 5. Formally,
Definition 3 A puzzle-system is a pair (G1, G2), where G1 is a randomized
challenge generator that takes as input 1k (with k security parameter) and a pair
of random bit strings r1, r2 ∈ {0, 1}
∗ and outputs k challenges 〈c1, . . . , ck〉 ←
G1
(
1k, r1, r2
)
. G2 is a randomized challenge generator that takes as input 1
k
(security parameter), a random bit string r1 ∈ {0, 1}
∗, and proposed answers
~a = 〈a1, ..., ak〉 to the challenges G1
(
1k, r1, r2
)
and outputs a challenge
cˆ← G2
(
1k, r1,~a
)
. We say that the puzzle-system is (α, β)-usable if
Pr
H
$
←H
[Accurate (H, cˆ, α)] ≥ β ,
whenever ~a = H
(
G1
(
1k, r1, r2
)
, σ0
)
, where Accurate (H, cˆ, α) denotes the
event that the human H can consistently solve cˆ with accuracy α.
In our authentication setting the random string r1 is extracted from the
user’s password using a strong pseudorandom function Extract. To provide
a concrete example of a puzzle-system, G1 could be a program that generates
a set of inkblot challenges 〈I1, . . . , Ik〉 using random bits r1, selects a random
permutation π : [k] → [k] using random bits r2, and returns 〈Ipi(1), . . . , Ipi(k)〉.
The human’s response to an Inkblot —H (Ij , σ0) — is whatever he/she imagines
when he sees the inkblot Ij for the first time (e.g., some people might imagine an
evil clown when they look at figure 1). Finally, G2 might generate Inkblots ~c =
〈I1, . . . , Ik〉 using random bits r1, and return the matching challenge cˆpi = (~c,~a).
In this case the matching challenge is for the user to match his labels with the
appropriate Inkblot images to recover the permutation π. Observe that the final
challenge — cˆpi — can only be generated after a round of interaction between
the computer and a human. By contrast, the challenges in a HOSP must be
generated automatically by a computer. Also notice that if G2 is executed with
5We note that a HOSP puzzle system (G) [14] can be modeled as a GOTCHA puzzle
system (G1, G2) where G1 does nothing and G2 simply runs G to generate the final challenge
cˆ directly.
8
a different random bit string r′1 then we do not require the resulting challenge to
be consistently recognizable (e.g., if the user enters in the wrong password then
authentication will fail regardless of how he solves the resulting challenge). For
example, if the user enters the wrong password the user might be asked to match
his labels 〈ℓpi(1), ..., ℓpi(k)〉 = H
(
〈Ipi(1), . . . , Ipi(k)〉, σ0
)
with Inkblots 〈I ′1, . . . , I
′
k〉
that he has never seen.
An adversary could attack a puzzle system by either (1) attempting to dis-
tinguish between the correct puzzle, and puzzles that might be meaningless to
the human, or (2) by solving the matching challenge directly.
We say that an algorithm A can distinguish distributions D1 and D2 with
advantage ǫ if ∣∣∣∣ Prx∼D1 [A (x) = 1]− Pry∼D2 [A (y) = 1]
∣∣∣∣ ≥ ǫ .
Our formal definition of a GOTCHA is found in definition 4. Intuitively,
definition 4 says that (1) The underlying puzzle-system should be usable —
so that legitimate users can authenticate. (2) It should be difficult for the
adversary to distinguish between the correct matching challenge (e.g., the one
that the user will see when he types in the correct password), and an incorrect
matching challenge (e.g., if the user enters the wrong password he will be asked
to match his labels with different Inkblot images), and (3) It should be difficult
for the adversary to distinguish between the user’s matching, and a random
matching drawn from a distribution R with sufficiently high minimum entropy.
Definition 4 A puzzle-system (G1, G2) is an (α, β, ǫ, δ, µ)-GOTCHA if (1) (G1, G2)
is (α, β)-usable (2) Given a human H ∈ H no probabilistic polynomial time al-
gorithm can distinguish between distributions
D1 =
{
H(G1(1k,r1,r2),σ0),
G2(1k,r1,H(G1(1k,r1,r2),σ0))
r1, r2
$
← {0, 1}n
}
and
D2 =
{
H(G1(1k,r1,r2),σ0),
G2(1k,r3,H(G1(1k,r1,r2),σ0))
r1, r2, r3
$
← {0, 1}n
}
with advantage greater than ǫ, and (3) Given a human H ∈ H, there is a
distribution R(c) with µ(m) bits of minimum entropy such that no probabilistic
polynomial time algorithm can distinguish between distributions
D3 =
{
H(G1(1k,r1,r2),σ0)
G2(1k,r1,H(G1(1k,r1,r2),σ0)),
H(G2(1k,r1,H(G1(1k,r1,r2),σ0)),σ0)
r1, r2
$
← {0, 1}n
}
and
D4 =
{
H(G1(1k,r1,r2),σ0)
G2(1k,r1,H(G1(1k,r1,r2),σ0)),
R(G2(1
m,r1,〈a1,...,am〉),σ0)
r1, r2
$
← {0, 1}n
}
with advantage greater then δ.
9
2.1 Password Storage and Offline Attacks
To protect users in the event of a server breach organizations are advised to store
salted password hashes — using a cryptographic hash function (h : {0, 1}∗ →
{0, 1}n) and a random bit string (s ∈ {0, 1}∗) [38]. For example, if a user (u)
chose the password (pw) the server would store the tuple (u, s, h (s, pw)). Any
adversary who has obtained (u, s, h (s, pw)) (e.g., through a server breach) may
mount a — fully automated — offline dictionary attack using powerful password
crackers like John the Ripper [17]. To verify a guess pw′ the adversary simply
computes h (s, pw′) and checks to see if this hash matches h (s, pw).
We assume that an adversary Adv who breaches the server can obtain the
code for h, as well as the code for any GOTCHAs used in the authentication
protocol. Given the code for h and the salt value s the adversary can construct
a function
VerifyHash (pw′) =
{
1 if h (s, pw) = h (s, pw′)
0 otherwise.
.
We also allow the adversary to have black box access to a GOTCHA solver
(e.g., a human). We use cH to denote the cost of querying a human and ch to
denote the cost of querying the function VerifyHash6, and we use nH (resp.
nh) to denote the number of queries to the human (resp. VerifyHash). Queries
to the human GOTCHA solver are much more expensive than queries to the
cryptographic hash function (cH ≫ ch) [31]. For technical reasons we limit our
analysis to conservative adversaries.
Definition 5 We say that an adversary Adv is conservative if (1) Adv uses
the cryptographic hash function h in a black box manner (e.g., the hash function
h and the stored hash value are only used to construct a subroutine VerifyHash
which is then used as a black box by Adv ), (2) The pseudorandom function
Extract is used as a black box, and (3) The adversary only queries a human
about challenges generated using a password guess.
It is reasonable to believe that our adversary is conservative. All existing pass-
word crackers (e.g., [17]) use the hash function as a black box, and it is difficult
to imagine that the adversary would benefit by querying a human solver about
Inkblots that are unrelated to the password.
We useD ⊆ {0, 1}∗ to denote a dictionary of likely guesses that the adversary
would like to try,
Cost (Adv, D) = (nhch + nHcH)
to denote the cost of the queries that the adversary makes to check each guess
in D, and Succeed (Adv, D, pw) to denote the event that the adversary makes
a query to VerifyHash that returns 1 (e.g., the adversary successfully finds
the user’s password pw). The adversary might use a computer program to try
6The value of ch may vary widely depending on the particular cryptographic hash function
— it is inexpensive to evaluate SHA1, but BCRYPT [35] may be very expensive to evaluate.
10
to solve some of the GOTCHAs — to save cost by not querying a human.
However, in this case the adversary might fail to crack the password because
the GOTCHA solver found the wrong solution to one of the challenges.
Definition 6 An adversary Adv is (C, γ,D)-successful if Cost (Adv, D) ≤ C,
and
Pr
pw
$
←D
[Succeed (Adv, D, pw)] ≥ γ .
Our attack model is slightly different from the attack model in [14]. They
assume that the adversary may ask a limited number of queries to a human
challenge solution oracle. Instead we adopt an economic model similar to [10],
and assume that the adversary is instead limited by a budget C, which may be
used to either evaluate the cryptographic hash function h or query a human H .
3 Inkblot Construction
Our candidate GOTCHA construction is based on Inkblots images. We use al-
gorithm 1 to generate inkblot images. Algorithm 1 takes as input random bits r1
and a security parameter k — which specifies the number of Inkblots to output.
Algorithm 1 makes use of the randomized subroutineDrawRandomEllipsePairs (I, t, width, height)
which draws t pairs of ellipses on the image I with the specified width and height.
The first ellipse in each pair is drawn at a random (x, y) coordinate on the left
half of the image with a randomly selected color and angle α of rotation, and
the second ellipse is mirrored on the right half of the image. Figure 1 is an
example of an Inkblot image generated by algorithm 1.
Algorithm 1 GenerateInkblotImages
Input: Security Parameter 1k, Random bit string r1 ∈ {0, 1}
∗.
for j = 1, . . . , k do
Ij ← new Blank Image ⊲ The following operations only use the random
bit string r1 as a source of randomness
DrawRandomEllipsePairs (Ij , 150, 60, 60)
DrawRandomEllipsePairs (Ij , 70, 20, 20)
DrawRandomEllipsePairs (Ij , 150, 60, 20)
return 〈I1, . . . , Ik〉 ⊲ Inkblot Images
Our candidate GOTCHA is given by the pair (G1, G2) — algorithms 2 and
3. G1 runs algorithm 1 to generate k Inkblot images, and then returns these
images in permuted order — using a function
GenerateRandomPermutation (k, r), which generates a random permuta-
tion π : [k] → [k] using random bits r. G2 also runs algorithm 1 to generate k
Inkblot images, and then outputs a matching challenge.
After the Inkblots 〈Ipi(1), . . . , Ipi(k)〉 have been generated, the human user is
queried to provide labels ℓpi(1), . . . , ℓpi(k) where
〈ℓpi(1), . . . , ℓpi(k)〉 = H
(
〈Ipi(1), . . . , Ipi(k)〉, σ0
)
.
11
Algorithm 2 G1
Input: Security Parameter 1k, Random bit strings r1, r2 ∈ {0, 1}
∗.
〈I1, . . . , Ik〉 ← GenerateInkblotImages (k, r1)
π ← GenerateRandomPermutation (k, r2)
return 〈Ipi(1), . . . , Ipi(k)〉
In our authentication setting the server would store the labels ℓpi(1), . . . , ℓpi(k) in
permuted order. The final challenge — generated by algorithm 3 — is to match
the Inkblot images I1, . . . , Ik with the user generated labels ℓ1, ..., ℓk to recover
the permutation π.
Algorithm 3 GenerateMatchingChallenge G2
Input: Security Parameter 1k, Random bits r1 ∈ {0, 1}
∗ and labels ~a =
〈ℓpi(1), . . . , ℓpi(k)〉.
〈I1, . . . , Ik〉 ← GenerateInkblotImages
(
1k, r1
)
return cˆpi = (~c,~a) ⊲ Matching Challenge
Observation: Notice that if the random bits provided as input toGenerateInkblotImages
and
GenerateMatchingChallenge match that the user will see the same Inkblot
images in the final matching challenge. However, if the random bits do not
match (e.g., because the user typed the wrong password in our authentication
protocol) then the user will see different Inkblot images. The labels ℓ1, . . . , ℓk
will be the same in both cases.
3.1 GOTCHA Authentication
To illustrate how our GOTCHAs can be used to defend against offline attacks
we present the following authentication protocols: Create Account (pro-
tocol 3.1) and Authenticate (protocol 3.2). Communication in both proto-
cols should take place over a secure channel. Both protocols involve several
rounds of interaction between the user and the server. To create a new ac-
count the user sends his username/password to the server, the server responds
by generating k Inkblot images I1, . . . , Ik, and the user provides a response
〈ℓ1, . . . , ℓk〉 = H (〈I1, . . . , Ik〉, σ0) based on his mental state at the time — the
server stores these labels in permuted order ℓpi(1), . . . , ℓpi(k)
7. To authenticate
later the user will have to match these labels with the corresponding inkblot
images to recover the permutation π.
In section 4 we argue that the adversary who wishes to mount a cost effective
offline attack needs to obtain constant feedback from a human. Following [14]
7For a general GOTCHA, protocol 3.1 would need to have an extra round of communica-
tion. The server would send the user the final challenge generated by G2 and the user would
respond with H (G2 (, ) , σ0). Protocol 3.1 takes advantage of the fact that pi = H (G2 (, ) , σ0)
is already known.
12
we assume that the function Extract : {0, 1}∗ → {0, 1}n is a strong randomness
extractor, which can be used to extract random strings from the user’s password.
Recall that h : {0, 1}∗ → {0, 1}∗ denotes a cryptographic hash function.
Protocol 3.1: Create Account
Security Parameters: k, n.
(User): Select username (u) and password (pw) and send (u, pw) to the
server.
(Server): Sends Inkblots 〈I1, . . . , Ik〉 to the user where:
r′
$
← {0, 1}n, r1 ← Extract (pw, r
′), r2
$
← {0, 1}n and
〈I1, . . . , Ik〉 ← GenerateInkblotImages
(
1k, r1
)
(User): Sends responses 〈ℓ1, ..., ℓk〉 back to the server where:
〈ℓ1, . . . , ℓk〉 ← H (〈I1, . . . , Ik〉, σ0).
(Server): Store the tuple t where t is computed as follows:
Salt: s
$
← {0, 1}n
π ← GenerateRandomPermutation (k, r2).
hpw ← h (u, s, pw, π(1), ..., π(k))
t←
(
u, r′, s, hpw, ℓpi(1), . . . , ℓpi(k)
)
Protocol 3.2: Authenticate
Security Parameters: k, n.
Usability Parameter: α
(User): Send username (u) and password (pw′) — pw′ may or may not be
correct.
(Server): Sends challenge cˆ to the user where cˆ is computed as follows:
Find t =
(
u, r′, s, hpw, ℓpi(1), . . . , ℓpi(k)
)
r′1 ← Extract (pw
′, r′)
〈I ′1, ..., I
′
k〉 ← GenerateInkblotImages (r
′
1, k)
cˆpi ←
(
〈I1, ..., Ik〉, 〈ℓpi(1), . . . , ℓpi(k)〉
)
(User): Solves cˆpi and sends the answer π
′ = H (cˆ, σt).
(Server):
for all π0 s.t dk (π0, π
′) ≤ α do
hpw,0 ← h (u, s, pw
′, π0(1), ..., π0(k))
if hpw,0 = hpw then
Authenticate
Deny
Our protocol could be updated to allow the user to reject challenges he
found confusing during account creation in protocol 3.1. In this case the server
would simply note that the first GOTCHA was confusing and generate a new
GOTCHA. Once our user has created an account he can login by following
protocol 3.2.
13
Claim 1 says that a legitimate user can successfully authenticate if our
Inkblot construction satisfies the usability requirements of a GOTCHA. The
proof of claim 1 can be found in appendix A.
Claim 1 If (G1, G2) is a (α, β, ǫ, δ, µ)-GOTCHA then at least β-fraction of
humans can successfully authenticate using protocol 3.2 after creating an account
using protocol 3.1.
One way to improve usability of our authentication protocol is to increase the
neighborhood of acceptably close matchings by increasing α. The disadvantage
is that the running time for the server in protocol 3.2 increases with the size of
α. Claim 2 bounds the time needed to enumerate over all close permuations.
The proof of claim 2 can be found in appendix A.
Claim 2 For all permutations π : [k]→ [k] and α ≥ 0
|{π′ dk (π, π
′) ≤ α}| ≤ 1 +
α∑
i=2
(
k
i
)
i! .
For example, if the user matches k = 10 Inkblots and we want to accept
matchings that are off by at most α = 5 entries then the server would need
to enumerate over at most 36, 091 permutations8. Organizations are already
advised to use password hash functions like BCRYPT [35] which intentionally
designed to be slower than standard cryptographic hash functions — often by
a factor of millions. Instead of making the hash function a million times slower
to evaluate the server might instead make the hash function a thousand times
slower to evaluate and use these extra computation cycles to enumerate over
close permutations. The organization’s trade-off is between: security, usability
and the resources that it needs to invest during the authentication process.
We observe that an adversary mounting an online attack would be naturally
rate limited because he would need to solve a GOTCHA for each new guess.
Protocol 3.2 could also be supplemented with a k-strikes policy — in which a
user is locked out for several hours after k incorrect login attempts — if desired.
3.2 User Study
To test our candidate GOTCHA construction we conducted an online user
study9. We recruited participants through Amazon’s Mechanical Turk to par-
ticipate in our study. The study was conducted in two phases. In phase 1 we
generated ten random Inkblot images for each participant, and asked each par-
ticipant to provide labels for their Inkblot images. Participants were advised
to use creative titles (e.g., evil clown, frog, lady with poofy dress) because they
would not need to remember the exact titles that they used. Participants were
paid $1 for completing this first phase. A total of 70 users completed phase 1.
8A more precise calculation reveals that there are exactly 13, 264 permutations s.t.
d10 (pi′, pi) ≤ 5 and a random permuation pi′ would only be accepted with probability
3.66× 10−3
9Our study protocol was approved for exemption by the Institutional Review Board (IRB)
at Carnegie Mellon University (IRB Protocol Number: HS13-219).
14
Figure 2: Phase 1
After our participants completed the first phase we waited ten days before
asking our participants to return and complete phase 2. During phase 2 we
showed each participant the Inkblot images they saw in phase 1 (in a random
order) as well as the titles that they created during phase 1 (in alphabetical
order). Participants were asked to match the labels with the appropriate image.
The purpose of the longer waiting time was to make sure that participants
had time to forget their images and their labels. Participants were paid an
additional $1 for completing phase 2 of the user study. At the beginning of the
user study we let participants know that they would be paid during phase 2
even if their answers were not correct. We adopted this policy to discourage
cheating (e.g., using screen captures from phase 1 to match the images and the
labels) and avoid positively biasing our results.
We measured the time it took each participant to complete phase 1. Our
results are summarized in table 1. It is quite likely that some participants left
15
Figure 3: Phase 2
Phase 1 Phase 2
Average 9.3 4.5
StdDev 9.6 3
Max 57.5 18.5
Min 1.4 1.6
Average ≤ 20 6.2 N/A
Table 1: Completion Times
their computer in the middle of the study and returned later to complete the
study (e.g., one user took 57.5 minutes to complete the study). While we could
not measure time away from the computer, we believe that it is likely that at
least 9 of our participants left the computer. Restricting our attention to the
other 61 participants who took at most 20 minutes we get an adjusted average
completion time of 6.2 minutes.
Fifty-eight of our participants returned to complete phase 2 by taking our
matching test. It took these participants 4.5 minutes on average to complete the
matching test. Seventeen of our participants correctly matched all ten of their
labels, and 69% of participants matched at least 5 out of ten labels correctly.
Our results are summarized in table 2.
Discussion Our user study provides evidence that our construction is at least
(0, 0.29)-usable or (5, 0.69)-usable. While this means that our Inkblot Matching
GOTCHA could be used by a significant fraction of the population to pro-
16
α-accurate # partici-
pants
# participants
58
|{pi′ d10(pi,pi′)≤α}|
10!
α = 0 17 0.29 2.76× 10−7
α = 2 22 0.38 1.27× 10−5
α = 3 26 0.45 7.88× 10−5
α = 4 34 0.59 6.00× 10−4
α = 5 40 0.69 3.66× 10−3
Table 2: Usability Results: Fraction of Participants who would have authenti-
cated with accuracy parameter α
tect their passwords during authentication it also means that the use of our
GOTCHA would have to be voluntary so that users who have difficulty won’t
get locked out of their accounts. Another approach would be to construct dif-
ferent GOTCHAs and allow users to choose which GOTCHA to use during
authentication.
Study Incentives: There is evidence that the lack of monetary incentives
to perform well on our matching test may have negatively influenced the results
(e.g., some participants may have rushed through phase 1 of the study because
their payment in round 2 was independent of their ability to match their labels
correctly). For example, none of our 18 fastest participants during phase 1
matched all of their labels correctly, and — excluding participants we believe
left their computer during phase 1 (e.g., took longer than 20 minutes) — on
average participants who failed to match at least five labels correctly took 2
minutes less time to complete phase 1 than participants who did.
Time: We imagine that some web services may be reluctant to adopt
GOTCHAs out of fear driving away customers who don’t want to spend time
labeling Inkblot images [21]. However, we believe that for many high security
applications (e.g., online banking) the extra security benefits of GOTCHAs will
outweigh the costs — GOTCHAs might even help a bank keep its customers by
providing extra assurance that users’ passwords are secure. We are looking at
modifying our Inkblot generation algorithm to produce Inkblots which require
less “mental effort” to label. In particular could techniques like Perlin Noise
[34] be used to generate Inkblots that can be labeled more quickly and matched
more accurately?
Accuracy: We believe that the usability of our Inkblot Matching GOTCHA
construction can still be improved. One simple way to improve the usability
of our GOTCHA construction would be to allow the user to reject Inkblot
images that were confusing. We also believe that usability could be improved
by providing users with specific strategies for creating their labels (e.g., we
found that simple labels like “a voodoo mask” were often mismatched, while
more elaborate stories like “A happy guy on the ground, protecting himself
from ticklers” were rarely mismatched).
17
3.3 An Open Challenge to the AI Community
We envision a rich interaction between the security community and the artifi-
cial intelligence community. To facilitate this interaction we present an open
challenge to break our GOTCHA scheme.
Challenge Setup We chose several random passwords
(pw1, ..., pw4)
$
← {0, 107} and pw5
$
← {0, 108}. We used a functionGenerateInkblots (pwi, 10)
to generate ten inkblots Ii1, ..., I
i
10 for each password, and we had a human label
each inkblot image 〈ℓi1, . . . , ℓ
i
10〉 ← H
(
〈Ii1, . . . , I
i
10〉, σ0
)
. We selected a random
permutation πi : [10]→ [10] for each account, and generated the tuple
Ti =
(
si, h (pwi, si, πi(1), ..., πi(10)) , ℓ
i
pii(1)
, ..., ℓipii(10)
)
,
where si is a randomly selected salt value and h is a cryptographic hash function.
We are releasing the source code that we used to generate the Inkblots and
evaluate the hash function h along with the tuples T1, ..., T5 — see
http://www.cs.cmu.edu/~jblocki/GOTCHA-Challenge.html.
Challenge: Recover each password pwi.
Approaches One way to accomplish this goal would be to enumerate over ev-
ery possible password guess pw′i and evaluate h (pw
′
i, si, π(1), ..., π(10)) for every
possible permutation π : [10] → [10]. However, the goal of this challenge is to
see if AI techniques can be applied to attack our GOTCHA construction. We
intentionally selected our passwords from a smaller space to make the challenge
more tractable for AI based attacks, but to discourage participants from try-
ing to brute force over all password/permutation pairs we used BCRYPT (Level
15)10 — an expensive hash function — to encrypt the passwords. Our implemen-
tation allows the Inkblot images to be generated very quickly from a password
guess pw’ so an AI program that can use the labels in the password file to dis-
tinguish between the correct Inkblots returned by GenerateInkblots (pwi, 10)
and incorrect Inkblots returned by GenerateInkblots (pw′i, 10) would be able
to quickly dismiss incorrect guesses. Similarly, an AI program which generates a
small set of likely permutations for each password guess could allow an attacker
to quickly dismiss incorrect guesses.
4 Analysis: Cost of Offline Attacks
In this section we argue that our password scheme (protocols 3.2 and 3.1) sig-
nificantly mitigates the threat of offline attacks. An informal interpretation of
our main technical result — Theorem 1 — is that either (1) the adversary’s
10The level parameter specifies the computation complexity of hashing. The amount of
work necessary to evaluate the BCRYPT hash function increases exponentially with the level
so in our case the work increases by a factor of 215.
18
offline attack is prohibitively expensive (2) there is a good chance that adver-
sary’s offline attack will fail, or (3) the underlying GOTCHA construction can
be broken. Observe that the security guarantees are still meaningful even if the
security parameters ǫ and δ are not negligably small.
Theorem 1 Suppose that our user selects his password uniformly at random
from a set D (e.g., pw
$
← D) and creates his account using protocol 3.1. If algo-
rithms 2 and 3 are an (ǫ, δ, µ)-GOTCHA then no conservative offline adversary
is
(
C, γ + ǫ+ δ + nH|D| , D
)
-successful for C < γ|D|2µ(k)ch + nHcH
Proof of Theorem 1. (Sketch) We use a hybrid argument. An adversary who
breaches the server is able to recover the tuple t =
(
u, r′, s, h (u, s, pw, π(1), . . . , π(k)) , ℓpi(1), . . . , ℓpi(k)
)
as well as the code for the cryptographic hash function h and the code for our
GOTCHA — (G1, G2).
1. World 0: W0 denotes the real world in which the adversary has recovered
the tuple
t0 =
(
u, r′, s, h (u, s, pw, π(1), . . . , π(k)) , ℓpi(1), . . . , ℓpi(k)
)
as well as the code for the cryptographic hash function h and the code for
our GOTCHA — (G1, G2). Because the adversary Adv is conservative it
constructs the function
VerifyHash (pw′, π′) =
{
1 if pw′ = pw and π′ = π
0 otherwise.
,
and usesVerifyHash as a blackbox. We say thatAdv queries a humanH
about password pw′ if it queriesH forH
(
GenerateInkblotImages
(
1k,Extract (pw′, r′)
))
,
and we let D′ ⊆ D denote the set of passwords for which the adversary
queries a human.
2. World 1: W1 denotes a hypothetical world that is similar to W0 except
that VerifyHash function the adversary uses as a blackbox is replaced
with the following incorrect version
VerifyHash1 (pw′, π′) = {
1 if pw′ /∈ D′, pw′ = pw and π′ = π
0 otherwise.
,
where D′ ⊆ D is a subset of passwords which denotes the set of passwords
for which the adversary makes queries to a human in the real world.
3. World 2: W2 denotes a hypothetical world that is similar to W1 except
that VerifyHash1 function the adversary uses as a blackbox is replaced
19
with the following incorrect version
VerifyHash2 (pw′, π′) =

1 if π′ = R
(
G2
(
1k,Extract (pw′, r′) , ℓ1, . . . , ℓk
))
and pw′ /∈ D′, pw′ = pw
0 otherwise.
,
where R is a distribution with minimum entropy µ(k) as in definition 4.
4. World 3: W3 denotes a hypothetical real world which is similar to world 2,
except that the labels ℓpi(1), . . . , ℓpi(k) are replaced with the labels ℓ
′
pi′(1), . . . , ℓ
′
pi′(k),
where π′ : [k] → [k] is a new random permutation, and the labels ℓ′i are
for a completely unrelated set of Inkblot challenges
ℓ′1, . . . , ℓk ← H
(
G1
(
1k, x1, x2
))
,
where x1, x2 ∈ {0, 1}
n are freshly chosen random value.
In world 3 it is easy to bound the adversary’s probability of success. No ad-
versary is (C, γ,D)-successful for C < γ|D|2µ(k)ch, because the fake Inkblot
labels are not correlated with the actual Inblots that were generated with
the real password. Our particular advesary cannot be (C, γ,D)-successful for
C < γ|D|2µ(k)ch + |D
′|cH . In world 2 the adversary might improve his chances
of success by looking at the Inblot labels, but by definition of (α, β, ǫ, δ, µ)-
GOTCHA his chances change by at most δ. In world 1 the adversary might fur-
ther improve his chances of success, but by definition of (α, β, ǫ, δ, µ)-GOTCHA
his chances improve by at most ǫ. Finally, in world 0 the adversary improves
his chances by at most |D′|/|D| by querying the human about passwords in D′.

5 Discussion
We conclude by discussing some key directions for future work.
Other GOTCHAConstructions Because GOTCHAs allow for human feed-
back during puzzle generation — unlike HOSPs [14] — our definition potentially
opens up a much wider space of potential GOTCHA constructions. One idea
might be to have a user rate/rank random items (e.g., movies, activities, foods).
By allowing human feedback we could allow the user to dismiss potentially con-
fusing items (e.g., movies he hasn’t seen, foods about which he has no strong
opinion). There is some evidence that this approach could provide security (e.g.,
Narayanan and Shmatikov showed that a Netflix user can often be uniquely
identified from a few movie ratings [32].).
20
Obfuscating CAPTCHAs If it were possible to efficiently obfuscate pro-
grams then it would be easy to construct GOTCHAs from CAPTCHAs (e.g.,
just obfuscate a program that returns the CAPTCHA without the answer).
Unfortunately, there is no general program obsfuscator [8]. However, the ap-
proach may not be entirely hopeless. Point functions [46] can be obfuscated,
and our application is similar to a point function — the puzzle generator G2
in an GOTCHA only needs to generate a human solvable puzzle for one input.
Recently, multilinear maps have been used to obfuscate conjunctions [13] and
to obfuscate NC1 circuits [23] 11. Could similar techniques be used obfuscate
CAPTCHAs?
Exploiting The Power of Interaction Can interaction be exploited and
used to improve security or usability in human-authentication? While inter-
action is an incredibly powerful tool in computer security (e.g., nonces [36],
zero-knowledge proofs [24], secure multiparty computation [48]) and in complex-
ity theory12, human authentication typically does not exploit interaction with
the human (e.g., the user simply enters his password). We view the idea behind
HOSPs and GOTCHAs — exploiting interaction to mitigate the threat of offline
attacks — as a positive step in this direction. Could interaction be exploited to
reduce memory burden on the user by allowing a user to reuse the same secret to
authenticate to multiple different servers? The human-authentication protocol
of Hopper, et al. [26] — based on the noisy parity problem — could be used by
a human to repeatedly authenticate over an insecure channel. Unfortunately,
the protocol is slow and tedious for a human to execute, and it can be broken
if the adversary is able to ask adaptive parity queries [30].
References
[1] Rockyou hack: From bad to worse.
http://techcrunch.com/2009/12/14/rockyou-hack-security-myspace-
facebook-passwords/, December 2009. Retrieved 9/27/2012.
[2] Update on playstation network/qriocity services.
http://blog.us.playstation.com/2011/04/22/update-on-playstation-
network-qriocity-services/, April 2011. Retrieved 5/22/2012.
[3] Data breach at ieee.org: 100k plaintext passwords. http://ieeelog.com/,
September 2012. Retrieved 9/27/2012.
11The later result used a weaker notion of obfuscation known as “indistinguishability obfus-
cation,” which (loosely) only guarantees that the adversary cannot distinguish between the
obfuscations of two circuits which compute the same function.
12A polynomial time verifier can verify PSPACE-complete languages by interacting with
a powerful prover [40], by contrast the same verifier can only check proofs of NP-Complete
languages without interaction.
21
[4] An update on linkedin member passwords compromised.
http://blog.linkedin.com/2012/06/06/linkedin-member-passwords-
compromised/, June 2012. Retrieved 9/27/2012.
[5] Zappos customer accounts breached. http://www.usatoday.com/tech/news/story/2012-
01-16/mark-smith-zappos-breach-tips/52593484/1, January 2012. Re-
trieved 5/22/2012.
[6] S. Alexander. Password protection for modern operating systems. ;login,
June 2004.
[7] A. Baddeley. Human memory: Theory and practice. Psychology Pr, 1997.
[8] B. Barak, O. Goldreich, R. Impagliazzo, S. Rudich, A. Sahai, S. Vadhan,
and K. Yang. On the (im) possibility of obfuscating programs. In Advances
in Cryptology-CRYPTO 2001, pages 1–18. Springer, 2001.
[9] S. Biddle. Anonymous leaks 90,000 military email accounts in lat-
est antisec attack. http://gizmodo.com/5820049/anonymous-leaks-90000-
military-email-accounts-in-latest-antisec-attack, July 2011. Retrieved
8/16/2011.
[10] J. Blocki, M. Blum, and A. Datta. Naturally rehearsing passwords. In
Advances in Cryptology-ASIACRYPT 2013 (to appear).
[11] J. Bonneau. The science of guessing: analyzing an anonymized corpus of
70 million passwords. In Proc. of Oakland, pages 538–552, 2012.
[12] J. Bonneau and S. Preibusch. The password thicket: technical and market
failures in human authentication on the web. In Proc. of WEIS, volume
2010, 2010.
[13] Z. Brakerski and G. N. Rothblum. Obfuscating conjunctions. In Advances
in Cryptology-CRYPTO 2013, pages 416–434. Springer, 2013.
[14] R. Canetti, S. Halevi, and M. Steiner. Mitigating dictionary attacks on
password-protected local storage. In Advances in Cryptology-CRYPTO
2006, pages 160–179. Springer, 2006.
[15] W. Daher and R. Canetti. Posh: A generalized captcha with security
applications. In Proceedings of the 1st ACM workshop on Workshop on
AISec, pages 1–10. ACM, 2008.
[16] M. Dailey and C. Namprempre. A text graphics character captcha for pass-
word authentication. In TENCON 2004. 2004 IEEE Region 10 Conference,
pages 45–48. IEEE, 2004.
[17] S. Designer. John the Ripper. http://www.openwall.com/john/, 1996-2010.
22
[18] K. Doel. Scary logins: Worst passwords of 2012 and how to fix them.
http://www.prweb.com/releases/2012/10/prweb10046001.htm, 2012. Re-
trieved 1/21/2013.
[19] J. Elson, J. R. Douceur, J. Howell, and J. Saul. Asirra: a captcha that
exploits interest-aligned manual image categorization. In Proc. of CCS.
[20] D. Florencio and C. Herley. A large-scale study of web password habits.
In Proceedings of the 16th international conference on World Wide Web,
pages 657–666. ACM, 2007.
[21] D. Floreˆncio and C. Herley. Where do security policies come from? In
Proceedings of the Sixth Symposium on Usable Privacy and Security, pages
1–14. ACM, 2010.
[22] M. Fossi, E. Johnson, D. Turner, T. Mack, J. Blackbird, D. McKinney,
M. K. Low, T. Adams, M. P. Laucht, and J. Gough. Symantec report on
the undergorund economy, November 2008. Retrieved 1/8/2013.
[23] S. Garg, C. Gentry, S. Halevi, M. Raykova, A. Sahai, and B. Waters. Can-
didate indistinguishability obfuscation and functional encryption for all cir-
cuits. In Proc. of FOCS (to appear), 2013.
[24] O. Goldreich, A. Sahai, and S. Vadhan. Can statistical zero knowledge
be made non-interactive? or on the relationship of SZK and NISZK. In
Advances in Cryptology-CRYPTO 1999, pages 467–484, 1999.
[25] D. Goodin. Why passwords have never been weaker and crackers have
never been stronger. http://arstechnica.com/security/2012/08/passwords-
under-assault/, 2012.
[26] N. J. Hopper and M. Blum. Secure human identification protocols. In
Advances in Cryptology-ASIACRYPT 2001, pages 52–66. Springer, 2001.
[27] Imperva. Consumer password worst practices. 2010. Retrived 1/22/2013.
[28] S. Komanduri, R. Shay, P. Kelley, M. Mazurek, L. Bauer, N. Christin,
L. Cranor, and S. Egelman. Of passwords and people: measuring the effect
of password-composition policies. In Proc. of CHI, pages 2595–2604, 2011.
[29] H. Kruger, T. Steyn, B. Medlin, and L. Drevin. An empirical assessment of
factors impeding effective password management. Journal of Information
Privacy and Security, 4(4):45–59, 2008.
[30] E. Kushilevitz and Y. Mansour. Learning decision trees using the Fourier
spectrum. SIAM J. Comput., 22(6):1331–1348, 1993.
[31] M. Motoyama, K. Levchenko, C. Kanich, D. McCoy, G. M. Voelker, and
S. Savage. Re: Captchas–understanding captcha-solving services in an eco-
nomic context. In USENIX Security Symposium, volume 10, 2010.
23
[32] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse
datasets. In Proc. of the 2008 IEEE Symposium on Security and Privacy,
pages 111–125. IEEE, 2008.
[33] P. Oechslin. Making a faster cryptanalytic time-memory trade-off. Ad-
vances in Cryptology-CRYPTO 2003, pages 617–630, 2003.
[34] K. Perlin. Implementing improved perlin noise. GPU Gems, pages 73–85,
2004.
[35] N. Provos and D. Mazieres. Bcrypt algorithm.
[36] P. Rogaway. Nonce-based symmetric encryption. In Fast Software Encryp-
tion, pages 348–358. Springer, 2004.
[37] G. Sauer, H. Hochheiser, J. Feng, and J. Lazar. Towards a universally
usable captcha. In Proceedings of the 4th Symposium on Usable Privacy
and Security, 2008.
[38] K. Scarfone and M. Souppaya. NIST special publication 800-118: Guide
to enterprise password management (draft), 2009.
[39] D. Seeley. Password cracking: A game of wits. Communications of the
ACM, 32(6):700–703, 1989.
[40] A. Shamir. Ip= pspace. Journal of the ACM (JACM), 39(4):869–877, 1992.
[41] A. Singer. No plaintext passwords. ;login: THE MAGAZINE OF USENIX
& SAGE, 26(7), November 2001. Retrieved 8/16/2011.
[42] A. Stubblefield and D. Simon. Inkblot authentication. Technical report,
2004.
[43] L. Von Ahn, M. Blum, N. Hopper, and J. Langford. Captcha: Using hard ai
problems for security. Advances in Cryptology-EUROCRYPT 2003, pages
646–646, 2003.
[44] L. Von Ahn, B. Maurer, C. McMillen, D. Abraham, and M. Blum. re-
captcha: Human-based character recognition via web security measures.
Science, 321(5895):1465–1468, 2008.
[45] M. J. Watkins and J. M. Gardiner. An appreciation of generate-recognize
theory of recall. Journal of Verbal Learning and Verbal Behavior, 18(6):687–
704, 1979.
[46] H. Wee. On obfuscating point functions. In Proc. of STOC, pages 523–532.
ACM, 2005.
[47] R. Witty, K. Brittain, and A. Allen. Justify identity management invest-
ment with metrics. Gartner Group report, 2004.
24
[48] A. C. Yao. Protocols for secure computations. In Proc. of FOCS, pages
160–164, 1982.
[49] A. Zonenberg. Distributed hash cracker: A cross-platform gpu-accelerated
password recovery system. Rensselaer Polytechnic Institute, page 27, 2009.
25
A Missing Proofs
Reminder of Claim 1. If (G1, G2) is a (α, β, ǫ, δ, µ)-GOTCHA then at
least β-fraction of humans can sucessfully authenticate using protocol 3.2 after
creating an account using protocol 3.1.
Proof of Claim 1. A legitimate user H ∈ H will use the same passwords in
protocols 3.1 and 3.2. Hence,
r′1 = Extract (pw
′, r′) = Extract (pw, r′) = r1 ,
and the final matching challenge cˆpi is the same one that would be generated by
G2
(
1k, r1, H
(
G1
(
1k, r1, r2
)
, σ0
))
. If cˆpi is consistently solvable with accuracy
α by H — by definition 4 this is the case for at least β-fraction of users — then
it follows that
dk (π, π
′, σt) ≤ α ,
where H
(
G1
(
1k, r1, r2
))
. For some π0 (namely π0 = π) s.t. dk (π0, π
′) ≤ α it
must be the case that
hpw,0 = h (u, s, pw
′, π0(1), ..., π0(k))
= h (u, s, pw, π(1), ..., π(k))
= hpw ,
and protocol 3.2 accepts. 
Reminder of Claim Claim 2. For all permutations π : [k]→ [k] and α ≥ 0
|{π′ dk (π, π
′) ≤ α}| ≤ 1 +
α∑
i=2
(
k
i
)
i! .
Proof of 2. It suffices to show that
(
k
j
)
j! ≥ |{π′ dk (π, π
′) = j}|. We first
choose the j unique indices i1, . . . , ij on which π and π
′ differ — there are
(
k
j
)
ways to do this. Once we have fixed our indices i1, . . . , ij we define π
′ (k) = π (k)
for each k /∈ {i1, . . . , ij}. Now j! upperbounds the number of ways of selecting
the remaining values π′ (ik) s.t. π (ik) 6= π
′ (ik) for all k ≤ j. 
B HOSP: Pre-Generated CAPTCHAs
The HOSP construction proposed by [14] was to simply fill several high capacity
hard drives with randomly generated CAPTCHAs — discarding the solutions.
Once we have compiled a database large D of CAPTCHAs we can use algo-
rithm 4 as our challenge generator — simply return a random CAPTCHA from
D. The advantage of this approach is that we can make use of already tested
CAPTCHA solutions so there is no need to make hardness assumptions about
new AI problems. The primary disadvantage of this approach is that the size
26
of the database D will be limited by economic considerations — storage isn’t
free. While |D| the number of CAPTCHAs that could be stored on a hard drive
may be large, it is not exponentially large. An adversary could theoretically
pay humans to solve every puzzle in D at which point the scheme would be
completely broken.
Algorithm 4 GenerateChallenge
Input: Random bits r ∈ {0, 1}n, Database D = {P1, ..., P2n} of CAPTCHAs
return Pr
Economic Cost Suppose that two 4 TB hard drives are filled will text CAPTCHAS
13. Let S be the space required to store one CAPTCHA, and let CH denote the
cost of paying a human to solve a CAPTCHA. We use the values S = 8 KB
14 and CH = $0.001
15. In this case |D| = 4 TB8KB ≈ 10
9 so we can store a
billion unsolved CAPTCHAs on the hard drives. It would cost the adversary
|D|CH = $1, 000, 000 to solve all of the CAPTCHAs — or $500, 000 to solve half
of them. The up front cost of this attack may be large, but once the adversary
has solved the CAPTCHAs he can execute offline dictionary attacks against ev-
ery user who had an account on the server. Many server breaches have resulted
in the release of password records for millions of accounts [5, 4, 2, 1]. If each
cracked password is worth between $4 and $30 [22] then it may be easily worth
the cost to pay humans to solve every CAPTCHA in D.
13At the time of submission a 4 TB hard drive can be purchased on Amazon for less than
$162.
14The exact value of S may vary slightly depending on the particular method used to
generate the CAPTCHA. When we compressed a text CAPTCHA using popular GIF format
the resulting files were consistently 8 KB.
15Motoyama, et al. estimated that spammers paid humans $1 to solve a thousand
CAPTCHAs [31]
27
"
46,"1 
 
Real-Time Illegal Parking Detection System Based on 
Deep Learning
Xuemei Xie †, Chenye Wang, Shu Chen, Guangming Shi, Zhifu Zhao 
School of Electronic Engineering, Xidian University 
xmxie@mail.xidian.edu.cn 
ABSTRACT 
The increasing illegal parking has become more and more serious. 
Nowadays the methods of detecting illegally parked vehicles are 
based on background segmentation. However, this method is 
weakly robust and sensitive to environment. Benefitting from 
deep learning, this paper proposes a novel illegal vehicle parking 
detection system. Illegal vehicles captured by camera are firstly 
located and classified by the famous Single Shot MultiBox 
Detector (SSD) algorithm. To improve the performance, we 
propose to optimize SSD by adjusting the aspect ratio of default 
box to accommodate with our dataset better. After that, a tracking 
and analysis of movement is adopted to judge the illegal vehicles 
in the region of interest (ROI). Experiments show that the system 
can achieve a 99% accuracy and real-time (25FPS) detection with 
strong robustness in complex environments.  
CCS Concepts 
• Information systems➝Information systems 
applications➝Decision support systems➝Data analytics. 
Keywords  
Single Shot MultiBox Detector, Tracking, Vehicle detection, 
Real-time, Deep learning. 
1. INTRODUCTION 
Illegal parking may cause a lot of problems such as traffic jam and 
risks of safety. However, its number has increased considerably in 
cities in recent years, which impose a heavy burden on its 
detection. To solve the problem of illegal parking, an accurate and 
real-time detection system is desired.  
Several methods have been proposed to build such a system [1-7], 
most of which are based on separation of foreground and 
background. Typically, vehicles are first extracted from 
background and then are tracked. An alarm will be triggered once 
a vehicle was found to be stationary for a certain period of time in  
 
the ROI. [1]-[3] extracts the foreground objects via sophisticated 
background modeling strategies. They achieve promising 
performance in simple traffic environment, but cannot work in 
crowded scenes. [4] proposes to subtract the background 
constructed by Gaussian Mixture Model to extract the foreground.  
And then a vehicle is recognized by detecting its wheels. It 
effectively separates the foreground and background. However, a 
vehicle cannot be detected when its wheels are occluded. 
Wahyono et al. [5] proposes to use background subtraction to 
obtain candidates of stationary regions, and verify a vehicle by 
exacting scalable histogram of oriented gradient (SHOG) features 
followed by a support vector machines (SVM) classification. It 
copes well with lighting changes, but it is hard to design the 
SHOG feature, and cannot deal with complex weather conditions.  
[6] and [7] also falls into this category. However, extracting 
foreground by background segmentation is easily affected by 
environments, such as illumination changing and the weather. 
Besides, occlusion is also difficult to deal with.  
In this paper, we propose an accurate and real-time illegal parking 
detection system based on Single Shot MultiBox Detector (SSD) 
[8], which can locate and identify objects at the same time without 
generating region proposal, and has the advantages of high 
recognition accuracy, high speed and strong robustness. After the 
detection of vehicles in the input image, all the ones in the ROI 
will be tracked and counted once it stops moving. To improve the 
performance, we propose to optimize SSD via adjusting the aspect 
ratio of default box by K-means to accommodate with the illegal 
detection problem. Experiments show that we achieve a 99% 
accuracy and a 25 FPS speed in illegal parking detection. And 
even with illumination changing and in complex weather, it is still 
robust and can achieve a high accuracy.  
This paper is organized as follows. Section 2 gives a brief 
introduction of SSD. The flowchart of the proposed system is 
given in Section 3.1. The improvement of SSD is discussed in 
Section 3.2. Section 3.3 describes the tracking of vehicles in ROI. 
The judgment of the motion state of a vehicle is described in 
Section 3.4. Experimental results are shown in Section 4 and we 
publish our dataset. Finally, Section 5 concludes our work. 
2. SINGLE SHOT MULTIBOX DETECTOR 
SSD is a method for detecting objects in images using a single 
deep neural network [9]. It can locate and identify objects in 
images at the same time without extracting the region proposal, 
which avoid the disadvantage of background segmentation.  
SSD is an end to end training deep neural network. There are a set 
of default boxes with different aspect ratios at each location in 
several feature maps which are used to locate and classify the 
objects in input images. During the training stage, all the default 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for 
components of this work owned by others than ACM must be honored. 
Abstracting with credit is permitted. To copy otherwise, or republish, to 
post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions@acm.org.  
ICDLT '17, June 2–4, 2017, Chengdu, China.  
© 2017 Association for Computing Machinery. 
ACM ISBN 978-1-4503-5232-1/17/06…$15.00 
http://dx.doi.org/10.1145/3094243.3094261 
2 
 
boxes are compared with ground truth. The default boxes which 
have high IOU with ground truth are treated as positives and the 
others are treated as negatives. The matched boxes are then 
trained to obtain their four offsets accounting to its ground truth to 
match the shapes of objects better. The loss function of SSD is 
given as follows: 
𝐿(𝑥, 𝑐, 𝑙, 𝑔) =
1
𝑁
(𝐿𝑐𝑜𝑛𝑓(𝑥, 𝑐) + 𝛼𝐿𝑙𝑜𝑐(𝑥, 𝑙, 𝑔)) 
where N is the number of matched default box for a ground truth. 
 𝐿𝑐𝑜𝑛𝑓 is confidence loss and 𝐿𝑙𝑜𝑐 is localization loss. 𝑥𝑖𝑗
𝑝 = {1,0} 
is an indicator for matching the 𝑖-th default box to the 𝑗-th ground 
truth box of category 𝑝. The localization loss is a Smooth L1 loss 
between the predicted box ( 𝑙 ) and the ground truth box ( 𝑔 ) 
parameters. The parameters of location and category are trained at 
the same time. 
VGG16
Conv6
(FC6)
Conv7
(FC7) Conv8_2
Conv9_2
Conv10_2
Conv11_2
N
o
n
-m
axim
al su
p
p
re
ssio
n
Input 
image
D
e
te
ctio
n
s: 6
3
1
6
 p
e
r C
lass
Conv4_3
Figure. 1. Single Shot MultiBox Detector network architecture. 
The network architecture of SSD is shown in Figure. 1. It is based 
on a convolutional network such as VGG16 [10], and appends 
several convolution layers to the end of the base network. All 
appended feature layer will contribute to the last predictions in the 
top layer of SSD, so it can naturally predict objects in different 
scales. In vehicle detection, there are many kinds of vehicles with 
different scales, and its scale will change with moving. Therefore, 
it is hard to detect objects with different scales in an image, but 
SSD can solve the problem well by using feature maps of multiple 
layers. 
SSD is an end to end detector for multiple categories. It can make 
fast prediction because of the eliminating of bounding box 
proposals and the subsequent pixel or feature resampling stage. 
For a 300*300 input image, SSD achieves 74.3% mAP on 
VOC2007 test at 59 FPS on a NVidia Titian X and 76.9% mAP 
for 512*512 input, outperforming Faster R-CNN [11] and YOLO 
[12]. The general framework with high quality and real-time 
detection results makes it attracting for vehicle detection. 
3. PROPOSED METHOD 
3.1 Introduction of The System 
As shown in Figure 2, our system mainly consists of four parts: 
video capturing, SSD-based vehicle detection, tracking and the 
analysis of movement. The input of the system can be a real-time 
signal come from a surveillance camera or a local video. Since the 
illegal regions in different scenarios varies, the ROI need to be 
marked manually. 
Flowchart of the proposed system is shown in Figure 3. For a 
given frame in a video, firstly we input the image into a trained 
SSD network to detect the location and category of all vehicles 
contained in the image by setting a suitable hyper-parameter. And 
then the vehicles will be tracked and its motion state will be 
analyzed if it is in the ROI. If it is found that a vehicle is in a 
quiescent state, the system will start to time it. And if the time 
before the state changes exceeds a given threshold, the vehicle 
will be identified as illegal parking and the alarm will be triggered. 
In order to detect new arrivals within the ROI, and prevent 
tracking drift problem, the system re-detects vehicles in the video 
every 25 frames. The newly detected box will be matched with the 
original box by calculating intersection-over-union (IOU) and 
inherits the timing information of the old box if matched. 
SSD detect 
module
Video capture 
module
Tracking 
module
Illegal judgment 
module
 
Figure 2. The composition of the proposed system. 
Vehicle in ROI?
Read frame 
by frame
N
Y
Y
Detection 
using SSD
Key frame
Tracking 
Motion 
analysis
Illegal parking？
Previous vehicle？
Update box 
position
Next 
vehicle
Triggering 
alert
Next 
vehicle
N
Y
N
New box
 
Figure 3. Flowchart of the proposed system. 
3.2 Improvement of SSD  
The original SSD is a network architecture designed for VOC 
dataset which have objects with different scales and shapes. In 
other words, it is a general detector. For an illegal parking system, 
however, the targets are only vehicles which have more single 
scales and aspect ratios. Big boxes and flat boxes won’t match any 
ground truth box, they are useless for us and increase the 
computational complexity. An unsuitable default box may bring a 
lot of background noise. All of these default boxes will reduce the 
accuracy. It is clear that if we train the original SSD network with 
our own dataset directly, both speed and accuracy of network will 
be unsatisfactory. So, the network must be improved to 
accommodate with our dataset better. 
In order to make SSD more suitable for our data set, we redesign 
the aspect ratios of the default boxes in original SSD. We propose 
to use k-means to improve SSD. K-means is a method to cluster 
the input data according to a characteristic of it. We use aspect 
ratios of all ground truth in dataset as input data. By setting a 
value of clustering centers K, We have got K clustering centers 
which are most closely matched with our data sets. One question 
3 
 
needs to be notice that these clustering centers cannot be too close, 
otherwise every default boxes will match the same ground truth. 
The roles of these boxes is completely repeated and the amount of 
calculations has increased. The value of K is set according to the 
distribution of the aspect ratios of the ground truth in the data set. 
Then we set the aspect ratios of default boxes to obtained ones. 
And after that we fine-tune the network based on original network 
parameters by our own dataset. 
In our implementation, K is set to 2. Accounting to the result of 
K-means, we set the aspect ratios to 0.5 and 0.7, respectively. The 
aspect ratios of original SSD are 0.3, 0.5, 1, 2, and 3, respectively. 
After trained by our dataset, the optimized model for vehicle 
detection is obtained. It has better performance for vehicle 
detection because the aspect radios of default boxes match our 
dataset better. The accuracy increased from 75% to 77%. The 
detect results are shown in Figure 4. 
 
(a) Detection in sunny weather 
 
(b) Detection in rainy weather 
Figure 4. The detection of optimized SSD 
3.3 Tracking 
The vehicles in ROI will be tracked after detection with the 
improved SSD model. Here we use the template matching to track 
the vehicles. In order to speed up, templates are converted to gray 
scale. At first, the detection results are selected as the original 
template, which is denoted as 𝑇(𝑥, 𝑦). After the arrival of a new 
frame, the entire image is searched to find out the best matched 
area with the template. For each candidate area 𝐼(𝑥, 𝑦) , we 
evaluate its score R(𝑥, 𝑦) with 𝑇(𝑥, 𝑦), as defined in Eq. (1). The 
region with the highest score is identified as the new location of 
the object.  
R(𝑥, 𝑦) = ∑ (𝑇′(𝑥′, 𝑦′) × 𝐼′(𝑥 + 𝑥′, 𝑦 + 𝑦′)) .            (1)𝑥′,𝑦′   
𝑇′(𝑥, 𝑦) and 𝐼′(𝑥, 𝑦) are the normalized image block of 𝑇(𝑥, 𝑦) 
and 𝐼(𝑥, 𝑦) , respectively. The normalization is established by 
subtracting the respective mean values and dividing them by the 
respective variance, which are given by 
𝑇′(𝑥, 𝑦) =
𝑇(𝑥, 𝑦) −
1
𝑤 × ℎ
∑ 𝑇(𝑥′, 𝑦′)𝑥′,𝑦′
√∑ 𝑇(𝑥′, 𝑦′)2𝑥′,𝑦′
 
and 
𝐼′(𝑥, 𝑦) =
𝐼(𝑥, 𝑦) −
1
𝑤 × ℎ
∑ 𝐼(𝑥′, 𝑦′)𝑥′,𝑦′
√∑ 𝐼(𝑥′, 𝑦′)2𝑥′,𝑦′
 
After that, the normalized images can reduce the affection of 
illumination changing. R(𝑥, 𝑦) evaluates the degree of similarity 
between the original template and the target one. The final result 
is normalized to -1 to 1, in which 1, -1 and 0 means that the two 
images are perfectly matched, exactly the opposite, and no linear 
relationship between the two templates, respectively. 
3.4 Analysis of Illegal Parking 
In order to determine whether a vehicle has an illegal behavior, 
the system analyzes the state of the vehicles being tracked during 
the whole tracking process. The distance that the vehicle moves in 
the region of interest is obtained by making the difference 
between the positions of the adjacent two frames. If the difference 
between the two positions is greater than a given threshold, it is 
considered to be moving. Otherwise, it is in a quiescent state, and 
the stop time of the vehicle is counted at the same time. The alarm 
will be triggered when the stop time of any vehicle exceeds the 
given threshold. 
One problem needs to be noticed that a new vehicle may enter 
into ROI at any moment. So the system detect every certain 
frames. It can prevent tracking drift problem at the same time. 
Since the SSD will always detect all the vehicles in an input 
image including the vehicles is being tracked, so we need to 
match the newly detected boxes with the boxes in last frame. Here 
we match two boxes by calculating the IOU between them. If an 
IOU value of two boxes is greater than the given threshold, we 
think the two boxes contain the same vehicle and the new box will 
inherit the timing information of the old box. 
4. EXPERIMENTAL RESULTS  
4.1 Implementation Setting 
SSD model is trained by our own dataset [13] which contains 
various kinds of traffic conditions including sunny, raining, 
shadow, crowd and smoothness. Our illegal parking detection 
system is implemented by C/C++ programming language.  In 
order to synchronize the detection and tracking module, we use 
two threads for them. The system runs on the PC equipped with 
Core i7-4790 CPU, at 3.60GHz frequency and 8GB RAM. In our 
experiments, the confidence threshold of SSD is set to 0.6. Once a 
vehicle stops for 15s, it will be identified as illegal parking, and 
system will begin to time for illegal vehicles until they moving. 
4.2 Detection Results 
Our system is evaluated on our own dataset described above. Our 
system can detect cars trucks and motorcycles. In this paper we 
only consider cars. The test results are shown in Figure 4. The red 
box represents the ROI which is drawn according to forbidden 
area. The yellow boxes represent illegal parking vehicles detected 
by the system. The number on the yellow box records the illegal 
parking time.  
4 
 
The detection process of the system in sunny days is presented by 
frame 330 to frame 1238 in Figure 5. There are three cars in the 
ROI in frame 330. As the two black cars enter into the ROI earlier 
than the white one, the two black cars are identified as illegal 
vehicles earlier in frame 467. One of the black car start to move in 
frame 697, so the system removes its illegal state. The stop time 
of the white car increases to 15s in frame 875, and thus it is 
identified as an illegal vehicle. The white car starts to move in the 
following frames. Another black car is the only illegal car kept to 
the end. Figure 6. shows the detection process in rainy day. This 
system achieved a 99% detection accuracy on our own test videos. 
5. CONCLUSION 
This paper proposes a deep learning based framework to detect 
illegal parking in ROI. It achieves high accuracy and real-time 
detection results. Besides, our system has a high degree of 
robustness and can adapt to a variety of complex environments.  
6. ACKNOWLEDGMENTS 
This work is supported by Natural Science Foundation (NSF) of 
China (61472301). 
7. REFERENCES 
[1] E. Herrero-Jaraba, Orrite-Uru, C. Uela, and J. Senar, 
""Detected motion classification with a double-background 
and a neighborhood-based difference,"" Pattern Recognition 
Letters, vol. 24, pp. 2079-2092, 2003. 
[2] L. Maddalena and A. Petrosino, ""Stopped Object Detection 
by Learning Foreground Model in Videos,"" IEEE 
Transactions on Neural Networks & Learning Systems, vol. 
24, pp. 723-735, 2013. 
[3] P. Fatih and H. Tetsuji, ""Robust Abandoned Object 
Detection Using Dual Foregrounds,"" EURASIP Journal on 
Advances in Signal Processing, vol. 2008, pp. 1-11, 2007. 
[4] C. Mu, M. Xing, and P. Zhang, ""Smart Detection of Vehicle 
in Illegal Parking Area by Fusing of Multi-features,"" in 
International Conference on Next Generation Mobile 
Applications, Services and Technologies, pp. 388-392, 2015. 
[5] Wahyono, A. Filonenko, and K. H. Jo, ""Illegally parked 
vehicle detection using adaptive dual background model,"" in 
Industrial Electronics Society, pp. 25-28, 2015. 
[6] S. Banerjee, P. Choudekar, and M. K. Muju, ""Real time car 
parking system using image processing,"" in International 
Conference on Electronics Computer Technology, pp. 99-103, 
2011. 
[7] W. Wahyono and K.-H. Jo, ""Cumulative Dual Foreground 
Differences For Illegally Parked Vehicles Detection,"" IEEE 
Transactions on Industrial Informatics, pp. 1-9, 2017. 
[8] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. 
Fu, et al., ""SSD: Single shot multibox detector,"" in European 
Conference on Computer Vision, pp. 21-37, 2016. 
[9] C. Mu, M. Xing, and P. Zhang, ""Smart Detection of Vehicle 
in Illegal Parking Area by Fusing of Multi-features,"" in 
International Conference on Next Generation Mobile 
Applications, Services and Technologies, pp. 388-392, 2015. 
[10] K. Simonyan and A. Zisserman, ""Very deep convolutional 
networks for large-scale image recognition,"" arXiv preprint 
arXiv:1409.1556, 2014. 
[11] R. Girshick, ""Fast R-CNN,"" Computer Sciencepp, pp. 1440-
1448, 2015. 
[12] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ""You 
only look once: Unified, real-time object detection,"" in 
Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, pp. 779-788, 2016. 
[13] http://see.xidian.edu.cn/faculty/gmshi/dataset.htm 
             
Frame 330                                                                                                                     Frame 467 
            
Frame 697                                                                                                                 Frame 875 
5 
 
           
Frame 1107                                                                                                              Frame 1238 
Figure 5. Detection in the case of sunny weather. There are three vehicles in ROI in frame 330. The two black cars are firstly identified as illegal vehicles 
according to their parking time in frame 467 and the white car follows in frame 875. When one of black car and the white car start to move, the system 
removes their illegal state. The black car is the only illegal car in the end in frame 1238. 
            
Frame 399                                                                                                                Frame 528 
Figure 6. Detection in the case of rainy weather 
"
47,"GAN Based Image Deblurring Using Dark Channel Prior
Shuang Zhang∗, Ada Zhen, Robert L. Stevenson; University of Notre Dame; Notre Dame, Indiana, 46556
Abstract
A conditional general adversarial network (GAN) is pro-
posed for image deblurring problem. It is tailored for image de-
blurring instead of just applying GAN on the deblurring problem.
Motivated by that, dark channel prior is carefully picked to be in-
corporated into the loss function for network training. To make
it more compatible with neuron networks, its original indifferen-
tiable form is discarded and L2 norm is adopted instead. On both
synthetic datasets and noisy natural images, the proposed network
shows improved deblurring performance and robustness to image
noise qualitatively and quantitatively. Additionally, compared to
the existing end-to-end deblurring networks, our network struc-
ture is light-weight, which ensures less training and testing time.
Introduction
Blur is a common artifact for images taken by hand-held
cameras. It is mostly caused by object motions, hand shake or
out-of-focus. The blurry image is often modeled as convolution
of a sharp image and a blur kernel. And the target of deblurring
is to restore a latent sharp image from the blurry one. Single im-
age deblurring, however, is a highly ill-posed problem, since it
contains insufficient information to recover a unique sharp image.
In the past few years, assorted constraints and regulation
schemes have been proposed to exclude implausible solutions.
Priors, like total variation prior [1], sparse image prior [2], heavy-
tailed gradient prior [3] and dark channel prior [4], are combined
with L1/L2 norm image regulation term to suppress ringing ar-
tifacts and improve quality. Zhen [5] takes advantage of inertia
sensor data to gain extra information and estimate spatially vary-
ing blur kernels. However, since the blur kernel in reality is more
complicated than the model, estimation of blur kernel is inaccu-
rate, which causes ringing artifacts. Furthermore, these methods
based on iterative optimization techniques are computationally in-
tensive.
Recently, Convolutional Neural Networks (CNN) and deep
learning related techniques have drawn a great attention in com-
puter vision and image processing. Their applications in image
deblurring demonstrate promising results. Sun [7] and Schuler
[6] use CNN to estimate the spatially-invariant blur kernel and ob-
tain latent image by tradition pipeline. Chakrabarti [13] trained a
neural network to predict complex Fourier coefficients of motion
kernel. Recently kernel-free end-to-end deblurring methods are
proposed by Nah et al. [8] and Kupyn et al. [9]. Nah [8] adopted
the multi-scale network to mimic conventional coarse-to-fine op-
timization methods, and proposed a new realistic blurry image
dataset with ground truth sharp images. The work of Kupyn [9]
trains the popular Generative Adversarial Network (GAN) on the
same dataset with fewer parameters, gains higher PSNR values
than that of Nah et al. [8] on the GOPRO dataset, and beats the
others on Kohler dataset [10] using SSIM. Although [9] performs
well based on metric scores, visually, its deblurred result suffers
Figure 1. Comparison. (a) Input blurry image. (b) Result of [9]. (c) Our
result.
grid artifacts, as illustrated in Fig. 1.
To address this artifact, we utilize the dark channel prior.
Dark channel is defined as minimal intensity among three color
channels of pixels in a local area. It was first proposed by He et
al. [11] for dehazing problem, based on the statistics that haze-
free outdoor images have a smaller dark channel than hazy im-
ages. Pan et al. [4] applied dark channel prior to image deblur-
ring. They theoretically and empirically proved that comparing
with blur images, the dark channel of sharp image is more sparse.
And their results demonstrate that dark channel prior contributes
to suppressing ringing and other artifacts. In order to enforce the
sparsity, they utilize a regulation term of L0 norm to count the
nonzero elements of dark channel maps. Unfortunately, L0 norm
is not differentiable, which makes it hard to utilize in back prop-
agation of neuron networks. Instead of using L0 norm, we adopt
L2 norm to directly compute difference of the dark channel maps
between groundtruth sharp images and deblurred images.
In this paper, we present a GAN based image deblurring net-
work using dark channel difference as loss function. The pro-
posed technique is not just a straightforward application of GAN.
This method focuses on how to combine traditional knowledge
with deep learning to make the network achieve better perfor-
mance. Compared to the previous GAN-based deblurring net-
work, the proposed network has less layers and weights. It
leads to less training and testing time, and more importantly
achieves favorable results. In addition, the original GOPRO train-
ing dataset consists of artificially created blurry images without
noise, which are usually different from the real blurry images.
To improve the quality of our trained network on more realistic
blurry images and increase network robustness, we add random
Gaussian noise with variance in a limited range onto the training
image patches. The comparison experiments show that our net-
work outperforms Kupyn et al. [9] for both GOPRO test dataset
and real noisy blurry images.
Related Work
Conditional General Adversarial Networks
GAN is first proposed by Goodfellow et al. [14] to train a
generative network in an adversarial process. It consists of two
ar
X
iv
:1
90
3.
00
10
7v
1 
 [c
s.C
V]
  2
8 F
eb
 20
19
Figure 2. Proposed Network. The proposed CGAN based network has two sub-networks: generator G and discriminator D. Generator restores sharp image
IS from input blurry image IB. IS¯ represents ground truth image. Discriminator can regard input pair (IS, IB) as ”fake” and (IS¯, IB) as ”real”. Except for the first layer
of discriminator and generator, each block in both generator and discriminator consists of a convolutional layer, batch normalization step [21] and an activation
function LeakyReLu [23]. The first layers are not normalized. The digit denotes the number of filters for each block. Dotted lines are skip connection layers in
decoder which come from layers with same size in encoder.
networks: a generator G and a discriminator D. Generator gen-
erates a fake sample from input noise z, while discriminator es-
timates the probability that the fake sample is from training data
rather than generated by generator. These two networks are si-
multaneously trained until discriminator cannot tell if the sample
is real or fake. This process can be summarized as a two-player
min-max game with the following function:
min
G
max
D
Ex¯∼Pdata(x¯) [logD(x¯)]+Ez∼Pz(z) [log(1−D(G(z)))] , (1)
where Pdata denotes distribution over training data x¯ and Pz is
distribution of input noise z. GAN has been applied to different
image restoration problems like super-resolution [16] and texture
transfer [17].
Mirza et al. [15] extend GAN into a conditional model (eq.
(2)), called Conditional Generative Adversarial Nets (CGAN), so
that GAN can make use of auxiliary information to direct both
generator and discriminator. Isola et at. [18] adopt CGAN ar-
chitecture to achieve general image-to-image translation. In [18],
more than just random noise z, similar image y is added as input
of the generator, where y and x¯ share part of features. y and x¯ can
be pairs of hazing and clear images about same scene, or different
color buildings with same structure. Based on network architec-
ture of [18], Kupyn et al. [9] utilize Wasserstein loss [19] and
perceptual loss [20] to train a CGAN for deblurring problem.
min
G
max
D
Ex¯∼Pdata(x¯) [logD(x¯,y)]+Ez∼Pz(z) [log(1−D(G(z,y),y))] ,
(2)
Dark Channel Prior
For an image I, the dark channel of a pixel p is defined by
He et al. [11] as
Dc(p) = min
q∈N (p)
(
min
c∈{r,g,b}
Ic(q)
)
, (3)
where p and q are pixel locations,N (p) denotes the image patch
centered at p, and Ic is the c-th color channel. As shown in eq.
(3), dark channel describes the minimum intensity in an image
patch. He et al. [11] observe that dark channel map D(I) in a
haze-free image tends to be zero. Pan et al. [4] use a less restric-
tive assumption that dark channel map D(I) is sparse rather than
zero. Inspired by this, they adopt L0-regulation term to enforce
less sparse dark channel in a deblurring process, where L0 norm
counts non-zero elements in a dark channel map.
Proposed Method
Network Architecture
The proposed network aims at obtaining a generator to re-
store sharp image IS from input blurry image IB. This generator
is trained with a discriminator using pairs of blurry image IB and
ground truth sharp image IS¯. This structure is shown in fig.2. Ex-
cept for the first layer of discriminator and generator, each block
in both generator and discriminator consists of a convolutional
layer, batch normalization step [21] and an activation function
LeakyReLu [23] with leaking rate α = 0.2. The first layers are
not normalized.
Generator The proposed generator adopts an encoder-
decoder framework to achieve image-to-image performance.
Similar to [18], the encoder consists of a sequence of convolu-
tional layers with stride = 2 and kernel size = 5. And the decoder
has a chain of transposed-convolutional layers with same size of
stride and kernel. Encoder represents the input image with a bot-
tleneck vector and decoder recovers an image with same size of
input from bottleneck vector. A skip architecture is applied by
inserting same size of layers from encoder after each layer of de-
coder. This skip connection refines the details in output image by
combining deep, coarse, semantic information and shallow, fine,
appearance information [22]. Dropout is also included in decoder
to avoid over-fitting.
Discriminator The proposed discriminator contains a series
of convolutional layers with stride = 2 and kernel size = 5. The
output of discriminator is a scalar, followed by a sigmoid function.
Loss Functions
According to eq. (2), we train discriminator and generator
alternatively. The loss function of discriminator is same as adver-
sarial loss:
Ld = Ex¯,y [logD(x¯,y)]+Ey,z [log(1−D(G(z,y),y))] . (4)
In the deblurring setting, y and x¯ denote blurry and sharp image,
respectively. The generator loss is defined as combination of ad-
Figure 3. Comparison with DeblurGAN [9]. From top to bottom: image from GOPRO dataset and real nature image. From left to right: blurry images, deblurred
results by [9] and our result.
versarial loss, content loss and dark channel loss:
Lg=Ey,z [log(1−D(G(z,y),y))]+λ1 ·Lcontent+λ2 ·Ldarkchannel ,
(5)
where λ1 = 100 and λ2 = 250 in our experiments.
Content loss We adopt the traditional content loss to direct
the output of generator to ground truth. Although both L1 and L2
norm are commonly used, L1 norm is chosen since it attains less
blurring result [18].
Lcontent = Ex¯,y,z [||x¯−G(y,z)||1] . (6)
Dark channel loss In order to suppress ringing and grid arti-
fact, dark channel prior is especially chosen. Pan et al. [4] exploit
L0 norm to count non-zero elements in a dark channel mapDc(I)
of an image I. Since L0 norm is indifferential, L2 norm is uti-
lized instead which calculates the distance of dark channel map
between ground truth and deblurred image.
Ldarkchannel = Ex¯,y,z [||Dc(x¯)−Dc(G(y,z))||2] . (7)
Unlike [9], we discard the perceptual loss [20]. Kupyn et
al. [9] employ the difference of one feature map in the VGG-19
[24] between ground truth and restored images as perceptual loss.
GAN is known for its ability to reserve perceptual feature of an
image. Adding an extra perceptual loss seems a noneffective re-
peat. Our experiment shows that perceptual loss doesn’t improve
the result, on the contrary, it leads to worse performance.
Experiments
Our network is implemented with Python code based on Ten-
sorflow [25].
Datasets
GOPRO dataset [8] is utilized for training and testing our
network. It contains 2103 paris of blurry and ground truth im-
ages in train dataset, and 1111 pairs in test dataset. Resolution
of the image are 720p. The blurry image is generated by averag-
ing a sequence (7-15) of continuous sharp images. Sharp image
in the middle of sequence is regarded as ground truth. GOPRO
dataset is regarded as benchmark by many deblurring algorithms
like [8] and [9]. Although GOPRO dataset is widely used, it only
employs noise-free images. For natural images, however, noise
always accompanies with blur. To test our model on more real
images, we add Gaussian noise with variance = 0.001 to origi-
nal GOPRO Large dataset and create a new GOPRO-noise dataset
with 1111 image pairs. A synthetic dataset in [9] is adopted for
training. Same as combination version of DeblurGAN in [9], we
use both GOPRO train dataset and synthetic dataset to train our
network.
Training Process
The proposed network is trained on NVIDIA GeForce GTX
1080 Ti GPU and tested on Mac Pro with 2.7 GHz Intel Core i5
CPU. Similar to [9], the input training pair is randomly croped as
size of 256×256 after downsampled by a factor of two. Weights
are initialized to follow Gaussian distribution with zeros mean and
standard deviation 0.02. For each iteration of optimization, 1 step
is performed on discriminator D, followed by 2 steps on genera-
tor G to prevent discriminator loss Ld from zero. The model is
trained for 15 epochs within 2 days, comparing with 200 epochs
for 6 days in [9] . Furthermore, despite of instability GAN’s train-
ing, our method converges to similar result for each and every
training task, which demonstrates the robustness of our GAN ar-
chitecture.
Table. 1 Average PSNR and SSIM.
Dataset Metrics [9] dc0 dc250 dc250p
Original PSNR 26.63 26.70 27.01 26.45
SSIM 0.8701 0.8798 0.8813 0.8680
Noisy PSNR 26.32 26.53 26.83 26.31
SSIM 0.8524 0.8697 0.8707 0.8604
Result and Comparison
Our test results are mainly compared with state-of-art GAN
based deblurring network DeblurGAN [9]. DeblurGAN defeats
deep learning networks [7] and [8] on GOPRO dataset. Since
the author posted the code online1, we compare our network with
DeblurGAN by directly adopting its uploaded network and latest
trained weights. We test our model on GOPRO and GOPRO-noise
test datasets.
Fig. 3 illustrates the deblurred results of [9] and our model.
Blurry image in the first row is picked in GOPRO-noise dataset
and the blurry one of second row is real natural image with motion
blur taken by camera. According to local patches, although [9]
can deal with blur but its results suffer from grid artifacts, while
our model with dark channel loss achieves sharper images without
grid artifacts. Furthermore, for motion blurry image (second row),
the sharp part in input image remains unchanged in our deblurred
result, but extra grid artifacts are added to result of [9].
The quantitative performance of the proposed network on
two dataset GOPRO and GOPRO-noise is shown in Tab. 1. In our
experiment, the coefficient of dark channel loss λ2 = 250(dc250).
The results are compared with same network without dark chan-
nel loss dc0, same network with extra perpetual loss dc250p as
well as DeblurGAN [9]. All test images are downsampled by fac-
tor of two. The perpetual loss follows what it is in [10]. The
proposed model performs best among the comparisons for both
noise-free and noisy dataset. DeblurGAN performs less well ow-
ing to its grid artifacts. Perceptual loss leads to a worse result.
Since GAN is good at preserving perceptual feature already, per-
ceptual loss brings no extra constraints for the network. Compar-
ison with dc=0 demonstrates that dark channel loss contributes to
better result.
Conclusion
To address deblurring problem using a CGAN based archi-
tecture and to tackle the issue with grid artifacts in GAN based
deblurring methods, this paper incorporates a dark channel prior.
The dark channel prior is employed by L2 norm rather than L0
in order to make it more friendly for network training. To vali-
date the deblurring result on more nature images, a noise involved
dataset is proposed. The proposed network shows a great deblur-
ring performance for both synthetic and real blurry images.
References
[1] Nicolas Dey, Laure Blanc-Feraud, C Zimmer, Z Kam, J-C Olivo-
Marin, and J Zerubia. A deconvolution method for confocal mi-
croscopy with total variation regularization. In Biomedical Imag-
ing: Nano to Macro, 2004. IEEE International Symposium on, pg.
1223−1226. (2004).
1https://github.com/KupynOrest/DeblurGAN
[2] Anat Levin, Rob Fergus, Fredo Durand, and William T Freeman. Im-
age and depth from a conventional camera with a coded aperture. In
ACM Transactions on Graphics (TOG), volume 26, pg. 70. (2007).
[3] Shan Qi, Jiaya Jia, and Aseem Agarwala. High-quality motion de-
blurring from a single image. In ACM Trans. Graph., volume 27, pg.
73. (2008).
[4] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming−Hsuan Yang.
Blind image deblurring using dark channel prior. In CVPR, pg.
1628−1636, (2016).
[5] Ruiwen Zhen and Robert L. Stevenson. Multi−image motion de-
blurring aided by inertial sensors. Journal of Electronic Imaging,
25(1):013027−013027, (2016).
[6] Christian J. Schuler, Michael Hirsch, Stefan Harmeling, and Bern-
hard Schlkopff. Learning to deblur. IEEE Trans. Pattern Anal. Mach.
Intell., 38(7):1439−1451, (2016).
[7] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learning a con-
volutional neural network for non-uniform motion blur removal. In
CVPR, pg. 769−777, (2015).
[8] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi−scale convolutional neural network for dynamic scene deblur-
ring. pg. 3883−3891, (2017).
[9] Kupyn, Orest Budzan, Volodymyr Mykhailych, Mykola Mishkin,
Dmytro Matas, and Jiri Matas. DeblurGAN: Blind Motion Deblur-
ring Using Conditional Adversarial Networks. CVPR camera-ready
(2018).
[10] Rolf Khler, Michael Hirsch, Betty Mohler, Bernhard Schlkopf, and
Stefan Harmeling. Recording and playback of camera shake: Bench-
marking blind deconvolution with a real−world database. In ECCV,
pg. 27−40, (2012).
[11] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal
using dark channel prior. In CVPR, pg. 1956−1963, (2019).
[12] Tae Hyun Kim, Seungjun Nah, and Kyoung Mu Lee. Dynamic scene
deblurring using a locally adaptive linear blur model. arXiv preprint
arXiv : 1603.04265, (2016).
[13] Ayan Chakrabarti. A neural approach to blind motion deblurring. In
ECCV, pg. 221−235. Springer, (2016).
[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Gen-
erative adversarial nets. In NIPS, pg. 2672-2680, (2014).
[15] Mehdi Mirza, Simon Osindero. Conditional generative adversarial
nets. CoRR, abs/1411.1784., (2014).
[16] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, An-
drew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Te-
jani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic
single image super-resolution using a generative adversarial network.
arXiv preprint arXiv:1609.04802, (2016).
[17] Chuan Li and Michael Wand. Precomputed real-time texture synthe-
sis with markovian generative adversarial networks. ECCV, (2016).
[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros.
Image-to-image translation with conditional adversarial networks. In
CoRR, abs /1611.07004, (2016).
[19] Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein
GAN. ArXiv, arXiv:1701.07875v3, (2017).
[20] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses
for real-time style transfer and super-resolution. In ECCV, (2016).
[21] Sergey Ioffe and Christian Szegedy. Batch normalization: Acceler-
ating deep network training by reducing internal covariate shift. In
Proceedings of the 32nd International Conference on International
Conference on Machine Learning - Volume 37 (ICML’15), Francis
Bach and David Blei (Eds.), Vol. 37. JMLR.org 448-456.(2015).
[22] E. Shelhamer, J. Long and T. Darrell, Fully Convolutional Networks
for Semantic Segmentation, in IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 39, no. 4, pg. 640-651, (2017).
[23] Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li. Empiricalevalua-
tionof rectified activations in convolutional network. arXiv preprint
arXiv:1505.00853, 2015.
[24] Karen Simonyan, Andrew Zisserman. Very Deep Convolutional
Networks for Large-Scale Image Recognition. ArXiv e- prints, Sept.
(2014).
[25] Abadi, Martn, Barham, Paul, Chen, Jianmin, Chen, Zhifeng, Davis,
Andy, Dean, Jeffrey, Devin, Matthieu, Ghemawat, Sanjay, Irving, Ge-
offrey, Isard, Michael, Kudlur, Manjunath, Levenberg, Josh, Monga,
Rajat, Moore, Sherry, Murray, Derek Gordon, Steiner, Benoit,
Tucker, Paul A., Vasudevan, Vijay, Warden, Pete, Wicke, Martin, Yu,
Yuan and Zhang, Xiaoqiang. TensorFlow: A system for large-scale
machine learning. CoRR abs/1605.08695 (2016) .
"
48,"Blood cell classification using the Houghtransform and convolutional neural networksMiguel A. Molina-Cabello, Ezequiel Lo´pez-Rubio, Rafael M. Luque-Baena,Mar´ıa Jesu´s Rodr´ıguez-Espinosa, and Karl Thurnhofer-HemsiDepartment of Computer Languages and Computer Science, University of Ma´laga,Bulevar Louis Pasteur, 35, 29071 Ma´laga, Spain{miguelangel,ezeqlr,rmluque,karlkhader}@lcc.uma.esmjesus.rodriguez.espinosa@hotmail.comAbstract. The detection of red blood cells in blood samples can becrucial for the disease detection in its early stages. The use of imageprocessing techniques can accelerate and improve the effectiveness andefficiency of this detection. In this work, the use of the Circle Houghtransform for cell detection and artificial neural networks for their iden-tification as a red blood cell is proposed. Specifically, the applicationof neural networks (MLP) as a standard classification technique with(MLP) is compared with new proposals related to deep learning such asconvolutional neural networks (CNNs). The different experiments car-ried out reveal the high classification ratio and show promising resultsafter the application of the CNNs.Keywords: blood cell detection, blood cell classification, circle Houghtransform, convolutional neural networks1 IntroductionDigital image processing is of paramount importance in various medicine fields,from images which are obtained in medical tests such as X-ray image, com-puted tomography, magnetic resonance imaging, ultrasound image and nuclearmedicine image, to images which are obtained in laboratory by microscopy. Inthis way, digital image processing allows to process the image to obtain a bettervisibility, to emphasize the required parts or to make analysis and predictions. Inaddition, image segmentation through image processing is essential for pathologydetection [12, 4].Nowadays, most of images obtained in laboratory by microscopy are digital-ized afterwards and processed by computers to make easier and faster the imageanalysis process. Among other fields, blood optical microscopy provides imagesamples whose study can supply very useful information about patient health.The techniques applied in hematology to count blood cells in blood samples,such as centrifugation techniques or hematology cell counters, are imprecise andinexact in many cases. Thus, the result can change significantly according tothe measure technique used [3, 8]. Moreover, automated cell counters are used2 Miguel A. Molina-Cabello et al.with less frequency than manual cell counters in laboratories. Thus, laboratoryoperations might be optimized by automated cell counters, considering manualcell counters for validation purposes [11].For a long time, many researches have been carried out to develop toolsor techniques which reduce the errors in the manual method, the cost and theslowness at blood cells count for pathology detection [9, 14]. In a blood sample,the percentage occupied by red blood cells in relation to the total blood is knownas hematocrit. The hematocrit value is very important for early detection ofseveral diseases. An appreciable increase or reduction of the hematocrit valuemight be a disease indication like anemia. In many cases, these diseases arenot serious but they may cause other severe problems. Thus, an early detectionmight be vital [16, 15].In this paper, a blood optical microscopy imaging analysis is performed bydetecting the red blood cells in blood samples using the Hough transform andneural networks. The Hough transform technique provides the detection of eachobject or cell. This technique is frequently applied on image processing becauseof its efficacy and low complexity for the shape and feature detection. At present,it is useful in medical imaging studies [5, 13].Neural networks have already been used for a wide variety of tasks in medicine,from medical imaging, signal processing to biomedical researches. In particular,multilayer perceptrons (MLP) are considered as universal approximators so theycan be adapted to solve different real problems inside the field of medical imag-ing [2, 6, 1]. In this approach, the method classifies the detected cells betweenred blood cells and other elements such as white blood cells or spurious objects,among others. Recently, convolutional neural networks, which are an evolution ofthe MLP, have been successfully applied on image recognition and classificationwith higher accuracy rates than traditional techniques. A comparative betweenthe two types of neural networks (MLP and CNN) is carried out.The rest of the paper is structured as follows. Section 2 presents the method-ology of the proposal, differentiating between the detection of blood cells andtheir subsequent classification. A complete suite of experiments is performed inSection 3, where both quantitative and qualitative results are showed over adataset of blood images. Finally, our conclusions are provided in Section 4.2 MethodologyOur proposal can be divided into two steps: the detection of the red blood cellsand their classification. An schema of our framework is represented in Figure 1.Section 2.1 describes how blood cells are detected inside the image regardless oftheir size. A classification process is performed from the detected cells in orderto identify the actual red blood cells. This stage is detailed in Section 2.2.2.1 Red blood cells detectionThe first step is the detection of the possible red blood cells that appear in theimage. For this purpose, we apply the Circle Hough Transform (CHT) to detectTitle Suppressed Due to Excessive Length 3Fig. 1. Schema of the operation of the proposalcircular objects in the image. This is because the geometric shape of the redblood cells is similar to a circle.The CHT is computed as follows. As known, the equation for a circle in twodimensions reads as follows:(x− a)2 + (y − b)2 = r2 (1)where (a, b) is the center of the circle, and r > 0 is the radius. The circleHough transform takes as input a set of N edge points in the image:S = {(xi, yi) ∈ R2|i ∈ {1, ..., N}} (2)Then a score for each tentative circle, given by its three parameters (a, b, r) ∈N3, is computed as the count of edge points which lie in that circle. To that end,the parametric equation of the circle is considered:(x, y) = (a+ r cos θ, b+ r sin θ) (3)After that the equation is rearranged as follows:(a, b) = (x+ r cos θ, y + r sin θ) (4)Finally, for each edge point in S, each tentative value of the radius r, andeach angle θ, a vote is cast for the resulting tentative circle (a, b, r), as givenby (4). The tentative circles with the highest number of votes are declared asdetected circles.In order to have an approximation of the radii r of the different blood cellsthat can be detected, a first search is accomplished because the scale of the imageis unknown. The operation is to reduce the size of the image by applying a scaleof 10 times (so that, the size in pixels of the image is reduced 100 times becausethe image is two-dimensional and the scale decreases each dimension size in 10times). After that, a circles search is made in the reduced image using a radiusrank between 2 and 10 pixels, counting the number of the detected circles witheach of these radii.The radius which presents the highest accumulation of circles will be selectedto the second and deeper search. In this case, a new circle search by using theCircle Hough Transform is made again in the original image (not the reducedone). A rank around the most numerous radius previously found, increasing4 Miguel A. Molina-Cabello et al.its size by 10 for adjusting to the original image, is applied as possible radiusinterval. In particular, a rank of difference 10 around the most numerous radiusis defined.2.2 Red blood cells classificationOnce we have detected the blood cells, we need to classify them into red bloodcells or other kind of blood cells. In this case we have considered two approachesto solve it.The first predictive model is based on a basic neural network for classificationtasks; specifically, the multilayer perceptron (MLP). This network consists of aninput layer, where each input pattern is the median color of each detected cell(that is, it has three values which belong to the RGB value), a hidden layer withM neurons and an output layer with one neuron whose value represents theprobability of being red blood cell or not. The estimated value for the number ofneurons in the hidden layer is M = 6. It has been empirically checked that thereis not much difference in the classification performance when this parameterincreases.The second analyzed model is the Alexnet convolutional neural network(CNN) [10]. Alexnet was developed by using several convolutional and fully-connected layers and its aim was to classify the IMAGENET dataset1. Thenetwork inputs are the images associated to the blood cells detected.In both cases, a balanced set of input samples has been considered to trainthe network, where only two classes are available: red blood cell or not. Theimages have been selected from the testing set of images and each one of themexhibits different blood cells. A dataset with all the images with the same di-mension (227x227 pixels) is required to train the CNN. Nevertheless, the bloodcell samples does not present this required size. Therefore, we have applied aresize process to solve this issue. Some different samples of these images can befound in Figure 2.Since not all the images show the same scale in blood cells, it is needed tocorrect it and calculate the real size of cells. Hence, it is possible to estimatethis value by knowing that the real size of the radius of a red blood cell is 3.9micrometers approximately (an average diameter of 7.8 micrometers) [7].3 ExperimentsIn this section, both the cell detection and cell classification approaches areapplied to a set of blood sample images of heterogeneous sizes. In order toevaluate our methodology, a selected set of 10 images, which exhibit several bloodcells and show different issues like overlapping objects, different illumination orscale in the image, is considered. They have been taken from the ASH Image1 http://www.image-net.orgTitle Suppressed Due to Excessive Length 5(a) (b)Fig. 2. Some samples of blood cell images used for the training process of the neuralnetworks. The resize process is already applied on each displayed region. (a) exhibitsred blood cells and (b) shows non red blood cells.Bank2. The selected images from this repository are named as follows: 1050,1051, 1068, 1071, 1072, 60095, 60475, 60550, 60802 and 60974.In addition, we have selected several blood cell images from the used datasetand we have labeled them. This has to carry out in order to train the networks.Due to the low number of non red blood cells that are presented, we have in-creased this number with data augmentation techniques.Furthermore, we have manually annotated the centroid of each red blood cellof each image to test the performance of the detection and classification of thiskind of cells, using this information as a ground truth.A robust performance of the goodness of the proposal is achieved due to a5-fold strategy is employed, where the 80 percent of the data form the trainingset and the remaining 20 percent comprise the test set. This process is repeated10 times and each time the order of the images is randomly selected. Moreover,the training cell images corresponding to an image is randomly chosen too, with20 red blood cell images and other 20 of non red blood cell images. After thetraining of the networks, the test set is used to measure the performance of theproposal.Some different well-known measures have been selected in order to comparethe performance of the detection and the classification of the blood cells from aquantitative point of view. In this work we have considered the spatial accuracy(S), the Accuracy (Acc) and the F-measure (Fm). All this measures providevalues in the interval [0, 1], where higher is better, and represent the percentageof hits of the system.2 http://imagebank.hematology.org6 Miguel A. Molina-Cabello et al.(a) (b) (c) (d)Fig. 3. Detection and classification of the red blood cells. First row exhibits the rawimages and second row shows the result provided by the proposal. Each column presentsan image of the tested dataset. Columns (a), (b), (c) and (d) corresponds to images1050, 1068, 60095 and 60974, respectively. Each blood cell detected is indicated witha circle and the color shows its classification: red is for red blood cell and blue for nonred blood cell.True positives (TP), True negatives (TN), False negatives (FN), false posi-tives (FP), the precision (PR), the recall (RC), the specificity (SP) and the per-centage of wrong classifications (PWC) rates are also used in this work. Amongall these measures, the spatial accuracy, the accuracy and the F-measure providea good overall evaluation of the performance of a given method, while FN mustbe considered against FP (lower is better), and PR against RC (higher is better).The definition of each measure can be described as follow:S =TPTP + FN + FPAcc =TP + TNTP + FP + FN + TN(5)RC =TPTP + FNPR =TPTP + FPSP =TNFP + TN(6)FNR =FNTP + FNFPR =FPFP + TN(7)F-measure = 2 ∗ PR ∗RCPR+RC(8)PWC = 100 ∗ FN + FPTP + FP + TN + FN(9)In order to test the performance of the detection of red blood cells we havecompared the centroids of the ground truth against the centroids of the esti-mated blood cell. Because we can estimate the scale of the test image and it iswell-known that the radius of a red blood cell is 3.9 micrometers [7], we haveconsidered that an estimated centroid is correctly detected if it has a distanceTitle Suppressed Due to Excessive Length 7Table 1. Performance of the detection of red blood cells. First column indicates theimage from the dataset and remaining columns show the yielded mean performanceachieved by the proposal in different measures. Each row represents a tested imagefrom the dataset and its mean performance, and last row exhibits the average of thevalues of each measure.Image TP FP FN RC FNR PR Fm S1050 63 7 2 0.969 0.031 0.900 0.933 0.8751051 70 9 26 0.729 0.271 0.886 0.800 0.6671068 129 50 12 0.915 0.085 0.721 0.806 0.6751071 55 10 7 0.887 0.113 0.846 0.866 0.7641072 52 11 4 0.929 0.071 0.825 0.874 0.77660095 41 18 1 0.976 0.024 0.695 0.812 0.68360475 199 32 11 0.948 0.052 0.861 0.902 0.82260550 82 40 6 0.932 0.068 0.672 0.781 0.64160802 77 24 4 0.951 0.049 0.762 0.846 0.73360974 97 27 5 0.951 0.049 0.782 0.858 0.752Average 0.919 0.081 0.795 0.848 0.739of 1.5 micrometers from the ground truth centroid. Furthermore, we only haveconsidered those ground truth centroids that appear inside the image. Accordingto this restrictions, the detection test does not consider the TN measure. On theother hand, the test of the performance of the classification is over the detectedblood cells.From a qualitative point of view, our proposal exhibits a good performancein the detection and the classification of the red blood cells. This information canbe observed in Figure 3. It can be appreciated how the proposal considers severalblood cells as red ones in the detection step. Nevertheless, after the applicationof the classification process, the proposal corrects these predictions. It must alsobe highlighted the detection of the overlapping blood cells or those that appearin the border of the image. Another problem is shown when the proposal detectstwo (or more) centroids for only one red blood cell.On the other hand, from a quantitative point of view. The performance ofthe detection of our proposal can be observed in Table 1. The average of bloodcell detection is around 0.74, mainly because the CHT not only detects redblood cells, but also any circular element that is present in the image. Thus, itis possible to detect several red blood cells in the location of larger white bloodcells, so it is essential to have a posterior stage that correctly detects or classifiesthose circles as red blood cells.The performance of the two proposed classification model is shown in Tables2 and 3. We compare the accuracy of these two models, as we can see in Figure4. Although both models obtain very good red cell classification rates, in gen-eral, the CNN proposal slightly improves the generated results by the standardneural network. This happens because by providing more input information (thecomplete image of each detected circle instead of a vector of three elements with8 Miguel A. Molina-Cabello et al.Table 2. Performance of the classification of red blood cells by employing the ba-sic neural network. First column indicates the image from the dataset and remainingcolumns show the yielded mean performance achieved by the proposal in different mea-sures. Each row represents a tested image from the dataset and its mean performance,and last row exhibits the average of the values of each measure.Image TP TN FP FN RC SP FPR FNR PWC PR Fm S Acc1050 65 5 0 0 1.00 1.000 0.000 0.00 0.000 1.000 1.000 1.000 1.0001051 70 6 3 0 1.00 0.667 0.333 0.00 3.797 0.959 0.979 0.959 0.9621068 138 0.4 42.6 0 1.00 0.009 0.991 0.00 23.536 0.764 0.866 0.764 0.7651071 56 6 3 0 1.00 0.667 0.333 0.00 4.615 0.949 0.974 0.949 0.9541072 57 3.3 2.7 0 1.00 0.550 0.450 0.00 4.286 0.955 0.977 0.955 0.95760095 48 11 2 0 1.00 0.846 0.154 0.00 3.279 0.960 0.980 0.960 0.96760475 225 2 7 0 1.00 0.222 0.778 0.00 2.991 0.970 0.985 0.970 0.97060550 86 30.8 7.2 0 1.00 0.811 0.189 0.00 5.806 0.923 0.960 0.923 0.94260802 84 11 7 0 1.00 0.611 0.389 0.00 6.863 0.923 0.960 0.923 0.93160974 106 3 17 0 1.00 0.150 0.850 0.00 13.492 0.862 0.926 0.862 0.865Average 1.00 0.553 0.447 0.00 6.867 0.926 0.961 0.926 0.931Table 3. Performance of the classification of red blood cells by employing the convolu-tional neural network. First column indicates the image from the dataset and remainingcolumns show the yielded mean performance achieved by the proposal in different mea-sures. Each row represents a tested image from the dataset and its mean performance,and last row exhibits the average of the values of each measure.Image TP TN FP FN RC SP FPR FNR PWC PR Fm S Acc1050 65 5 0 0 1.000 1.000 0.000 0.000 0.000 1.000 1.000 1.000 1.0001051 70 6 3 0 1.000 0.667 0.333 0.000 3.797 0.959 0.979 0.959 0.9621068 136.9 3.8 39.2 1.1 0.992 0.088 0.912 0.008 22.265 0.777 0.872 0.773 0.7771071 56 7 2 0 1.000 0.778 0.222 0.000 3.077 0.966 0.982 0.966 0.9691072 57 5 1 0 1.000 0.833 0.167 0.000 1.587 0.983 0.991 0.983 0.98460095 48 11 2 0 1.000 0.846 0.154 0.000 3.279 0.960 0.980 0.960 0.96760475 223.2 2 7 1.8 0.992 0.222 0.778 0.008 3.761 0.970 0.981 0.962 0.96260550 86 31 7 0 1.000 0.816 0.184 0.000 5.645 0.925 0.961 0.925 0.94460802 83.2 11 7 0.8 0.990 0.611 0.389 0.010 7.647 0.922 0.955 0.914 0.92460974 105.7 3.1 16.9 0.3 0.997 0.155 0.845 0.003 13.651 0.862 0.925 0.860 0.863Average 0.997 0.602 0.398 0.003 6.471 0.932 0.963 0.930 0.935the medium color) the CNN can better discern which circles correspond to thered blood cells, despite the considerable increase of complexity of this secondproposal. Possibly, the development of CNNs adapted to this specific issue man-ages to improve the classification result in images with smaller cell sizes (image60974 ) and with low illumination (image 1068 ).4 ConclusionIn this work, a methodology for the detection of red blood cells in medical imagesis presented, based on the use of the circle Hough transform (CHT) for detectionTitle Suppressed Due to Excessive Length 9105010511068107110726009560475605506080260974OverallImage0.70.80.91AccNN CNNFig. 4. Accuracy comparison between the performance achieved by the basic neuralnetwork and the convolutional neural network in each image from the dataset. Thebars of the overall represent the obtained average accuracy.and neural networks for classification. This classification module is necessarysince not all the detected circles correspond to red blood cells, which impliesan identification process to ensure which circle corresponds to a red blood cellor not. Specifically, two alternatives have been tested for this classification, astandard neuronal network (multilayer perceptron, MLP) and a convolutionalneuronal network (CNN). A set of heterogeneous medical images has been usedfor the evaluation of the different proposals. It is observed that the results of thefirst detection module are remarkable, with success rates close to 75%, while theclassification rates are close to 94% on average for both CNN and MLP. Althoughthere is some improvement in favor of the CNNs, the implementation of ad hocCNNs associated with this issue could still improve the results obtained.References1. Baxt, W.G.: Application of artificial neural networks to clinical medicine. Thelancet 346(8983), 1135–1138 (1995)2. Clark, J.: Neural network modelling. Physics in Medicine and Biology 36(10), 1259(1991)3. Cornbleet, J.: Spurious results from automated hematology cell counters. Labora-tory Medicine 14(8), 509–514 (2016)4. Davis, R., Boyers, S.: The role of digital image analysis in reproductive biology andmedicine. Archives of pathology & laboratory medicine 116(4), 351–363 (1992)5. Ecabert, O., Thiran, J.P.: Adaptive hough transform for the detection of naturalshapes under weak affine transformations. Pattern Recognition Letters 25(12),1411–1419 (2004)10 Miguel A. Molina-Cabello et al.6. Egbert, D.D., Kaburlasos, V.G., Goodman, P.H.: Neural network discriminationof subtle image patterns. In: Neural Networks, 1990., 1990 IJCNN InternationalJoint Conference on, pp. 517–524. IEEE (1990)7. Fedosov, D.A., Caswell, B., Karniadakis, G.E.: A multiscale red blood cell modelwith accurate mechanics, rheology, and dynamics. Biophysical journal 98(10),2215–2225 (2010)8. Imeri, F., Herklotz, R., Risch, L., Arbetsleitner, C., Zerlauth, M., Risch, G.M., Hu-ber, A.R.: Stability of hematological analytes depends on the hematology analyserused: a stability study with bayer advia 120, beckman coulter lh 750 and sysmexxe 2100. Clinica chimica acta 397(1), 68–71 (2008)9. Krause, J.: Automated differentials in the hematology laboratory. American jour-nal of clinical pathology 93(4 Suppl 1), S11–6 (1990)10. Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet classification with deep con-volutional neural networks. Advances in Neural Information Processing Systems25, 1097–1105 (2012)11. Lantis, K.L., Harris, R.J., Davis, G., Renner, N., Finn, W.G.: Elimination ofinstrument-driven reflex manual differential leukocyte counts: optimization of man-ual blood smear review criteria in a high-volume automated hematology laboratory.American journal of clinical pathology 119(5), 656–662 (2003)12. McAuliffe, M.J., Lalonde, F.M., McGarry, D., Gandler, W., Csaky, K., Trus,B.L.: Medical image processing, analysis and visualization in clinical research. In:Computer-Based Medical Systems, 2001. CBMS 2001. Proceedings. 14th IEEESymposium on, pp. 381–386. IEEE (2001)13. Philip, K.P., Dove, E.L., McPherson, D.D., Gotteiner, N.L., Stanford, W., Chan-dran, K.B.: The fuzzy hough transform-feature extraction in medical images. IEEETransactions on Medical Imaging 13(2), 235–240 (1994)14. Schmitt, J.M., Zhou, G.X., Miller, J.: Measurement of blood hematocrit by dual-wavelength near-ir photoplethysmography. In: Physiological monitoring and earlydetection diagnostic methods, vol. 1641, pp. 150–162. International Society forOptics and Photonics (1992)15. Tang, Z., Lee, J.H., Louie, R.F., Kost, G.J.: Effects of different hematocrit levelson glucose measurements with handheld meters for point-of-care testing. Archivesof pathology & laboratory medicine 124(8), 1135–1140 (2000)16. Wennecke, G.: Hematocrit-a review of different analytical methods. RadiometerMedical ApS (2004)"
49,"Imagination Based Sample Construction for Zero-Shot Learning
Gang Yang
Renmin University of China
Beijing, China
yanggang@ruc.edu.cn
Jinlu Liu
Renmin University of China
Beijing, China
liujinlu@ruc.edu.cn
Xirong Li∗
Renmin University of China
Beijing, China
xirong@ruc.edu.cn
ABSTRACT
Zero-shot learning (ZSL) which aims to recognize unseen classes
with no labeled training sample, efficiently tackles the problem of
missing labeled data in image retrieval. Nowadays there are mainly
two types of popular methods for ZSL to recognize images of unseen
classes: probabilistic reasoning and feature projection. Different
from these existing types of methods, we propose a new method:
sample construction to deal with the problem of ZSL. Our proposed
method, called Imagination Based Sample Construction (IBSC), in-
novatively constructs image samples of target classes in feature
space by mimicking human associative cognition process. Based
on an association between attribute and feature, target samples are
constructed from different parts of various samples. Furthermore,
dissimilarity representation is employed to select high-quality con-
structed samples which are used as labeled data to train a specific
classifier for those unseen classes. In this way, zero-shot learning is
turned into a supervised learning problem. As far as we know, it is
the first work to construct samples for ZSL thus, our work is viewed
as a baseline for future sample construction methods. Experiments
on four benchmark datasets show the superiority of our proposed
method.
CCS CONCEPTS
• Information systems→ Image search;
KEYWORDS
Imagination, Sample Construction, Zero-Shot Learning
ACM Reference Format:
Gang Yang, Jinlu Liu, and Xirong Li. 2018. Imagination Based Sample Con-
struction for Zero-Shot Learning. In SIGIR ’18: The 41st International ACM
SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3209978.3210096
1 INTRODUCTION
Development of image retrieval, especially of fine-grained image
retrieval is more or less impeded by the problem of missing la-
beled data due to increasing annotation costs. As zero-shot learning
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210096
Feature
0 1 … 0 … 1 1
1 0 … 1 … 0 0
… …
1 1 … 0 … 0 0
0 1 … 1 … 1 0
At
tr
ib
ut
e
tiger
ca
t4
ca
t3
ca
t2
fu
rr
y
w
hi
te
ha
ir
le
ss
ha
nd
s
pa
w
s
ho
rn
s
Attribute
Feature
Feature-Attribute Relation Sample Construction Sample Screening
Feature Space
   
ca
t1
Figure 1: Imagination Based Sample Construction: samples
of unseen class “tiger” are constructed from samples of its
similar class “cat” in feature space.
(ZSL) realizes image classification of certain classes which have no
labeled training samples, it has been drawing much attention in
recent years [5]. But task of ZSL is difficult because it lacks labeled
training samples of some classes, called unseen classes, to directly
train classifiers. Just given labeled training samples of seen classes,
ZSL aims to achieve unseen classes recognition by building rela-
tionship between unseen classes and seen classes. Nowadays, there
are mainly two popular methods for ZSL: probability reasoning
based on attribute prediction [4] and feature projection among dif-
ferent embeddings [6]. The first method usually predicts attribute
probability to calculate class maximum likelihood while the second
method mainly bridges different embedding spaces to exploit space
projection and feature mapping for ZSL. However, existing meth-
ods have several flaws. On the one hand, there is inherent error
accumulation in probabilistic reasoning. On the other hand, most
embedding methods apply complex deep network to realize space
projection, which is widely believed to have little interpretability
of projection process and takes a lot of time to train the network.
Different from existing two types of methods, we propose a new
type of method for ZSL: sample construction. Our proposed method,
Imagination Based Sample Construction (IBSC), is based on human
associative cognition process to directly construct samples of un-
seen classes. Thus, unseen class recognition can be realized via
learning from the constructed samples.
Human can visualize unseen objects through referring some
already known objects and assembling their visual components
based on imagination [7]. A human, who never see a tiger before
but has seen some cats yet, can speculate the species at the first
sight of a real tiger if he knows the description of tiger or attribute
relationship between tiger and cat. By mimicking human associa-
tive cognition process, we construct samples of unseen classes from
samples of seen classes in feature space, based on a relationship be-
tween feature and attribute. Our proposed method is schematically
ar
X
iv
:1
81
0.
12
14
5v
1 
 [c
s.C
V]
  2
9 O
ct 
20
18
displayed in Figure 1. As shown in Figure 1, each attribute is re-
lated to specific dimensions of image feature. For example, samples
which don’t have attribute “paws” are different from samples with
“paws” in certain feature dimensions. Based on attribute-feature
relation, an image feature can be reconstructed from other samples
to express different attributes. If use feature dimension related to
“paws” to replace original feature dimension of samples without
“paws”, the reconstructed samples will have a new characteristic.
Generally, it is reasonable to choose seen classes with large similar-
ity to unseen classes as reference basis when constructing target
samples of unseen classes. After samples are constructed through
splicing different samples of seen classes, the constructed samples
of higher quality need to be picked out. Hence, we adopt the idea
about dissimilarity representation [3] to measure representativeness
of the constructed samples. Although our method is designed to
classify images of unseen classes, no more new classifiers need
constructing, as ZSL has been simplified into a traditional super-
vised classification problem where most existing classifiers can be
used. We experiment on four benchmark datasets. Compared with
state-of-the-art approaches, comprehensive results demonstrate the
superiority of our proposed method. Furthermore, our work can be
viewed as a baseline for future sample construction works for ZSL.
2 IMAGINATION BASED SAMPLE
CONSTRUCTION
We propose a newmethod, Imagination Based Sample Construction
(IBSC), to directly construct exemplar samples of unseen classes.
Problem definition is as follows. Classes are split into two types:
seen classes S = {Si }K si=1 and unseen classes U = {Ui }K
u
i=1. Only
labeled samples D = {(xi ,yi )}ni=1 of seen classes are given. At-
tribute A = {Ai }Ki=1 of each class is utilized to associate unseen
classes with seen classes (K = Ks + Ku ), where Ai = (a1i , ...,adi ) is
a d-dimensional vector. Our target is to recognize unseen classes
which have no labeled data at training stage. We firstly build a rela-
tionship between attribute and feature. This relationship indicates
whether an image feature can characterize certain attributes that
is to say, attribute-relevant feature is selected by the relationship.
Then we choose similar (source) classes to construct samples of
unseen (target) classes. When constructing new samples, attribute
difference between source classes and target classes is measured
to instruct the process of attribute-relevant feature substitution.
After target samples are constructed, dissimilarity representation is
adopted to filter out unsuitable constructed samples and to reserve
the most representable samples which are used to train classifiers
for unseen classes. Then ZSL problem is turned into a supervised
learning problem.
2.1 Feature Selection per Attribute
Class attributes are related to partial image features strongly. For
example, features related to attribute “paws” show what animals
with paws look like in an image. That is to say, different combina-
tions of feature dimension express diverse attribute characteristics.
If relationship between attribute and feature is built, samples can
be constructed from different features to characterize unseen class
attributes. Thus, we aim to construct an attribute-feature relation
as shown in the left part of Figure 1. We use relation value Ri j to
denote relation between attribute and feature, where Ri j = {0, 1}.
And value 1 means that the j-th dimension of feature can distin-
guish different classes in terms of the i-th dimension of attribute
while value 0 is opposite. For efficient feature selection, we employ
linear support vector classification (SVC) with ℓ1 regularization to
select feature per attribute. SVC can automatically select features
relevant to certain attribute. The selected features have class dis-
tinguishing ability in terms of different attributes, which are called
attribute-relevant features in this paper.
2.2 Sample Construction
Sample construction in our method refers to constructing new
samples of unseen classes by splicing relevant parts in the selected
samples of seen classes. Seen classes which contain these samples
are called source classes and the selected samples are called source
samples in this paper. To realize sample construction, we take three
steps as follows.
Step 1. Source Classes Selection Because samples are con-
structed to train a classifier for unseen classes, the constructed
samples should maximally characterize unseen classes. Samples
constructed from source classes which have large similarity and
small attribute difference with unseen classes should have better
characteristic expression ability. Under this assumption, we use
class similarity and attribute difference to instruct source class
selection. The similarity between two classes is measured by ℓ2
distance of class attribute vectors:
ϕi j =
∥Ai −Aj ∥2 − µ1
σ1
(1)
where µ1/σ1 is the mean value/standard deviation of ℓ2 distances
between any two classes. Attribute difference is defined as Eq. (2):
ψi j =
∑d
n=1 |ani − anj | − µ2
σ2
(2)
where ani /anj is the n-th dimension of attribute Ai/Aj . Similarly,
µ2/σ2 is the mean value/standard deviation of total absolute differ-
ence of attribute values.
A source class set {S1, ..., Sk } of unseen class Ui , is selected
among seen classes by class similarity values. In source class set,
source class S1 has the largest class similarity to Ui while Sk has
the smallest class similarity. Regarding class attributes, combi-
nation of source class attributes can be viewed as a virtual at-
tribute vector of an unseen class. For examples, if a real attribute
vector of unseen class Ui is represented in binary value such as
Ai = (0, 1, 1, ..., 0, 1, 0), the corresponding virtual attribute vector
should have small Hamming distance to Ai . Hamming distance
is calculated as a reference to determine the number k of source
classes. We put more emphasis on selecting the most similar class
S1 because feature splicing is made on samples of the most similar
class to construct new samples. When selecting more suitable seen
classes as the most similar source classes of all unseen classes, we
optimize:
argmin
{Sj }
Ku∑
i=1
ϕi j +ψi j (3)
{Sj } is the most similar source classes set of unseen classes after
adjustment by optimizing Eq. (3). After adjustment, the most similar
Attribute Space
dog
cat
tiger
dolphin
Feature Space
dog
cat
tiger
dolphin
0.19
0.80
0.72
0.20
0.80
0.70
Figure 2:Dissimilarity representation bridges attribute space
and feature space: a class can be represented by relative dis-
tance to other classes in different spaces. By knowing dis-
similarity representation of unseen class “tiger” in attribute
space is (0.20, 0.80, 0.70), we can infer the class with similar
dissimilarity representation (0.19, 0.80, 0.72) in feature space
to be “tiger”.
classes of unseen classes have no repetitive classes, which enhance
inter-class distinguishing degree when sample construction. That is
to say, the second similar class S2 will replace the original most sim-
ilar class S1 to be the new S1 if total value of Eq. (3) is smaller when
original S1 is used as the most similar class of other unseen class.
Adjusting in this way, inter-class discriminability and whole simi-
larity between source classes and unseen classes are simultaneously
optimized.
Step 2. Sample SelectionWe use all samples of source class S1
selected via above strategies to construct target samples of unseen
classUi . Features which are relevant to different attributes between
S1 andUi are replaced by new features. The new features are picked
from samples of other classes in {S2, ..., Sk }. Thus, it is necessary to
make reasonable selection of these samples in {S2, ..., Sk }. Know-
ing which dimensions of feature are attribute-irrelevant, we define
those dimensions where relation value R equals to 0 as environment
information. To select more reasonable samples of other classes in
{S2, ..., Sk }, we employ environment information tomeasure fitness
degree of a sample. Fitness degree is measured by ℓ2 distance of en-
vironment information among different samples and samples with
more similar environment information are selected to be candidate
source samples when constructing target samples. Furthermore,
we pre-train classifiers for each attribute by SVM which are used
to measure attribute prediction capability by attribute prediction
probability. Samples selected by environment information with the
largest attribute prediction probability are most suitable to be used
in sample construction.
Step 3. Feature Construction Target samples of unseen classes
can be constructed based on samples of source classes. Main pro-
cess of construction is that features which are related to attributes
shared by the most similar source class and unseen class are re-
tained while the rest features are replaced by corresponding sample
features of other classes. New features will be picked from samples
selected at previous steps, which have strong attribute prediction
ability and similar environment information with original source
samples. Process of feature replacement is repeated until features
of all attributes of unseen class are spliced on the source samples,
as shown in the middle part of Figure 1. Hence, the constructed
features can be viewed as samples belonging to unseen classes,
which generally characterize unseen classes.
2.3 Sample Screening
There are some noisy samples in the constructed samples which
are not suitable to represent unseen classes. Because dissimilarity
representation can bridge different spaces to represent a class in an
unified form, we apply dissimilarity representation to screen the
constructed samples of higher quality [3]. As shown in Figure 2,
classes in different spaces are represented in different forms but
they are unified into one form which is expressed by the relative
relation with other classes. That is, a well constructed “tiger” sample
in feature space has the same dissimilarity representation of “tiger”
in attribute space. So dissimilarity representation enables class
attribute to instruct sample screening in feature space. Here we use
Eq. (4) to represent relative relation among classes:
di j =
∥ci − c j ∥22
θ2
(4)
In attribute space, c is a class attribute vector while in feature
space, c is the center of each class. In two spaces, θ2 has the same
meaning: sum of ℓ2 distance among classes. Unseen classes dissimi-
larity representation in attribute space is denoted as a normalized
vector Da = (d1, ...,dK s ). As aiming to screen the constructed
samples of high-quality, we denote each sample instead of each
class in form of normalized vector Df = (d1, ...,dK s ) in feature
space, where relative relation is measured between each sample
and centers of other classes. In terms of dissimilarity representation,
quality of the constructed samples is measured by difference value
|Df − Da |, which is to say that samples of small difference value
are screened to be training data of unseen classes. Given abundant
screened samples, zero-shot learning is turned into a supervised
learning problem where unseen classes can be classified by existing
classifiers.
3 EXPERIMENTS AND ANALYSIS
Datasets We conduct experiments on four benchmark datasets:
Animals with Attributes (AwA1) [5], Animals with Attributes 2
(AwA2) [12], SUN Attribute (SUN) [8], and Caltech-USCD-Birds-
200-2011 (CUB) [10]. Both AwA1 and AwA2 have 40 seen classes
and 10 unseen classes, totally containing 30,475 and 37,322 images.
As for SUN, there are 707 seen classes and 10 unseen classes, adding
up to 14,340 images. CUB is a set which has 200 bird species with
150/50 seen/unseen classes. Attribute dimensions of four datasets
are 85, 85,102 and 312.
Setup In experiments, continuous attribute is utilized to measure
class similarity and attribute difference while binary attribute is
used to determine the quantity k of source classes of each unseen
class. The quantity k is set to 5 in our experiments. Since deep
Convolutional Neural Network (CNN) feature has been proven
to have the best feature expression ability of images, we use fc7
layer output of VGG-19 network as image feature which is a 4096-
dimensional vector [9].
Analysis As shown in Figure 3, the constructed samples dis-
tribute closely to real samples of of unseen classes. Thus, the con-
structed samples have strong ability to characterize unseen classes
and using them as training data is proven to be reasonable. We
compare top-1 accuracy with several state-of-the-art methods on
four benchmark datasets. Comprehensive results comparison is
(a) chimpanzee (b) humpback whale
Figure 3: t-SNE visualization of the constructed samples af-
ter screening and real samples of two unseen classes in
AwA1 (better viewed in color).
Table 1: Comparison to existing approaches in top-1 accu-
racy (in %) on four benchmark datasets. The best is marked
in bold. IBSC: samples without screening; IBSCS : samples af-
ter screening.
Approach AwA1 AwA2 SUN CUB
LAT EM[11] 72.1 - - 45.6
SYNCo−vs−o [1] 69.7 - 62.8 53.4
SYNCcs [1] 68.4 - 52.9 51.6
SYNCstruct [1] 72.9 - 62.7 54.7
EXEM (1NN )[2] 76.2 - 69.6 56.3
EXEM (1NNs)[2] 76.5 - 67.3 58.5
I BSC 74.6 62.4 75.5 36.7
I BSCs 82.6 67.0 80.1 38.4
shown in Table 1. It can be clearly seen that our method outper-
forms on three datasets, especially on AwA1 and SUN. Our method
constructs representative samples of high quality as shown in Fig-
ure 3. Classifier trained on these constructed samples has good
classification ability because the samples are constructed to express
characteristic of each attribute of unseen classes. Samples espe-
cially after screening are more typical to represent unseen classes
and there is a significant increment of classification accuracy by
taking screening strategy. Moreover, previous embedding methods
inevitably have information loss when associating different spaces
but our method can reduce information loss by turning zero-shot
learning problem into a supervised learning problem. It’s notable
that there are no construction methods before to tackle zero-shot
learning problem therefore, the result comparison with previous
methods is more convictive. The relative poor performance on
CUB is due to inter-class similarity among unseen classes. There
are 200 species of birds in dataset where several species are ex-
tremely similar in visual. If source classes have large inter-class
similarity, samples constructed from them have weak characteristic
expression ability thus, classifier trained on these samples has weak
class distinguishing ability.
We take several basic sample construction strategies to com-
pare with IBSC, as shown in Figure 4. M1 uses samples in source
classes as training data without any changing while M2 and M3
randomly change feature values. From Figure 4, we observe that
sample construction based on attribute-feature relation is more
reasonable compared with random construction. Although random
construction is rough and simple, but in terms of type of methods,
it is more comparable than embedding methods.
M1 M2 M3 IBSC IBSCs
Methods
10
20
30
40
50
60
70
80
90
Acc
ura
cy(
%)
AWA1
AWA2
SUN
CUB
Figure 4: Top-1 accuracy of different sample construction
methods. Basic ways of constructing samples: M1: samples
of the most similar classes without changing;M2: randomly
change value on random feature dimensions of samples in
M1; M3: randomly change value on attribute-relevant fea-
ture dimensions of samples inM1. IBSC/IBSCS : as illustrated
in Table 1.
4 CONCLUSIONS
We propose a novel method, Imagination Based Sample Construc-
tion, to directly construct samples of unseen classes by referring
to human associative cognition process. Target samples are con-
structed by splicing different parts of selected samples of seen
classes, which have been proven to have strong capability to char-
acterize unseen classes. The constructed samples are used as labeled
data to train a classifier for unseen classes. In this way, we simplify
the problem of ZSL into a supervised learning problem. Comprehen-
sive result comparison of four benchmark datasets illustrates the
superiority of our method. Moreover, it is the first work concerning
sample construction method for ZSL. Therefore, our work can be
viewed as a baseline for future sample construction works.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China (61773385, 61672523) .
REFERENCES
[1] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha. 2016. Synthesized
Classifiers for Zero-Shot Learning. In CVPR.
[2] Soravit Changpinyo, Wei-Lun Chao, and Fei Sha. 2017. Predicting Visual Exem-
plars of Unseen Classes for Zero-Shot Learning. In ICCV.
[3] Robert P.W. Duin and ElÅĳbieta PÄŹkalska. 2012. The dissimilarity space. Pattern
Recognition Letters (2012).
[4] Dinesh Jayaraman and Kristen Grauman. 2014. Zero-shot recognition with
unreliable attributes. In NIPS.
[5] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. 2014. Attribute-
Based Classification for Zero-Shot Visual Object Categorization. IEEE TPAMI
(2014).
[6] Xirong Li, Shuai Liao, Weiyu Lan, Xiaoyong Du, and Gang Yang. 2015. Zero-shot
Image Tagging by Hierarchical Semantic Embedding. In ACM SIGIR.
[7] Christopher Papadopoulos, Brett K. Hayes, and Ben R. Newell. 2011. Noncate-
gorical approaches to feature prediction with uncertain categories. Memory &
Cognition (2011).
[8] Genevieve Patterson and James Hays. 2012. SUN attribute database: Discovering,
annotating, and recognizing scene attributes. In CVPR.
[9] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In ICLR.
[10] CatherineWah, Steve Branson, PeterWelinder, Pietro Perona, and Serge Belongie.
2011. The Caltech-UCSD Birds-200-2011 Dataset. Advances in Water Resources
(2011).
[11] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh N. Nguyen, Matthias Hein,
and Bernt Schiele. 2016. Latent Embeddings for Zero-Shot Classification. In
CVPR.
[12] Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata. 2017.
Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the
Ugly. arXiv preprint arXiv:1707.00600 (2017).
"
